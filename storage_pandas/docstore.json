{"docstore/metadata": {"bc7d624c-2119-435b-a459-acd86e08602f": {"doc_hash": "067bb4bb51d1aabbaff8e65097ea98902323e632d3bac5913649d1a592b8666b"}, "27077e63-6c4e-4e8f-847c-b2826930e37c": {"doc_hash": "b2c8a06980dcd29015c487ab90f1b581c60d651376d9f20ea3cbf6c539a85f6f"}, "34287ea1-78e3-43a6-a980-6de997889711": {"doc_hash": "4d7bff8371c0bed30366710ada31b85510b2e55984952f4e513f9cd9cfd2d7ea"}, "e5e853a1-4909-4444-ba33-4f87e328dac1": {"doc_hash": "3a27769f892a69cdd64d32801f5dee67c2719b60b7f3489f98ed550660968a31"}, "5d343073-d22e-4d68-9b5d-d787c7876e34": {"doc_hash": "f16032386c1f4f6b6614943062147476ff4b12390e60018651b34c3ba8604439"}, "4a080934-2d32-42b0-9ede-ebd3a21e348f": {"doc_hash": "e0f13ea06d3773d0c981ab34268f9248786cf98612fab496d6b55e96529796cf"}, "add4f3bc-0cac-4c98-9f35-458bfcf7a7b1": {"doc_hash": "0aaa3525ea0a9e6df96b1546936297f081afcabaca3d8f2611d199bf3f9bf0a9"}, "62313f7a-3a1f-4ba8-9153-d3c6ac15bb5f": {"doc_hash": "a915388b18baa0aadf23be4c160438cc7908a83d61dfaa58eb226f9142cd2031"}, "bad62121-bf17-40e4-b7fe-8aa0da990b07": {"doc_hash": "992927eb8f6720af61d5127af50434327610a3091f825887965c5db9d9901f2f"}, "1e0c5143-d113-41f9-a488-93841c6500c4": {"doc_hash": "edd0642668a0fdbc1d2aa3def6fcf853337a9f192485155b85ae76fc1c2fc397"}, "3c5ae70b-da7b-4dc0-bada-fbfa4aaecc65": {"doc_hash": "076d1c468818a956401baf757f2dc8b0cd7c6cdcdb19877342afb7dfd1a4f1f5"}, "e824d9ac-d2f1-473a-8cca-33e65aa70514": {"doc_hash": "84ca31beaadb2c63c9fd04206aadd6ac9ccdb9e569b86d19300631fd486c2944"}, "fe8c7730-ddeb-41d5-a965-cbe095b5420f": {"doc_hash": "10453e3c5e254118176b96f2f5383abfd2b980b7416c543bab3db615d5e7b68a"}, "9c41d8c3-63d7-4ac6-97d0-83f5e18e1801": {"doc_hash": "27ca291aa1f798d74eb64a3bf4a173ea0c1d6f79e0d63574bb8b8d8fd117d0c0"}, "64a3fc7e-0e53-42d3-b9de-fbffed97c33c": {"doc_hash": "2507e7d5dd1e37e8c9720e9a309705311d5700a3a7dbb461457b966ab5a021a8"}, "07b2633d-a631-43a6-8f4b-c7309ae146e7": {"doc_hash": "117a91d2190c04b7a2df2d8cae86d08518b1e1baf44d5910a906f40fc7251d26"}, "2077c564-8323-44a1-8f02-2ff71b0ee284": {"doc_hash": "527d6723f157d3ca86e04eade2e4732a1b18788f8c10b4f5126c580637fbaae4"}, "84a07c53-8b16-4a5a-a1a4-615d115788ca": {"doc_hash": "6a52f663c24ad6e77be36e0806fa059e50d3a568a4279c80480b086a40998596"}, "e50f3f16-6d22-4beb-a024-d7abf5728a48": {"doc_hash": "4f0ed7142a028b787a8c32419268f624bc195bc9510ce8364f633583faebec9e"}, "dc5b4859-b6a6-4025-ac74-e3f749977be4": {"doc_hash": "bbe779a2b74c4e7ed7918080fc4a741b9baab4bc488ad39b0497ec8ecf3b9c9c"}, "c1ea5030-933f-4c1f-ab6e-56f9bd5b23d5": {"doc_hash": "6f625c2e304358fd3a9be14c3b06dbb2439d6533605003341418af88d8cfb316"}, "4873f5b2-ea28-4505-9ff0-6f3c215165df": {"doc_hash": "645ab0b58acde3fc4dc259ea3d4ae27f7ccb5f718891d96952d875caad0d7535"}, "d2355fbc-894d-4042-8d71-ff8b2e4e8cfe": {"doc_hash": "47d273f6df61c2680e5cf04c08665a6d2659d3571ecc10830f63868d83ada981"}, "33c7adcc-aaf3-4767-b401-847312309d55": {"doc_hash": "8f5db03fc9fda68e74e8012646628a13b66bab2adbfcd9fc2abbc9be84bf3113"}, "90a5f4f1-c741-4993-956a-6919bc569203": {"doc_hash": "663c199ae555f2a75e3d56222b9dcbe7f0f4721d1bd1aeab2a378564dd3f8e29"}, "1b99c246-601e-459b-b6c2-6204df0857c1": {"doc_hash": "ff7f7b135251954e69e617c2bf53a2fd99e9e6628de4f03bfb1597fa3c9bf766"}, "a7824b7e-19a5-4bcc-b257-da50aee60760": {"doc_hash": "43a384b92abe1ea4d4b64c6431498a17c20e2f523152043de21a67026d4723c8"}, "98ab5d7a-951d-4d4d-a6b4-58f05ba80232": {"doc_hash": "99301705fc3ef3d346037e4d84e775d32ec89bc9f03ca8809622520dbcd35010"}, "60fddf11-79d3-4f39-8ed7-b296a017e5f3": {"doc_hash": "34559fb0cb8de9a56b41e691a1cba87015d651eddf10882192e5f8698cc2b422"}, "8d20605e-75a1-4de2-9bc2-a8e1d9f68832": {"doc_hash": "19b882023ab7bf397231c74351f5530dfaad2383bf0b9aff770d86f9260c485f"}, "d4e3998f-4a09-4348-95ab-522afe7a8b7f": {"doc_hash": "24badfee07d1df63cfe8ed0e6769d7e8b36e805489c6a579571eb79a554184da"}, "d6e37960-3ea8-4f3d-92a9-71ed568dbb72": {"doc_hash": "13ee7bd5eefd1abc01854ee8898f51639f8cdec41f149afb9663731ee6a21206"}, "7a8b13f6-7189-46a0-a586-b69124285cf9": {"doc_hash": "9fec48c37b21390909e90dd5ba9cd1efecbd221d6d89020974d685ed0e9eca44"}, "1a0b4f10-7ee1-46e7-a586-b46dc8aa20dc": {"doc_hash": "e8188023f0934258684dc405b46857b608f42c6ab37edfa914479ce3615bced1"}, "91bcbd11-db05-4ea8-a233-5220f4b7766c": {"doc_hash": "ae72cd1f1371933b948ff8d3092f06f37e009e7a6acb2c1f4da6a65dd8311b10"}, "aed9c18d-2432-409d-9887-452d5bf37042": {"doc_hash": "2686566d1c2390c6bdce48ee899cdcda13f74fa2b12febe01044136c35daec2a"}, "4ff3c89f-987f-4d39-8869-34da98026e2d": {"doc_hash": "52836268f14c92d376ef0e9cdc32f5cf45de8323539ac3432cd16f96514a5c28"}, "fe1dbc93-df3d-46dd-9998-95963bf2b2c0": {"doc_hash": "deaae74c2b999e0353e3bf71766c4552b982b9f77b96b47085f6af77a254c171"}, "020707c1-897f-4c18-938d-af72116d7af5": {"doc_hash": "fcd432703ad992bfab6868083f7f6275052e74b4c85319f770b72ffdc19a5ce1"}, "9af307b6-80a3-497d-a0c0-818f76bdee30": {"doc_hash": "22f993221e9cdbaeb8589d42073de5ec0e8f46f2b2c3e14167e04a8bb96bd7a9"}, "5de2ecf8-7986-42e4-a156-095d24df8b88": {"doc_hash": "5e4f92fbe2b4f23ebf37e9eec8820a0d25a272e3febaec890ee96d1812abc94b"}, "b0a339be-2e41-4f48-8972-77ad66ea3de6": {"doc_hash": "f2351bbefe5d9f5a2f6953b337b37ec4ccb3a7d7036fe1fae70a8add868fc177"}, "3845c5d8-3e19-48df-b0ec-18f0005a9289": {"doc_hash": "85a8611544ab98e8c01d324939a1575bd21c67be7281cc0e6535ead623b81170"}, "5869dd75-6922-4a2f-9e89-74c00a51d1d9": {"doc_hash": "4bda3d254ff5799f8aeec296b20e122a12bf224e096a92abc5dc812e3e368e0f"}, "b9b4b83e-b958-4e6a-b982-113240952463": {"doc_hash": "d7b2bee670a2847636209151a0aa57fb2320d7c06d682a72c828cc1d7c4be409"}, "6880905a-3e81-48b7-9515-882a36192f1c": {"doc_hash": "5e9f36bbb1af9e09952e2d8fe61a2a8b4397915446195668a5a5c258a23dae2c"}, "eb7a97ff-b65c-49a1-a85b-1343ab69a244": {"doc_hash": "28b07b247e1c30c99dc87d4860be612137fcf3aa0b87a730b19ca4f39267672e"}, "a05aeb87-b978-47ed-918c-5f4d8e675971": {"doc_hash": "c63b45f873cfe7bcb4054df54863cd69821f7fcae8c5b587a0da25cbd895bd46"}, "0714cd08-53e0-49a1-8b98-143d58cab6be": {"doc_hash": "fef73f2772aba3977bd689ff7cc21a1cb9241dca5f8951f811a0ca869746b64c"}, "8aca9fc9-8d18-47ce-9f85-a54172b03b3a": {"doc_hash": "286ca137021e8841d32bdd7a1cdfb33ca5ddcd607e08bc4959495bdc6c1e9870"}, "2e430396-b7ac-4d84-aa68-ce9186a58b66": {"doc_hash": "e29f01af651bb77a3ed8e002171d5a142c9b7620ed79d091240a83b6def82610"}, "b5df003e-3745-4368-b100-c7159a274bef": {"doc_hash": "ad34b8f0f3b01716873f9e208b6a1bd23f6ff2079bd545251d7ce80709502654"}, "c552fae2-67f4-4942-984e-ddf5d22639ef": {"doc_hash": "1feda7fea7b1ff2e0192f2cc975a5d3491010b6925d8c23967327c42de56a69a"}, "95172a2e-3b72-46a9-87c9-b8510807ae32": {"doc_hash": "dd138000cef05ea16010c6992843c1bdf9daf291b5ba8bf32f0a29ee77992a50"}, "44bf1b1b-0aa0-4875-b9bf-9ed48f21e0e4": {"doc_hash": "94b2e9bcb011ec00487dea4dbbe83dd73a2dc2df0f4e3925149a5f10210f1542"}, "2e075493-319a-4650-9bc8-b5737210bfef": {"doc_hash": "8c348a5da76a5ddcf616860d0d250f3739ca89d311e5d0151d63fac07b6c348f"}, "ad83b1b1-be3d-4745-a766-66364ea1fad0": {"doc_hash": "df8d9f14e92b42d2aa3256a7041109d03371ae209a397cda4fc0ef36abffefc3"}, "5a022d8f-5593-4a88-bbae-51390f4e901c": {"doc_hash": "db05626b2bee8ed7df6b8d1136cb8f612b71b0d640401575c45b28235e8ba5f1"}, "8d9f2722-ba9a-4424-be92-6241c197a883": {"doc_hash": "30d6206655c70221baa83fb668410af18112f5e27ae373bfc6c8bf9681f7d339"}, "f7794030-6743-4ce5-a623-b44c5f86ee64": {"doc_hash": "534993508b4867ca71cb80bc62c252e0bd6fbc0292ee51ad731ba59ad1f76693"}, "0792b78e-c22a-4dce-92ba-9ce97388a7fe": {"doc_hash": "944edc07c21e5a01e392d810ecd7fd987e1e6c1c1eefd1e56fb1ad33094ed561"}, "7fdce276-149f-4429-b3d9-0c4047f598bc": {"doc_hash": "18cb98b3fb1fc3219d097d8552e001e4d3deddd913bb8b506c3bfc3759432323"}, "3075b510-798f-47cd-88ed-f0a82fee5b13": {"doc_hash": "331878db8574fa88f09e49dcdf740b8de4d03494e597f815de4dbe75bbde8bde"}, "7babdc5b-e97d-4e9b-bb93-f2305ab5f9f5": {"doc_hash": "7cb61ff49c906f7506015c7be60078457b21057a746af8c1fa2f039b26713660"}, "4f12c2b4-b82d-4fee-a208-b710e9a4acbf": {"doc_hash": "bc001c3ae8b4cdd69135589f97919f13c698bde7efc9fa91b0a64157e484a414"}, "69725fae-7801-4fd9-ad21-5681431c98b1": {"doc_hash": "7c6e4ccc56ef1df232d2b3aa553098bce33266f17e42b0e7b77091ec10956718"}, "d293a774-e372-4a2f-91d3-996ae539b8c0": {"doc_hash": "7d42e3186339b2701181948f92485c3bd1eba5da37e4efa5dff0def144ccecc9"}, "f63e30b1-fdc0-4c96-9270-96e84a5ffde2": {"doc_hash": "0101ef85d2c2664a596e67030c956248c42d5b24deffadbb51dd7cc138a525b6"}, "158a2b2a-be10-4cc6-ae00-78a57dcd1505": {"doc_hash": "4150321b0ad5c6f8be9718dc28f53bc71f0ec1580c82dc92579e5cab36ad4bdf"}, "ae01c664-6559-4e6d-99ff-d38c59e3edb8": {"doc_hash": "0585095be865e11ee5bec329f8b34a59a182b5c09c776fb4815a1ebf72d1fc49"}, "6104effe-e002-4020-94ca-7edc75d64da6": {"doc_hash": "7a7493739f1c796f01faaeb0e32e4f8cffe0f5e4acd86d68184a1a348ddada28"}, "dd0e3112-c89c-457d-b957-3d5171c61020": {"doc_hash": "4a70ed3a0b15ab394bd6d17e684842080a7514f4424fc59c659f13a4bc4b8df6"}, "2643c806-ff4b-4c17-8eab-f6cb3fbe2fc4": {"doc_hash": "70a76be635fc484705d2f26e66e8d19dddd785a03ebfd4be87f6258ea80fd838"}, "23d22d2a-9cce-4389-b4b7-79fa3aea94d6": {"doc_hash": "f98f60837c6a5f6069ea42342a63033bf9b94effc4e2b7ae425e1c6494367e0c"}, "1ea2926c-7965-496c-bb49-9cb111429f09": {"doc_hash": "2fde1712e3149c302356963ae77d9638e00a6b40c6c31ba30a0eeb562a782d58"}, "27387f47-10bf-4ffe-8a1b-a4eeb53d110f": {"doc_hash": "63d3fd2e5bfaeda6df5504087411e260dc694b618d3787c98d779fa28cd2939a"}, "2d37df91-f945-4d5f-8b93-f08c14930587": {"doc_hash": "4b9e2992388fccc5a0d6d1b96401de05525622e9c6d7eb9aa106cf9a195219a8"}, "e7055960-da50-489a-9f89-becde79d4fa3": {"doc_hash": "67ed91e066032d8a13594b5e98b766bf7b3b3f1f39e4f31b59ad95bf3f34134c"}, "a46267e0-fc16-41db-b435-690d01d8d346": {"doc_hash": "f99a5595ef7c6bb55ed53c52868ebaba6c7b45207f4295edae94a05afa515a9e"}, "abffe0d8-2f97-4638-b134-eba5bad092d9": {"doc_hash": "a859933a7d7aa20ebdfd0568724653b3cd55c25c58b873077b118a5b6a301cd4"}, "d75f968f-8269-4cc8-9d01-8673f954d93b": {"doc_hash": "0e75794b6f58b103903f1ff03fc5e280937d1c31febc0dcae3942be0387ee24d"}, "94c3ddf1-c5b0-46ea-a01b-ba06597a0257": {"doc_hash": "067bb4bb51d1aabbaff8e65097ea98902323e632d3bac5913649d1a592b8666b"}, "6564dd1f-ee92-4f53-a610-ee6b9adfbffd": {"doc_hash": "b2c8a06980dcd29015c487ab90f1b581c60d651376d9f20ea3cbf6c539a85f6f"}, "a349f424-78b1-43e7-b62a-97286915f0c5": {"doc_hash": "4d7bff8371c0bed30366710ada31b85510b2e55984952f4e513f9cd9cfd2d7ea"}, "f172870c-d9be-40b2-9f0e-e92422671aef": {"doc_hash": "c2722fc334e85240d2d11e600a21fd288cc79c353eda5bbfeca2a91f7161222c"}, "c69e3536-6e2c-4050-ba5b-2f8dc4598a8b": {"doc_hash": "ed09047f8a3c7aa573566ce584866ff78e729bc98d7c0e42b2052dfa489435ed"}, "62fb35bd-3509-448c-8d1d-c99d6436c8e9": {"doc_hash": "641c8d64fdb6b70ea704ffa0dfeeffa73b108c5d67a95ae7624dc4aa1a9ce8e7"}, "677acef4-073a-42e0-830a-385d4ea5ad10": {"doc_hash": "a270976884c60382f7018e9c7a3832aa95b1560198a8381a9f2d725a23e42f15"}, "7624479a-2d1d-4358-9275-575ac96b60a2": {"doc_hash": "f7c30285ae904de8003a953428ff87261648dad6f68a6cb18272cf7d63fc8bb3"}, "39cf6e79-6c78-4336-83bf-1cf54714ecf0": {"doc_hash": "e0f13ea06d3773d0c981ab34268f9248786cf98612fab496d6b55e96529796cf"}, "33f59b41-fcc3-40fb-ae44-b3675d16f889": {"doc_hash": "0aaa3525ea0a9e6df96b1546936297f081afcabaca3d8f2611d199bf3f9bf0a9"}, "a4e6218a-1121-46c2-8275-501fae1421a8": {"doc_hash": "a915388b18baa0aadf23be4c160438cc7908a83d61dfaa58eb226f9142cd2031"}, "4684885b-777b-4951-935f-ab4b67ed6741": {"doc_hash": "992927eb8f6720af61d5127af50434327610a3091f825887965c5db9d9901f2f"}, "7c1048a7-4db7-456d-abfa-48ab9dc73fdc": {"doc_hash": "4f231dda412328c36db75be318d8258e47190110577dfe92006d3448716001bc"}, "9367a334-f80a-41c0-868f-e2ccfe8961a8": {"doc_hash": "e559ec5beb22b80a37417a6318838a63092a29945879dd419476406f749cfb62"}, "3c5f69a6-4c2c-4fe1-8e22-24e313617ac5": {"doc_hash": "b3bcd680c170693101aa07876c36e59cad01e3c8ea0466755aa1ef2af283ebf2"}, "b0d94955-4cd2-4760-9515-f8259a793481": {"doc_hash": "71a281d1ab4e2100cc65304ac0008518a1b487b9b2e54dbddf12a7ca48ca0e50"}, "da8d91d6-ed6c-43ba-9ccc-1e83007e6e7d": {"doc_hash": "0e56d4084c5829fbce11d1120050f9924f59678ce9758bef85e4f929080577f5"}, "e154535f-3641-4196-b91d-7b678b8a91a5": {"doc_hash": "b50de4fd268d1051e969b0ab09357f68cf46613231c49a7ee93fe624b58beeca"}, "bd52ef85-1714-4ac2-85ff-c9be597aa6aa": {"doc_hash": "5858c6bb2e2ada98154e43d2345090261d43450199108923497089a2a218c252"}, "30aa99db-6465-4145-8a88-6794c35f8753": {"doc_hash": "1e15a73e11522cebb2374b84ec3406afe7dc4f3b3477c696aecb2dc94b5b1d9d"}, "996f2509-b79c-4409-be16-d20b71d81752": {"doc_hash": "1b6452726c5e189327c4d1ec07e2f83ff1c64d349c0431312c58e7a882a2b1b9"}, "38661e94-5c0f-4690-b0d1-ac445b6d80ef": {"doc_hash": "12bab8eac796d8f81fc074216153f156ed270eb7acf1cd9b79e51aa2256807e4"}, "4f74907f-ef3d-460b-8e35-4b3d74fab429": {"doc_hash": "27ca291aa1f798d74eb64a3bf4a173ea0c1d6f79e0d63574bb8b8d8fd117d0c0"}, "7c2686fc-bd93-48b5-ad37-3e148503332a": {"doc_hash": "91e8e56484bba4798a3459e7ba05d531dd8488adf3104417a5904d6a4415b385"}, "04594028-0f9f-44a4-9983-b38f47011523": {"doc_hash": "d1ddac98da67b2d5e86505585f0208969bb0f28b612555ae7a963c94378f3e05"}, "6f51bec6-9029-41cf-b01f-9e79b348fb2e": {"doc_hash": "decd8414bf67406b752e13e90e31706511a4a052a78deed50c6b0be4f6b9cea2"}, "7e192e23-51ab-4d5a-976e-e8bc0b263f88": {"doc_hash": "80603b45aa4f3790e4a1e4b2de7b0ae167a6dbe1e00cffe70adf97a0bf9e13ea"}, "d43f2696-03c2-495a-aead-52ce79fd171f": {"doc_hash": "eb024b93e6be89ace89d9a24eac4f5e90daadea5e99a6281f84f20b067839828"}, "eaf359bd-841a-4c0b-8f8e-52c42d809558": {"doc_hash": "0173d1d9948e41ab3f63a45c17613747c2d57851f88b9d4176fbb058fd6d2d11"}, "f529ddd1-1b6d-435a-a223-8648cdefb16f": {"doc_hash": "c9c2928361db6cc3368e0fadab595890d325da01ea74e28bb8b399c5d77f505c"}, "b80d3d3c-f5d8-42af-88b0-f24031c57a4a": {"doc_hash": "007c7b5e2356865a6cd31bb02d3bd3569dd89eac9232de6061675cf141f56c28"}, "f295b5a0-1ac1-459e-afab-dc0b8d6b2938": {"doc_hash": "2f46ac5fe31be5a122117c595de739e9c30eb2ad30e1129347f5b904d22a20c5"}, "9df4f39e-151b-4d21-af40-08ad18644b7b": {"doc_hash": "a3cbdb0e52bd08970d7ce47ab2afdf4305276c006cf114c78db1952a906401cc"}, "03815282-3be3-4701-8929-6ed693dee5e3": {"doc_hash": "7395b082992d5dccd7fee0a57632c8b9fe8d240300f866c07fc6d4ada0ae64d3"}, "dda3a56d-2dda-499d-9e6d-1d53f79dadcf": {"doc_hash": "a4f6cbb4e26d868f51e1e55bc077b657a637f3200e4bb7fe42676ac5425f4e0e"}, "ceff3af3-c557-4213-8ed3-f9fed6a08563": {"doc_hash": "f726875a88f3552c8e7100680036fad2b0a6e6734998cf578ed85221f7b43e49"}, "4e0b94bc-4c79-440e-93cf-4ad7f942f146": {"doc_hash": "eb942343aced4ed0d2ca549f2b1aed1212e87bb1e914d9bab968287497b5b532"}, "18881ff8-e516-4bb1-a578-d4dca5b33539": {"doc_hash": "097254a92673cc9a44a77617c6faa6e284ac53f6ab79726ba569cde67dd9f997"}, "26e841fd-1de5-4dc6-8d32-254d0bb704f0": {"doc_hash": "6f625c2e304358fd3a9be14c3b06dbb2439d6533605003341418af88d8cfb316"}, "eef3fcf5-193c-4437-a9dd-ab19324c43be": {"doc_hash": "645ab0b58acde3fc4dc259ea3d4ae27f7ccb5f718891d96952d875caad0d7535"}, "a4533459-f113-4119-b938-cbf0ef059885": {"doc_hash": "47d273f6df61c2680e5cf04c08665a6d2659d3571ecc10830f63868d83ada981"}, "49e1faa0-e4b9-436c-9942-f3d3e080119d": {"doc_hash": "8f5db03fc9fda68e74e8012646628a13b66bab2adbfcd9fc2abbc9be84bf3113"}, "ca3fee36-ee0a-4025-b701-04b830a92960": {"doc_hash": "ea26f6cac972fcb591f27a9871ae9b193f142c4c8ac19e9018b11683d219875a"}, "37df59b9-6ca4-465e-9f99-d1d052c9889b": {"doc_hash": "cbcaa4872b68921ae13ff773e8a94ba9c43f1e802337a049c450c20485b1e9e0"}, "58934a8f-7ef3-4b88-a91e-58ee98fb17e6": {"doc_hash": "ff7f7b135251954e69e617c2bf53a2fd99e9e6628de4f03bfb1597fa3c9bf766"}, "407d41c9-e450-4b3d-b6e1-120008343de2": {"doc_hash": "43a384b92abe1ea4d4b64c6431498a17c20e2f523152043de21a67026d4723c8"}, "6991f8fe-830e-4092-a448-d3725a29b8d1": {"doc_hash": "99301705fc3ef3d346037e4d84e775d32ec89bc9f03ca8809622520dbcd35010"}, "0ed3b7c6-0caf-4c8b-9e02-6ddfd94a68be": {"doc_hash": "34559fb0cb8de9a56b41e691a1cba87015d651eddf10882192e5f8698cc2b422"}, "769d4c4c-0175-470e-a8e8-693753a79e6d": {"doc_hash": "19b882023ab7bf397231c74351f5530dfaad2383bf0b9aff770d86f9260c485f"}, "2572b393-5836-45af-8598-53158a7f0140": {"doc_hash": "24badfee07d1df63cfe8ed0e6769d7e8b36e805489c6a579571eb79a554184da"}, "a768d8b1-7fa0-42d6-8066-41d6b75c8c3b": {"doc_hash": "13ee7bd5eefd1abc01854ee8898f51639f8cdec41f149afb9663731ee6a21206"}, "5139742e-8c33-4947-a166-91a8ada187ed": {"doc_hash": "9fec48c37b21390909e90dd5ba9cd1efecbd221d6d89020974d685ed0e9eca44"}, "0b02797d-d880-46f7-b388-8b813a18a1b2": {"doc_hash": "e8188023f0934258684dc405b46857b608f42c6ab37edfa914479ce3615bced1"}, "57e1ff1a-4206-4620-b180-bc33bd24a717": {"doc_hash": "ae72cd1f1371933b948ff8d3092f06f37e009e7a6acb2c1f4da6a65dd8311b10"}, "1c99371a-6d7f-4fe7-b863-ec3c15c11d42": {"doc_hash": "2686566d1c2390c6bdce48ee899cdcda13f74fa2b12febe01044136c35daec2a"}, "ac883b34-a3dc-4272-ace6-963de5b1efc3": {"doc_hash": "52836268f14c92d376ef0e9cdc32f5cf45de8323539ac3432cd16f96514a5c28"}, "c4da50de-971a-42b8-bc48-b1cd25fd4242": {"doc_hash": "deaae74c2b999e0353e3bf71766c4552b982b9f77b96b47085f6af77a254c171"}, "9463748d-09dc-4575-baab-4e6146ecab33": {"doc_hash": "fcd432703ad992bfab6868083f7f6275052e74b4c85319f770b72ffdc19a5ce1"}, "5776e04d-ad02-492a-9595-724f9fcb53ce": {"doc_hash": "6f63cd591b1ae27865d7d7a5b1be9a6f25d67ef2873323dc7fed084be8277e6a"}, "5226fc89-7380-4827-8e10-f9aeba28660e": {"doc_hash": "9586a3693deff02c339eb0ac7e784da6857d69550d219d7ed6b8353bd9af5267"}, "933cff03-c575-4676-80aa-b9df6ae0a9d2": {"doc_hash": "d783950c52cfc08e6ba74dbe4c482dfd9a21a8f589c3f21ece4d7d9e70756de0"}, "9b9de79a-f7bb-432c-b7f8-ee7b61814c10": {"doc_hash": "5e4f92fbe2b4f23ebf37e9eec8820a0d25a272e3febaec890ee96d1812abc94b"}, "2dc4ad44-2d3a-4910-92ca-e34fa4aee76e": {"doc_hash": "2bd9f366f78bbe7cb79ba339630b8c22b643900526562be18ef196f3706558b1"}, "4ae57d5d-1d9f-4252-b938-d1b82464e557": {"doc_hash": "1d02f7035c05dd89261d74ab3335c33f500cab906da7b02673a30e24ca476ef8"}, "527b4528-cfb9-4be4-94d2-3fcd80d71247": {"doc_hash": "e38dc848a6d9b8488f8f2b1b712e11491cb10e7ffd2daa75cf8850032fbecbb8"}, "ebbabac7-6197-4418-8f42-d98ea40f3b86": {"doc_hash": "983809e3f1ed1fb7a4e28107ede5cd33a17e30a0c59088a3ea1a5f152f0bcd9e"}, "a663c5b2-5d86-4dcc-9b63-f33d11112e5b": {"doc_hash": "df8efec766ce843b47489319e537d81499e997abf3c7d45eaa82f5a9a58ca8f8"}, "50f7fc01-f9a0-469c-b1db-c214292955fb": {"doc_hash": "4630855fed73b5b848e7d6c62829597b5440f48d7c0111288dcd730b82acaa76"}, "45da16ef-dcde-4119-938d-e42807c56343": {"doc_hash": "d09c690f3a50c4bea03e5a5f1137136b7c29f0f69bef3c690ef1cdc552ca13ce"}, "ea328907-8117-4cd7-b729-fcd45783f39c": {"doc_hash": "3e6367e1aaa0c0e34e7bc1f43346fc2ff1052d8c361dbc4c42a55f591e7acda1"}, "f81228ad-d7fa-4f0b-8b9a-f193673d5e4f": {"doc_hash": "e98b6eaff085b00ba8c39e0b1e21cb4ec3f22b99a7209dfb450c5aff879181f7"}, "4bd48c93-35cc-4432-93b7-f496ede75e56": {"doc_hash": "dcb187c3a71f5d9b3f3a90835b71def56e8250fb83735c70e02cf2e79fb2241e"}, "174ab571-498d-472f-b8e1-6e3c583e7a5d": {"doc_hash": "7f7f0c30eb081cf4a1cedd92b7cb6dba78aedd16526e45ae8be0fe336e589c6f"}, "1071bcb7-42cc-404f-b711-342db505f532": {"doc_hash": "3e6367e1aaa0c0e34e7bc1f43346fc2ff1052d8c361dbc4c42a55f591e7acda1"}, "28c01d3f-9a0f-4e4b-9765-9ab3da6a321b": {"doc_hash": "33a9a132f02e8d9cd2b5fd868f40674655129b459ac17c69a4bd050f4d9f48a3"}, "7befe10f-2bf9-423c-8ecd-60467953fa9e": {"doc_hash": "5676e893f2476d5cc36ba1ef278bddc82a0e8656469f1fc667ee3634ae707460"}, "5728e558-3a32-4fc9-84d3-6f84336dbb1b": {"doc_hash": "3839c1487b25e44538d64c3bf34db36b40b58c404828d265bc4abc3925324dac"}, "c323a586-a0df-4d0e-bdf2-26e4f305e39f": {"doc_hash": "e42ff4423a80a8f1e16adea6d06819bc13da3afdc35d788501573378d9ddcedc"}, "f5752f7b-4a97-4587-bb31-36f754f3e0b1": {"doc_hash": "286ca137021e8841d32bdd7a1cdfb33ca5ddcd607e08bc4959495bdc6c1e9870"}, "8190afad-75df-4275-9a87-f8ef2bb47374": {"doc_hash": "e29f01af651bb77a3ed8e002171d5a142c9b7620ed79d091240a83b6def82610"}, "4f24f5ed-0c02-4433-862e-4d9661a565fc": {"doc_hash": "ad34b8f0f3b01716873f9e208b6a1bd23f6ff2079bd545251d7ce80709502654"}, "ef4a1199-a5bf-4ede-a330-f5f760425ea3": {"doc_hash": "1feda7fea7b1ff2e0192f2cc975a5d3491010b6925d8c23967327c42de56a69a"}, "709d7431-7e5b-4f36-b7ac-b624f15c67c4": {"doc_hash": "dd138000cef05ea16010c6992843c1bdf9daf291b5ba8bf32f0a29ee77992a50"}, "d8ee9482-e0c1-4599-8bfd-b06e3d0fccbb": {"doc_hash": "94b2e9bcb011ec00487dea4dbbe83dd73a2dc2df0f4e3925149a5f10210f1542"}, "e0207929-9fc7-4e6d-96e1-cd947a7897ec": {"doc_hash": "8c348a5da76a5ddcf616860d0d250f3739ca89d311e5d0151d63fac07b6c348f"}, "29a8001a-d314-474c-83ad-8e1af11864fe": {"doc_hash": "df8d9f14e92b42d2aa3256a7041109d03371ae209a397cda4fc0ef36abffefc3"}, "6d9b81a9-65e0-40a8-9921-2faf24156cbc": {"doc_hash": "db05626b2bee8ed7df6b8d1136cb8f612b71b0d640401575c45b28235e8ba5f1"}, "b01adedf-ecf1-454c-a4b1-fe91b485fd75": {"doc_hash": "30d6206655c70221baa83fb668410af18112f5e27ae373bfc6c8bf9681f7d339"}, "ba378902-1ece-4e34-a676-3390e8f55544": {"doc_hash": "038734afffdb530105fc4be741063f1a2aafa81e91c38731f65a08ff76439a8e"}, "7783bb37-2070-46b6-a2e5-56ee9a7b9048": {"doc_hash": "109186a43b34231796e0cd364951c18444a4f3f59d31095bdbc62b903fa51629"}, "09eeda75-c5f5-4247-ba31-fb770511b3fa": {"doc_hash": "944edc07c21e5a01e392d810ecd7fd987e1e6c1c1eefd1e56fb1ad33094ed561"}, "bc163a48-8492-40bb-b81e-35b0f0b7bc5f": {"doc_hash": "18cb98b3fb1fc3219d097d8552e001e4d3deddd913bb8b506c3bfc3759432323"}, "5bb93fd6-50a1-40b5-aa75-ea3e5843e87e": {"doc_hash": "331878db8574fa88f09e49dcdf740b8de4d03494e597f815de4dbe75bbde8bde"}, "e47d98dd-1f34-42e7-8a0e-ba9548ea22d3": {"doc_hash": "7f62fd5585106c14dcf910e41d3d6375bdb505706b798c8a641f96e6b0b10645"}, "822aa6b1-11da-480f-91b1-cf740e52f101": {"doc_hash": "54fb1449429894c1366e4279cba948e7cf6bf5365d129c82b74fb93229db9696"}, "dd33def3-77f9-4df4-a879-852780f9fb86": {"doc_hash": "eb39fdd136a94cfcccc943c50ed5e0440811779771636b96828989e6c3957ebe"}, "d8fe4b83-f51b-4764-b630-78e12845aef9": {"doc_hash": "c9c2928361db6cc3368e0fadab595890d325da01ea74e28bb8b399c5d77f505c"}, "3ab0c41b-19ef-41dc-bc9d-c4253badeb72": {"doc_hash": "7c6e4ccc56ef1df232d2b3aa553098bce33266f17e42b0e7b77091ec10956718"}, "03aad595-36e9-4bb4-bf59-861b745024ef": {"doc_hash": "7d42e3186339b2701181948f92485c3bd1eba5da37e4efa5dff0def144ccecc9"}, "5dd517a0-9f85-4ad6-81a6-cf09a9283d7c": {"doc_hash": "0101ef85d2c2664a596e67030c956248c42d5b24deffadbb51dd7cc138a525b6"}, "ebf89484-857f-4198-bf2e-4d03147c9583": {"doc_hash": "4150321b0ad5c6f8be9718dc28f53bc71f0ec1580c82dc92579e5cab36ad4bdf"}, "d27b50a4-d2f1-4564-b28d-51935ec380cc": {"doc_hash": "d68597e2cc0e9f8f186e94c19a0dc59f288a4e076bd28c84dd3872ffc585d15f"}, "e7d1eaae-ed92-43e2-8bdd-5481f7dc6433": {"doc_hash": "70558bcc88d9f4eb9dd46a3d4928689b4422e966117ac4a9426e366db9a1b9d9"}, "b41da508-e92f-4d3f-a2da-23486ab1b7fa": {"doc_hash": "7a7493739f1c796f01faaeb0e32e4f8cffe0f5e4acd86d68184a1a348ddada28"}, "ed6cbbc5-d833-4214-a6a4-a702a4d8d5d3": {"doc_hash": "29b689ee2dcd08bc6efe40fd6e6cd2c329b90f152f38b6a17db4c3d7b004b435"}, "6f31858b-1ffe-4157-83fb-ef2518ab5f42": {"doc_hash": "f85f1923e32b8385f5f84db974a9cef131402a54b9cb21a0e6303744389d707b"}, "66ec1340-36b7-40ac-91a2-985effb605f1": {"doc_hash": "70a76be635fc484705d2f26e66e8d19dddd785a03ebfd4be87f6258ea80fd838"}, "de4b5d1d-f2e2-4b1c-ad66-fcab87cc1815": {"doc_hash": "f98f60837c6a5f6069ea42342a63033bf9b94effc4e2b7ae425e1c6494367e0c"}, "637e2577-4fa4-4623-b6f3-befb0b131326": {"doc_hash": "3c9aef1029f6728e9f19beb6c1ee6f5b9e67eef58111376603fa5a9fe954d5f7"}, "f9cde05f-6afc-47ae-988d-0cdb4d0b0bb6": {"doc_hash": "efdc85eadeb70ac2526a8660694dd9daeab3bdd6497faed497039e8fb4c816a5"}, "bc51f29d-9ad7-4b2c-bcd3-b67258cba7c4": {"doc_hash": "63d3fd2e5bfaeda6df5504087411e260dc694b618d3787c98d779fa28cd2939a"}, "dfe2a23b-1918-4c22-ad17-47b8aa453736": {"doc_hash": "4b9e2992388fccc5a0d6d1b96401de05525622e9c6d7eb9aa106cf9a195219a8"}, "3fe1a6e1-9104-499b-b141-59610b26b78c": {"doc_hash": "67ed91e066032d8a13594b5e98b766bf7b3b3f1f39e4f31b59ad95bf3f34134c"}, "ac5bb15b-71e4-4c11-82c3-8025abb2e7ce": {"doc_hash": "88fcdbec3bcf0e8ad3454db47d9dd730adf36f78845076db0b47e0bd6bdc4f6e"}, "5a91fddf-4980-41fd-af0f-9e9d54e41d89": {"doc_hash": "72ec95f0d4cff0b9ac4f245c7d1904660d2b4311035c3895a206c2fc5c064965"}, "e71c834d-c4a0-486f-9d4e-3e3c7fb80bf2": {"doc_hash": "a859933a7d7aa20ebdfd0568724653b3cd55c25c58b873077b118a5b6a301cd4"}, "f34d4cb8-03a4-45a3-bef8-b8185899d5fc": {"doc_hash": "d1abffa3a7c2e7939c842b503095801e6ac863b8b134b75858f16f333be5b8f7"}, "3f4c3679-f158-47b8-a304-e507f1e10f1b": {"doc_hash": "ecb2730a4f70e1ba8098b0169b8cebcc60153b094716037036f0180165e75788"}}, "docstore/data": {"94c3ddf1-c5b0-46ea-a01b-ba06597a0257": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame\u3010Content\u3011The pandas.DataFrame class is a two-dimensional, size-mutable, potentially heterogeneous tabular data structure that can contain labeled axes (rows and columns). It can be thought of as a dict-like container for Series objects.The class constructor takes several parameters:- data: This can be an ndarray, a list of dicts, a dict of Series, or a DataFrame. The data can also be in the form of an iterable or a dataclass. If data is a dict, the column order follows insertion order. If a dict contains Series with defined indices, the data is aligned based on those indices.- index: This parameter specifies the index or array-like object to use for the resulting DataFrame. If no indexing information is part of the input data and no index is provided, a default RangeIndex will be used.- columns: This parameter specifies the column labels to use for the resulting DataFrame when the data does not have them. If the data already contains column labels, column selection will be performed based on these labels.- dtype: This parameter allows you to force a specific data type for the DataFrame. Only a single data type is allowed.- copy: This parameter specifies whether to copy the data from the inputs. The default behavior depends on the type of data. For dict data, the default is copy=True, while for DataFrame or 2d ndarray input, the default is copy=False.The DataFrame class provides various methods and attributes for data manipulation, analysis, and visualization. Some important methods include:- head(): Returns the first n rows of the DataFrame.- tail(): Returns the last n rows of the DataFrame.- describe(): Generates descriptive statistics of the DataFrame.- info(): Prints a concise summary of the DataFrame.- shape: Returns a tuple representing the dimensionality of the DataFrame.- dtype: Returns the data types of the DataFrame.- columns: Returns the column labels of the DataFrame.- index: Returns the index (row labels) of the DataFrame.- dtypes: Returns the data types in the DataFrame.- values: Returns a Numpy representation of the DataFrame.The DataFrame class also provides various arithmetic and logical operations, such as addition, subtraction, multiplication, division, and comparison. These operations align on both row and column labels.The DataFrame can be constructed from various data types, including dictionaries, numpy ndarrays, dataclasses, and Series/DataFrames. The class also provides methods for reading data from files such as CSV and clipboard, as well as methods for writing data to files in formats like CSV, Excel, HDF5, Parquet, and Stata.In addition to these methods, the DataFrame class also supports indexing and slicing, grouping and aggregation, merging and joining, reshaping and pivoting, and handling missing values.Overall, the pandas DataFrame class provides a powerful tool for manipulating and analyzing tabular data, with a wide range of functionality and flexibility.", "doc_id": "94c3ddf1-c5b0-46ea-a01b-ba06597a0257", "embedding": null, "doc_hash": "067bb4bb51d1aabbaff8e65097ea98902323e632d3bac5913649d1a592b8666b", "extra_info": null, "node_info": {"start": 0, "end": 2995, "_node_type": "1"}, "relationships": {"1": "bc7d624c-2119-435b-a459-acd86e08602f"}}, "__type__": "1"}, "6564dd1f-ee92-4f53-a610-ee6b9adfbffd": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.head\u3010Content\u3011pandas.DataFrame.head#DataFrame.head(n=5)[source]#Return the firstnrows.This function returns the firstnrows for the object based\non position. It is useful for quickly testing if your object\nhas the right type of data in it.For negative values ofn, this function returns all rows except\nthe last|n|rows, equivalent todf[:n].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:same type as callerThe firstnrows of the caller object.See alsoDataFrame.tailReturns the lastnrows.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the first 5 lines>>>df.head()animal0  alligator1        bee2     falcon3       lion4     monkeyViewing the firstnlines (three in this case)>>>df.head(3)animal0  alligator1        bee2     falconFor negative values ofn>>>df.head(-3)animal0  alligator1        bee2     falcon3       lion4     monkey5     parrot", "doc_id": "6564dd1f-ee92-4f53-a610-ee6b9adfbffd", "embedding": null, "doc_hash": "b2c8a06980dcd29015c487ab90f1b581c60d651376d9f20ea3cbf6c539a85f6f", "extra_info": null, "node_info": {"start": 0, "end": 1177, "_node_type": "1"}, "relationships": {"1": "27077e63-6c4e-4e8f-847c-b2826930e37c"}}, "__type__": "1"}, "a349f424-78b1-43e7-b62a-97286915f0c5": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.tail\u3010Content\u3011pandas.DataFrame.tail#DataFrame.tail(n=5)[source]#Return the lastnrows.This function returns lastnrows from the object based on\nposition. It is useful for quickly verifying data, for example,\nafter sorting or appending rows.For negative values ofn, this function returns all rows except\nthe first|n|rows, equivalent todf[|n|:].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:type of callerThe lastnrows of the caller object.See alsoDataFrame.headThe firstnrows of the caller object.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the last 5 lines>>>df.tail()animal4  monkey5  parrot6   shark7   whale8   zebraViewing the lastnlines (three in this case)>>>df.tail(3)animal6  shark7  whale8  zebraFor negative values ofn>>>df.tail(-3)animal3    lion4  monkey5  parrot6   shark7   whale8   zebra", "doc_id": "a349f424-78b1-43e7-b62a-97286915f0c5", "embedding": null, "doc_hash": "4d7bff8371c0bed30366710ada31b85510b2e55984952f4e513f9cd9cfd2d7ea", "extra_info": null, "node_info": {"start": 0, "end": 1141, "_node_type": "1"}, "relationships": {"1": "34287ea1-78e3-43a6-a980-6de997889711"}}, "__type__": "1"}, "f172870c-d9be-40b2-9f0e-e92422671aef": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.describe\u3010Content\u3011pandas.DataFrame.describe#DataFrame.describe(percentiles=None,include=None,exclude=None)[source]#Generate descriptive statistics.Descriptive statistics include those that summarize the central\ntendency, dispersion and shape of a\ndataset\u2019s distribution, excludingNaNvalues.Analyzes both numeric and object series, as well\nasDataFramecolumn sets of mixed data types. The output\nwill vary depending on what is provided. Refer to the notes\nbelow for more detail.Parameters:percentileslist-like of numbers, optionalThe percentiles to include in the output. All should\nfall between 0 and 1. The default is[.25,.5,.75], which returns the 25th, 50th, and\n75th percentiles.include\u2018all\u2019, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored\nforSeries. Here are the options:\u2018all\u2019 : All columns of the input will be included in the output.A list-like of dtypes : Limits the results to the\nprovided data types.\nTo limit the result to numeric types submitnumpy.number. To limit it instead to object columns submit\nthenumpy.objectdata type. Strings\ncan also be used in the style ofselect_dtypes(e.g.df.describe(include=['O'])). To\nselect pandas categorical columns, use'category'None (default) : The result will include all numeric columns.excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored\nforSeries. Here are the options:A list-like of dtypes : Excludes the provided data types\nfrom the result. To exclude numeric types submitnumpy.number. To exclude object columns submit the data\ntypenumpy.object. Strings can also be used in the style ofselect_dtypes(e.g.df.describe(exclude=['O'])). To\nexclude pandas categorical columns, use'category'None (default) : The result will exclude nothing.Returns:Series or DataFrameSummary statistics of the Series or Dataframe provided.See alsoDataFrame.countCount number of non-NA/null observations.DataFrame.maxMaximum of the values in the object.DataFrame.minMinimum of the values in the object.DataFrame.meanMean of the values.DataFrame.stdStandard deviation of the observations.DataFrame.select_dtypesSubset of a DataFrame including/excluding columns based on their dtype.NotesFor numeric data, the result\u2019s index will includecount,mean,std,min,maxas well as lower,50and\nupper percentiles. By default the lower percentile is25and the\nupper percentile is75. The50percentile is the\nsame as the median.For object data (e.g. strings or timestamps), the result\u2019s index\nwill includecount,unique,top, andfreq. Thetopis the most common value. Thefreqis the most common value\u2019s\nfrequency. Timestamps also include thefirstandlastitems.If multiple object values have the highest count, then thecountandtopresults will be arbitrarily chosen from\namong those with the highest count.For mixed data types provided via aDataFrame, the default is to\nreturn only an analysis of numeric columns. If the dataframe consists\nonly of object and categorical data without any numeric columns, the\ndefault is to return an analysis of both the object and categorical\ncolumns. Ifinclude='all'is provided as an option, the result\nwill include a union of attributes of each type.Theincludeandexcludeparameters can be used to limit\nwhich columns in aDataFrameare analyzed for the output.\nThe parameters are ignored when analyzing aSeries.ExamplesDescribing a numericSeries.>>>s=pd.Series([1,2,3])>>>s.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%     ", "doc_id": "f172870c-d9be-40b2-9f0e-e92422671aef", "embedding": null, "doc_hash": "c2722fc334e85240d2d11e600a21fd288cc79c353eda5bbfeca2a91f7161222c", "extra_info": null, "node_info": {"start": 0, "end": 3578, "_node_type": "1"}, "relationships": {"1": "e5e853a1-4909-4444-ba33-4f87e328dac1", "3": "c69e3536-6e2c-4050-ba5b-2f8dc4598a8b"}}, "__type__": "1"}, "c69e3536-6e2c-4050-ba5b-2f8dc4598a8b": {"__data__": {"text": "an analysis of numeric columns. If the dataframe consists\nonly of object and categorical data without any numeric columns, the\ndefault is to return an analysis of both the object and categorical\ncolumns. Ifinclude='all'is provided as an option, the result\nwill include a union of attributes of each type.Theincludeandexcludeparameters can be used to limit\nwhich columns in aDataFrameare analyzed for the output.\nThe parameters are ignored when analyzing aSeries.ExamplesDescribing a numericSeries.>>>s=pd.Series([1,2,3])>>>s.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0dtype: float64Describing a categoricalSeries.>>>s=pd.Series(['a','a','b','c'])>>>s.describe()count     4unique    3top       afreq      2dtype: objectDescribing a timestampSeries.>>>s=pd.Series([...np.datetime64(\"2000-01-01\"),...np.datetime64(\"2010-01-01\"),...np.datetime64(\"2010-01-01\")...])>>>s.describe()count                      3mean     2006-09-01 08:00:00min      2000-01-01 00:00:0025%      2004-12-31 12:00:0050%      2010-01-01 00:00:0075%      2010-01-01 00:00:00max      2010-01-01 00:00:00dtype: objectDescribing aDataFrame. By default only numeric fields\nare returned.>>>df=pd.DataFrame({'categorical':pd.Categorical(['d','e','f']),...'numeric':[1,2,3],...'object':['a','b','c']...})>>>df.describe()numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Describing all columns of aDataFrameregardless of data type.>>>df.describe(include='all')categorical  numeric objectcount            3      3.0      3unique           3      NaN      3top              f      NaN      afreq             1      NaN      1mean           NaN      2.0    NaNstd            NaN      1.0    NaNmin            NaN      1.0    NaN25%            NaN      1.5    NaN50%            NaN      2.0    NaN75%            NaN      2.5    NaNmax            NaN      3.0    NaNDescribing a column from aDataFrameby accessing it as\nan attribute.>>>df.numeric.describe()count    3.0mean     2.0std      1.0min      1.025%     ", "doc_id": "c69e3536-6e2c-4050-ba5b-2f8dc4598a8b", "embedding": null, "doc_hash": "ed09047f8a3c7aa573566ce584866ff78e729bc98d7c0e42b2052dfa489435ed", "extra_info": null, "node_info": {"start": 3066, "end": 5167, "_node_type": "1"}, "relationships": {"1": "e5e853a1-4909-4444-ba33-4f87e328dac1", "2": "f172870c-d9be-40b2-9f0e-e92422671aef", "3": "62fb35bd-3509-448c-8d1d-c99d6436c8e9"}}, "__type__": "1"}, "62fb35bd-3509-448c-8d1d-c99d6436c8e9": {"__data__": {"text": "         NaN      1.0    NaN25%            NaN      1.5    NaN50%            NaN      2.0    NaN75%            NaN      2.5    NaNmax            NaN      3.0    NaNDescribing a column from aDataFrameby accessing it as\nan attribute.>>>df.numeric.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0Name: numeric, dtype: float64Including only numeric columns in aDataFramedescription.>>>df.describe(include=[np.number])numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Including only string columns in aDataFramedescription.>>>df.describe(include=[object])objectcount       3unique      3top         afreq        1Including only categorical columns from aDataFramedescription.>>>df.describe(include=['category'])categoricalcount            3unique           3top              dfreq             1Excluding numeric columns from aDataFramedescription.>>>df.describe(exclude=[np.number])categorical objectcount            3      3unique           3      3top              f      afreq             1      1Excluding object columns from aDataFramedescription.>>>df.describe(exclude=[object])categorical  numericcount            3      3.0unique           3      NaNtop              f      NaNfreq             1      NaNmean           NaN      2.0std            NaN      1.0min            NaN      1.025%            NaN      1.550%            NaN      2.075%            NaN      2.5max            NaN      3.0", "doc_id": "62fb35bd-3509-448c-8d1d-c99d6436c8e9", "embedding": null, "doc_hash": "641c8d64fdb6b70ea704ffa0dfeeffa73b108c5d67a95ae7624dc4aa1a9ce8e7", "extra_info": null, "node_info": {"start": 5369, "end": 6892, "_node_type": "1"}, "relationships": {"1": "e5e853a1-4909-4444-ba33-4f87e328dac1", "2": "c69e3536-6e2c-4050-ba5b-2f8dc4598a8b"}}, "__type__": "1"}, "677acef4-073a-42e0-830a-385d4ea5ad10": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.info\u3010Content\u3011pandas.DataFrame.info#DataFrame.info(verbose=None,buf=None,max_cols=None,memory_usage=None,show_counts=None)[source]#Print a concise summary of a DataFrame.This method prints information about a DataFrame including\nthe index dtype and columns, non-null values and memory usage.Parameters:verbosebool, optionalWhether to print the full summary. By default, the setting inpandas.options.display.max_info_columnsis followed.bufwritable buffer, defaults to sys.stdoutWhere to send the output. By default, the output is printed to\nsys.stdout. Pass a writable buffer if you need to further process\nthe output.max_colsint, optionalWhen to switch from the verbose to the truncated output. If the\nDataFrame has more thanmax_colscolumns, the truncated output\nis used. By default, the setting inpandas.options.display.max_info_columnsis used.memory_usagebool, str, optionalSpecifies whether total memory usage of the DataFrame\nelements (including the index) should be displayed. By default,\nthis follows thepandas.options.display.memory_usagesetting.True always show memory usage. False never shows memory usage.\nA value of \u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d.\nMemory usage is shown in human-readable units (base-2\nrepresentation). Without deep introspection a memory estimation is\nmade based in column dtype and number of rows assuming values\nconsume the same memory amount for corresponding dtypes. With deep\nmemory introspection, a real memory usage calculation is performed\nat the cost of computational resources. See theFrequently Asked Questionsfor more\ndetails.show_countsbool, optionalWhether to show the non-null counts. By default, this is shown\nonly if the DataFrame is smaller thanpandas.options.display.max_info_rowsandpandas.options.display.max_info_columns. A value of True always\nshows the counts, and False never shows the counts.Returns:NoneThis method prints a summary of a DataFrame and returns None.See alsoDataFrame.describeGenerate descriptive statistics of DataFrame columns.DataFrame.memory_usageMemory usage of DataFrame columns.Examples>>>int_values=[1,2,3,4,5]>>>text_values=['alpha','beta','gamma','delta','epsilon']>>>float_values=[0.0,0.25,0.5,0.75,1.0]>>>df=pd.DataFrame({\"int_col\":int_values,\"text_col\":text_values,...\"float_col\":float_values})>>>dfint_col text_col  float_col0        1    alpha       0.001        2     beta       0.252        3    gamma       0.503        4    delta       0.754        5  epsilon       1.00Prints information of all columns:>>>df.info(verbose=True)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Data columns (total 3 columns):#   Column     Non-Null Count  Dtype---  ------     --------------  -----0   int_col    5 non-null      int641   text_col   5 non-null      object2   float_col  5 non-null      float64dtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPrints a summary of columns count and its dtypes but not per column\ninformation:>>>df.info(verbose=False)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Columns: 3 entries, int_col to float_coldtypes: float64(1), int64(1),", "doc_id": "677acef4-073a-42e0-830a-385d4ea5ad10", "embedding": null, "doc_hash": "a270976884c60382f7018e9c7a3832aa95b1560198a8381a9f2d725a23e42f15", "extra_info": null, "node_info": {"start": 0, "end": 3180, "_node_type": "1"}, "relationships": {"1": "5d343073-d22e-4d68-9b5d-d787c7876e34", "3": "7624479a-2d1d-4358-9275-575ac96b60a2"}}, "__type__": "1"}, "7624479a-2d1d-4358-9275-575ac96b60a2": {"__data__": {"text": "5 entries, 0 to 4Data columns (total 3 columns):#   Column     Non-Null Count  Dtype---  ------     --------------  -----0   int_col    5 non-null      int641   text_col   5 non-null      object2   float_col  5 non-null      float64dtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPrints a summary of columns count and its dtypes but not per column\ninformation:>>>df.info(verbose=False)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Columns: 3 entries, int_col to float_coldtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPipe output of DataFrame.info to buffer instead of sys.stdout, get\nbuffer content and writes to a text file:>>>importio>>>buffer=io.StringIO()>>>df.info(buf=buffer)>>>s=buffer.getvalue()>>>withopen(\"df_info.txt\",\"w\",...encoding=\"utf-8\")asf:...f.write(s)260Thememory_usageparameter allows deep introspection mode, specially\nuseful for big DataFrames and fine-tune memory optimization:>>>random_strings_array=np.random.choice(['a','b','c'],10**6)>>>df=pd.DataFrame({...'column_1':np.random.choice(['a','b','c'],10**6),...'column_2':np.random.choice(['a','b','c'],10**6),...'column_3':np.random.choice(['a','b','c'],10**6)...})>>>df.info()<class 'pandas.core.frame.DataFrame'>RangeIndex: 1000000 entries, 0 to 999999Data columns (total 3 columns):#   Column    Non-Null Count    Dtype---  ------    --------------    -----0   column_1  1000000 non-null  object1   column_2  1000000 non-null  object2   column_3  1000000 non-null  objectdtypes: object(3)memory usage: 22.9+ MB>>>df.info(memory_usage='deep')<class 'pandas.core.frame.DataFrame'>RangeIndex: 1000000 entries, 0 to 999999Data columns (total 3 columns):#   Column    Non-Null Count    Dtype---  ------    --------------    -----0   column_1  1000000 non-null  object1   column_2  1000000 non-null  object2   column_3  1000000 non-null  objectdtypes: object(3)memory usage: 165.9 MB", "doc_id": "7624479a-2d1d-4358-9275-575ac96b60a2", "embedding": null, "doc_hash": "f7c30285ae904de8003a953428ff87261648dad6f68a6cb18272cf7d63fc8bb3", "extra_info": null, "node_info": {"start": 2643, "end": 4555, "_node_type": "1"}, "relationships": {"1": "5d343073-d22e-4d68-9b5d-d787c7876e34", "2": "677acef4-073a-42e0-830a-385d4ea5ad10"}}, "__type__": "1"}, "39cf6e79-6c78-4336-83bf-1cf54714ecf0": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.columns\u3010Content\u3011pandas.DataFrame.columns#DataFrame.columns#The column labels of the DataFrame.Examples>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>dfA  B0    1  31    2  4>>>df.columnsIndex(['A', 'B'], dtype='object')", "doc_id": "39cf6e79-6c78-4336-83bf-1cf54714ecf0", "embedding": null, "doc_hash": "e0f13ea06d3773d0c981ab34268f9248786cf98612fab496d6b55e96529796cf", "extra_info": null, "node_info": {"start": 0, "end": 272, "_node_type": "1"}, "relationships": {"1": "4a080934-2d32-42b0-9ede-ebd3a21e348f"}}, "__type__": "1"}, "33f59b41-fcc3-40fb-ae44-b3675d16f889": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.index\u3010Content\u3011pandas.DataFrame.index#DataFrame.index#The index (row labels) of the DataFrame.The index of a DataFrame is a series of labels that identify each row.\nThe labels can be integers, strings, or any other hashable type. The index\nis used for label-based access and alignment, and can be accessed or\nmodified using this attribute.Returns:pandas.IndexThe index labels of the DataFrame.See alsoDataFrame.columnsThe column labels of the DataFrame.DataFrame.to_numpyConvert the DataFrame to a NumPy array.Examples>>>df=pd.DataFrame({'Name':['Alice','Bob','Aritra'],...'Age':[25,30,35],...'Location':['Seattle','New York','Kona']},...index=([10,20,30]))>>>df.indexIndex([10, 20, 30], dtype='int64')In this example, we create a DataFrame with 3 rows and 3 columns,\nincluding Name, Age, and Location information. We set the index labels to\nbe the integers 10, 20, and 30. We then access theindexattribute of the\nDataFrame, which returns anIndexobject containing the index labels.>>>df.index=[100,200,300]>>>dfName  Age Location100  Alice   25  Seattle200    Bob   30 New York300  Aritra  35    KonaIn this example, we modify the index labels of the DataFrame by assigning\na new list of labels to theindexattribute. The DataFrame is then\nupdated with the new labels, and the output shows the modified DataFrame.", "doc_id": "33f59b41-fcc3-40fb-ae44-b3675d16f889", "embedding": null, "doc_hash": "0aaa3525ea0a9e6df96b1546936297f081afcabaca3d8f2611d199bf3f9bf0a9", "extra_info": null, "node_info": {"start": 0, "end": 1367, "_node_type": "1"}, "relationships": {"1": "add4f3bc-0cac-4c98-9f35-458bfcf7a7b1"}}, "__type__": "1"}, "a4e6218a-1121-46c2-8275-501fae1421a8": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.dtypes\u3010Content\u3011pandas.DataFrame.dtypes#propertyDataFrame.dtypes[source]#Return the dtypes in the DataFrame.This returns a Series with the data type of each column.\nThe result\u2019s index is the original DataFrame\u2019s columns. Columns\nwith mixed types are stored with theobjectdtype. Seethe User Guidefor more.Returns:pandas.SeriesThe data type of each column.Examples>>>df=pd.DataFrame({'float':[1.0],...'int':[1],...'datetime':[pd.Timestamp('20180310')],...'string':['foo']})>>>df.dtypesfloat              float64int                  int64datetime    datetime64[ns]string              objectdtype: object", "doc_id": "a4e6218a-1121-46c2-8275-501fae1421a8", "embedding": null, "doc_hash": "a915388b18baa0aadf23be4c160438cc7908a83d61dfaa58eb226f9142cd2031", "extra_info": null, "node_info": {"start": 0, "end": 655, "_node_type": "1"}, "relationships": {"1": "62313f7a-3a1f-4ba8-9153-d3c6ac15bb5f"}}, "__type__": "1"}, "4684885b-777b-4951-935f-ab4b67ed6741": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.shape\u3010Content\u3011pandas.DataFrame.shape#propertyDataFrame.shape[source]#Return a tuple representing the dimensionality of the DataFrame.See alsondarray.shapeTuple of array dimensions.Examples>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>df.shape(2, 2)>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4],...'col3':[5,6]})>>>df.shape(2, 3)", "doc_id": "4684885b-777b-4951-935f-ab4b67ed6741", "embedding": null, "doc_hash": "992927eb8f6720af61d5127af50434327610a3091f825887965c5db9d9901f2f", "extra_info": null, "node_info": {"start": 0, "end": 388, "_node_type": "1"}, "relationships": {"1": "bad62121-bf17-40e4-b7fe-8aa0da990b07"}}, "__type__": "1"}, "7c1048a7-4db7-456d-abfa-48ab9dc73fdc": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.groupby\u3010Content\u3011pandas.DataFrame.groupby#DataFrame.groupby(by=None,axis=_NoDefault.no_default,level=None,as_index=True,sort=True,group_keys=True,observed=_NoDefault.no_default,dropna=True)[source]#Group DataFrame using a mapper or by a Series of columns.A groupby operation involves some combination of splitting the\nobject, applying a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.Parameters:bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby.\nIfbyis a function, it\u2019s called on each value of the object\u2019s\nindex. If a dict or Series is passed, the Series or dict VALUES\nwill be used to determine the groups (the Series\u2019 values are first\naligned; see.align()method). If a list or ndarray of length\nequal to the selected axis is passed (see thegroupby user guide),\nthe values are used as-is to determine the groups. A label or list\nof labels may be passed to group by the columns inself.\nNotice that a tuple is interpreted as a (single) key.axis{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0Split along rows (0) or columns (1). ForSeriesthis parameter\nis unused and defaults to 0.Deprecated since version 2.1.0:Will be removed and behave like axis=0 in a future version.\nForaxis=1, doframe.T.groupby(...)instead.levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular\nlevel or levels. Do not specify bothbyandlevel.as_indexbool, default TrueReturn object with group labels as the\nindex. Only relevant for DataFrame input. as_index=False is\neffectively \u201cSQL-style\u201d grouped output. This argument has no effect\non filtrations (see thefiltrations in the user guide),\nsuch ashead(),tail(),nth()and in transformations\n(see thetransformations in the user guide).sortbool, default TrueSort group keys. Get better performance by turning this off.\nNote this does not influence the order of observations within each\ngroup. Groupby preserves the order of rows within each group. If False,\nthe groups will appear in the same order as they did in the original DataFrame.\nThis argument has no effect on filtrations (see thefiltrations in the user guide),\nsuch ashead(),tail(),nth()and in transformations\n(see thetransformations in the user guide).Changed in version 2.0.0:Specifyingsort=Falsewith an ordered categorical grouper will no\nlonger sort the values.group_keysbool, default TrueWhen calling apply and thebyargument produces a like-indexed\n(i.e.a transform) result, add group keys to\nindex to identify pieces. By default group keys are not included\nwhen the result\u2019s index (and column) labels match the inputs, and\nare included otherwise.Changed in version 1.5.0:Warns thatgroup_keyswill no longer be ignored when the\nresult fromapplyis a like-indexed Series or DataFrame.\nSpecifygroup_keysexplicitly to include the group keys or\nnot.Changed in version 2.0.0:group_keysnow defaults toTrue.observedbool, default FalseThis only applies if any of the groupers are Categoricals.\nIf True: only show observed values for categorical groupers.\nIf False: show all values for categorical groupers.Deprecated since version 2.1.0:The default value will change to True in a future version of pandas.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together\nwith row/column will be dropped.\nIf False, NA values will also be treated as the key in groups.Returns:pandas.api.typing.DataFrameGroupByReturns a groupby object that contains information about the groups.See", "doc_id": "7c1048a7-4db7-456d-abfa-48ab9dc73fdc", "embedding": null, "doc_hash": "4f231dda412328c36db75be318d8258e47190110577dfe92006d3448716001bc", "extra_info": null, "node_info": {"start": 0, "end": 3616, "_node_type": "1"}, "relationships": {"1": "1e0c5143-d113-41f9-a488-93841c6500c4", "3": "9367a334-f80a-41c0-868f-e2ccfe8961a8"}}, "__type__": "1"}, "9367a334-f80a-41c0-868f-e2ccfe8961a8": {"__data__": {"text": "Series or DataFrame.\nSpecifygroup_keysexplicitly to include the group keys or\nnot.Changed in version 2.0.0:group_keysnow defaults toTrue.observedbool, default FalseThis only applies if any of the groupers are Categoricals.\nIf True: only show observed values for categorical groupers.\nIf False: show all values for categorical groupers.Deprecated since version 2.1.0:The default value will change to True in a future version of pandas.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together\nwith row/column will be dropped.\nIf False, NA values will also be treated as the key in groups.Returns:pandas.api.typing.DataFrameGroupByReturns a groupby object that contains information about the groups.See alsoresampleConvenience method for frequency conversion and resampling of time series.NotesSee theuser guidefor more\ndetailed usage and examples, including splitting an object into groups,\niterating through groups, selecting a group, aggregation, and more.Examples>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>dfAnimal  Max Speed0  Falcon      380.01  Falcon      370.02  Parrot       24.03  Parrot       26.0>>>df.groupby(['Animal']).mean()Max SpeedAnimalFalcon      375.0Parrot       25.0Hierarchical IndexesWe can groupby different levels of a hierarchical index\nusing thelevelparameter:>>>arrays=[['Falcon','Falcon','Parrot','Parrot'],...['Captive','Wild','Captive','Wild']]>>>index=pd.MultiIndex.from_arrays(arrays,names=('Animal','Type'))>>>df=pd.DataFrame({'Max Speed':[390.,350.,30.,20.]},...index=index)>>>dfMax SpeedAnimal TypeFalcon Captive      390.0Wild         350.0Parrot Captive       30.0Wild          20.0>>>df.groupby(level=0).mean()Max SpeedAnimalFalcon      370.0Parrot       25.0>>>df.groupby(level=\"Type\").mean()Max SpeedTypeCaptive      210.0Wild         185.0We can also choose to include NA in group keys or not by settingdropnaparameter, the default setting isTrue.>>>l=[[1,2,3],[1,None,4],[2,1,3],[1,2,2]]>>>df=pd.DataFrame(l,columns=[\"a\",\"b\",\"c\"])>>>df.groupby(by=[\"b\"]).sum()a   cb1.0 2   32.0 2   5>>>df.groupby(by=[\"b\"],dropna=False).sum()a   cb1.0 2   32.0 2   5NaN 1   4>>>l=[[\"a\",12,12],[None,12.3,33.],[\"b\",12.3,123],[\"a\",1,1]]>>>df=pd.DataFrame(l,columns=[\"a\",\"b\",\"c\"])>>>df.groupby(by=\"a\").sum()b     caa   13.0   13.0b   12.3  123.0>>>df.groupby(by=\"a\",dropna=False).sum()b     caa   13.0   13.0b   12.3  123.0NaN 12.3   33.0When using.apply(), usegroup_keysto include or exclude the\ngroup keys. Thegroup_keysargument defaults toTrue(include).>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>df.groupby(\"Animal\",group_keys=True)[['Max Speed']].apply(lambdax:x)Max", "doc_id": "9367a334-f80a-41c0-868f-e2ccfe8961a8", "embedding": null, "doc_hash": "e559ec5beb22b80a37417a6318838a63092a29945879dd419476406f749cfb62", "extra_info": null, "node_info": {"start": 2978, "end": 5735, "_node_type": "1"}, "relationships": {"1": "1e0c5143-d113-41f9-a488-93841c6500c4", "2": "7c1048a7-4db7-456d-abfa-48ab9dc73fdc", "3": "3c5f69a6-4c2c-4fe1-8e22-24e313617ac5"}}, "__type__": "1"}, "3c5f69a6-4c2c-4fe1-8e22-24e313617ac5": {"__data__": {"text": "    caa   13.0   13.0b   12.3  123.0>>>df.groupby(by=\"a\",dropna=False).sum()b     caa   13.0   13.0b   12.3  123.0NaN 12.3   33.0When using.apply(), usegroup_keysto include or exclude the\ngroup keys. Thegroup_keysargument defaults toTrue(include).>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>df.groupby(\"Animal\",group_keys=True)[['Max Speed']].apply(lambdax:x)Max SpeedAnimalFalcon 0      380.01      370.0Parrot 2       24.03       26.0>>>df.groupby(\"Animal\",group_keys=False)[['Max Speed']].apply(lambdax:x)Max Speed0      380.01      370.02       24.03       26.0", "doc_id": "3c5f69a6-4c2c-4fe1-8e22-24e313617ac5", "embedding": null, "doc_hash": "b3bcd680c170693101aa07876c36e59cad01e3c8ea0466755aa1ef2af283ebf2", "extra_info": null, "node_info": {"start": 5947, "end": 6577, "_node_type": "1"}, "relationships": {"1": "1e0c5143-d113-41f9-a488-93841c6500c4", "2": "9367a334-f80a-41c0-868f-e2ccfe8961a8"}}, "__type__": "1"}, "b0d94955-4cd2-4760-9515-f8259a793481": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.merge\u3010Content\u3011pandas.DataFrame.merge#DataFrame.merge(right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=None,indicator=False,validate=None)[source]#Merge DataFrame or named Series objects with a database-style join.A named Series object is treated as a DataFrame with a single named column.The join is done on columns or indexes. If joining columns on\ncolumns, the DataFrame indexeswill be ignored. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen performing a cross merge, no column specifications to merge on are\nallowed.WarningIf both key columns contain rows where the key is a null value, those\nrows will be matched against each other. This is different from usual SQL\njoin behaviour and can lead to unexpected results.Parameters:rightDataFrame or named SeriesObject to merge with.how{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019, \u2018cross\u2019}, default \u2018inner\u2019Type of merge to be performed.left: use only keys from left frame, similar to a SQL left outer join;\npreserve key order.right: use only keys from right frame, similar to a SQL right outer join;\npreserve key order.outer: use union of keys from both frames, similar to a SQL full outer\njoin; sort keys lexicographically.inner: use intersection of keys from both frames, similar to a SQL inner\njoin; preserve the order of the left keys.cross: creates the cartesian product from both frames, preserves the order\nof the left keys.onlabel or listColumn or index level names to join on. These must be found in both\nDataFrames. Ifonis None and not merging on indexes then this defaults\nto the intersection of the columns in both DataFrames.left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also\nbe an array or list of arrays of the length of the left DataFrame.\nThese arrays are treated as if they are columns.right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also\nbe an array or list of arrays of the length of the right DataFrame.\nThese arrays are treated as if they are columns.left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a\nMultiIndex, the number of keys in the other DataFrame (either the index\nor a number of columns) must match the number of levels.right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as\nleft_index.sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False,\nthe order of the join keys depends on the join type (how keyword).suffixeslist-like, default is (\u201c_x\u201d, \u201c_y\u201d)A length-2 sequence where each element is optionally a string\nindicating the suffix to add to overlapping column names inleftandrightrespectively. Pass a value ofNoneinstead\nof a string to indicate that the column name fromleftorrightshould be left as-is, with no suffix. At least one of the\nvalues must not be None.copybool, default TrueIf False, avoid copy if possible.NoteThecopykeyword will change behavior in pandas 3.0.Copy-on-Writewill be enabled by default, which means that all methods with acopykeyword will use a lazy copy mechanism to defer the copy and\nignore thecopykeyword. Thecopykeyword will be removed in a\nfuture version of pandas.You can already get the future behavior and improvements through\nenabling copy on writepd.options.mode.copy_on_write=Trueindicatorbool or str, default FalseIf True, adds a column to the output DataFrame called \u201c_merge\u201d with\ninformation on the source of each row. The column can be given a different\nname by providing a string argument. The column will have", "doc_id": "b0d94955-4cd2-4760-9515-f8259a793481", "embedding": null, "doc_hash": "71a281d1ab4e2100cc65304ac0008518a1b487b9b2e54dbddf12a7ca48ca0e50", "extra_info": null, "node_info": {"start": 0, "end": 3765, "_node_type": "1"}, "relationships": {"1": "3c5ae70b-da7b-4dc0-bada-fbfa4aaecc65", "3": "da8d91d6-ed6c-43ba-9ccc-1e83007e6e7d"}}, "__type__": "1"}, "da8d91d6-ed6c-43ba-9ccc-1e83007e6e7d": {"__data__": {"text": "suffix. At least one of the\nvalues must not be None.copybool, default TrueIf False, avoid copy if possible.NoteThecopykeyword will change behavior in pandas 3.0.Copy-on-Writewill be enabled by default, which means that all methods with acopykeyword will use a lazy copy mechanism to defer the copy and\nignore thecopykeyword. Thecopykeyword will be removed in a\nfuture version of pandas.You can already get the future behavior and improvements through\nenabling copy on writepd.options.mode.copy_on_write=Trueindicatorbool or str, default FalseIf True, adds a column to the output DataFrame called \u201c_merge\u201d with\ninformation on the source of each row. The column can be given a different\nname by providing a string argument. The column will have a Categorical\ntype with the value of \u201cleft_only\u201d for observations whose merge key only\nappears in the left DataFrame, \u201cright_only\u201d for observations\nwhose merge key only appears in the right DataFrame, and \u201cboth\u201d\nif the observation\u2019s merge key is found in both DataFrames.validatestr, optionalIf specified, checks if merge is of specified type.\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both\nleft and right datasets.\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left\ndataset.\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right\ndataset.\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.Returns:DataFrameA DataFrame of the two merged objects.See alsomerge_orderedMerge with optional filling/interpolation.merge_asofMerge on nearest keys.DataFrame.joinSimilar method using indices.Examples>>>df1=pd.DataFrame({'lkey':['foo','bar','baz','foo'],...'value':[1,2,3,5]})>>>df2=pd.DataFrame({'rkey':['foo','bar','baz','foo'],...'value':[5,6,7,8]})>>>df1lkey value0   foo      11   bar      22   baz      33   foo      5>>>df2rkey value0   foo      51   bar      62   baz      73   foo      8Merge df1 and df2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, appended.>>>df1.merge(df2,left_on='lkey',right_on='rkey')lkey  value_x rkey  value_y0  foo        1  foo        51  foo        1  foo        82  bar        2  bar        63  baz        3  baz        74  foo        5  foo        55  foo        5  foo        8Merge DataFrames df1 and df2 with specified left and right suffixes\nappended to any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',...suffixes=('_left','_right'))lkey  value_left rkey  value_right0  foo           1  foo            51  foo           1  foo            82  bar           2  bar            63  baz           3  baz    ", "doc_id": "da8d91d6-ed6c-43ba-9ccc-1e83007e6e7d", "embedding": null, "doc_hash": "0e56d4084c5829fbce11d1120050f9924f59678ce9758bef85e4f929080577f5", "extra_info": null, "node_info": {"start": 3127, "end": 5717, "_node_type": "1"}, "relationships": {"1": "3c5ae70b-da7b-4dc0-bada-fbfa4aaecc65", "2": "b0d94955-4cd2-4760-9515-f8259a793481", "3": "e154535f-3641-4196-b91d-7b678b8a91a5"}}, "__type__": "1"}, "e154535f-3641-4196-b91d-7b678b8a91a5": {"__data__": {"text": " foo        5  foo        8Merge DataFrames df1 and df2 with specified left and right suffixes\nappended to any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',...suffixes=('_left','_right'))lkey  value_left rkey  value_right0  foo           1  foo            51  foo           1  foo            82  bar           2  bar            63  baz           3  baz            74  foo           5  foo            55  foo           5  foo            8Merge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',suffixes=(False,False))Traceback (most recent call last):...ValueError:columns overlap but no suffix specified:Index(['value'], dtype='object')>>>df1=pd.DataFrame({'a':['foo','bar'],'b':[1,2]})>>>df2=pd.DataFrame({'a':['foo','baz'],'c':[3,4]})>>>df1a  b0   foo  11   bar  2>>>df2a  c0   foo  31   baz  4>>>df1.merge(df2,how='inner',on='a')a  b  c0   foo  1  3>>>df1.merge(df2,how='left',on='a')a  b  c0   foo  1  3.01   bar  2  NaN>>>df1=pd.DataFrame({'left':['foo','bar']})>>>df2=pd.DataFrame({'right':[7,8]})>>>df1left0   foo1   bar>>>df2right0   71   8>>>df1.merge(df2,how='cross')left  right0   foo      71   foo      82   bar      73   bar      8", "doc_id": "e154535f-3641-4196-b91d-7b678b8a91a5", "embedding": null, "doc_hash": "b50de4fd268d1051e969b0ab09357f68cf46613231c49a7ee93fe624b58beeca", "extra_info": null, "node_info": {"start": 5977, "end": 7236, "_node_type": "1"}, "relationships": {"1": "3c5ae70b-da7b-4dc0-bada-fbfa4aaecc65", "2": "da8d91d6-ed6c-43ba-9ccc-1e83007e6e7d"}}, "__type__": "1"}, "bd52ef85-1714-4ac2-85ff-c9be597aa6aa": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.sort_values\u3010Content\u3011pandas.DataFrame.sort_values#DataFrame.sort_values(by,*,axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last',ignore_index=False,key=None)[source]#Sort by the values along either axis.Parameters:bystr or list of strName or list of names to sort by.ifaxisis 0 or\u2018index\u2019thenbymay contain index\nlevels and/or column labels.ifaxisis 1 or\u2018columns\u2019thenbymay contain column\nlevels and/or index labels.axis\u201c{0 or \u2018index\u2019, 1 or \u2018columns\u2019}\u201d, default 0Axis to be sorted.ascendingbool or list of bool, default TrueSort ascending vs. descending. Specify list for multiple sort\norders. If this is a list of bools, must match the length of\nthe by.inplacebool, default FalseIf True, perform operation in-place.kind{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019Choice of sorting algorithm. See alsonumpy.sort()for more\ninformation.mergesortandstableare the only stable algorithms. For\nDataFrames, this option is only applied when sorting on a single\ncolumn or label.na_position{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019Puts NaNs at the beginning iffirst;lastputs NaNs at the\nend.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.keycallable, optionalApply the key function to the values\nbefore sorting. This is similar to thekeyargument in the\nbuiltinsorted()function, with the notable difference that\nthiskeyfunction should bevectorized. It should expect aSeriesand return a Series with the same shape as the input.\nIt will be applied to each column inbyindependently.Returns:DataFrame or NoneDataFrame with sorted values or None ifinplace=True.See alsoDataFrame.sort_indexSort a DataFrame by the index.Series.sort_valuesSimilar method for a Series.Examples>>>df=pd.DataFrame({...'col1':['A','A','B',np.nan,'D','C'],...'col2':[2,1,9,8,7,4],...'col3':[0,1,9,4,2,3],...'col4':['a','B','c','D','e','F']...})>>>dfcol1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FSort by col1>>>df.sort_values(by=['col1'])col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort by multiple columns>>>df.sort_values(by=['col1','col2'])col1  col2  col3 col41    A     1     1    B0    A     2     0    a2    B     9     9    c5    C     4     3    F4    D     7     2  ", "doc_id": "bd52ef85-1714-4ac2-85ff-c9be597aa6aa", "embedding": null, "doc_hash": "5858c6bb2e2ada98154e43d2345090261d43450199108923497089a2a218c252", "extra_info": null, "node_info": {"start": 0, "end": 2495, "_node_type": "1"}, "relationships": {"1": "e824d9ac-d2f1-473a-8cca-33e65aa70514", "3": "30aa99db-6465-4145-8a88-6794c35f8753"}}, "__type__": "1"}, "30aa99db-6465-4145-8a88-6794c35f8753": {"__data__": {"text": "B2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort by multiple columns>>>df.sort_values(by=['col1','col2'])col1  col2  col3 col41    A     1     1    B0    A     2     0    a2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort Descending>>>df.sort_values(by='col1',ascending=False)col1  col2  col3 col44    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    B3  NaN     8     4    DPutting NAs first>>>df.sort_values(by='col1',ascending=False,na_position='first')col1  col2  col3 col43  NaN     8     4    D4    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    BSorting with a key function>>>df.sort_values(by='col4',key=lambdacol:col.str.lower())col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FNatural sort with the key argument,\nusing thenatsort <https://github.com/SethMMorton/natsort>package.>>>df=pd.DataFrame({...\"time\":['0hr','128hr','72hr','48hr','96hr'],...\"value\":[10,20,30,40,50]...})>>>dftime  value0    0hr     101  128hr     202   72hr     303   48hr     404   96hr     50>>>fromnatsortimportindex_natsorted>>>df.sort_values(...by=\"time\",...key=lambdax:np.argsort(index_natsorted(df[\"time\"]))...)time  value0    0hr     103   48hr     402   72hr     304   96hr     501  128hr     20", "doc_id": "30aa99db-6465-4145-8a88-6794c35f8753", "embedding": null, "doc_hash": "1e15a73e11522cebb2374b84ec3406afe7dc4f3b3477c696aecb2dc94b5b1d9d", "extra_info": null, "node_info": {"start": 2208, "end": 3724, "_node_type": "1"}, "relationships": {"1": "e824d9ac-d2f1-473a-8cca-33e65aa70514", "2": "bd52ef85-1714-4ac2-85ff-c9be597aa6aa"}}, "__type__": "1"}, "996f2509-b79c-4409-be16-d20b71d81752": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.drop\u3010Content\u3011pandas.DataFrame.drop#DataFrame.drop(labels=None,*,axis=0,index=None,columns=None,level=None,inplace=False,errors='raise')[source]#Drop specified labels from rows or columns.Remove rows or columns by specifying label names and corresponding\naxis, or by directly specifying index or column names. When using a\nmulti-index, labels on different levels can be removed by specifying\nthe level. See theuser guidefor more information about the now unused levels.Parameters:labelssingle label or list-likeIndex or column labels to drop. A tuple will be used as a single\nlabel and not treated as a list-like.axis{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0Whether to drop labels from the index (0 or \u2018index\u2019) or\ncolumns (1 or \u2018columns\u2019).indexsingle label or list-likeAlternative to specifying axis (labels,axis=0is equivalent toindex=labels).columnssingle label or list-likeAlternative to specifying axis (labels,axis=1is equivalent tocolumns=labels).levelint or level name, optionalFor MultiIndex, level from which the labels will be removed.inplacebool, default FalseIf False, return a copy. Otherwise, do operation\nin place and return None.errors{\u2018ignore\u2019, \u2018raise\u2019}, default \u2018raise\u2019If \u2018ignore\u2019, suppress error and only existing labels are\ndropped.Returns:DataFrame or NoneReturns DataFrame or None DataFrame with the specified\nindex or column labels removed or None if inplace=True.Raises:KeyErrorIf any of the labels is not found in the selected axis.See alsoDataFrame.locLabel-location based indexer for selection by label.DataFrame.dropnaReturn DataFrame with labels on given axis omitted where (all or any) data are missing.DataFrame.drop_duplicatesReturn DataFrame with duplicate rows removed, optionally only considering certain columns.Series.dropReturn Series with specified index labels removed.Examples>>>df=pd.DataFrame(np.arange(12).reshape(3,4),...columns=['A','B','C','D'])>>>dfA  B   C   D0  0  1   2   31  4  5   6   72  8  9  10  11Drop columns>>>df.drop(['B','C'],axis=1)A   D0  0   31  4   72  8  11>>>df.drop(columns=['B','C'])A   D0  0   31  4   72  8  11Drop a row by index>>>df.drop([0,1])A  B   C   D2  8  9  10  11Drop columns and/or rows of MultiIndex DataFrame>>>midx=pd.MultiIndex(levels=[['llama','cow','falcon'],...['speed','weight','length']],...codes=[[0,0,0,1,1,1,2,2,2],...[0,1,2,0,1,2,0,1,2]])>>>df=pd.DataFrame(index=midx,columns=['big','small'],...data=[[45,30],[200,100],[1.5,1],[30,20],...[250,150],[1.5,0.8],[320,250],...[1,0.8],[0.3,0.2]])>>>dfbig     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0weight  1.0     0.8length  0.3     0.2Drop a specific index combination from the MultiIndex\nDataFrame, i.e., drop the combination'falcon'and'weight', which deletes only the", "doc_id": "996f2509-b79c-4409-be16-d20b71d81752", "embedding": null, "doc_hash": "1b6452726c5e189327c4d1ec07e2f83ff1c64d349c0431312c58e7a882a2b1b9", "extra_info": null, "node_info": {"start": 0, "end": 2895, "_node_type": "1"}, "relationships": {"1": "fe8c7730-ddeb-41d5-a965-cbe095b5420f", "3": "38661e94-5c0f-4690-b0d1-ac445b6d80ef"}}, "__type__": "1"}, "38661e94-5c0f-4690-b0d1-ac445b6d80ef": {"__data__": {"text": "    smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0weight  1.0     0.8length  0.3     0.2Drop a specific index combination from the MultiIndex\nDataFrame, i.e., drop the combination'falcon'and'weight', which deletes only the corresponding row>>>df.drop(index=('falcon','weight'))big     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0length  0.3     0.2>>>df.drop(index='cow',columns='small')bigllama   speed   45.0weight  200.0length  1.5falcon  speed   320.0weight  1.0length  0.3>>>df.drop(index='length',level=1)big     smallllama   speed   45.0    30.0weight  200.0   100.0cow     speed   30.0    20.0weight  250.0   150.0falcon  speed   320.0   250.0weight  1.0     0.8", "doc_id": "38661e94-5c0f-4690-b0d1-ac445b6d80ef", "embedding": null, "doc_hash": "12bab8eac796d8f81fc074216153f156ed270eb7acf1cd9b79e51aa2256807e4", "extra_info": null, "node_info": {"start": 2549, "end": 3469, "_node_type": "1"}, "relationships": {"1": "fe8c7730-ddeb-41d5-a965-cbe095b5420f", "2": "996f2509-b79c-4409-be16-d20b71d81752"}}, "__type__": "1"}, "4f74907f-ef3d-460b-8e35-4b3d74fab429": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.isnull\u3010Content\u3011pandas.DataFrame.isnull#DataFrame.isnull()[source]#DataFrame.isnull is an alias for DataFrame.isna.Detect missing values.Return a boolean same-sized object indicating if the values are NA.\nNA values, such as None ornumpy.NaN, gets mapped to True\nvalues.\nEverything else gets mapped to False values. Characters such as empty\nstrings''ornumpy.infare not considered NA values\n(unless you setpandas.options.mode.use_inf_as_na=True).Returns:DataFrameMask of bool values for each element in DataFrame that\nindicates whether an element is an NA value.See alsoDataFrame.isnullAlias of isna.DataFrame.notnaBoolean inverse of isna.DataFrame.dropnaOmit axes labels with missing values.isnaTop-level isna.ExamplesShow which entries in a DataFrame are NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.isna()age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  FalseShow which entries in a Series are NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.isna()0    False1    False2     Truedtype: bool", "doc_id": "4f74907f-ef3d-460b-8e35-4b3d74fab429", "embedding": null, "doc_hash": "27ca291aa1f798d74eb64a3bf4a173ea0c1d6f79e0d63574bb8b8d8fd117d0c0", "extra_info": null, "node_info": {"start": 0, "end": 1429, "_node_type": "1"}, "relationships": {"1": "9c41d8c3-63d7-4ac6-97d0-83f5e18e1801"}}, "__type__": "1"}, "7c2686fc-bd93-48b5-ad37-3e148503332a": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.fillna\u3010Content\u3011pandas.DataFrame.fillna#DataFrame.fillna(value=None,*,method=None,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values using the specified method.Parameters:valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a\ndict/Series/DataFrame of values specifying which value to use for\neach index (for a Series) or column (for a DataFrame). Values not\nin the dict/Series/DataFrame will not be filled. This value cannot\nbe a list.method{\u2018backfill\u2019, \u2018bfill\u2019, \u2018ffill\u2019, None}, default NoneMethod to use for filling holes in reindexed Series:ffill: propagate last valid observation forward to next valid.backfill / bfill: use next valid observation to fill gap.Deprecated since version 2.1.0:Use ffill or bfill instead.axis{0 or \u2018index\u2019} for Series, {0 or \u2018index\u2019, 1 or \u2018columns\u2019} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any\nother views on this object (e.g., a no-copy slice for a column in a\nDataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive\nNaN values to forward/backward fill. In other words, if there is\na gap with more than this number of consecutive NaNs, it will only\nbe partially filled. If method is not specified, this is the\nmaximum number of entries along the entire axis where NaNs will be\nfilled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,\nor the string \u2018infer\u2019 which will try to downcast to an appropriate\nequal type (e.g. float64 to int64 if possible).Deprecated since version 2.2.0.Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.See alsoffillFill values by propagating the last valid observation to next valid.bfillFill values by using the next valid observation to fill the gap.interpolateFill NaN values using interpolation.reindexConform object to new index.asfreqConvert TimeSeries to specified frequency.Examples>>>df=pd.DataFrame([[np.nan,2,np.nan,0],...[3,4,np.nan,1],...[np.nan,np.nan,np.nan,np.nan],...[np.nan,3,np.nan,4]],...columns=list(\"ABCD\"))>>>dfA    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  NaN  NaN NaN  NaN3  NaN  3.0 NaN  4.0Replace all NaN elements with 0s.>>>df.fillna(0)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  0.03  0.0  3.0  0.0  4.0Replace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1,\n2, and 3 respectively.>>>values={\"A\":0,\"B\":1,\"C\":2,\"D\":3}>>>df.fillna(value=values)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  2.0  1.02  0.0  1.0  2.0  3.03  0.0  3.0  2.0  4.0Only replace the first", "doc_id": "7c2686fc-bd93-48b5-ad37-3e148503332a", "embedding": null, "doc_hash": "91e8e56484bba4798a3459e7ba05d531dd8488adf3104417a5904d6a4415b385", "extra_info": null, "node_info": {"start": 0, "end": 2836, "_node_type": "1"}, "relationships": {"1": "64a3fc7e-0e53-42d3-b9de-fbffed97c33c", "3": "04594028-0f9f-44a4-9983-b38f47011523"}}, "__type__": "1"}, "04594028-0f9f-44a4-9983-b38f47011523": {"__data__": {"text": " 0.0  1.02  0.0  0.0  0.0  0.03  0.0  3.0  0.0  4.0Replace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1,\n2, and 3 respectively.>>>values={\"A\":0,\"B\":1,\"C\":2,\"D\":3}>>>df.fillna(value=values)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  2.0  1.02  0.0  1.0  2.0  3.03  0.0  3.0  2.0  4.0Only replace the first NaN element.>>>df.fillna(value=values,limit=1)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  NaN  1.02  NaN  1.0  NaN  3.03  NaN  3.0  NaN  4.0When filling using a DataFrame, replacement happens along\nthe same column names and same indices>>>df2=pd.DataFrame(np.zeros((4,4)),columns=list(\"ABCE\"))>>>df.fillna(df2)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  NaN3  0.0  3.0  0.0  4.0Note that column D is not affected since it is not present in df2.", "doc_id": "04594028-0f9f-44a4-9983-b38f47011523", "embedding": null, "doc_hash": "d1ddac98da67b2d5e86505585f0208969bb0f28b612555ae7a963c94378f3e05", "extra_info": null, "node_info": {"start": 2510, "end": 3319, "_node_type": "1"}, "relationships": {"1": "64a3fc7e-0e53-42d3-b9de-fbffed97c33c", "2": "7c2686fc-bd93-48b5-ad37-3e148503332a"}}, "__type__": "1"}, "6f51bec6-9029-41cf-b01f-9e79b348fb2e": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.to_csv\u3010Content\u3011pandas.DataFrame.to_csv#DataFrame.to_csv(path_or_buf=None,*,sep=',',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,mode='w',encoding=None,compression='infer',quoting=None,quotechar='\"',lineterminator=None,chunksize=None,date_format=None,doublequote=True,escapechar=None,decimal='.',errors='strict',storage_options=None)[source]#Write object to a comma-separated values (csv) file.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like\nobject implementing a write() function. If None, the result is\nreturned as a string. If a non-binary file object is passed, it should\nbe opened withnewline=\u2019\u2019, disabling universal newlines. If a binary\nfile object is passed,modemight need to contain a\u2018b\u2019.sepstr, default \u2018,\u2019String of length 1. Field delimiter for the output file.na_repstr, default \u2018\u2019Missing data representation.float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes\nprecedence over other numeric formatting parameters, like decimal.columnssequence, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is\nassumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, andheaderandindexare True, then the index names are used. A\nsequence should be given if the object uses MultiIndex. If\nFalse do not print fields for index names. Use index_label=False\nfor easier importing in R.mode{\u2018w\u2019, \u2018x\u2019, \u2018a\u2019}, default \u2018w\u2019Forwarded to eitheropen(mode=)orfsspec.open(mode=)to control\nthe file opening. Typical values include:\u2018w\u2019, truncate the file first.\u2018x\u2019, exclusive creation, failing if the file already exists.\u2018a\u2019, append to the end of file if it exists.encodingstr, optionalA string representing the encoding to use in the output file,\ndefaults to \u2018utf-8\u2019.encodingis not supported ifpath_or_bufis a non-binary file object.compressionstr or dict, default \u2018infer\u2019For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 is\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, \u2018.tar\u2019, \u2018.tar.gz\u2019, \u2018.tar.xz\u2019 or \u2018.tar.bz2\u2019\n(otherwise no compression).\nSet toNonefor no compression.\nCan also be a dict with key'method'set\nto one of {'zip','gzip','bz2','zstd','xz','tar'} and\nother key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.\nAs an example, the following could be passed for faster compression and to create\na reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key \u2018method\u2019 as compression mode\nand other entries as additional compression options if\ncompression", "doc_id": "6f51bec6-9029-41cf-b01f-9e79b348fb2e", "embedding": null, "doc_hash": "decd8414bf67406b752e13e90e31706511a4a052a78deed50c6b0be4f6b9cea2", "extra_info": null, "node_info": {"start": 0, "end": 3083, "_node_type": "1"}, "relationships": {"1": "07b2633d-a631-43a6-8f4b-c7309ae146e7", "3": "7e192e23-51ab-4d5a-976e-e8bc0b263f88"}}, "__type__": "1"}, "7e192e23-51ab-4d5a-976e-e8bc0b263f88": {"__data__": {"text": "no compression.\nCan also be a dict with key'method'set\nto one of {'zip','gzip','bz2','zstd','xz','tar'} and\nother key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.\nAs an example, the following could be passed for faster compression and to create\na reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key \u2018method\u2019 as compression mode\nand other entries as additional compression options if\ncompression mode is \u2018zip\u2019.Passing compression options as keys in dict is\nsupported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set afloat_formatthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC\nwill treat them as non-numeric.quotecharstr, default \u2018\"\u2019String of length 1. Character used to quote fields.lineterminatorstr, optionalThe newline character or character sequence to use in the output\nfile. Defaults toos.linesep, which depends on the OS in which\nthis method is called (\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).Changed in version 1.5.0:Previously was line_terminator, changed for consistency with\nread_csv and the standard library \u2018csv\u2019 module.chunksizeint or NoneRows to write at a time.date_formatstr, default NoneFormat string for datetime objects.doublequotebool, default TrueControl quoting ofquotecharinside a field.escapecharstr, default NoneString of length 1. Character used to escapesepandquotecharwhen appropriate.decimalstr, default \u2018.\u2019Character recognized as decimal separator. E.g. use \u2018,\u2019 for\nEuropean data.errorsstr, default \u2018strict\u2019Specifies how encoding and decoding errors are to be handled.\nSee the errors argument foropen()for a full list\nof options.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\nare forwarded tourllib.request.Requestas header options. For other\nURLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are\nforwarded tofsspec.open. Please seefsspecandurllibfor more\ndetails, and for more examples on storage options referhere.Returns:None or strIf path_or_buf is None, returns the resulting csv format as a\nstring. Otherwise returns None.See alsoread_csvLoad a CSV file into a DataFrame.to_excelWrite DataFrame to an Excel file.ExamplesCreate \u2018out.csv\u2019 containing \u2018df\u2019 without indices>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv('out.csv',index=False)Create \u2018out.zip\u2019 containing \u2018out.csv\u2019>>>df.to_csv(index=False)'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first\nneed to create it using either Pathlib or", "doc_id": "7e192e23-51ab-4d5a-976e-e8bc0b263f88", "embedding": null, "doc_hash": "80603b45aa4f3790e4a1e4b2de7b0ae167a6dbe1e00cffe70adf97a0bf9e13ea", "extra_info": null, "node_info": {"start": 2545, "end": 5620, "_node_type": "1"}, "relationships": {"1": "07b2633d-a631-43a6-8f4b-c7309ae146e7", "2": "6f51bec6-9029-41cf-b01f-9e79b348fb2e", "3": "d43f2696-03c2-495a-aead-52ce79fd171f"}}, "__type__": "1"}, "d43f2696-03c2-495a-aead-52ce79fd171f": {"__data__": {"text": "containing \u2018df\u2019 without indices>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv('out.csv',index=False)Create \u2018out.zip\u2019 containing \u2018out.csv\u2019>>>df.to_csv(index=False)'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first\nneed to create it using either Pathlib or os:>>>frompathlibimportPath>>>filepath=Path('folder/subfolder/out.csv')>>>filepath.parent.mkdir(parents=True,exist_ok=True)>>>df.to_csv(filepath)>>>importos>>>os.makedirs('folder/subfolder',exist_ok=True)>>>df.to_csv('folder/subfolder/out.csv')", "doc_id": "d43f2696-03c2-495a-aead-52ce79fd171f", "embedding": null, "doc_hash": "eb024b93e6be89ace89d9a24eac4f5e90daadea5e99a6281f84f20b067839828", "extra_info": null, "node_info": {"start": 5617, "end": 6404, "_node_type": "1"}, "relationships": {"1": "07b2633d-a631-43a6-8f4b-c7309ae146e7", "2": "7e192e23-51ab-4d5a-976e-e8bc0b263f88"}}, "__type__": "1"}, "eaf359bd-841a-4c0b-8f8e-52c42d809558": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.to_excel\u3010Content\u3011pandas.DataFrame.to_excel#DataFrame.to_excel(excel_writer,*,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to\nspecify a target file name. To write to multiple sheets it is necessary to\ncreate anExcelWriterobject with a target file name, and specify a sheet\nin the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.\nWith all data written to the file it is necessary to save the changes.\nNote that creating anExcelWriterobject with a file name that already\nexists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default \u2018Sheet1\u2019Name of sheet which will contain DataFrame.na_repstr, default \u2018\u2019Missing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=\"%.2f\"will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is\nassumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A\nsequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this\nvia the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default \u2018inf\u2019Representation for infinity (there is no native representation for\ninfinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that\nis to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\nare forwarded tourllib.request.Requestas header options. For other\nURLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are\nforwarded tofsspec.open. Please seefsspecandurllibfor more\ndetails, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),\nto_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further\ndata without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row", "doc_id": "eaf359bd-841a-4c0b-8f8e-52c42d809558", "embedding": null, "doc_hash": "0173d1d9948e41ab3f63a45c17613747c2d57851f88b9d4176fbb058fd6d2d11", "extra_info": null, "node_info": {"start": 0, "end": 3480, "_node_type": "1"}, "relationships": {"1": "2077c564-8323-44a1-8f02-2ff71b0ee284", "3": "f529ddd1-1b6d-435a-a223-8648cdefb16f"}}, "__type__": "1"}, "f529ddd1-1b6d-435a-a223-8648cdefb16f": {"__data__": {"text": "keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),\nto_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further\ndata without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(\"output.xlsx\")To specify the sheet name:>>>df1.to_excel(\"output.xlsx\",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is\nnecessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,\nyou can pass theenginekeyword (the default engine is\nautomatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')", "doc_id": "f529ddd1-1b6d-435a-a223-8648cdefb16f", "embedding": null, "doc_hash": "c9c2928361db6cc3368e0fadab595890d325da01ea74e28bb8b399c5d77f505c", "extra_info": null, "node_info": {"start": 2813, "end": 4285, "_node_type": "1"}, "relationships": {"1": "2077c564-8323-44a1-8f02-2ff71b0ee284", "2": "eaf359bd-841a-4c0b-8f8e-52c42d809558"}}, "__type__": "1"}, "b80d3d3c-f5d8-42af-88b0-f24031c57a4a": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.to_json\u3010Content\u3011pandas.DataFrame.to_json#DataFrame.to_json(path_or_buf=None,*,orient=None,date_format=None,double_precision=10,force_ascii=True,date_unit='ms',default_handler=None,lines=False,compression='infer',index=None,indent=None,storage_options=None,mode='w')[source]#Convert the object to a JSON string.Note NaN\u2019s and None will be converted to null and datetime objects\nwill be converted to UNIX timestamps.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like\nobject implementing a write() function. If None, the result is\nreturned as a string.orientstrIndication of expected JSON string format.Series:default is \u2018index\u2019allowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018table\u2019}.DataFrame:default is \u2018columns\u2019allowed values are: {\u2018split\u2019, \u2018records\u2019, \u2018index\u2019, \u2018columns\u2019,\n\u2018values\u2019, \u2018table\u2019}.The format of the JSON string:\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns],\n\u2018data\u2019 -> [values]}\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]\u2018index\u2019 : dict like {index -> {column -> value}}\u2018columns\u2019 : dict like {column -> {index -> value}}\u2018values\u2019 : just the values array\u2018table\u2019 : dict like {\u2018schema\u2019: {schema}, \u2018data\u2019: {data}}Describing the data, where data component is likeorient='records'.date_format{None, \u2018epoch\u2019, \u2018iso\u2019}Type of date conversion. \u2018epoch\u2019 = epoch milliseconds,\n\u2018iso\u2019 = ISO8601. The default depends on theorient. Fororient='table', the default is \u2018iso\u2019. For all other orients,\nthe default is \u2018epoch\u2019.double_precisionint, default 10The number of decimal places to use when encoding\nfloating point values. The possible maximal value is 15.\nPassing double_precision greater than 15 will raise a ValueError.force_asciibool, default TrueForce encoded string to be ASCII.date_unitstr, default \u2018ms\u2019 (milliseconds)The time unit to encode to, governs timestamp and ISO8601\nprecision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019 for second, millisecond,\nmicrosecond, and nanosecond respectively.default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a\nsuitable format for JSON. Should receive a single argument which is\nthe object to convert and return a serialisable object.linesbool, default FalseIf \u2018orient\u2019 is \u2018records\u2019 write out line-delimited json format. Will\nthrow ValueError if incorrect \u2018orient\u2019 since others are not\nlist-like.compressionstr or dict, default \u2018infer\u2019For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 is\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, \u2018.tar\u2019, \u2018.tar.gz\u2019, \u2018.tar.xz\u2019 or \u2018.tar.bz2\u2019\n(otherwise no compression).\nSet toNonefor no compression.\nCan also be a dict with key'method'set\nto one of {'zip','gzip','bz2','zstd','xz','tar'} and\nother key-value pairs are", "doc_id": "b80d3d3c-f5d8-42af-88b0-f24031c57a4a", "embedding": null, "doc_hash": "007c7b5e2356865a6cd31bb02d3bd3569dd89eac9232de6061675cf141f56c28", "extra_info": null, "node_info": {"start": 0, "end": 2893, "_node_type": "1"}, "relationships": {"1": "84a07c53-8b16-4a5a-a1a4-615d115788ca", "3": "f295b5a0-1ac1-459e-afab-dc0b8d6b2938"}}, "__type__": "1"}, "f295b5a0-1ac1-459e-afab-dc0b8d6b2938": {"__data__": {"text": "or dict, default \u2018infer\u2019For on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018path_or_buf\u2019 is\npath-like, then detect compression from the following extensions: \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, \u2018.tar\u2019, \u2018.tar.gz\u2019, \u2018.tar.xz\u2019 or \u2018.tar.bz2\u2019\n(otherwise no compression).\nSet toNonefor no compression.\nCan also be a dict with key'method'set\nto one of {'zip','gzip','bz2','zstd','xz','tar'} and\nother key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.\nAs an example, the following could be passed for faster compression and to create\na reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.indexbool or None, default NoneThe index is only used when \u2018orient\u2019 is \u2018split\u2019, \u2018index\u2019, \u2018column\u2019,\nor \u2018table\u2019. Of these, \u2018index\u2019 and \u2018column\u2019 do not supportindex=False.indentint, optionalLength of whitespace used to indent each record.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\nare forwarded tourllib.request.Requestas header options. For other\nURLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are\nforwarded tofsspec.open. Please seefsspecandurllibfor more\ndetails, and for more examples on storage options referhere.modestr, default \u2018w\u2019 (writing)Specify the IO mode for output when supplying a path_or_buf.\nAccepted args are \u2018w\u2019 (writing) and \u2018a\u2019 (append) only.\nmode=\u2019a\u2019 is only supported when lines is True and orient is \u2018records\u2019.Returns:None or strIf path_or_buf is None, returns the resulting json format as a\nstring. Otherwise returns None.See alsoread_jsonConvert a JSON string to pandas object.NotesThe behavior ofindent=0varies from the stdlib, which does not\nindent the output but does insert newlines. Currently,indent=0and the defaultindent=Noneare equivalent in pandas, though this\nmay change in a future release.orient='table'contains a \u2018pandas_version\u2019 field under \u2018schema\u2019.\nThis stores the version ofpandasused in the latest revision of the\nschema.Examples>>>fromjsonimportloads,dumps>>>df=pd.DataFrame(...[[\"a\",\"b\"],[\"c\",\"d\"]],...index=[\"row 1\",\"row 2\"],...columns=[\"col 1\",\"col 2\"],...)>>>result=df.to_json(orient=\"split\")>>>parsed=loads(result)>>>dumps(parsed,indent=4){\"columns\": [\"col 1\",\"col 2\"],\"index\": [\"row 1\",\"row 2\"],\"data\": [[\"a\",\"b\"],[\"c\",\"d\"]]}Encoding/decoding a Dataframe using'records'formatted JSON.\nNote that index labels are not preserved with this encoding.>>>result=df.to_json(orient=\"records\")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[{\"col 1\": \"a\",\"col 2\": \"b\"},{\"col 1\": \"c\",\"col 2\": \"d\"}]Encoding/decoding a Dataframe using'index'formatted", "doc_id": "f295b5a0-1ac1-459e-afab-dc0b8d6b2938", "embedding": null, "doc_hash": "2f46ac5fe31be5a122117c595de739e9c30eb2ad30e1129347f5b904d22a20c5", "extra_info": null, "node_info": {"start": 2517, "end": 5367, "_node_type": "1"}, "relationships": {"1": "84a07c53-8b16-4a5a-a1a4-615d115788ca", "2": "b80d3d3c-f5d8-42af-88b0-f24031c57a4a", "3": "9df4f39e-151b-4d21-af40-08ad18644b7b"}}, "__type__": "1"}, "9df4f39e-151b-4d21-af40-08ad18644b7b": {"__data__": {"text": "1\",\"row 2\"],...columns=[\"col 1\",\"col 2\"],...)>>>result=df.to_json(orient=\"split\")>>>parsed=loads(result)>>>dumps(parsed,indent=4){\"columns\": [\"col 1\",\"col 2\"],\"index\": [\"row 1\",\"row 2\"],\"data\": [[\"a\",\"b\"],[\"c\",\"d\"]]}Encoding/decoding a Dataframe using'records'formatted JSON.\nNote that index labels are not preserved with this encoding.>>>result=df.to_json(orient=\"records\")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[{\"col 1\": \"a\",\"col 2\": \"b\"},{\"col 1\": \"c\",\"col 2\": \"d\"}]Encoding/decoding a Dataframe using'index'formatted JSON:>>>result=df.to_json(orient=\"index\")>>>parsed=loads(result)>>>dumps(parsed,indent=4){\"row 1\": {\"col 1\": \"a\",\"col 2\": \"b\"},\"row 2\": {\"col 1\": \"c\",\"col 2\": \"d\"}}Encoding/decoding a Dataframe using'columns'formatted JSON:>>>result=df.to_json(orient=\"columns\")>>>parsed=loads(result)>>>dumps(parsed,indent=4){\"col 1\": {\"row 1\": \"a\",\"row 2\": \"c\"},\"col 2\": {\"row 1\": \"b\",\"row 2\": \"d\"}}Encoding/decoding a Dataframe using'values'formatted JSON:>>>result=df.to_json(orient=\"values\")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[[\"a\",\"b\"],[\"c\",\"d\"]]Encoding with Table Schema:>>>result=df.to_json(orient=\"table\")>>>parsed=loads(result)>>>dumps(parsed,indent=4){\"schema\": {\"fields\": [{\"name\": \"index\",\"type\": \"string\"},{\"name\": \"col 1\",\"type\": \"string\"},{\"name\": \"col 2\",\"type\": \"string\"}],\"primaryKey\": [\"index\"],\"pandas_version\": \"1.4.0\"},\"data\": [{\"index\": \"row 1\",\"col 1\": \"a\",\"col 2\": \"b\"},{\"index\": \"row 2\",\"col 1\": \"c\",\"col 2\": \"d\"}]}", "doc_id": "9df4f39e-151b-4d21-af40-08ad18644b7b", "embedding": null, "doc_hash": "a3cbdb0e52bd08970d7ce47ab2afdf4305276c006cf114c78db1952a906401cc", "extra_info": null, "node_info": {"start": 5214, "end": 6684, "_node_type": "1"}, "relationships": {"1": "84a07c53-8b16-4a5a-a1a4-615d115788ca", "2": "f295b5a0-1ac1-459e-afab-dc0b8d6b2938"}}, "__type__": "1"}, "03815282-3be3-4701-8929-6ed693dee5e3": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.to_sql\u3010Content\u3011pandas.DataFrame.to_sql#DataFrame.to_sql(name,con,*,schema=None,if_exists='fail',index=True,index_label=None,chunksize=None,dtype=None,method=None)[source]#Write records stored in a DataFrame to a SQL database.Databases supported by SQLAlchemy[1]are supported. Tables can be\nnewly created, appended to, or overwritten.Parameters:namestrName of SQL table.consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that\nlibrary. Legacy support is provided for sqlite3.Connection objects. The user\nis responsible for engine disposal and connection closure for the SQLAlchemy\nconnectable. Seehere.\nIf passing a sqlalchemy.engine.Connection which is already in a transaction,\nthe transaction will not be committed. If passing a sqlite3.Connection,\nit will not be possible to roll back the record insertion.schemastr, optionalSpecify the schema (if database flavor supports this). If None, use\ndefault schema.if_exists{\u2018fail\u2019, \u2018replace\u2019, \u2018append\u2019}, default \u2018fail\u2019How to behave if the table already exists.fail: Raise a ValueError.replace: Drop the table before inserting new values.append: Insert new values to the existing table.indexbool, default TrueWrite DataFrame index as a column. Usesindex_labelas the column\nname in the table. Creates a table index for this column.index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) andindexis True, then the index names are used.\nA sequence should be given if the DataFrame uses MultiIndex.chunksizeint, optionalSpecify the number of rows in each batch to be written at a time.\nBy default, all rows will be written at once.dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the\nkeys should be the column names and the values should be the\nSQLAlchemy types or strings for the sqlite3 legacy mode. If a\nscalar is provided, it will be applied to all columns.method{None, \u2018multi\u2019, callable}, optionalControls the SQL insertion clause used:None : Uses standard SQLINSERTclause (one per row).\u2018multi\u2019: Pass multiple values in a singleINSERTclause.callable with signature(pd_table,conn,keys,data_iter).Details and a sample callable implementation can be found in the\nsectioninsert method.Returns:None or intNumber of rows affected by to_sql. None is returned if the callable\npassed intomethoddoes not return an integer number of rows.The number of returned rows affected is the sum of therowcountattribute ofsqlite3.Cursoror SQLAlchemy connectable which may not\nreflect the exact number of written rows as stipulated in thesqlite3orSQLAlchemy.New in version 1.4.0.Raises:ValueErrorWhen the table already exists andif_existsis \u2018fail\u2019 (the\ndefault).See alsoread_sqlRead a DataFrame from a table.NotesTimezone aware datetime columns will be written asTimestampwithtimezonetype with SQLAlchemy if supported by the\ndatabase. Otherwise, the datetimes will be stored as timezone unaware\ntimestamps local to the original timezone.References[1]https://docs.sqlalchemy.org[2]https://www.python.org/dev/peps/pep-0249/ExamplesCreate an in-memory SQLite database.>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine('sqlite://',echo=False)Create a table from scratch with 3 rows.>>>df=pd.DataFrame({'name':['User 1','User 2','User 3']})>>>dfname0  User 11  User 22  User 3>>>df.to_sql(name='users',con=engine)3>>>fromsqlalchemyimporttext>>>withengine.connect()asconn:...conn.execute(text(\"SELECT * FROM", "doc_id": "03815282-3be3-4701-8929-6ed693dee5e3", "embedding": null, "doc_hash": "7395b082992d5dccd7fee0a57632c8b9fe8d240300f866c07fc6d4ada0ae64d3", "extra_info": null, "node_info": {"start": 0, "end": 3554, "_node_type": "1"}, "relationships": {"1": "e50f3f16-6d22-4beb-a024-d7abf5728a48", "3": "dda3a56d-2dda-499d-9e6d-1d53f79dadcf"}}, "__type__": "1"}, "dda3a56d-2dda-499d-9e6d-1d53f79dadcf": {"__data__": {"text": "with SQLAlchemy if supported by the\ndatabase. Otherwise, the datetimes will be stored as timezone unaware\ntimestamps local to the original timezone.References[1]https://docs.sqlalchemy.org[2]https://www.python.org/dev/peps/pep-0249/ExamplesCreate an in-memory SQLite database.>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine('sqlite://',echo=False)Create a table from scratch with 3 rows.>>>df=pd.DataFrame({'name':['User 1','User 2','User 3']})>>>dfname0  User 11  User 22  User 3>>>df.to_sql(name='users',con=engine)3>>>fromsqlalchemyimporttext>>>withengine.connect()asconn:...conn.execute(text(\"SELECT * FROM users\")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]Ansqlalchemy.engine.Connectioncan also be passed tocon:>>>withengine.begin()asconnection:...df1=pd.DataFrame({'name':['User 4','User 5']})...df1.to_sql(name='users',con=connection,if_exists='append')2This is allowed to support operations that require that the same\nDBAPI connection is used for the entire operation.>>>df2=pd.DataFrame({'name':['User 6','User 7']})>>>df2.to_sql(name='users',con=engine,if_exists='append')2>>>withengine.connect()asconn:...conn.execute(text(\"SELECT * FROM users\")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),(0, 'User 4'), (1, 'User 5'), (0, 'User 6'),(1, 'User 7')]Overwrite the table with justdf2.>>>df2.to_sql(name='users',con=engine,if_exists='replace',...index_label='id')2>>>withengine.connect()asconn:...conn.execute(text(\"SELECT * FROM users\")).fetchall()[(0, 'User 6'), (1, 'User 7')]Usemethodto define a callable insertion method to do nothing\nif there\u2019s a primary key conflict on a table in a PostgreSQL database.>>>fromsqlalchemy.dialects.postgresqlimportinsert>>>definsert_on_conflict_nothing(table,conn,keys,data_iter):...# \"a\" is the primary key in \"conflict_table\"...data=[dict(zip(keys,row))forrowindata_iter]...stmt=insert(table.table).values(data).on_conflict_do_nothing(index_elements=[\"a\"])...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=\"conflict_table\",con=conn,if_exists=\"append\",method=insert_on_conflict_nothing)0For MySQL, a callable to update columnsbandcif there\u2019s a conflict\non a primary key.>>>fromsqlalchemy.dialects.mysqlimportinsert>>>definsert_on_conflict_update(table,conn,keys,data_iter):...# update columns \"b\" and \"c\" on primary key conflict...data=[dict(zip(keys,row))forrowindata_iter]...stmt=(...insert(table.table)....values(data)...)...stmt=stmt.on_duplicate_key_update(b=stmt.inserted.b,c=stmt.inserted.c)...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=\"conflict_table\",con=conn,if_exists=\"append\",method=insert_on_conflict_update)2Specify the dtype (especially useful for integers with missing values).\nNotice that while pandas is forced to store the data as floating point,\nthe database supports nullable integers. When fetching the data with\nPython, we get back integer scalars.>>>df=pd.DataFrame({\"A\":[1,None,2]})>>>dfA0  1.01  NaN2 ", "doc_id": "dda3a56d-2dda-499d-9e6d-1d53f79dadcf", "embedding": null, "doc_hash": "a4f6cbb4e26d868f51e1e55bc077b657a637f3200e4bb7fe42676ac5425f4e0e", "extra_info": null, "node_info": {"start": 2977, "end": 5952, "_node_type": "1"}, "relationships": {"1": "e50f3f16-6d22-4beb-a024-d7abf5728a48", "2": "03815282-3be3-4701-8929-6ed693dee5e3", "3": "ceff3af3-c557-4213-8ed3-f9fed6a08563"}}, "__type__": "1"}, "ceff3af3-c557-4213-8ed3-f9fed6a08563": {"__data__": {"text": "the dtype (especially useful for integers with missing values).\nNotice that while pandas is forced to store the data as floating point,\nthe database supports nullable integers. When fetching the data with\nPython, we get back integer scalars.>>>df=pd.DataFrame({\"A\":[1,None,2]})>>>dfA0  1.01  NaN2  2.0>>>fromsqlalchemy.typesimportInteger>>>df.to_sql(name='integers',con=engine,index=False,...dtype={\"A\":Integer()})3>>>withengine.connect()asconn:...conn.execute(text(\"SELECT * FROM integers\")).fetchall()[(1,), (None,), (2,)]", "doc_id": "ceff3af3-c557-4213-8ed3-f9fed6a08563", "embedding": null, "doc_hash": "f726875a88f3552c8e7100680036fad2b0a6e6734998cf578ed85221f7b43e49", "extra_info": null, "node_info": {"start": 6233, "end": 6757, "_node_type": "1"}, "relationships": {"1": "e50f3f16-6d22-4beb-a024-d7abf5728a48", "2": "dda3a56d-2dda-499d-9e6d-1d53f79dadcf"}}, "__type__": "1"}, "4e0b94bc-4c79-440e-93cf-4ad7f942f146": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.to_dict\u3010Content\u3011pandas.DataFrame.to_dict#DataFrame.to_dict(orient='dict',*,into=<class'dict'>,index=True)[source]#Convert the DataFrame to a dictionary.The type of the key-value pairs can be customized with the parameters\n(see below).Parameters:orientstr {\u2018dict\u2019, \u2018list\u2019, \u2018series\u2019, \u2018split\u2019, \u2018tight\u2019, \u2018records\u2019, \u2018index\u2019}Determines the type of the values of the dictionary.\u2018dict\u2019 (default) : dict like {column -> {index -> value}}\u2018list\u2019 : dict like {column -> [values]}\u2018series\u2019 : dict like {column -> Series(values)}\u2018split\u2019 : dict like\n{\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 -> [values]}\u2018tight\u2019 : dict like\n{\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 -> [values],\n\u2018index_names\u2019 -> [index.names], \u2018column_names\u2019 -> [column.names]}\u2018records\u2019 : list like\n[{column -> value}, \u2026 , {column -> value}]\u2018index\u2019 : dict like {index -> {column -> value}}New in version 1.4.0:\u2018tight\u2019 as an allowed value for theorientargumentintoclass, default dictThe collections.abc.MutableMapping subclass used for all Mappings\nin the return value. Can be the actual class or an empty\ninstance of the mapping type you want. If you want a\ncollections.defaultdict, you must pass it initialized.indexbool, default TrueWhether to include the index item (and index_names item iforientis \u2018tight\u2019) in the returned dictionary. Can only beFalsewhenorientis \u2018split\u2019 or \u2018tight\u2019.New in version 2.0.0.Returns:dict, list or collections.abc.MutableMappingReturn a collections.abc.MutableMapping object representing the\nDataFrame. The resulting transformation depends on theorientparameter.See alsoDataFrame.from_dictCreate a DataFrame from a dictionary.DataFrame.to_jsonConvert a DataFrame to JSON format.Examples>>>df=pd.DataFrame({'col1':[1,2],...'col2':[0.5,0.75]},...index=['row1','row2'])>>>dfcol1  col2row1     1  0.50row2     2  0.75>>>df.to_dict(){'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}You can specify the return orientation.>>>df.to_dict('series'){'col1': row1    1row2    2Name: col1, dtype: int64,'col2': row1    0.50row2    0.75Name: col2, dtype: float64}>>>df.to_dict('split'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]]}>>>df.to_dict('records')[{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]>>>df.to_dict('index'){'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}>>>df.to_dict('tight'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}You can also specify the mapping", "doc_id": "4e0b94bc-4c79-440e-93cf-4ad7f942f146", "embedding": null, "doc_hash": "eb942343aced4ed0d2ca549f2b1aed1212e87bb1e914d9bab968287497b5b532", "extra_info": null, "node_info": {"start": 0, "end": 2592, "_node_type": "1"}, "relationships": {"1": "dc5b4859-b6a6-4025-ac74-e3f749977be4", "3": "18881ff8-e516-4bb1-a578-d4dca5b33539"}}, "__type__": "1"}, "18881ff8-e516-4bb1-a578-d4dca5b33539": {"__data__": {"text": "'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]]}>>>df.to_dict('records')[{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]>>>df.to_dict('index'){'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}>>>df.to_dict('tight'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}You can also specify the mapping type.>>>fromcollectionsimportOrderedDict,defaultdict>>>df.to_dict(into=OrderedDict)OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])If you want adefaultdict, you need to initialize it:>>>dd=defaultdict(list)>>>df.to_dict('records',into=dd)[defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]", "doc_id": "18881ff8-e516-4bb1-a578-d4dca5b33539", "embedding": null, "doc_hash": "097254a92673cc9a44a77617c6faa6e284ac53f6ab79726ba569cde67dd9f997", "extra_info": null, "node_info": {"start": 2179, "end": 3012, "_node_type": "1"}, "relationships": {"1": "dc5b4859-b6a6-4025-ac74-e3f749977be4", "2": "4e0b94bc-4c79-440e-93cf-4ad7f942f146"}}, "__type__": "1"}, "26e841fd-1de5-4dc6-8d32-254d0bb704f0": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011DataFrame\u3010Section\u3011pandas.DataFrame.to_string\u3010Content\u3011pandas.DataFrame.to_string#DataFrame.to_string(buf=None,*,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',line_width=None,min_rows=None,max_colwidth=None,encoding=None)[source]#Render a DataFrame to a console-friendly tabular output.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spaceint, list or dict of int, optionalThe minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..headerbool or list of str, optionalWrite out the column names. If a list of columns is given, it is assumed to be aliases for the column names.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default \u2018NaN\u2019String representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columns\u2019 elements by position or\nname.\nThe result of each function must be a unicode string.\nList/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columns\u2019 elements if they are\nfloats. This function must return a unicode string and will be\napplied only to the non-NaNelements, withNaNbeing\nhandled byna_rep.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print\nevery multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from\nthe print configuration (controlled by set_option), \u2018right\u2019 out\nof the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default \u2018.\u2019Character recognized as decimal separator, e.g. \u2018,\u2019 in Europe.line_widthint, optionalWidth to wrap a line in characters.min_rowsint, optionalThe number of rows to display in the console in a truncated repr\n(when number of rows is abovemax_rows).max_colwidthint, optionalMax width to truncate each column in characters. By default, no limit.encodingstr, default \u201cutf-8\u201dSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns\nNone.See alsoto_htmlConvert DataFrame to HTML.Examples>>>d={'col1':[1,2,3],'col2':[4,5,6]}>>>df=pd.DataFrame(d)>>>print(df.to_string())col1  col20     1     41     2     52     3     6", "doc_id": "26e841fd-1de5-4dc6-8d32-254d0bb704f0", "embedding": null, "doc_hash": "6f625c2e304358fd3a9be14c3b06dbb2439d6533605003341418af88d8cfb316", "extra_info": null, "node_info": {"start": 0, "end": 3095, "_node_type": "1"}, "relationships": {"1": "c1ea5030-933f-4c1f-ab6e-56f9bd5b23d5"}}, "__type__": "1"}, "eef3fcf5-193c-4437-a9dd-ab19324c43be": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.andrews_curves\u3010Content\u3011pandas.plotting.andrews_curves#pandas.plotting.andrews_curves(frame,class_column,ax=None,samples=200,color=None,colormap=None,**kwargs)[source]#Generate a matplotlib plot for visualizing clusters of multivariate data.Andrews curves have the functional form:\\[f(t) = \\frac{x_1}{\\sqrt{2}} + x_2 \\sin(t) + x_3 \\cos(t) +\nx_4 \\sin(2t) + x_5 \\cos(2t) + \\cdots\\]Where\\(x\\)coefficients correspond to the values of each dimension\nand\\(t\\)is linearly spaced between\\(-\\pi\\)and\\(+\\pi\\).\nEach row of frame then corresponds to a single curve.Parameters:frameDataFrameData to be plotted, preferably normalized to (0.0, 1.0).class_columnlabelName of the column containing class names.axaxes object, default NoneAxes to use.samplesintNumber of points to plot in each curve.colorstr, list[str] or tuple[str], optionalColors to use for the different classes. Colors can be strings\nor 3-element floating point RGB values.colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If a string, load colormap with that\nname from matplotlib.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.AxesExamples>>>df=pd.read_csv(...'https://raw.githubusercontent.com/pandas-dev/'...'pandas/main/pandas/tests/io/data/csv/iris.csv'...)>>>pd.plotting.andrews_curves(df,'Name')", "doc_id": "eef3fcf5-193c-4437-a9dd-ab19324c43be", "embedding": null, "doc_hash": "645ab0b58acde3fc4dc259ea3d4ae27f7ccb5f718891d96952d875caad0d7535", "extra_info": null, "node_info": {"start": 0, "end": 1376, "_node_type": "1"}, "relationships": {"1": "4873f5b2-ea28-4505-9ff0-6f3c215165df"}}, "__type__": "1"}, "a4533459-f113-4119-b938-cbf0ef059885": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.autocorrelation_plot\u3010Content\u3011pandas.plotting.autocorrelation_plot#pandas.plotting.autocorrelation_plot(series,ax=None,**kwargs)[source]#Autocorrelation plot for time series.Parameters:seriesSeriesThe time series to visualize.axMatplotlib axis object, optionalThe matplotlib axis object to use.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.AxesExamplesThe horizontal lines in the plot correspond to 95% and 99% confidence bands.The dashed line is 99% confidence band.>>>spacing=np.linspace(-9*np.pi,9*np.pi,num=1000)>>>s=pd.Series(0.7*np.random.rand(1000)+0.3*np.sin(spacing))>>>pd.plotting.autocorrelation_plot(s)", "doc_id": "a4533459-f113-4119-b938-cbf0ef059885", "embedding": null, "doc_hash": "47d273f6df61c2680e5cf04c08665a6d2659d3571ecc10830f63868d83ada981", "extra_info": null, "node_info": {"start": 0, "end": 699, "_node_type": "1"}, "relationships": {"1": "d2355fbc-894d-4042-8d71-ff8b2e4e8cfe"}}, "__type__": "1"}, "49e1faa0-e4b9-436c-9942-f3d3e080119d": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.bootstrap_plot\u3010Content\u3011pandas.plotting.bootstrap_plot#pandas.plotting.bootstrap_plot(series,fig=None,size=50,samples=500,**kwds)[source]#Bootstrap plot on mean, median and mid-range statistics.The bootstrap plot is used to estimate the uncertainty of a statistic\nby relying on random sampling with replacement[1]. This function will\ngenerate bootstrapping plots for mean, median and mid-range statistics\nfor the given number of samples of the given size.[1]\u201cBootstrapping (statistics)\u201d inhttps://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29Parameters:seriespandas.SeriesSeries from where to get the samplings for the bootstrapping.figmatplotlib.figure.Figure, default NoneIf given, it will use thefigreference for plotting instead of\ncreating a new one with default parameters.sizeint, default 50Number of data points to consider during each sampling. It must be\nless than or equal to the length of theseries.samplesint, default 500Number of times the bootstrap procedure is performed.**kwdsOptions to pass to matplotlib plotting method.Returns:matplotlib.figure.FigureMatplotlib figure.See alsopandas.DataFrame.plotBasic plotting for DataFrame objects.pandas.Series.plotBasic plotting for Series objects.ExamplesThis example draws a basic bootstrap plot for a Series.>>>s=pd.Series(np.random.uniform(size=100))>>>pd.plotting.bootstrap_plot(s)<Figure size 640x480 with 6 Axes>", "doc_id": "49e1faa0-e4b9-436c-9942-f3d3e080119d", "embedding": null, "doc_hash": "8f5db03fc9fda68e74e8012646628a13b66bab2adbfcd9fc2abbc9be84bf3113", "extra_info": null, "node_info": {"start": 0, "end": 1438, "_node_type": "1"}, "relationships": {"1": "33c7adcc-aaf3-4767-b401-847312309d55"}}, "__type__": "1"}, "ca3fee36-ee0a-4025-b701-04b830a92960": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.boxplot\u3010Content\u3011pandas.plotting.boxplot#pandas.plotting.boxplot(data,column=None,by=None,ax=None,fontsize=None,rot=0,grid=True,figsize=None,layout=None,return_type=None,**kwargs)[source]#Make a box plot from DataFrame columns.Make a box-and-whisker plot from DataFrame columns, optionally grouped\nby some other columns. A box plot is a method for graphically depicting\ngroups of numerical data through their quartiles.\nThe box extends from the Q1 to Q3 quartile values of the data,\nwith a line at the median (Q2). The whiskers extend from the edges\nof box to show the range of the data. By default, they extend no more than1.5 * IQR (IQR = Q3 - Q1)from the edges of the box, ending at the farthest\ndata point within that interval. Outliers are plotted as separate dots.For further details see\nWikipedia\u2019s entry forboxplot.Parameters:dataDataFrameThe data to visualize.columnstr or list of str, optionalColumn name or list of names, or vector.\nCan be any valid input topandas.DataFrame.groupby().bystr or array-like, optionalColumn in the DataFrame topandas.DataFrame.groupby().\nOne box-plot will be done per value of columns inby.axobject of class matplotlib.axes.Axes, optionalThe matplotlib axes to be used by boxplot.fontsizefloat or strTick label font size in points or as a string (e.g.,large).rotfloat, default 0The rotation angle of labels (in degrees)\nwith respect to the screen coordinate system.gridbool, default TrueSetting this to True will show the grid.figsizeA tuple (width, height) in inchesThe size of the figure to create in matplotlib.layouttuple (rows, columns), optionalFor example, (3, 5) will display the subplots\nusing 3 rows and 5 columns, starting from the top-left.return_type{\u2018axes\u2019, \u2018dict\u2019, \u2018both\u2019} or None, default \u2018axes\u2019The kind of object to return. The default isaxes.\u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on.\u2018dict\u2019 returns a dictionary whose values are the matplotlib\nLines of the boxplot.\u2018both\u2019 returns a namedtuple with the axes and dict.when grouping withby, a Series mapping columns toreturn_typeis returned.Ifreturn_typeisNone, a NumPy array\nof axes with the same shape aslayoutis returned.**kwargsAll other plotting keyword arguments to be passed tomatplotlib.pyplot.boxplot().Returns:resultSee Notes.See alsopandas.Series.plot.histMake a histogram.matplotlib.pyplot.boxplotMatplotlib equivalent plot.NotesThe return type depends on thereturn_typeparameter:\u2018axes\u2019 : object of class matplotlib.axes.Axes\u2018dict\u2019 : dict of matplotlib.lines.Line2D objects\u2018both\u2019 : a namedtuple with structure (ax, lines)For data grouped withby, return a Series of the above or a numpy\narray:Seriesarray(forreturn_type=None)Usereturn_type='dict'when you want to tweak the appearance\nof the lines after plotting. In this case a dict containing the Lines\nmaking up the boxes, caps, fliers, medians, and whiskers is returned.ExamplesBoxplots can be created for every column in the dataframe\nbydf.boxplot()or indicating the columns to be used:>>>np.random.seed(1234)>>>df=pd.DataFrame(np.random.randn(10,4),...columns=['Col1','Col2','Col3','Col4'])>>>boxplot=df.boxplot(column=['Col1','Col2','Col3'])Boxplots of variables distributions grouped by the values of a third\nvariable can be created using the", "doc_id": "ca3fee36-ee0a-4025-b701-04b830a92960", "embedding": null, "doc_hash": "ea26f6cac972fcb591f27a9871ae9b193f142c4c8ac19e9018b11683d219875a", "extra_info": null, "node_info": {"start": 0, "end": 3286, "_node_type": "1"}, "relationships": {"1": "90a5f4f1-c741-4993-956a-6919bc569203", "3": "37df59b9-6ca4-465e-9f99-d1d052c9889b"}}, "__type__": "1"}, "37df59b9-6ca4-465e-9f99-d1d052c9889b": {"__data__": {"text": "Series of the above or a numpy\narray:Seriesarray(forreturn_type=None)Usereturn_type='dict'when you want to tweak the appearance\nof the lines after plotting. In this case a dict containing the Lines\nmaking up the boxes, caps, fliers, medians, and whiskers is returned.ExamplesBoxplots can be created for every column in the dataframe\nbydf.boxplot()or indicating the columns to be used:>>>np.random.seed(1234)>>>df=pd.DataFrame(np.random.randn(10,4),...columns=['Col1','Col2','Col3','Col4'])>>>boxplot=df.boxplot(column=['Col1','Col2','Col3'])Boxplots of variables distributions grouped by the values of a third\nvariable can be created using the optionby. For instance:>>>df=pd.DataFrame(np.random.randn(10,2),...columns=['Col1','Col2'])>>>df['X']=pd.Series(['A','A','A','A','A',...'B','B','B','B','B'])>>>boxplot=df.boxplot(by='X')A list of strings (i.e.['X','Y']) can be passed to boxplot\nin order to group the data by combination of the variables in the x-axis:>>>df=pd.DataFrame(np.random.randn(10,3),...columns=['Col1','Col2','Col3'])>>>df['X']=pd.Series(['A','A','A','A','A',...'B','B','B','B','B'])>>>df['Y']=pd.Series(['A','B','A','B','A',...'B','A','B','A','B'])>>>boxplot=df.boxplot(column=['Col1','Col2'],by=['X','Y'])The layout of boxplot can be adjusted giving a tuple tolayout:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...layout=(2,1))Additional formatting can be done to the boxplot, like suppressing the grid\n(grid=False), rotating the labels in the x-axis (i.e.rot=45)\nor changing the fontsize (i.e.fontsize=15):>>>boxplot=df.boxplot(grid=False,rot=45,fontsize=15)The parameterreturn_typecan be used to select the type of element\nreturned byboxplot. Whenreturn_type='axes'is selected,\nthe matplotlib axes on which the boxplot is drawn are returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],return_type='axes')>>>type(boxplot)<class 'matplotlib.axes._axes.Axes'>When grouping withby, a Series mapping columns toreturn_typeis returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...return_type='axes')>>>type(boxplot)<class 'pandas.core.series.Series'>Ifreturn_typeisNone, a NumPy array of axes with the same shape\naslayoutis returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...return_type=None)>>>type(boxplot)<class 'numpy.ndarray'>", "doc_id": "37df59b9-6ca4-465e-9f99-d1d052c9889b", "embedding": null, "doc_hash": "cbcaa4872b68921ae13ff773e8a94ba9c43f1e802337a049c450c20485b1e9e0", "extra_info": null, "node_info": {"start": 2643, "end": 4923, "_node_type": "1"}, "relationships": {"1": "90a5f4f1-c741-4993-956a-6919bc569203", "2": "ca3fee36-ee0a-4025-b701-04b830a92960"}}, "__type__": "1"}, "58934a8f-7ef3-4b88-a91e-58ee98fb17e6": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.deregister_matplotlib_converters\u3010Content\u3011pandas.plotting.deregister_matplotlib_converters#pandas.plotting.deregister_matplotlib_converters()[source]#Remove pandas formatters and converters.Removes the custom converters added byregister(). This\nattempts to set the state of the registry back to the state before\npandas registered its own units. Converters for pandas\u2019 own types like\nTimestamp and Period are removed completely. Converters for types\npandas overwrites, likedatetime.datetime, are restored to their\noriginal value.See alsoregister_matplotlib_convertersRegister pandas formatters and converters with matplotlib.ExamplesThe following line is done automatically by pandas so\nthe plot can be rendered:>>>pd.plotting.register_matplotlib_converters()>>>df=pd.DataFrame({'ts':pd.period_range('2020',periods=2,freq='M'),...'y':[1,2]...})>>>plot=df.plot.line(x='ts',y='y')Unsetting the register manually an error will be raised:>>>pd.set_option(\"plotting.matplotlib.register_converters\",...False)>>>df.plot.line(x='ts',y='y')Traceback (most recent call last):TypeError:float() argument must be a string or a real number, not 'Period'", "doc_id": "58934a8f-7ef3-4b88-a91e-58ee98fb17e6", "embedding": null, "doc_hash": "ff7f7b135251954e69e617c2bf53a2fd99e9e6628de4f03bfb1597fa3c9bf766", "extra_info": null, "node_info": {"start": 0, "end": 1191, "_node_type": "1"}, "relationships": {"1": "1b99c246-601e-459b-b6c2-6204df0857c1"}}, "__type__": "1"}, "407d41c9-e450-4b3d-b6e1-120008343de2": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.lag_plot\u3010Content\u3011pandas.plotting.lag_plot#pandas.plotting.lag_plot(series,lag=1,ax=None,**kwds)[source]#Lag plot for time series.Parameters:seriesSeriesThe time series to visualize.lagint, default 1Lag length of the scatter plot.axMatplotlib axis object, optionalThe matplotlib axis object to use.**kwdsMatplotlib scatter method keyword arguments.Returns:matplotlib.axes.AxesExamplesLag plots are most commonly used to look for patterns in time series data.Given the following time series>>>np.random.seed(5)>>>x=np.cumsum(np.random.normal(loc=1,scale=5,size=50))>>>s=pd.Series(x)>>>s.plot()A lag plot withlag=1returns>>>pd.plotting.lag_plot(s,lag=1)<Axes: xlabel='y(t)', ylabel='y(t + 1)'>", "doc_id": "407d41c9-e450-4b3d-b6e1-120008343de2", "embedding": null, "doc_hash": "43a384b92abe1ea4d4b64c6431498a17c20e2f523152043de21a67026d4723c8", "extra_info": null, "node_info": {"start": 0, "end": 744, "_node_type": "1"}, "relationships": {"1": "a7824b7e-19a5-4bcc-b257-da50aee60760"}}, "__type__": "1"}, "6991f8fe-830e-4092-a448-d3725a29b8d1": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.parallel_coordinates\u3010Content\u3011pandas.plotting.parallel_coordinates#pandas.plotting.parallel_coordinates(frame,class_column,cols=None,ax=None,color=None,use_columns=False,xticks=None,colormap=None,axvlines=True,axvlines_kwds=None,sort_labels=False,**kwargs)[source]#Parallel coordinates plotting.Parameters:frameDataFrameclass_columnstrColumn name containing class names.colslist, optionalA list of column names to use.axmatplotlib.axis, optionalMatplotlib axis object.colorlist or tuple, optionalColors to use for the different classes.use_columnsbool, optionalIf true, columns will be used as xticks.xtickslist or tuple, optionalA list of values to use for xticks.colormapstr or matplotlib colormap, default NoneColormap to use for line colors.axvlinesbool, optionalIf true, vertical lines will be added at each xtick.axvlines_kwdskeywords, optionalOptions to be passed to axvline method for vertical lines.sort_labelsbool, default FalseSort class_column labels, useful when assigning colors.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.AxesExamples>>>df=pd.read_csv(...'https://raw.githubusercontent.com/pandas-dev/'...'pandas/main/pandas/tests/io/data/csv/iris.csv'...)>>>pd.plotting.parallel_coordinates(...df,'Name',color=('#556270','#4ECDC4','#C7F464')...)", "doc_id": "6991f8fe-830e-4092-a448-d3725a29b8d1", "embedding": null, "doc_hash": "99301705fc3ef3d346037e4d84e775d32ec89bc9f03ca8809622520dbcd35010", "extra_info": null, "node_info": {"start": 0, "end": 1348, "_node_type": "1"}, "relationships": {"1": "98ab5d7a-951d-4d4d-a6b4-58f05ba80232"}}, "__type__": "1"}, "0ed3b7c6-0caf-4c8b-9e02-6ddfd94a68be": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.plot_params\u3010Content\u3011pandas.plotting.plot_params#pandas.plotting.plot_params={'xaxis.compat':False}#Stores pandas plotting options.Allows for parameter aliasing so you can just use parameter names that are\nthe same as the plot function parameters, but is stored in a canonical\nformat that makes it easy to breakdown into groups later.Examples>>>np.random.seed(42)>>>df=pd.DataFrame({'A':np.random.randn(10),...'B':np.random.randn(10)},...index=pd.date_range(\"1/1/2000\",...freq='4MS',periods=10))>>>withpd.plotting.plot_params.use(\"x_compat\",True):..._=df[\"A\"].plot(color=\"r\")..._=df[\"B\"].plot(color=\"g\")", "doc_id": "0ed3b7c6-0caf-4c8b-9e02-6ddfd94a68be", "embedding": null, "doc_hash": "34559fb0cb8de9a56b41e691a1cba87015d651eddf10882192e5f8698cc2b422", "extra_info": null, "node_info": {"start": 0, "end": 656, "_node_type": "1"}, "relationships": {"1": "60fddf11-79d3-4f39-8ed7-b296a017e5f3"}}, "__type__": "1"}, "769d4c4c-0175-470e-a8e8-693753a79e6d": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.radviz\u3010Content\u3011pandas.plotting.radviz#pandas.plotting.radviz(frame,class_column,ax=None,color=None,colormap=None,**kwds)[source]#Plot a multidimensional dataset in 2D.Each Series in the DataFrame is represented as a evenly distributed\nslice on a circle. Each data point is rendered in the circle according to\nthe value on each Series. Highly correlatedSeriesin theDataFrameare placed closer on the unit circle.RadViz allow to project a N-dimensional data set into a 2D space where the\ninfluence of each dimension can be interpreted as a balance between the\ninfluence of all dimensions.More info available at theoriginal articledescribing RadViz.Parameters:frameDataFrameObject holding the data.class_columnstrColumn name containing the name of the data point category.axmatplotlib.axes.Axes, optionalA plot instance to which to add the information.colorlist[str] or tuple[str], optionalAssign a color to each category. Example: [\u2018blue\u2019, \u2018green\u2019].colormapstr ormatplotlib.colors.Colormap, default NoneColormap to select colors from. If string, load colormap with that\nname from matplotlib.**kwdsOptions to pass to matplotlib scatter plotting method.Returns:matplotlib.axes.AxesSee alsopandas.plotting.andrews_curvesPlot clustering visualization.Examples>>>df=pd.DataFrame(...{...'SepalLength':[6.5,7.7,5.1,5.8,7.6,5.0,5.4,4.6,6.7,4.6],...'SepalWidth':[3.0,3.8,3.8,2.7,3.0,2.3,3.0,3.2,3.3,3.6],...'PetalLength':[5.5,6.7,1.9,5.1,6.6,3.3,4.5,1.4,5.7,1.0],...'PetalWidth':[1.8,2.2,0.4,1.9,2.1,1.0,1.5,0.2,2.1,0.2],...'Category':[...'virginica',...'virginica',...'setosa',...'virginica',...'virginica',...'versicolor',...'versicolor',...'setosa',...'virginica',...'setosa'...]...}...)>>>pd.plotting.radviz(df,'Category')", "doc_id": "769d4c4c-0175-470e-a8e8-693753a79e6d", "embedding": null, "doc_hash": "19b882023ab7bf397231c74351f5530dfaad2383bf0b9aff770d86f9260c485f", "extra_info": null, "node_info": {"start": 0, "end": 1768, "_node_type": "1"}, "relationships": {"1": "8d20605e-75a1-4de2-9bc2-a8e1d9f68832"}}, "__type__": "1"}, "2572b393-5836-45af-8598-53158a7f0140": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.register_matplotlib_converters\u3010Content\u3011pandas.plotting.register_matplotlib_converters#pandas.plotting.register_matplotlib_converters()[source]#Register pandas formatters and converters with matplotlib.This function modifies the globalmatplotlib.units.registrydictionary. pandas adds custom converters forpd.Timestamppd.Periodnp.datetime64datetime.datetimedatetime.datedatetime.timeSee alsoderegister_matplotlib_convertersRemove pandas formatters and converters.ExamplesThe following line is done automatically by pandas so\nthe plot can be rendered:>>>pd.plotting.register_matplotlib_converters()>>>df=pd.DataFrame({'ts':pd.period_range('2020',periods=2,freq='M'),...'y':[1,2]...})>>>plot=df.plot.line(x='ts',y='y')Unsetting the register manually an error will be raised:>>>pd.set_option(\"plotting.matplotlib.register_converters\",...False)>>>df.plot.line(x='ts',y='y')Traceback (most recent call last):TypeError:float() argument must be a string or a real number, not 'Period'", "doc_id": "2572b393-5836-45af-8598-53158a7f0140", "embedding": null, "doc_hash": "24badfee07d1df63cfe8ed0e6769d7e8b36e805489c6a579571eb79a554184da", "extra_info": null, "node_info": {"start": 0, "end": 1029, "_node_type": "1"}, "relationships": {"1": "d4e3998f-4a09-4348-95ab-522afe7a8b7f"}}, "__type__": "1"}, "a768d8b1-7fa0-42d6-8066-41d6b75c8c3b": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.scatter_matrix\u3010Content\u3011pandas.plotting.scatter_matrix#pandas.plotting.scatter_matrix(frame,alpha=0.5,figsize=None,ax=None,grid=False,diagonal='hist',marker='.',density_kwds=None,hist_kwds=None,range_padding=0.05,**kwargs)[source]#Draw a matrix of scatter plots.Parameters:frameDataFramealphafloat, optionalAmount of transparency applied.figsize(float,float), optionalA tuple (width, height) in inches.axMatplotlib axis object, optionalgridbool, optionalSetting this to True will show the grid.diagonal{\u2018hist\u2019, \u2018kde\u2019}Pick between \u2018kde\u2019 and \u2018hist\u2019 for either Kernel Density Estimation or\nHistogram plot in the diagonal.markerstr, optionalMatplotlib marker type, default \u2018.\u2019.density_kwdskeywordsKeyword arguments to be passed to kernel density estimate plot.hist_kwdskeywordsKeyword arguments to be passed to hist function.range_paddingfloat, default 0.05Relative extension of axis range in x and y with respect to\n(x_max - x_min) or (y_max - y_min).**kwargsKeyword arguments to be passed to scatter function.Returns:numpy.ndarrayA matrix of scatter plots.Examples>>>df=pd.DataFrame(np.random.randn(1000,4),columns=['A','B','C','D'])>>>pd.plotting.scatter_matrix(df,alpha=0.2)array([[<Axes: xlabel='A', ylabel='A'>, <Axes: xlabel='B', ylabel='A'>,<Axes: xlabel='C', ylabel='A'>, <Axes: xlabel='D', ylabel='A'>],[<Axes: xlabel='A', ylabel='B'>, <Axes: xlabel='B', ylabel='B'>,<Axes: xlabel='C', ylabel='B'>, <Axes: xlabel='D', ylabel='B'>],[<Axes: xlabel='A', ylabel='C'>, <Axes: xlabel='B', ylabel='C'>,<Axes: xlabel='C', ylabel='C'>, <Axes: xlabel='D', ylabel='C'>],[<Axes: xlabel='A', ylabel='D'>, <Axes: xlabel='B', ylabel='D'>,<Axes: xlabel='C', ylabel='D'>, <Axes: xlabel='D', ylabel='D'>]],dtype=object)", "doc_id": "a768d8b1-7fa0-42d6-8066-41d6b75c8c3b", "embedding": null, "doc_hash": "13ee7bd5eefd1abc01854ee8898f51639f8cdec41f149afb9663731ee6a21206", "extra_info": null, "node_info": {"start": 0, "end": 1760, "_node_type": "1"}, "relationships": {"1": "d6e37960-3ea8-4f3d-92a9-71ed568dbb72"}}, "__type__": "1"}, "5139742e-8c33-4947-a166-91a8ada187ed": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Plotting\u3010Section\u3011pandas.plotting.table\u3010Content\u3011pandas.plotting.table#pandas.plotting.table(ax,data,**kwargs)[source]#Helper function to convert DataFrame and Series to matplotlib.table.Parameters:axMatplotlib axes objectdataDataFrame or SeriesData for table contents.**kwargsKeyword arguments to be passed to matplotlib.table.table.\nIfrowLabelsorcolLabelsis not specified, data index or column\nname will be used.Returns:matplotlib table objectExamples>>>importmatplotlib.pyplotasplt>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>fix,ax=plt.subplots()>>>ax.axis('off')(0.0, 1.0, 0.0, 1.0)>>>table=pd.plotting.table(ax,df,loc='center',...cellLoc='center',colWidths=list([.2,.2]))", "doc_id": "5139742e-8c33-4947-a166-91a8ada187ed", "embedding": null, "doc_hash": "9fec48c37b21390909e90dd5ba9cd1efecbd221d6d89020974d685ed0e9eca44", "extra_info": null, "node_info": {"start": 0, "end": 695, "_node_type": "1"}, "relationships": {"1": "7a8b13f6-7189-46a0-a586-b69124285cf9"}}, "__type__": "1"}, "0b02797d-d880-46f7-b388-8b813a18a1b2": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.groups\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.groups#propertyDataFrameGroupBy.groups[source]#Dict {group name -> group labels}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).groups{'a': ['a', 'a'], 'b': ['b']}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"])>>>dfa  b  c0  1  2  31  1  5  62  7  8  9>>>df.groupby(by=[\"a\"]).groups{1: [0, 1], 7: [2]}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').groups{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}", "doc_id": "0b02797d-d880-46f7-b388-8b813a18a1b2", "embedding": null, "doc_hash": "e8188023f0934258684dc405b46857b608f42c6ab37edfa914479ce3615bced1", "extra_info": null, "node_info": {"start": 0, "end": 876, "_node_type": "1"}, "relationships": {"1": "1a0b4f10-7ee1-46e7-a586-b46dc8aa20dc"}}, "__type__": "1"}, "57e1ff1a-4206-4620-b180-bc33bd24a717": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.groups\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.groups#propertySeriesGroupBy.groups[source]#Dict {group name -> group labels}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).groups{'a': ['a', 'a'], 'b': ['b']}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"])>>>dfa  b  c0  1  2  31  1  5  62  7  8  9>>>df.groupby(by=[\"a\"]).groups{1: [0, 1], 7: [2]}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').groups{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}", "doc_id": "57e1ff1a-4206-4620-b180-bc33bd24a717", "embedding": null, "doc_hash": "ae72cd1f1371933b948ff8d3092f06f37e009e7a6acb2c1f4da6a65dd8311b10", "extra_info": null, "node_info": {"start": 0, "end": 867, "_node_type": "1"}, "relationships": {"1": "91bcbd11-db05-4ea8-a233-5220f4b7766c"}}, "__type__": "1"}, "1c99371a-6d7f-4fe7-b863-ec3c15c11d42": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.indices\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.indices#propertyDataFrameGroupBy.indices[source]#Dict {group name -> group indices}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).indices{'a': array([0, 1]), 'b': array([2])}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"owl\",\"toucan\",\"eagle\"])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[\"a\"]).indices{1: array([0, 1]), 7: array([2])}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').indicesdefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],Timestamp('2023-02-01 00:00:00'): [2, 3]})", "doc_id": "1c99371a-6d7f-4fe7-b863-ec3c15c11d42", "embedding": null, "doc_hash": "2686566d1c2390c6bdce48ee899cdcda13f74fa2b12febe01044136c35daec2a", "extra_info": null, "node_info": {"start": 0, "end": 992, "_node_type": "1"}, "relationships": {"1": "aed9c18d-2432-409d-9887-452d5bf37042"}}, "__type__": "1"}, "ac883b34-a3dc-4272-ace6-963de5b1efc3": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.indices\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.indices#propertySeriesGroupBy.indices[source]#Dict {group name -> group indices}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).indices{'a': array([0, 1]), 'b': array([2])}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"owl\",\"toucan\",\"eagle\"])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[\"a\"]).indices{1: array([0, 1]), 7: array([2])}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').indicesdefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],Timestamp('2023-02-01 00:00:00'): [2, 3]})", "doc_id": "ac883b34-a3dc-4272-ace6-963de5b1efc3", "embedding": null, "doc_hash": "52836268f14c92d376ef0e9cdc32f5cf45de8323539ac3432cd16f96514a5c28", "extra_info": null, "node_info": {"start": 0, "end": 983, "_node_type": "1"}, "relationships": {"1": "4ff3c89f-987f-4d39-8869-34da98026e2d"}}, "__type__": "1"}, "c4da50de-971a-42b8-bc48-b1cd25fd4242": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.get_group\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.get_group#DataFrameGroupBy.get_group(name,obj=None)[source]#Construct DataFrame from group with provided name.Parameters:nameobjectThe name of the group to get as a DataFrame.objDataFrame, default NoneThe DataFrame to take the DataFrame out of. If\nit is None, the object groupby was called on will\nbe used.Deprecated since version 2.1.0:The obj is deprecated and will be removed in a future version.\nDodf.iloc[gb.indices.get(name)]instead ofgb.get_group(name,obj=df).Returns:same type as objExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).get_group(\"a\")a    1a    2dtype: int64For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"owl\",\"toucan\",\"eagle\"])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[\"a\"]).get_group((1,))a  b  cowl     1  2  3toucan  1  5  6For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').get_group('2023-01-01')2023-01-01    12023-01-15    2dtype: int64", "doc_id": "c4da50de-971a-42b8-bc48-b1cd25fd4242", "embedding": null, "doc_hash": "deaae74c2b999e0353e3bf71766c4552b982b9f77b96b47085f6af77a254c171", "extra_info": null, "node_info": {"start": 0, "end": 1353, "_node_type": "1"}, "relationships": {"1": "fe1dbc93-df3d-46dd-9998-95963bf2b2c0"}}, "__type__": "1"}, "9463748d-09dc-4575-baab-4e6146ecab33": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.get_group\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.get_group#SeriesGroupBy.get_group(name,obj=None)[source]#Construct DataFrame from group with provided name.Parameters:nameobjectThe name of the group to get as a DataFrame.objDataFrame, default NoneThe DataFrame to take the DataFrame out of. If\nit is None, the object groupby was called on will\nbe used.Deprecated since version 2.1.0:The obj is deprecated and will be removed in a future version.\nDodf.iloc[gb.indices.get(name)]instead ofgb.get_group(name,obj=df).Returns:same type as objExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).get_group(\"a\")a    1a    2dtype: int64For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"owl\",\"toucan\",\"eagle\"])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[\"a\"]).get_group((1,))a  b  cowl     1  2  3toucan  1  5  6For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').get_group('2023-01-01')2023-01-01    12023-01-15    2dtype: int64", "doc_id": "9463748d-09dc-4575-baab-4e6146ecab33", "embedding": null, "doc_hash": "fcd432703ad992bfab6868083f7f6275052e74b4c85319f770b72ffdc19a5ce1", "extra_info": null, "node_info": {"start": 0, "end": 1344, "_node_type": "1"}, "relationships": {"1": "020707c1-897f-4c18-938d-af72116d7af5"}}, "__type__": "1"}, "5776e04d-ad02-492a-9595-724f9fcb53ce": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.Grouper\u3010Content\u3011pandas.Grouper#classpandas.Grouper(*args,**kwargs)[source]#A Grouper allows the user to specify a groupby instruction for an object.This specification will select a column via the key parameter, or if the\nlevel and/or axis parameters are given, a level of the index of the target\nobject.Ifaxisand/orlevelare passed as keywords to bothGrouperandgroupby, the values passed toGroupertake precedence.Parameters:keystr, defaults to NoneGroupby key, which selects the grouping column of the target.levelname/number, defaults to NoneThe level for the target index.freqstr / frequency object, defaults to NoneThis will groupby the specified frequency if the target selection\n(via key or level) is a datetime-like object. For full specification\nof available frequencies, please seehere.axisstr, int, defaults to 0Number/name of the axis.sortbool, default to FalseWhether to sort the resulting labels.closed{\u2018left\u2019 or \u2018right\u2019}Closed end of interval. Only whenfreqparameter is passed.label{\u2018left\u2019 or \u2018right\u2019}Interval boundary to use for labeling.\nOnly whenfreqparameter is passed.convention{\u2018start\u2019, \u2018end\u2019, \u2018e\u2019, \u2018s\u2019}If grouper is PeriodIndex andfreqparameter is passed.originTimestamp or str, default \u2018start_day\u2019The timestamp on which to adjust the grouping. The timezone of origin must\nmatch the timezone of the index.\nIf string, must be one of the following:\u2018epoch\u2019:originis 1970-01-01\u2018start\u2019:originis the first value of the timeseries\u2018start_day\u2019:originis the first day at midnight of the timeseries\u2018end\u2019:originis the last value of the timeseries\u2018end_day\u2019:originis the ceiling midnight of the last dayNew in version 1.3.0.offsetTimedelta or str, default is NoneAn offset timedelta added to the origin.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together with\nrow/column will be dropped. If False, NA values will also be treated as\nthe key in groups.Returns:Grouper or pandas.api.typing.TimeGrouperA TimeGrouper is returned iffreqis notNone. Otherwise, a Grouper\nis returned.Examplesdf.groupby(pd.Grouper(key=\"Animal\"))is equivalent todf.groupby('Animal')>>>df=pd.DataFrame(...{...\"Animal\":[\"Falcon\",\"Parrot\",\"Falcon\",\"Falcon\",\"Parrot\"],...\"Speed\":[100,5,200,300,15],...}...)>>>dfAnimal  Speed0  Falcon    1001  Parrot      52  Falcon    2003  Falcon    3004  Parrot     15>>>df.groupby(pd.Grouper(key=\"Animal\")).mean()SpeedAnimalFalcon  200.0Parrot   10.0Specify a resample operation on the column \u2018Publish date\u2019>>>df=pd.DataFrame(...{...\"Publish date\":[...pd.Timestamp(\"2000-01-02\"),...pd.Timestamp(\"2000-01-02\"),...pd.Timestamp(\"2000-01-09\"),...pd.Timestamp(\"2000-01-16\")...],...\"ID\":[0,1,2,3],...\"Price\":[10,20,30,40]...}...)>>>dfPublish date  ID  Price0   2000-01-02   0     101   2000-01-02   1     202   2000-01-09   2     303   2000-01-16   3     40>>>df.groupby(pd.Grouper(key=\"Publish date\",freq=\"1W\")).mean()ID  PricePublish date2000-01-02    0.5   15.02000-01-09    2.0  ", "doc_id": "5776e04d-ad02-492a-9595-724f9fcb53ce", "embedding": null, "doc_hash": "6f63cd591b1ae27865d7d7a5b1be9a6f25d67ef2873323dc7fed084be8277e6a", "extra_info": null, "node_info": {"start": 0, "end": 2969, "_node_type": "1"}, "relationships": {"1": "9af307b6-80a3-497d-a0c0-818f76bdee30", "3": "5226fc89-7380-4827-8e10-f9aeba28660e"}}, "__type__": "1"}, "5226fc89-7380-4827-8e10-f9aeba28660e": {"__data__": {"text": "date  ID  Price0   2000-01-02   0     101   2000-01-02   1     202   2000-01-09   2     303   2000-01-16   3     40>>>df.groupby(pd.Grouper(key=\"Publish date\",freq=\"1W\")).mean()ID  PricePublish date2000-01-02    0.5   15.02000-01-09    2.0   30.02000-01-16    3.0   40.0If you want to adjust the start of the bins based on a fixed timestamp:>>>start,end='2000-10-01 23:30:00','2000-10-02 00:30:00'>>>rng=pd.date_range(start,end,freq='7min')>>>ts=pd.Series(np.arange(len(rng))*3,index=rng)>>>ts2000-10-01 23:30:00     02000-10-01 23:37:00     32000-10-01 23:44:00     62000-10-01 23:51:00     92000-10-01 23:58:00    122000-10-02 00:05:00    152000-10-02 00:12:00    182000-10-02 00:19:00    212000-10-02 00:26:00    24Freq: 7min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min')).sum()2000-10-01 23:14:00     02000-10-01 23:31:00     92000-10-01 23:48:00    212000-10-02 00:05:00    542000-10-02 00:22:00    24Freq: 17min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min',origin='epoch')).sum()2000-10-01 23:18:00     02000-10-01 23:35:00    182000-10-01 23:52:00    272000-10-02 00:09:00    392000-10-02 00:26:00    24Freq: 17min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min',origin='2000-01-01')).sum()2000-10-01 23:24:00     32000-10-01 23:41:00    152000-10-01 23:58:00    452000-10-02 00:15:00    45Freq: 17min, dtype: int64If you want to adjust the start of the bins with anoffsetTimedelta, the two\nfollowing lines are equivalent:>>>ts.groupby(pd.Grouper(freq='17min',origin='start')).sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min',offset='23h30min')).sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17min, dtype: int64To replace the use of the deprecatedbaseargument, you can now useoffset,\nin this example it is equivalent to havebase=2:>>>ts.groupby(pd.Grouper(freq='17min',offset='2min')).sum()2000-10-01 23:16:00     02000-10-01 23:33:00     92000-10-01 23:50:00    362000-10-02 00:07:00  ", "doc_id": "5226fc89-7380-4827-8e10-f9aeba28660e", "embedding": null, "doc_hash": "9586a3693deff02c339eb0ac7e784da6857d69550d219d7ed6b8353bd9af5267", "extra_info": null, "node_info": {"start": 2794, "end": 4886, "_node_type": "1"}, "relationships": {"1": "9af307b6-80a3-497d-a0c0-818f76bdee30", "2": "5776e04d-ad02-492a-9595-724f9fcb53ce", "3": "933cff03-c575-4676-80aa-b9df6ae0a9d2"}}, "__type__": "1"}, "933cff03-c575-4676-80aa-b9df6ae0a9d2": {"__data__": {"text": "23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17min, dtype: int64To replace the use of the deprecatedbaseargument, you can now useoffset,\nin this example it is equivalent to havebase=2:>>>ts.groupby(pd.Grouper(freq='17min',offset='2min')).sum()2000-10-01 23:16:00     02000-10-01 23:33:00     92000-10-01 23:50:00    362000-10-02 00:07:00    392000-10-02 00:24:00    24Freq: 17min, dtype: int64", "doc_id": "933cff03-c575-4676-80aa-b9df6ae0a9d2", "embedding": null, "doc_hash": "d783950c52cfc08e6ba74dbe4c482dfd9a21a8f589c3f21ece4d7d9e70756de0", "extra_info": null, "node_info": {"start": 4673, "end": 5116, "_node_type": "1"}, "relationships": {"1": "9af307b6-80a3-497d-a0c0-818f76bdee30", "2": "5226fc89-7380-4827-8e10-f9aeba28660e"}}, "__type__": "1"}, "9b9de79a-f7bb-432c-b7f8-ee7b61814c10": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.NamedAgg\u3010Content\u3011pandas.NamedAgg#classpandas.NamedAgg(column,aggfunc)[source]#Helper for column specific aggregation with control over output column names.Subclass of typing.NamedTuple.Parameters:columnHashableColumn label in the DataFrame to apply aggfunc.aggfuncfunction or strFunction to apply to the provided column. If string, the name of a built-in\npandas function.Examples>>>df=pd.DataFrame({\"key\":[1,1,2],\"a\":[-1,0,1],1:[10,11,12]})>>>agg_a=pd.NamedAgg(column=\"a\",aggfunc=\"min\")>>>agg_1=pd.NamedAgg(column=1,aggfunc=lambdax:np.mean(x))>>>df.groupby(\"key\").agg(result_a=agg_a,result_1=agg_1)result_a  result_1key1          -1      10.52           1      12.0AttributesaggfuncAlias for field number 1columnAlias for field number 0Methodscount(value,/)Return number of occurrences of value.index(value[,start,stop])Return first index of value.", "doc_id": "9b9de79a-f7bb-432c-b7f8-ee7b61814c10", "embedding": null, "doc_hash": "5e4f92fbe2b4f23ebf37e9eec8820a0d25a272e3febaec890ee96d1812abc94b", "extra_info": null, "node_info": {"start": 0, "end": 892, "_node_type": "1"}, "relationships": {"1": "5de2ecf8-7986-42e4-a156-095d24df8b88"}}, "__type__": "1"}, "2dc4ad44-2d3a-4910-92ca-e34fa4aee76e": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.apply\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.apply#SeriesGroupBy.apply(func,*args,**kwargs)[source]#Apply functionfuncgroup-wise and combine the results together.The function passed toapplymust take a series as its first\nargument and return a DataFrame, Series or scalar.applywill\nthen take care of combining the results back together into a single\ndataframe or series.applyis therefore a highly flexible\ngrouping method.Whileapplyis a very flexible method, its downside is that\nusing it can be quite a bit slower than using more specific methods\nlikeaggortransform. Pandas offers a wide range of method that will\nbe much faster than usingapplyfor their specific purposes, so try to\nuse them before reaching forapply.Parameters:funccallableA callable that takes a series as its first argument, and\nreturns a dataframe, a series or a scalar. In addition the\ncallable may take positional and keyword arguments.include_groupsbool, default TrueWhen True, will attempt to applyfuncto the groupings in\nthe case that they are columns of the DataFrame. If this raises a\nTypeError, the result will be computed with the groupings excluded.\nWhen False, the groupings will be excluded when applyingfunc.New in version 2.2.0.Deprecated since version 2.2.0:Setting include_groups to True is deprecated. Only the value\nFalse will be allowed in a future version of pandas.args, kwargstuple and dictOptional positional and keyword arguments to pass tofunc.Returns:Series or DataFrameSee alsopipeApply function to the full GroupBy object instead of to each group.aggregateApply aggregate function to the GroupBy object.transformApply function column-by-column to the GroupBy object.Series.applyApply a function to a Series.DataFrame.applyApply a function to each row or column of a DataFrame.NotesChanged in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>s=pd.Series([0,1,2],index='a a b'.split())>>>g1=s.groupby(s.index,group_keys=False)>>>g2=s.groupby(s.index,group_keys=True)Fromsabove we can see thatghas two groups,aandb.\nNotice thatg1haveg2have two groups,aandb, and only\ndiffer in theirgroup_keysargument. Callingapplyin various ways,\nwe can get different grouping results:Example 1: The function passed toapplytakes a Series as\nits argument and returns a Series.applycombines the result for\neach group together into a new Series.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc.>>>g1.apply(lambdax:x*2ifx.name=='a'elsex/2)a    0.0a    2.0b    1.0dtype: float64In the above, the groups are not part of the index. We can have them included\nby usingg2wheregroup_keys=True:>>>g2.apply(lambdax:x*2ifx.name=='a'elsex/2)a  a    0.0a    2.0b  b    1.0dtype: float64Example 2: The function passed toapplytakes a Series as\nits argument and returns a scalar.applycombines the result for\neach group together into a Series, including setting the index as\nappropriate:>>>g1.apply(lambdax:x.max()-x.min())a    1b    0dtype: int64Thegroup_keysargument has no effect here because the result is not\nlike-indexed (i.e.a transform) when compared\nto the input.>>>g2.apply(lambdax:x.max()-x.min())a    1b    0dtype:", "doc_id": "2dc4ad44-2d3a-4910-92ca-e34fa4aee76e", "embedding": null, "doc_hash": "2bd9f366f78bbe7cb79ba339630b8c22b643900526562be18ef196f3706558b1", "extra_info": null, "node_info": {"start": 0, "end": 3445, "_node_type": "1"}, "relationships": {"1": "b0a339be-2e41-4f48-8972-77ad66ea3de6", "3": "4ae57d5d-1d9f-4252-b938-d1b82464e557"}}, "__type__": "1"}, "4ae57d5d-1d9f-4252-b938-d1b82464e557": {"__data__": {"text": " a    0.0a    2.0b  b    1.0dtype: float64Example 2: The function passed toapplytakes a Series as\nits argument and returns a scalar.applycombines the result for\neach group together into a Series, including setting the index as\nappropriate:>>>g1.apply(lambdax:x.max()-x.min())a    1b    0dtype: int64Thegroup_keysargument has no effect here because the result is not\nlike-indexed (i.e.a transform) when compared\nto the input.>>>g2.apply(lambdax:x.max()-x.min())a    1b    0dtype: int64", "doc_id": "4ae57d5d-1d9f-4252-b938-d1b82464e557", "embedding": null, "doc_hash": "1d02f7035c05dd89261d74ab3335c33f500cab906da7b02673a30e24ca476ef8", "extra_info": null, "node_info": {"start": 2967, "end": 3451, "_node_type": "1"}, "relationships": {"1": "b0a339be-2e41-4f48-8972-77ad66ea3de6", "2": "2dc4ad44-2d3a-4910-92ca-e34fa4aee76e"}}, "__type__": "1"}, "527b4528-cfb9-4be4-94d2-3fcd80d71247": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.apply\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.apply#DataFrameGroupBy.apply(func,*args,include_groups=True,**kwargs)[source]#Apply functionfuncgroup-wise and combine the results together.The function passed toapplymust take a dataframe as its first\nargument and return a DataFrame, Series or scalar.applywill\nthen take care of combining the results back together into a single\ndataframe or series.applyis therefore a highly flexible\ngrouping method.Whileapplyis a very flexible method, its downside is that\nusing it can be quite a bit slower than using more specific methods\nlikeaggortransform. Pandas offers a wide range of method that will\nbe much faster than usingapplyfor their specific purposes, so try to\nuse them before reaching forapply.Parameters:funccallableA callable that takes a dataframe as its first argument, and\nreturns a dataframe, a series or a scalar. In addition the\ncallable may take positional and keyword arguments.include_groupsbool, default TrueWhen True, will attempt to applyfuncto the groupings in\nthe case that they are columns of the DataFrame. If this raises a\nTypeError, the result will be computed with the groupings excluded.\nWhen False, the groupings will be excluded when applyingfunc.New in version 2.2.0.Deprecated since version 2.2.0:Setting include_groups to True is deprecated. Only the value\nFalse will be allowed in a future version of pandas.args, kwargstuple and dictOptional positional and keyword arguments to pass tofunc.Returns:Series or DataFrameSee alsopipeApply function to the full GroupBy object instead of to each group.aggregateApply aggregate function to the GroupBy object.transformApply function column-by-column to the GroupBy object.Series.applyApply a function to a Series.DataFrame.applyApply a function to each row or column of a DataFrame.NotesChanged in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':'a a b'.split(),...'B':[1,2,3],...'C':[4,6,5]})>>>g1=df.groupby('A',group_keys=False)>>>g2=df.groupby('A',group_keys=True)Notice thatg1andg2have two groups,aandb, and only\ndiffer in theirgroup_keysargument. Callingapplyin various ways,\nwe can get different grouping results:Example 1: below the function passed toapplytakes a DataFrame as\nits argument and returns a DataFrame.applycombines the result for\neach group together into a new DataFrame:>>>g1[['B','C']].apply(lambdax:x/x.sum())B    C0  0.333333  0.41  0.666667  0.62  1.000000  1.0In the above, the groups are not part of the index. We can have them included\nby usingg2wheregroup_keys=True:>>>g2[['B','C']].apply(lambdax:x/x.sum())B    CAa 0  0.333333  0.41  0.666667  0.6b 2  1.000000  1.0Example 2: The function passed toapplytakes a DataFrame as\nits argument and returns a Series.applycombines the result for\neach group together into a new DataFrame.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc.>>>g1[['B','C']].apply(lambdax:x.astype(float).max()-x.min())B    CAa  1.0  2.0b  0.0  0.0>>>g2[['B','C']].apply(lambdax:x.astype(float).max()-x.min())B    CAa ", "doc_id": "527b4528-cfb9-4be4-94d2-3fcd80d71247", "embedding": null, "doc_hash": "e38dc848a6d9b8488f8f2b1b712e11491cb10e7ffd2daa75cf8850032fbecbb8", "extra_info": null, "node_info": {"start": 0, "end": 3377, "_node_type": "1"}, "relationships": {"1": "3845c5d8-3e19-48df-b0ec-18f0005a9289", "3": "ebbabac7-6197-4418-8f42-d98ea40f3b86"}}, "__type__": "1"}, "ebbabac7-6197-4418-8f42-d98ea40f3b86": {"__data__": {"text": "   CAa 0  0.333333  0.41  0.666667  0.6b 2  1.000000  1.0Example 2: The function passed toapplytakes a DataFrame as\nits argument and returns a Series.applycombines the result for\neach group together into a new DataFrame.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc.>>>g1[['B','C']].apply(lambdax:x.astype(float).max()-x.min())B    CAa  1.0  2.0b  0.0  0.0>>>g2[['B','C']].apply(lambdax:x.astype(float).max()-x.min())B    CAa  1.0  2.0b  0.0  0.0Thegroup_keysargument has no effect here because the result is not\nlike-indexed (i.e.a transform) when compared\nto the input.Example 3: The function passed toapplytakes a DataFrame as\nits argument and returns a scalar.applycombines the result for\neach group together into a Series, including setting the index as\nappropriate:>>>g1.apply(lambdax:x.C.max()-x.B.min(),include_groups=False)Aa    5b    2dtype: int64", "doc_id": "ebbabac7-6197-4418-8f42-d98ea40f3b86", "embedding": null, "doc_hash": "983809e3f1ed1fb7a4e28107ede5cd33a17e30a0c59088a3ea1a5f152f0bcd9e", "extra_info": null, "node_info": {"start": 2904, "end": 3808, "_node_type": "1"}, "relationships": {"1": "3845c5d8-3e19-48df-b0ec-18f0005a9289", "2": "527b4528-cfb9-4be4-94d2-3fcd80d71247"}}, "__type__": "1"}, "a663c5b2-5d86-4dcc-9b63-f33d11112e5b": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.agg\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.agg#SeriesGroupBy.agg(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either\nwork when passed a Series or when passed to Series.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']None, in which case**kwargsare used with Named Aggregation. Here the\noutput has one column for each element in**kwargs. The name of the\ncolumn is keyword, whereas the value determines the aggregation used to compute\nthe values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported\nwith this engine.If the'numba'engine is chosen, the function must be\na user defined function withvaluesandindexas the\nfirst and second arguments respectively in the function signature.\nEach group\u2019s index will be passed to the user defined function\nand optionally available for use.Deprecated since version 2.1.0:Passing a dictionary is deprecated and will raise in a future version\nof pandas. Pass a list of aggregations instead.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be\napplied to the function**kwargsIffuncis None,**kwargsare used to define the output names and\naggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:SeriesSee alsoSeries.groupby.applyApply function func group-wise and combine the results together.Series.groupby.transformTransforms the Series on each group based on the given function.Series.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.groupby([1,1,2,2]).min()1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg('min')1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg(['min','max'])min  max1    1    22    3    4The output column names can be controlled by passing\nthe desired column names and aggregations as keyword arguments.>>>s.groupby([1,1,2,2]).agg(...minimum='min',...maximum='max',...)minimum  maximum1        1        22        3        4Changed in", "doc_id": "a663c5b2-5d86-4dcc-9b63-f33d11112e5b", "embedding": null, "doc_hash": "df8efec766ce843b47489319e537d81499e997abf3c7d45eaa82f5a9a58ca8f8", "extra_info": null, "node_info": {"start": 0, "end": 3387, "_node_type": "1"}, "relationships": {"1": "5869dd75-6922-4a2f-9e89-74c00a51d1d9", "3": "50f7fc01-f9a0-469c-b1db-c214292955fb"}}, "__type__": "1"}, "50f7fc01-f9a0-469c-b1db-c214292955fb": {"__data__": {"text": "int64>>>s.groupby([1,1,2,2]).min()1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg('min')1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg(['min','max'])min  max1    1    22    3    4The output column names can be controlled by passing\nthe desired column names and aggregations as keyword arguments.>>>s.groupby([1,1,2,2]).agg(...minimum='min',...maximum='max',...)minimum  maximum1        1        22        3        4Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>s.groupby([1,1,2,2]).agg(lambdax:x.astype(float).min())1    1.02    3.0dtype: float64", "doc_id": "50f7fc01-f9a0-469c-b1db-c214292955fb", "embedding": null, "doc_hash": "4630855fed73b5b848e7d6c62829597b5440f48d7c0111288dcd730b82acaa76", "extra_info": null, "node_info": {"start": 2953, "end": 3568, "_node_type": "1"}, "relationships": {"1": "5869dd75-6922-4a2f-9e89-74c00a51d1d9", "2": "a663c5b2-5d86-4dcc-9b63-f33d11112e5b"}}, "__type__": "1"}, "45da16ef-dcde-4119-938d-e42807c56343": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.agg\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.agg#DataFrameGroupBy.agg(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either\nwork when passed a DataFrame or when passed to DataFrame.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.None, in which case**kwargsare used with Named Aggregation. Here the\noutput has one column for each element in**kwargs. The name of the\ncolumn is keyword, whereas the value determines the aggregation used to compute\nthe values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported\nwith this engine.If the'numba'engine is chosen, the function must be\na user defined function withvaluesandindexas the\nfirst and second arguments respectively in the function signature.\nEach group\u2019s index will be passed to the user defined function\nand optionally available for use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be\napplied to the function**kwargsIffuncis None,**kwargsare used to define the output names and\naggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:DataFrameSee alsoDataFrame.groupby.applyApply function func group-wise and combine the results together.DataFrame.groupby.transformTransforms the Series on each group based on the given function.DataFrame.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Examples>>>data={\"A\":[1,1,2,2],...\"B\":[1,2,3,4],...\"C\":[0.362838,0.227877,1.267767,-0.562860]}>>>df=pd.DataFrame(data)>>>dfA  B         C0  1  1  0.3628381  1  2  0.2278772  2  3  1.2677673  2  4 -0.562860The aggregation is for each column.>>>df.groupby('A').agg('min')B         CA1  1  0.2278772  3 -0.562860Multiple aggregations>>>df.groupby('A').agg(['min','max'])B             Cmin max       min       maxA1   1   2  0.227877  0.3628382   3   4 -0.562860 ", "doc_id": "45da16ef-dcde-4119-938d-e42807c56343", "embedding": null, "doc_hash": "d09c690f3a50c4bea03e5a5f1137136b7c29f0f69bef3c690ef1cdc552ca13ce", "extra_info": null, "node_info": {"start": 0, "end": 3288, "_node_type": "1"}, "relationships": {"1": "b9b4b83e-b958-4e6a-b982-113240952463", "3": "ea328907-8117-4cd7-b729-fcd45783f39c"}}, "__type__": "1"}, "ea328907-8117-4cd7-b729-fcd45783f39c": {"__data__": {"text": " B         C0  1  1  0.3628381  1  2  0.2278772  2  3  1.2677673  2  4 -0.562860The aggregation is for each column.>>>df.groupby('A').agg('min')B         CA1  1  0.2278772  3 -0.562860Multiple aggregations>>>df.groupby('A').agg(['min','max'])B             Cmin max       min       maxA1   1   2  0.227877  0.3628382   3   4 -0.562860  1.267767Select a column for aggregation>>>df.groupby('A').B.agg(['min','max'])min  maxA1    1    22    3    4User-defined function for aggregation>>>df.groupby('A').agg(lambdax:sum(x)+2)B          CA1       5       2.5907152       9       2.704907Different aggregations per column>>>df.groupby('A').agg({'B':['min','max'],'C':'sum'})B             Cmin max       sumA1   1   2  0.5907152   3   4  0.704907To control the output names with different aggregations per column,\npandas supports \u201cnamed aggregation\u201d>>>df.groupby(\"A\").agg(...b_min=pd.NamedAgg(column=\"B\",aggfunc=\"min\"),...c_sum=pd.NamedAgg(column=\"C\",aggfunc=\"sum\")...)b_min     c_sumA1      1  0.5907152      3  0.704907The keywords are theoutputcolumn namesThe values are tuples whose first element is the column to select\nand the second element is the aggregation to apply to that column.\nPandas provides thepandas.NamedAggnamedtuple with the fields['column','aggfunc']to make it clearer what the arguments are.\nAs usual, the aggregation can be a callable or a string alias.SeeNamed aggregationfor more.Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>df.groupby(\"A\")[[\"B\"]].agg(lambdax:x.astype(float).min())BA1   1.02   3.0", "doc_id": "ea328907-8117-4cd7-b729-fcd45783f39c", "embedding": null, "doc_hash": "3e6367e1aaa0c0e34e7bc1f43346fc2ff1052d8c361dbc4c42a55f591e7acda1", "extra_info": null, "node_info": {"start": 2954, "end": 4532, "_node_type": "1"}, "relationships": {"1": "b9b4b83e-b958-4e6a-b982-113240952463", "2": "45da16ef-dcde-4119-938d-e42807c56343"}}, "__type__": "1"}, "f81228ad-d7fa-4f0b-8b9a-f193673d5e4f": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.aggregate\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.aggregate#SeriesGroupBy.aggregate(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either\nwork when passed a Series or when passed to Series.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']None, in which case**kwargsare used with Named Aggregation. Here the\noutput has one column for each element in**kwargs. The name of the\ncolumn is keyword, whereas the value determines the aggregation used to compute\nthe values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported\nwith this engine.If the'numba'engine is chosen, the function must be\na user defined function withvaluesandindexas the\nfirst and second arguments respectively in the function signature.\nEach group\u2019s index will be passed to the user defined function\nand optionally available for use.Deprecated since version 2.1.0:Passing a dictionary is deprecated and will raise in a future version\nof pandas. Pass a list of aggregations instead.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be\napplied to the function**kwargsIffuncis None,**kwargsare used to define the output names and\naggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:SeriesSee alsoSeries.groupby.applyApply function func group-wise and combine the results together.Series.groupby.transformTransforms the Series on each group based on the given function.Series.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.groupby([1,1,2,2]).min()1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg('min')1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg(['min','max'])min  max1    1    22    3    4The output column names can be controlled by passing\nthe desired column names and aggregations as keyword arguments.>>>s.groupby([1,1,2,2]).agg(...minimum='min',...maximum='max',...)minimum  maximum1        1        22        3       ", "doc_id": "f81228ad-d7fa-4f0b-8b9a-f193673d5e4f", "embedding": null, "doc_hash": "e98b6eaff085b00ba8c39e0b1e21cb4ec3f22b99a7209dfb450c5aff879181f7", "extra_info": null, "node_info": {"start": 0, "end": 3393, "_node_type": "1"}, "relationships": {"1": "6880905a-3e81-48b7-9515-882a36192f1c", "3": "4bd48c93-35cc-4432-93b7-f496ede75e56"}}, "__type__": "1"}, "4bd48c93-35cc-4432-93b7-f496ede75e56": {"__data__": {"text": "  4dtype: int64>>>s.groupby([1,1,2,2]).min()1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg('min')1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg(['min','max'])min  max1    1    22    3    4The output column names can be controlled by passing\nthe desired column names and aggregations as keyword arguments.>>>s.groupby([1,1,2,2]).agg(...minimum='min',...maximum='max',...)minimum  maximum1        1        22        3        4Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>s.groupby([1,1,2,2]).agg(lambdax:x.astype(float).min())1    1.02    3.0dtype: float64", "doc_id": "4bd48c93-35cc-4432-93b7-f496ede75e56", "embedding": null, "doc_hash": "dcb187c3a71f5d9b3f3a90835b71def56e8250fb83735c70e02cf2e79fb2241e", "extra_info": null, "node_info": {"start": 2961, "end": 3586, "_node_type": "1"}, "relationships": {"1": "6880905a-3e81-48b7-9515-882a36192f1c", "2": "f81228ad-d7fa-4f0b-8b9a-f193673d5e4f"}}, "__type__": "1"}, "174ab571-498d-472f-b8e1-6e3c583e7a5d": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.aggregate\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.aggregate#DataFrameGroupBy.aggregate(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either\nwork when passed a DataFrame or when passed to DataFrame.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.None, in which case**kwargsare used with Named Aggregation. Here the\noutput has one column for each element in**kwargs. The name of the\ncolumn is keyword, whereas the value determines the aggregation used to compute\nthe values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported\nwith this engine.If the'numba'engine is chosen, the function must be\na user defined function withvaluesandindexas the\nfirst and second arguments respectively in the function signature.\nEach group\u2019s index will be passed to the user defined function\nand optionally available for use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be\napplied to the function**kwargsIffuncis None,**kwargsare used to define the output names and\naggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:DataFrameSee alsoDataFrame.groupby.applyApply function func group-wise and combine the results together.DataFrame.groupby.transformTransforms the Series on each group based on the given function.DataFrame.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Examples>>>data={\"A\":[1,1,2,2],...\"B\":[1,2,3,4],...\"C\":[0.362838,0.227877,1.267767,-0.562860]}>>>df=pd.DataFrame(data)>>>dfA  B         C0  1  1  0.3628381  1  2  0.2278772  2  3  1.2677673  2  4 -0.562860The aggregation is for each column.>>>df.groupby('A').agg('min')B         CA1  1  0.2278772  3 -0.562860Multiple aggregations>>>df.groupby('A').agg(['min','max'])B             Cmin max       min       maxA1   1   2  0.227877  0.3628382   3   4 -0.562860", "doc_id": "174ab571-498d-472f-b8e1-6e3c583e7a5d", "embedding": null, "doc_hash": "7f7f0c30eb081cf4a1cedd92b7cb6dba78aedd16526e45ae8be0fe336e589c6f", "extra_info": null, "node_info": {"start": 0, "end": 3305, "_node_type": "1"}, "relationships": {"1": "eb7a97ff-b65c-49a1-a85b-1343ab69a244", "3": "1071bcb7-42cc-404f-b711-342db505f532"}}, "__type__": "1"}, "1071bcb7-42cc-404f-b711-342db505f532": {"__data__": {"text": " B         C0  1  1  0.3628381  1  2  0.2278772  2  3  1.2677673  2  4 -0.562860The aggregation is for each column.>>>df.groupby('A').agg('min')B         CA1  1  0.2278772  3 -0.562860Multiple aggregations>>>df.groupby('A').agg(['min','max'])B             Cmin max       min       maxA1   1   2  0.227877  0.3628382   3   4 -0.562860  1.267767Select a column for aggregation>>>df.groupby('A').B.agg(['min','max'])min  maxA1    1    22    3    4User-defined function for aggregation>>>df.groupby('A').agg(lambdax:sum(x)+2)B          CA1       5       2.5907152       9       2.704907Different aggregations per column>>>df.groupby('A').agg({'B':['min','max'],'C':'sum'})B             Cmin max       sumA1   1   2  0.5907152   3   4  0.704907To control the output names with different aggregations per column,\npandas supports \u201cnamed aggregation\u201d>>>df.groupby(\"A\").agg(...b_min=pd.NamedAgg(column=\"B\",aggfunc=\"min\"),...c_sum=pd.NamedAgg(column=\"C\",aggfunc=\"sum\")...)b_min     c_sumA1      1  0.5907152      3  0.704907The keywords are theoutputcolumn namesThe values are tuples whose first element is the column to select\nand the second element is the aggregation to apply to that column.\nPandas provides thepandas.NamedAggnamedtuple with the fields['column','aggfunc']to make it clearer what the arguments are.\nAs usual, the aggregation can be a callable or a string alias.SeeNamed aggregationfor more.Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>df.groupby(\"A\")[[\"B\"]].agg(lambdax:x.astype(float).min())BA1   1.02   3.0", "doc_id": "1071bcb7-42cc-404f-b711-342db505f532", "embedding": null, "doc_hash": "3e6367e1aaa0c0e34e7bc1f43346fc2ff1052d8c361dbc4c42a55f591e7acda1", "extra_info": null, "node_info": {"start": 2972, "end": 4550, "_node_type": "1"}, "relationships": {"1": "eb7a97ff-b65c-49a1-a85b-1343ab69a244", "2": "174ab571-498d-472f-b8e1-6e3c583e7a5d"}}, "__type__": "1"}, "28c01d3f-9a0f-4e4b-9765-9ab3da6a321b": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.transform\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.transform#SeriesGroupBy.transform(func,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Call function producing a same-indexed Series on each group.Returns a Series having the same indexes as the original object\nfilled with the transformed values.Parameters:ffunction, strFunction to apply to each group. See the Notes section below for requirements.Accepted inputs are:StringPython functionNumba JIT function withengine='numba'specified.Only passing a single function is supported with this engine.\nIf the'numba'engine is chosen, the function must be\na user defined function withvaluesandindexas the\nfirst and second arguments respectively in the function signature.\nEach group\u2019s index will be passed to the user defined function\nand optionally available for use.If a string is chosen, then it needs to be the name\nof the groupby method you want to use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or the global settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be\napplied to the function**kwargsKeyword arguments to be passed into func.Returns:SeriesSee alsoSeries.groupby.applyApply functionfuncgroup-wise and combine the results together.Series.groupby.aggregateAggregate using one or more operations over the specified axis.Series.transformCallfuncon self producing a Series with the same axis shape as self.NotesEach group is endowed the attribute \u2018name\u2019 in case you need to know\nwhich group you are working on.The current implementation imposes three requirements on f:f must return a value that either has the same shape as the input\nsubframe or can be broadcast to the shape of the input subframe.\nFor example, iffreturns a scalar it will be broadcast to have the\nsame shape as the input subframe.if this is a DataFrame, f must support application column-by-column\nin the subframe. If f also supports application to the entire subframe,\nthen a fast path is used starting from the second chunk.f must not mutate groups. Mutation is not supported and may\nproduce unexpected results. SeeMutating with User Defined Function (UDF) methodsfor more details.When usingengine='numba', there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Changed in version 2.0.0:When using.transformon a grouped DataFrame and the transformation function\nreturns a DataFrame, pandas now aligns the result\u2019s index\nwith the input\u2019s index. You can call.to_numpy()on the\nresult of the transformation function to avoid alignment.Examples>>>ser=pd.Series([390.0,350.0,30.0,20.0],...index=[\"Falcon\",\"Falcon\",\"Parrot\",\"Parrot\"],...name=\"Max Speed\")>>>grouped=ser.groupby([1,1,2,2])>>>grouped.transform(lambdax:(x-x.mean())/x.std())Falcon    0.707107Falcon   -0.707107Parrot    0.707107Parrot   -0.707107Name: Max Speed, dtype: float64Broadcast result of the", "doc_id": "28c01d3f-9a0f-4e4b-9765-9ab3da6a321b", "embedding": null, "doc_hash": "33a9a132f02e8d9cd2b5fd868f40674655129b459ac17c69a4bd050f4d9f48a3", "extra_info": null, "node_info": {"start": 0, "end": 3576, "_node_type": "1"}, "relationships": {"1": "a05aeb87-b978-47ed-918c-5f4d8e675971", "3": "7befe10f-2bf9-423c-8ecd-60467953fa9e"}}, "__type__": "1"}, "7befe10f-2bf9-423c-8ecd-60467953fa9e": {"__data__": {"text": "using.transformon a grouped DataFrame and the transformation function\nreturns a DataFrame, pandas now aligns the result\u2019s index\nwith the input\u2019s index. You can call.to_numpy()on the\nresult of the transformation function to avoid alignment.Examples>>>ser=pd.Series([390.0,350.0,30.0,20.0],...index=[\"Falcon\",\"Falcon\",\"Parrot\",\"Parrot\"],...name=\"Max Speed\")>>>grouped=ser.groupby([1,1,2,2])>>>grouped.transform(lambdax:(x-x.mean())/x.std())Falcon    0.707107Falcon   -0.707107Parrot    0.707107Parrot   -0.707107Name: Max Speed, dtype: float64Broadcast result of the transformation>>>grouped.transform(lambdax:x.max()-x.min())Falcon    40.0Falcon    40.0Parrot    10.0Parrot    10.0Name: Max Speed, dtype: float64>>>grouped.transform(\"mean\")Falcon    370.0Falcon    370.0Parrot     25.0Parrot     25.0Name: Max Speed, dtype: float64Changed in version 1.3.0.The resulting dtype will reflect the return value of the passedfunc,\nfor example:>>>grouped.transform(lambdax:x.astype(int).max())Falcon    390Falcon    390Parrot     30Parrot     30Name: Max Speed, dtype: int64", "doc_id": "7befe10f-2bf9-423c-8ecd-60467953fa9e", "embedding": null, "doc_hash": "5676e893f2476d5cc36ba1ef278bddc82a0e8656469f1fc667ee3634ae707460", "extra_info": null, "node_info": {"start": 3012, "end": 4078, "_node_type": "1"}, "relationships": {"1": "a05aeb87-b978-47ed-918c-5f4d8e675971", "2": "28c01d3f-9a0f-4e4b-9765-9ab3da6a321b"}}, "__type__": "1"}, "5728e558-3a32-4fc9-84d3-6f84336dbb1b": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.transform\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.transform#DataFrameGroupBy.transform(func,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Call function producing a same-indexed DataFrame on each group.Returns a DataFrame having the same indexes as the original object\nfilled with the transformed values.Parameters:ffunction, strFunction to apply to each group. See the Notes section below for requirements.Accepted inputs are:StringPython functionNumba JIT function withengine='numba'specified.Only passing a single function is supported with this engine.\nIf the'numba'engine is chosen, the function must be\na user defined function withvaluesandindexas the\nfirst and second arguments respectively in the function signature.\nEach group\u2019s index will be passed to the user defined function\nand optionally available for use.If a string is chosen, then it needs to be the name\nof the groupby method you want to use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or the global settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be\napplied to the function**kwargsKeyword arguments to be passed into func.Returns:DataFrameSee alsoDataFrame.groupby.applyApply functionfuncgroup-wise and combine the results together.DataFrame.groupby.aggregateAggregate using one or more operations over the specified axis.DataFrame.transformCallfuncon self producing a DataFrame with the same axis shape as self.NotesEach group is endowed the attribute \u2018name\u2019 in case you need to know\nwhich group you are working on.The current implementation imposes three requirements on f:f must return a value that either has the same shape as the input\nsubframe or can be broadcast to the shape of the input subframe.\nFor example, iffreturns a scalar it will be broadcast to have the\nsame shape as the input subframe.if this is a DataFrame, f must support application column-by-column\nin the subframe. If f also supports application to the entire subframe,\nthen a fast path is used starting from the second chunk.f must not mutate groups. Mutation is not supported and may\nproduce unexpected results. SeeMutating with User Defined Function (UDF) methodsfor more details.When usingengine='numba', there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,\nsee the examples below.Changed in version 2.0.0:When using.transformon a grouped DataFrame and the transformation function\nreturns a DataFrame, pandas now aligns the result\u2019s index\nwith the input\u2019s index. You can call.to_numpy()on the\nresult of the transformation function to avoid alignment.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':['one','one','two','three',...'two','two'],...'C':[1,5,5,2,5,5],...'D':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')[['C','D']]>>>grouped.transform(lambdax:(x-x.mean())/x.std())C         D0 -1.154701", "doc_id": "5728e558-3a32-4fc9-84d3-6f84336dbb1b", "embedding": null, "doc_hash": "3839c1487b25e44538d64c3bf34db36b40b58c404828d265bc4abc3925324dac", "extra_info": null, "node_info": {"start": 0, "end": 3563, "_node_type": "1"}, "relationships": {"1": "0714cd08-53e0-49a1-8b98-143d58cab6be", "3": "c323a586-a0df-4d0e-bdf2-26e4f305e39f"}}, "__type__": "1"}, "c323a586-a0df-4d0e-bdf2-26e4f305e39f": {"__data__": {"text": "DataFrame and the transformation function\nreturns a DataFrame, pandas now aligns the result\u2019s index\nwith the input\u2019s index. You can call.to_numpy()on the\nresult of the transformation function to avoid alignment.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':['one','one','two','three',...'two','two'],...'C':[1,5,5,2,5,5],...'D':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')[['C','D']]>>>grouped.transform(lambdax:(x-x.mean())/x.std())C         D0 -1.154701 -0.5773501  0.577350  0.0000002  0.577350  1.1547013 -1.154701 -1.0000004  0.577350 -0.5773505  0.577350  1.000000Broadcast result of the transformation>>>grouped.transform(lambdax:x.max()-x.min())C    D0  4.0  6.01  3.0  8.02  4.0  6.03  3.0  8.04  4.0  6.05  3.0  8.0>>>grouped.transform(\"mean\")C    D0  3.666667  4.01  4.000000  5.02  3.666667  4.03  4.000000  5.04  3.666667  4.05  4.000000  5.0Changed in version 1.3.0.The resulting dtype will reflect the return value of the passedfunc,\nfor example:>>>grouped.transform(lambdax:x.astype(int).max())C  D0  5  81  5  92  5  83  5  94  5  85  5  9", "doc_id": "c323a586-a0df-4d0e-bdf2-26e4f305e39f", "embedding": null, "doc_hash": "e42ff4423a80a8f1e16adea6d06819bc13da3afdc35d788501573378d9ddcedc", "extra_info": null, "node_info": {"start": 3070, "end": 4164, "_node_type": "1"}, "relationships": {"1": "0714cd08-53e0-49a1-8b98-143d58cab6be", "2": "5728e558-3a32-4fc9-84d3-6f84336dbb1b"}}, "__type__": "1"}, "f5752f7b-4a97-4587-bb31-36f754f3e0b1": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.pipe\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.pipe#SeriesGroupBy.pipe(func,*args,**kwargs)[source]#Apply afuncwith arguments to this GroupBy object and return its result.Use.pipewhen you want to improve readability by chaining together\nfunctions that expect Series, DataFrames, GroupBy or Resampler objects.\nInstead of writing>>>h=lambdax,arg2,arg3:x+1-arg2*arg3>>>g=lambdax,arg1:x*5/arg1>>>f=lambdax:x**4>>>df=pd.DataFrame([[\"a\",4],[\"b\",5]],columns=[\"group\",\"value\"])>>>h(g(f(df.groupby('group')),arg1=1),arg2=2,arg3=3)You can write>>>(df.groupby('group')....pipe(f)....pipe(g,arg1=1)....pipe(h,arg2=2,arg3=3))which is much more readable.Parameters:funccallable or tuple of (callable, str)Function to apply to this GroupBy object or, alternatively,\na(callable, data_keyword)tuple wheredata_keywordis a\nstring indicating the keyword ofcallablethat expects the\nGroupBy object.argsiterable, optionalPositional arguments passed intofunc.kwargsdict, optionalA dictionary of keyword arguments passed intofunc.Returns:the return type offunc.See alsoSeries.pipeApply a function with arguments to a series.DataFrame.pipeApply a function with arguments to a dataframe.applyApply function to each group instead of to the full GroupBy object.NotesSee morehereExamples>>>df=pd.DataFrame({'A':'a b a b'.split(),'B':[1,2,3,4]})>>>dfA  B0  a  11  b  22  a  33  b  4To get the difference between each groups maximum and minimum value in one\npass, you can do>>>df.groupby('A').pipe(lambdax:x.max()-x.min())BAa  2b  2", "doc_id": "f5752f7b-4a97-4587-bb31-36f754f3e0b1", "embedding": null, "doc_hash": "286ca137021e8841d32bdd7a1cdfb33ca5ddcd607e08bc4959495bdc6c1e9870", "extra_info": null, "node_info": {"start": 0, "end": 1571, "_node_type": "1"}, "relationships": {"1": "8aca9fc9-8d18-47ce-9f85-a54172b03b3a"}}, "__type__": "1"}, "8190afad-75df-4275-9a87-f8ef2bb47374": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.pipe\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.pipe#DataFrameGroupBy.pipe(func,*args,**kwargs)[source]#Apply afuncwith arguments to this GroupBy object and return its result.Use.pipewhen you want to improve readability by chaining together\nfunctions that expect Series, DataFrames, GroupBy or Resampler objects.\nInstead of writing>>>h=lambdax,arg2,arg3:x+1-arg2*arg3>>>g=lambdax,arg1:x*5/arg1>>>f=lambdax:x**4>>>df=pd.DataFrame([[\"a\",4],[\"b\",5]],columns=[\"group\",\"value\"])>>>h(g(f(df.groupby('group')),arg1=1),arg2=2,arg3=3)You can write>>>(df.groupby('group')....pipe(f)....pipe(g,arg1=1)....pipe(h,arg2=2,arg3=3))which is much more readable.Parameters:funccallable or tuple of (callable, str)Function to apply to this GroupBy object or, alternatively,\na(callable, data_keyword)tuple wheredata_keywordis a\nstring indicating the keyword ofcallablethat expects the\nGroupBy object.argsiterable, optionalPositional arguments passed intofunc.kwargsdict, optionalA dictionary of keyword arguments passed intofunc.Returns:the return type offunc.See alsoSeries.pipeApply a function with arguments to a series.DataFrame.pipeApply a function with arguments to a dataframe.applyApply function to each group instead of to the full GroupBy object.NotesSee morehereExamples>>>df=pd.DataFrame({'A':'a b a b'.split(),'B':[1,2,3,4]})>>>dfA  B0  a  11  b  22  a  33  b  4To get the difference between each groups maximum and minimum value in one\npass, you can do>>>df.groupby('A').pipe(lambdax:x.max()-x.min())BAa  2b  2", "doc_id": "8190afad-75df-4275-9a87-f8ef2bb47374", "embedding": null, "doc_hash": "e29f01af651bb77a3ed8e002171d5a142c9b7620ed79d091240a83b6def82610", "extra_info": null, "node_info": {"start": 0, "end": 1580, "_node_type": "1"}, "relationships": {"1": "2e430396-b7ac-4d84-aa68-ce9186a58b66"}}, "__type__": "1"}, "4f24f5ed-0c02-4433-862e-4d9661a565fc": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.filter\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.filter#DataFrameGroupBy.filter(func,dropna=True,*args,**kwargs)[source]#Filter elements from groups that don\u2019t satisfy a criterion.Elements from groups are filtered if they do not satisfy the\nboolean criterion specified by func.Parameters:funcfunctionCriterion to apply to each group. Should return True or False.dropnaboolDrop groups that do not pass the filter. True by default; if False,\ngroups that evaluate False are filled with NaNs.Returns:DataFrameNotesEach subframe is endowed the attribute \u2018name\u2019 in case you need to know\nwhich group you are working on.Functions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':[1,2,3,4,5,6],...'C':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')>>>grouped.filter(lambdax:x['B'].mean()>3.)A  B    C1  bar  2  5.03  bar  4  1.05  bar  6  9.0", "doc_id": "4f24f5ed-0c02-4433-862e-4d9661a565fc", "embedding": null, "doc_hash": "ad34b8f0f3b01716873f9e208b6a1bd23f6ff2079bd545251d7ce80709502654", "extra_info": null, "node_info": {"start": 0, "end": 1106, "_node_type": "1"}, "relationships": {"1": "b5df003e-3745-4368-b100-c7159a274bef"}}, "__type__": "1"}, "ef4a1199-a5bf-4ede-a330-f5f760425ea3": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.SeriesGroupBy.filter\u3010Content\u3011pandas.core.groupby.SeriesGroupBy.filter#SeriesGroupBy.filter(func,dropna=True,*args,**kwargs)[source]#Filter elements from groups that don\u2019t satisfy a criterion.Elements from groups are filtered if they do not satisfy the\nboolean criterion specified by func.Parameters:funcfunctionCriterion to apply to each group. Should return True or False.dropnaboolDrop groups that do not pass the filter. True by default; if False,\ngroups that evaluate False are filled with NaNs.Returns:SeriesNotesFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':[1,2,3,4,5,6],...'C':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')>>>df.groupby('A').B.filter(lambdax:x.mean()>3.)1    23    45    6Name: B, dtype: int64", "doc_id": "ef4a1199-a5bf-4ede-a330-f5f760425ea3", "embedding": null, "doc_hash": "1feda7fea7b1ff2e0192f2cc975a5d3491010b6925d8c23967327c42de56a69a", "extra_info": null, "node_info": {"start": 0, "end": 985, "_node_type": "1"}, "relationships": {"1": "c552fae2-67f4-4942-984e-ddf5d22639ef"}}, "__type__": "1"}, "709d7431-7e5b-4f36-b7ac-b624f15c67c4": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.all\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.all#DataFrameGroupBy.all(skipna=True)[source]#Return True if all values in the group are truthful, else False.Parameters:skipnabool, default TrueFlag to ignore nan values during truth testing.Returns:Series or DataFrameDataFrame or Series of boolean values, where a value is True if all elements\nare True within its respective group, False otherwise.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,0],index=lst)>>>sera    1a    2b    0dtype: int64>>>ser.groupby(level=0).all()a     Trueb    Falsedtype: boolFor DataFrameGroupBy:>>>data=[[1,0,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"ostrich\",\"penguin\",\"parrot\"])>>>dfa  b  costrich  1  0  3penguin  1  5  6parrot   7  8  9>>>df.groupby(by=[\"a\"]).all()b      ca1  False   True7   True   True", "doc_id": "709d7431-7e5b-4f36-b7ac-b624f15c67c4", "embedding": null, "doc_hash": "dd138000cef05ea16010c6992843c1bdf9daf291b5ba8bf32f0a29ee77992a50", "extra_info": null, "node_info": {"start": 0, "end": 1056, "_node_type": "1"}, "relationships": {"1": "95172a2e-3b72-46a9-87c9-b8510807ae32"}}, "__type__": "1"}, "d8ee9482-e0c1-4599-8bfd-b06e3d0fccbb": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.any\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.any#DataFrameGroupBy.any(skipna=True)[source]#Return True if any value in the group is truthful, else False.Parameters:skipnabool, default TrueFlag to ignore nan values during truth testing.Returns:Series or DataFrameDataFrame or Series of boolean values, where a value is True if any element\nis True within its respective group, False otherwise.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,0],index=lst)>>>sera    1a    2b    0dtype: int64>>>ser.groupby(level=0).any()a     Trueb    Falsedtype: boolFor DataFrameGroupBy:>>>data=[[1,0,3],[1,0,6],[7,1,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"ostrich\",\"penguin\",\"parrot\"])>>>dfa  b  costrich  1  0  3penguin  1  0  6parrot   7  1  9>>>df.groupby(by=[\"a\"]).any()b      ca1  False   True7   True   True", "doc_id": "d8ee9482-e0c1-4599-8bfd-b06e3d0fccbb", "embedding": null, "doc_hash": "94b2e9bcb011ec00487dea4dbbe83dd73a2dc2df0f4e3925149a5f10210f1542", "extra_info": null, "node_info": {"start": 0, "end": 1052, "_node_type": "1"}, "relationships": {"1": "44bf1b1b-0aa0-4875-b9bf-9ed48f21e0e4"}}, "__type__": "1"}, "e0207929-9fc7-4e6d-96e1-cd947a7897ec": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.bfill\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.bfill#DataFrameGroupBy.bfill(limit=None)[source]#Backward fill the values.Parameters:limitint, optionalLimit of how many values to fill.Returns:Series or DataFrameObject with missing values filled.See alsoSeries.bfillBackward fill the missing values in the dataset.DataFrame.bfillBackward fill the missing values in the dataset.Series.fillnaFill NaN values of a Series.DataFrame.fillnaFill NaN values of a DataFrame.ExamplesWith Series:>>>index=['Falcon','Falcon','Parrot','Parrot','Parrot']>>>s=pd.Series([None,1,None,None,3],index=index)>>>sFalcon    NaNFalcon    1.0Parrot    NaNParrot    NaNParrot    3.0dtype: float64>>>s.groupby(level=0).bfill()Falcon    1.0Falcon    1.0Parrot    3.0Parrot    3.0Parrot    3.0dtype: float64>>>s.groupby(level=0).bfill(limit=1)Falcon    1.0Falcon    1.0Parrot    NaNParrot    3.0Parrot    3.0dtype: float64With DataFrame:>>>df=pd.DataFrame({'A':[1,None,None,None,4],...'B':[None,None,5,None,7]},index=index)>>>dfA         BFalcon  1.0       NaNFalcon  NaN       NaNParrot  NaN       5.0Parrot  NaN       NaNParrot  4.0       7.0>>>df.groupby(level=0).bfill()A         BFalcon  1.0       NaNFalcon  NaN       NaNParrot  4.0       5.0Parrot  4.0       7.0Parrot  4.0       7.0>>>df.groupby(level=0).bfill(limit=1)A         BFalcon  1.0       NaNFalcon  NaN       NaNParrot  NaN       5.0Parrot  4.0       7.0Parrot  4.0       7.0", "doc_id": "e0207929-9fc7-4e6d-96e1-cd947a7897ec", "embedding": null, "doc_hash": "8c348a5da76a5ddcf616860d0d250f3739ca89d311e5d0151d63fac07b6c348f", "extra_info": null, "node_info": {"start": 0, "end": 1491, "_node_type": "1"}, "relationships": {"1": "2e075493-319a-4650-9bc8-b5737210bfef"}}, "__type__": "1"}, "29a8001a-d314-474c-83ad-8e1af11864fe": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.corr\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.corr#DataFrameGroupBy.corr(method='pearson',min_periods=1,numeric_only=False)[source]#Compute pairwise correlation of columns, excluding NA/null values.Parameters:method{\u2018pearson\u2019, \u2018kendall\u2019, \u2018spearman\u2019} or callableMethod of correlation:pearson : standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: callable with input two 1d ndarraysand returning a float. Note that the returned matrix from corr\nwill have 1 along the diagonals and will be symmetric\nregardless of the callable\u2019s behavior.min_periodsint, optionalMinimum number of observations required per pair of columns\nto have a valid result. Currently only available for Pearson\nand Spearman correlation.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:DataFrameCorrelation matrix.See alsoDataFrame.corrwithCompute pairwise correlation with another DataFrame or Series.Series.corrCompute the correlation between two Series.NotesPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.Pearson correlation coefficientKendall rank correlation coefficientSpearman\u2019s rank correlation coefficientExamples>>>defhistogram_intersection(a,b):...v=np.minimum(a,b).sum().round(decimals=1)...returnv>>>df=pd.DataFrame([(.2,.3),(.0,.6),(.6,.0),(.2,.1)],...columns=['dogs','cats'])>>>df.corr(method=histogram_intersection)dogs  catsdogs   1.0   0.3cats   0.3   1.0>>>df=pd.DataFrame([(1,1),(2,np.nan),(np.nan,3),(4,4)],...columns=['dogs','cats'])>>>df.corr(min_periods=3)dogs  catsdogs   1.0   NaNcats   NaN   1.0", "doc_id": "29a8001a-d314-474c-83ad-8e1af11864fe", "embedding": null, "doc_hash": "df8d9f14e92b42d2aa3256a7041109d03371ae209a397cda4fc0ef36abffefc3", "extra_info": null, "node_info": {"start": 0, "end": 1809, "_node_type": "1"}, "relationships": {"1": "ad83b1b1-be3d-4745-a766-66364ea1fad0"}}, "__type__": "1"}, "6d9b81a9-65e0-40a8-9921-2faf24156cbc": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.corrwith\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.corrwith#DataFrameGroupBy.corrwith(other,axis=_NoDefault.no_default,drop=False,method='pearson',numeric_only=False)[source]#Compute pairwise correlation.Pairwise correlation is computed between rows or columns of\nDataFrame with rows or columns of Series or DataFrame. DataFrames\nare first aligned along both axes before computing the\ncorrelations.Parameters:otherDataFrame, SeriesObject with which to compute correlations.axis{0 or \u2018index\u2019, 1 or \u2018columns\u2019}, default 0The axis to use. 0 or \u2018index\u2019 to compute row-wise, 1 or \u2018columns\u2019 for\ncolumn-wise.dropbool, default FalseDrop missing indices from result.method{\u2018pearson\u2019, \u2018kendall\u2019, \u2018spearman\u2019} or callableMethod of correlation:pearson : standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: callable with input two 1d ndarraysand returning a float.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:SeriesPairwise correlations.See alsoDataFrame.corrCompute pairwise correlation of columns.Examples>>>index=[\"a\",\"b\",\"c\",\"d\",\"e\"]>>>columns=[\"one\",\"two\",\"three\",\"four\"]>>>df1=pd.DataFrame(np.arange(20).reshape(5,4),index=index,columns=columns)>>>df2=pd.DataFrame(np.arange(16).reshape(4,4),index=index[:4],columns=columns)>>>df1.corrwith(df2)one      1.0two      1.0three    1.0four     1.0dtype: float64>>>df2.corrwith(df1,axis=1)a    1.0b    1.0c    1.0d    1.0e    NaNdtype: float64", "doc_id": "6d9b81a9-65e0-40a8-9921-2faf24156cbc", "embedding": null, "doc_hash": "db05626b2bee8ed7df6b8d1136cb8f612b71b0d640401575c45b28235e8ba5f1", "extra_info": null, "node_info": {"start": 0, "end": 1645, "_node_type": "1"}, "relationships": {"1": "5a022d8f-5593-4a88-bbae-51390f4e901c"}}, "__type__": "1"}, "b01adedf-ecf1-454c-a4b1-fe91b485fd75": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.count\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.count#DataFrameGroupBy.count()[source]#Compute count of group, excluding missing values.Returns:Series or DataFrameCount of values within each group.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,np.nan],index=lst)>>>sera    1.0a    2.0b    NaNdtype: float64>>>ser.groupby(level=0).count()a    2b    0dtype: int64For DataFrameGroupBy:>>>data=[[1,np.nan,3],[1,np.nan,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[\"a\",\"b\",\"c\"],...index=[\"cow\",\"horse\",\"bull\"])>>>dfa         b     ccow     1       NaN     3horse   1       NaN     6bull    7       8.0     9>>>df.groupby(\"a\").count()b   ca1   0   27   1   1For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').count()2023-01-01    22023-02-01    2Freq: MS, dtype: int64", "doc_id": "b01adedf-ecf1-454c-a4b1-fe91b485fd75", "embedding": null, "doc_hash": "30d6206655c70221baa83fb668410af18112f5e27ae373bfc6c8bf9681f7d339", "extra_info": null, "node_info": {"start": 0, "end": 1167, "_node_type": "1"}, "relationships": {"1": "8d9f2722-ba9a-4424-be92-6241c197a883"}}, "__type__": "1"}, "ba378902-1ece-4e34-a676-3390e8f55544": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.cov\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.cov#DataFrameGroupBy.cov(min_periods=None,ddof=1,numeric_only=False)[source]#Compute pairwise covariance of columns, excluding NA/null values.Compute the pairwise covariance among the series of a DataFrame.\nThe returned data frame is thecovariance matrixof the columns\nof the DataFrame.Both NA and null values are automatically excluded from the\ncalculation. (See the note below about bias from missing values.)\nA threshold can be set for the minimum number of\nobservations for each value created. Comparisons with observations\nbelow this threshold will be returned asNaN.This method is generally used for the analysis of time series data to\nunderstand the relationship between different measures\nacross time.Parameters:min_periodsint, optionalMinimum number of observations required per pair of columns\nto have a valid result.ddofint, default 1Delta degrees of freedom. The divisor used in calculations\nisN-ddof, whereNrepresents the number of elements.\nThis argument is applicable only when nonanis in the dataframe.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:DataFrameThe covariance matrix of the series of the DataFrame.See alsoSeries.covCompute covariance with another Series.core.window.ewm.ExponentialMovingWindow.covExponential weighted sample covariance.core.window.expanding.Expanding.covExpanding sample covariance.core.window.rolling.Rolling.covRolling sample covariance.NotesReturns the covariance matrix of the DataFrame\u2019s time series.\nThe covariance is normalized by N-ddof.For DataFrames that have Series that are missing data (assuming that\ndata ismissing at random)\nthe returned covariance matrix will be an unbiased estimate\nof the variance and covariance between the member Series.However, for many applications this estimate may not be acceptable\nbecause the estimate covariance matrix is not guaranteed to be positive\nsemi-definite. This could lead to estimate correlations having\nabsolute values which are greater than one, and/or a non-invertible\ncovariance matrix. SeeEstimation of covariance matricesfor more details.Examples>>>df=pd.DataFrame([(1,2),(0,3),(2,0),(1,1)],...columns=['dogs','cats'])>>>df.cov()dogs      catsdogs  0.666667 -1.000000cats -1.000000  1.666667>>>np.random.seed(42)>>>df=pd.DataFrame(np.random.randn(1000,5),...columns=['a','b','c','d','e'])>>>df.cov()a         b         c         d         ea  0.998438 -0.020161  0.059277 -0.008943  0.014144b -0.020161  1.059352 -0.008543 -0.024738  0.009826c  0.059277 -0.008543  1.010670 -0.001486 -0.000271d -0.008943 -0.024738 -0.001486  0.921297 -0.013692e  0.014144  0.009826 -0.000271 -0.013692  0.977795Minimum number of periodsThis method also supports an optionalmin_periodskeyword\nthat specifies the required minimum number of non-NA observations for\neach column pair in order to have a valid", "doc_id": "ba378902-1ece-4e34-a676-3390e8f55544", "embedding": null, "doc_hash": "038734afffdb530105fc4be741063f1a2aafa81e91c38731f65a08ff76439a8e", "extra_info": null, "node_info": {"start": 0, "end": 3035, "_node_type": "1"}, "relationships": {"1": "f7794030-6743-4ce5-a623-b44c5f86ee64", "3": "7783bb37-2070-46b6-a2e5-56ee9a7b9048"}}, "__type__": "1"}, "7783bb37-2070-46b6-a2e5-56ee9a7b9048": {"__data__": {"text": "        ea  0.998438 -0.020161  0.059277 -0.008943  0.014144b -0.020161  1.059352 -0.008543 -0.024738  0.009826c  0.059277 -0.008543  1.010670 -0.001486 -0.000271d -0.008943 -0.024738 -0.001486  0.921297 -0.013692e  0.014144  0.009826 -0.000271 -0.013692  0.977795Minimum number of periodsThis method also supports an optionalmin_periodskeyword\nthat specifies the required minimum number of non-NA observations for\neach column pair in order to have a valid result:>>>np.random.seed(42)>>>df=pd.DataFrame(np.random.randn(20,3),...columns=['a','b','c'])>>>df.loc[df.index[:5],'a']=np.nan>>>df.loc[df.index[5:10],'b']=np.nan>>>df.cov(min_periods=12)a         b         ca  0.316741       NaN -0.150812b       NaN  1.248003  0.191417c -0.150812  0.191417  0.895202", "doc_id": "7783bb37-2070-46b6-a2e5-56ee9a7b9048", "embedding": null, "doc_hash": "109186a43b34231796e0cd364951c18444a4f3f59d31095bdbc62b903fa51629", "extra_info": null, "node_info": {"start": 2579, "end": 3339, "_node_type": "1"}, "relationships": {"1": "f7794030-6743-4ce5-a623-b44c5f86ee64", "2": "ba378902-1ece-4e34-a676-3390e8f55544"}}, "__type__": "1"}, "09eeda75-c5f5-4247-ba31-fb770511b3fa": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011GroupBy\u3010Section\u3011pandas.core.groupby.DataFrameGroupBy.cumcount\u3010Content\u3011pandas.core.groupby.DataFrameGroupBy.cumcount#DataFrameGroupBy.cumcount(ascending=True)[source]#Number each item in each group from 0 to the length of that group - 1.Essentially this is equivalent toself.apply(lambdax:pd.Series(np.arange(len(x)),x.index))Parameters:ascendingbool, default TrueIf False, number in reverse, from length of group - 1 to 0.Returns:SeriesSequence number of each element within each group.See alsongroupNumber the groups themselves.Examples>>>df=pd.DataFrame([['a'],['a'],['a'],['b'],['b'],['a']],...columns=['A'])>>>dfA0  a1  a2  a3  b4  b5  a>>>df.groupby('A').cumcount()0    01    12    23    04    15    3dtype: int64>>>df.groupby('A').cumcount(ascending=False)0    31    22    13    14    05    0dtype: int64", "doc_id": "09eeda75-c5f5-4247-ba31-fb770511b3fa", "embedding": null, "doc_hash": "944edc07c21e5a01e392d810ecd7fd987e1e6c1c1eefd1e56fb1ad33094ed561", "extra_info": null, "node_info": {"start": 0, "end": 831, "_node_type": "1"}, "relationships": {"1": "0792b78e-c22a-4dce-92ba-9ce97388a7fe"}}, "__type__": "1"}, "bc163a48-8492-40bb-b81e-35b0f0b7bc5f": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_csv\u3010Content\u3011The `pandas.read_csv()` function is used to read a comma-separated values (csv) file into a DataFrame. It has multiple parameters that allow for customization of the import process.Here is a summary of the important parameters:- `filepath_or_buffer`: Accepts a string path or file-like object. Can also accept a URL.- `sep`: Specifies the character or regex pattern to treat as the delimiter. The default is ','.- `header`: Specifies the row number(s) containing column labels. By default, it will infer the column names from the first line of the file.- `names`: Specifies a sequence of column labels to apply. If the file has a header row, this parameter should be set to 0 to override the column names.- `index_col`: Specifies the column(s) to use as row labels. Can be specified by column labels or indices.- `usecols`: Specifies a subset of columns to select, either by column labels or indices.- `dtype`: Specifies the data type(s) to apply to the dataset or individual columns.- `skiprows`: Specifies line numbers to skip at the start of the file.- `na_values`: Specifies additional strings to recognize as NaN.- `parse_dates`: Specifies columns to parse as dates.- `encoding`: Specifies the encoding of the file.- `quotechar`: Specifies the character used to denote the start and end of a quoted item. Quoted items can include the delimiter, and it will be ignored.- `quoting`: Specifies the field quoting behavior. Can be one of four constants.- `skip_blank_lines`: Specifies whether to skip over blank lines rather than interpreting them as NaN values.- `float_precision`: Specifies the converter the C engine should use for floating-point values.The function returns a DataFrame, which is a two-dimensional data structure with labeled axes.Example usage:```import pandas as pddata = pd.read_csv('data.csv')print(data)```This will read the 'data.csv' file and store its contents into a DataFrame named 'data'.", "doc_id": "bc163a48-8492-40bb-b81e-35b0f0b7bc5f", "embedding": null, "doc_hash": "18cb98b3fb1fc3219d097d8552e001e4d3deddd913bb8b506c3bfc3759432323", "extra_info": null, "node_info": {"start": 0, "end": 1985, "_node_type": "1"}, "relationships": {"1": "7fdce276-149f-4429-b3d9-0c4047f598bc"}}, "__type__": "1"}, "5bb93fd6-50a1-40b5-aa75-ea3e5843e87e": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_csv\u3010Content\u3011The Numpy reference is describing the `to_csv` function in the pandas.DataFrame module. The `to_csv` function is used to write a pandas DataFrame object to a comma-separated values (csv) file. The function has several parameters that allow you to customize the output file.The first parameter, `path_or_buf`, specifies the path to the output file. It can be a string representing the file path, a path object, a file-like object, or None. If None, the result is returned as a string. The `sep` parameter specifies the field delimiter for the output file, which is a comma by default.The `na_rep` parameter specifies the representation for missing data, which is an empty string by default. The `float_format` parameter allows you to specify the format string for floating-point numbers. If a Callable is given, it takes precedence over other numeric formatting parameters.The `columns` parameter specifies the columns to write to the file, and the `header` parameter controls whether or not to write out the column names. The `index` parameter controls whether to write row names (index), and the `index_label` parameter specifies the column label for the index column(s) if desired.The `mode` parameter controls the file opening mode, such as 'w' for truncating the file first, 'x' for exclusive creation, or 'a' for appending to the end of the file. The `encoding` parameter specifies the encoding to use in the output file, and the `compression` parameter allows for on-the-fly compression of the output data.The `quoting` parameter specifies the quoting behavior, and the `quotechar` parameter specifies the character used to quote fields. The `lineterminator` parameter specifies the newline character or character sequence to use in the output file.Other parameters include `chunksize` to specify the number of rows to write at a time, `date_format` to specify the format string for datetime objects, `doublequote` to control the quoting of quotechar inside a field, `escapechar` to specify the character used to escape sep and quotechar when appropriate, `decimal` to specify the character recognized as the decimal separator, `errors` to specify how encoding and decoding errors are handled, and `storage_options` for extra options that make sense for a particular storage connection.The function returns None if the `path_or_buf` parameter is provided, or the resulting csv format as a string if it is None.The reference also includes examples of how to use the `to_csv` function to create a csv file and to write a csv file to a new folder or nested folder using pathlib or os.Overall, the `to_csv` function in the pandas.DataFrame module provides a flexible way to write DataFrame objects to csv files with customizable settings for various aspects of the output file.", "doc_id": "5bb93fd6-50a1-40b5-aa75-ea3e5843e87e", "embedding": null, "doc_hash": "331878db8574fa88f09e49dcdf740b8de4d03494e597f815de4dbe75bbde8bde", "extra_info": null, "node_info": {"start": 0, "end": 2853, "_node_type": "1"}, "relationships": {"1": "3075b510-798f-47cd-88ed-f0a82fee5b13"}}, "__type__": "1"}, "e47d98dd-1f34-42e7-8a0e-ba9548ea22d3": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_excel\u3010Content\u3011The `pandas.read_excel` function is used to read an Excel file into a pandas DataFrame. It supports various file extensions including xls, xlsx, xlsm, xlsb, odf, ods, and odt. The function can read from a local file system or a URL.The `io` parameter specifies the file or file-like object to be read. It can be a string path, bytes, ExcelFile, xlrd.Book, path object, or file-like object. If a string path is provided, it can be a valid URL starting with http, ftp, s3, or file, or a local file path starting with file://localhost/.The `sheet_name` parameter is used to specify which sheet(s) of the Excel file to read. It can be a string, integer, list, or None. If a string is provided, it is interpreted as the sheet name. If an integer is provided, it is interpreted as the zero-indexed sheet position. If a list is provided, it is interpreted as a list of sheet names or positions. If None is provided, all worksheets are read.The `header` parameter specifies which row to use as the column labels of the parsed DataFrame. It can be an integer or a list of integers. If a list of integers is passed, those row positions will be combined into a MultiIndex. Use None if there is no header.The `names` parameter is a list of column names to use. If the file contains no header row, this parameter should be explicitly passed with header=None.The `index_col` parameter specifies which column to use as the row labels of the DataFrame. It can be an integer, string, or a list of integers or strings. If None is passed, there is no such column. If a list is passed, those columns will be combined into a MultiIndex. If a subset of data is selected with `usecols`, the `index_col` is based on the subset. Missing values will be forward-filled to allow roundtripping with `to_excel`, but you can avoid forward-filling by using `set_index` after reading the data instead of `index_col`.The `usecols` parameter specifies which columns to parse. It can be a string, list-like object, or a callable function. If None, all columns are parsed. If a string is provided, it indicates a comma-separated list of Excel column letters and column ranges. If a list of integers is provided, it indicates the column numbers to be parsed. If a list of strings is provided, it indicates the column names to be parsed. If a callable is provided, it evaluates each column name against it and parses the column if the callable returns True.The `dtype` parameter can be used to specify the data types for the data or columns in the DataFrame. It can be either a type name or a dictionary of column names to types. For example, {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32}. If None is used, it infers the dtype of each column based on the data. The `dtype` parameter is not applied if `converters` are specified.The `engine` parameter specifies the Excel engine to be used. It can be one of the following strings: 'openpyxl', 'calamine', 'odf', 'pyxlsb', or 'xlrd'. If the `engine` parameter is set to None, the function will determine the engine based on the file type.Other parameters include `converters` for specifying conversion functions for values in certain columns, `true_values` and `false_values` for specifying values to consider as True and False, `skiprows` for specifying rows to skip at the start of the file, `nrows` for specifying the number of rows to parse, `na_values` for specifying additional strings to recognize as NA/NaN, `keep_default_na` for specifying whether to include the default NaN values when parsing the data, `na_filter` for specifying whether to detect missing value markers, `verbose` for indicating the number of NA values placed in non-numeric", "doc_id": "e47d98dd-1f34-42e7-8a0e-ba9548ea22d3", "embedding": null, "doc_hash": "7f62fd5585106c14dcf910e41d3d6375bdb505706b798c8a641f96e6b0b10645", "extra_info": null, "node_info": {"start": 0, "end": 3716, "_node_type": "1"}, "relationships": {"1": "7babdc5b-e97d-4e9b-bb93-f2305ab5f9f5", "3": "822aa6b1-11da-480f-91b1-cf740e52f101"}}, "__type__": "1"}, "822aa6b1-11da-480f-91b1-cf740e52f101": {"__data__": {"text": "'odf', 'pyxlsb', or 'xlrd'. If the `engine` parameter is set to None, the function will determine the engine based on the file type.Other parameters include `converters` for specifying conversion functions for values in certain columns, `true_values` and `false_values` for specifying values to consider as True and False, `skiprows` for specifying rows to skip at the start of the file, `nrows` for specifying the number of rows to parse, `na_values` for specifying additional strings to recognize as NA/NaN, `keep_default_na` for specifying whether to include the default NaN values when parsing the data, `na_filter` for specifying whether to detect missing value markers, `verbose` for indicating the number of NA values placed in non-numeric columns, `parse_dates` for specifying how to parse date columns, `date_parser` for specifying a function for converting string columns to datetime instances, `date_format` for specifying the format of dates, `thousands` for specifying the thousands separator for parsing string columns to numeric, `decimal` for specifying the character to recognize as the decimal point, `comment` for indicating comments in the input file, `skipfooter` for specifying the number of rows to skip at the end of the file, `storage_options` for specifying extra options for a specific storage connection, `dtype_backend` for specifying the back-end data type applied to the resultant DataFrame, and `engine_kwargs` for specifying arbitrary keyword arguments passed to the Excel engine.The function returns a DataFrame or a dictionary of DataFrames, depending on the sheet_name argument. If a single sheet is read, a DataFrame is returned. If multiple sheets are read, a dictionary of DataFrames is returned, where the keys are the sheet names or positions.There are also examples provided in the documentation showing how to use the function with different parameters.", "doc_id": "822aa6b1-11da-480f-91b1-cf740e52f101", "embedding": null, "doc_hash": "54fb1449429894c1366e4279cba948e7cf6bf5365d129c82b74fb93229db9696", "extra_info": null, "node_info": {"start": 2970, "end": 4866, "_node_type": "1"}, "relationships": {"1": "7babdc5b-e97d-4e9b-bb93-f2305ab5f9f5", "2": "e47d98dd-1f34-42e7-8a0e-ba9548ea22d3"}}, "__type__": "1"}, "dd33def3-77f9-4df4-a879-852780f9fb86": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_excel\u3010Content\u3011pandas.DataFrame.to_excel#DataFrame.to_excel(excel_writer,*,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to\nspecify a target file name. To write to multiple sheets it is necessary to\ncreate anExcelWriterobject with a target file name, and specify a sheet\nin the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.\nWith all data written to the file it is necessary to save the changes.\nNote that creating anExcelWriterobject with a file name that already\nexists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default \u2018Sheet1\u2019Name of sheet which will contain DataFrame.na_repstr, default \u2018\u2019Missing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=\"%.2f\"will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is\nassumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A\nsequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this\nvia the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default \u2018inf\u2019Representation for infinity (there is no native representation for\ninfinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that\nis to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\nare forwarded tourllib.request.Requestas header options. For other\nURLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are\nforwarded tofsspec.open. Please seefsspecandurllibfor more\ndetails, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),\nto_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further\ndata without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row", "doc_id": "dd33def3-77f9-4df4-a879-852780f9fb86", "embedding": null, "doc_hash": "eb39fdd136a94cfcccc943c50ed5e0440811779771636b96828989e6c3957ebe", "extra_info": null, "node_info": {"start": 0, "end": 3483, "_node_type": "1"}, "relationships": {"1": "4f12c2b4-b82d-4fee-a208-b710e9a4acbf", "3": "d8fe4b83-f51b-4764-b630-78e12845aef9"}}, "__type__": "1"}, "d8fe4b83-f51b-4764-b630-78e12845aef9": {"__data__": {"text": "keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),\nto_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further\ndata without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(\"output.xlsx\")To specify the sheet name:>>>df1.to_excel(\"output.xlsx\",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is\nnecessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,\nyou can pass theenginekeyword (the default engine is\nautomatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')", "doc_id": "d8fe4b83-f51b-4764-b630-78e12845aef9", "embedding": null, "doc_hash": "c9c2928361db6cc3368e0fadab595890d325da01ea74e28bb8b399c5d77f505c", "extra_info": null, "node_info": {"start": 2816, "end": 4288, "_node_type": "1"}, "relationships": {"1": "4f12c2b4-b82d-4fee-a208-b710e9a4acbf", "2": "dd33def3-77f9-4df4-a879-852780f9fb86"}}, "__type__": "1"}, "3ab0c41b-19ef-41dc-bc9d-c4253badeb72": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.ExcelWriter\u3010Content\u3011The Numpy reference is a description of the `pandas.ExcelWriter` class in the pandas library. This class is used for writing DataFrame objects into excel sheets. The default engine used is `xlsxwriter` for xlsx files, if it is installed, otherwise `openpyxl` for ods files. The class provides several parameters to customize the writing process.The `path` parameter specifies the path to the xls or xlsx or ods file. The `engine` parameter is optional and specifies the engine to use for writing. If not specified, it defaults to `io.excel.<extension>.writer`. The `date_format` and `datetime_format` parameters allow you to specify the format strings for dates and datetime objects written into Excel files.The `mode` parameter specifies the file mode to use, either 'w' for write or 'a' for append. The `storage_options` parameter is optional and provides extra options for a particular storage connection, such as host, port, username, password, etc. The `if_sheet_exists` parameter specifies how to behave when trying to write to a sheet that already exists in append mode. It can be set to 'error' to raise a ValueError, 'new' to create a new sheet, 'replace' to delete the contents of the sheet before writing to it, or 'overlay' to write contents to the existing sheet without removing the existing contents.The `engine_kwargs` parameter is optional and allows you to pass keyword arguments to the engine. These arguments are then passed to the respective engine functions.The class can be used as a context manager or you can manually call the `close()` method to save and close any opened file handles.The reference provides several examples of how to use the `pandas.ExcelWriter` class. It shows how to write DataFrames to separate sheets in a single file, set the date and datetime format, append to an existing Excel file, write multiple DataFrames to a single sheet, store the file in RAM, and pack the file into a zip archive. It also demonstrates how to specify additional arguments to the underlying engine.The reference also provides information about the attributes and methods of the `pandas.ExcelWriter` class. The `book` attribute is a Book instance, the `date_format` attribute is the format string for dates written into Excel files, the `datetime_format` attribute is the format string for datetime objects written into Excel files, the `engine` attribute is the name of the engine being used, the `if_sheet_exists` attribute specifies how to behave when writing to an existing sheet in append mode, and the `sheets` attribute is a mapping of sheet names to sheet objects. The class also provides the `supported_extensions` attribute, which is a list of extensions that the writer engine supports.In addition, the class provides the `check_extension(ext)` method, which checks the extension of the given path against the supported extensions of the writer. It also provides the `close()` method, which is a synonym for `save()` to make it more file-like.", "doc_id": "3ab0c41b-19ef-41dc-bc9d-c4253badeb72", "embedding": null, "doc_hash": "7c6e4ccc56ef1df232d2b3aa553098bce33266f17e42b0e7b77091ec10956718", "extra_info": null, "node_info": {"start": 0, "end": 3047, "_node_type": "1"}, "relationships": {"1": "69725fae-7801-4fd9-ad21-5681431c98b1"}}, "__type__": "1"}, "03aad595-36e9-4bb4-bf59-861b745024ef": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_json\u3010Content\u3011The function `pandas.read_json()` is used to convert a JSON string into a pandas object, such as a Series or DataFrame. It takes several parameters:- `path_or_buf`: A valid JSON string, path, or file-like object. It can be a file path, a URL, or a file-like object (e.g., a file handle or `StringIO`).- `orient`: An optional parameter indicating the expected JSON string format. It can be one of the following: 'split', 'records', 'index', 'columns', 'values', or 'table'. The default value depends on the `typ` parameter.- `typ`: The type of object to recover. It can be either 'frame' (default) or 'series'.- `dtype`: If True, infer dtypes; if a dictionary of column to dtype, use those; if False, don't infer dtypes at all.- `convert_axes`: Try to convert the axes to the proper dtypes.- `convert_dates`: If True, default datelike columns may be converted (depending on `keep_default_dates`). If False, no dates will be converted. If a list of column names is provided, then those columns will be converted, along with the default datelike columns.- `keep_default_dates`: If parsing dates (`convert_dates` is not False), try to parse the default datelike columns.- `precise_float`: Set to True to enable the usage of higher precision (strtod) function when decoding string to double values.- `date_unit`: The timestamp unit to detect if converting dates. The default behavior is to try and detect the correct precision, but you can pass one of 's', 'ms', 'us', or 'ns' to force parsing only seconds, milliseconds, microseconds, or nanoseconds, respectively.- `encoding`: The encoding to use to decode bytes. Default is 'utf-8'.- `encoding_errors`: How encoding errors are treated. Default is 'strict'.- `lines`: Read the file as a JSON object per line. Default is False.- `chunksize`: Return a JsonReader object for iteration.- `compression`: For on-the-fly decompression of on-disk data. Default is 'infer'.- `nrows`: The number of lines from the line-delimited JSON file to be read.- `storage_options`: Extra options that make sense for a particular storage connection, such as host, port, username, password, etc.- `dtype_backend`: The back-end data type applied to the resultant DataFrame. Default is 'numpy_nullable'.- `engine`: Parser engine to use. Default is 'ujson', and 'pyarrow' is also available when `lines` is True.The function returns either a Series, DataFrame, or a JsonReader object if `chunksize` is not 0 or None.The documentation provides multiple usage examples with different parameter combinations. It includes examples of encoding and decoding DataFrames using various formats, such as 'split', 'index', 'records', and 'table'. It also demonstrates the use of the `dtype_backend` parameter for specifying the data type of the resultant DataFrame.", "doc_id": "03aad595-36e9-4bb4-bf59-861b745024ef", "embedding": null, "doc_hash": "7d42e3186339b2701181948f92485c3bd1eba5da37e4efa5dff0def144ccecc9", "extra_info": null, "node_info": {"start": 0, "end": 2841, "_node_type": "1"}, "relationships": {"1": "d293a774-e372-4a2f-91d3-996ae539b8c0"}}, "__type__": "1"}, "5dd517a0-9f85-4ad6-81a6-cf09a9283d7c": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_json\u3010Content\u3011The `to_json()` function in pandas is used to convert a DataFrame object into a JSON string. It has several parameters that allow you to customize the output format and behavior.The `path_or_buf` parameter specifies the destination where the JSON string will be written. It can be a string representing a file path, a path object, a file-like object with a `write()` function, or None, in which case the result is returned as a string.The `orient` parameter indicates the format of the JSON string. For a Series object, the default is 'index', and allowed values are 'split', 'records', 'index', and 'table'. For a DataFrame object, the default is 'columns', and allowed values are 'split', 'records', 'index', 'columns', 'values', and 'table'.The `date_format` parameter determines the type of date conversion in the JSON string. It can be set to None, 'epoch' (epoch milliseconds), or 'iso' (ISO8601).The `double_precision` parameter specifies the number of decimal places to use when encoding floating-point values.The `force_ascii` parameter determines whether the encoded string should be forced to be ASCII.The `date_unit` parameter specifies the time unit to encode to, governing timestamp and ISO8601 precision. The options are 's', 'ms' (default), 'us', and 'ns' for second, millisecond, microsecond, and nanosecond, respectively.The `default_handler` parameter is a callable that is called if an object cannot be converted to a suitable format for JSON.The `lines` parameter is a boolean indicating whether to write out the JSON string in line-delimited format when the `orient` is set to 'records'.The `compression` parameter allows for on-the-fly compression of the output data. It can be set to 'infer' to detect compression from the file extension if a file path is provided, or it can be a dict with a key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'}, with other key-value pairs forwarded to the corresponding compression file object.The `index` parameter is only used when the `orient` is set to 'split', 'index', 'column', or 'table'. It specifies whether to include the index in the JSON string output.The `indent` parameter specifies the length of whitespace used to indent each record in the JSON string.The `storage_options` parameter is a dictionary of extra options that make sense for a particular storage connection, such as host, port, username, password, etc.The `mode` parameter specifies the IO mode for output when supplying a `path_or_buf`. Accepted values are 'w' (writing) and 'a' (append), with 'a' only supported when `lines` is True and `orient` is 'records'.The function returns None if `path_or_buf` is provided, or the resulting JSON format as a string if `path_or_buf` is None.The examples provided demonstrate how to encode and decode a DataFrame using different formats such as 'split', 'records', 'index', 'columns', 'values', and 'table'.", "doc_id": "5dd517a0-9f85-4ad6-81a6-cf09a9283d7c", "embedding": null, "doc_hash": "0101ef85d2c2664a596e67030c956248c42d5b24deffadbb51dd7cc138a525b6", "extra_info": null, "node_info": {"start": 0, "end": 2979, "_node_type": "1"}, "relationships": {"1": "f63e30b1-fdc0-4c96-9270-96e84a5ffde2"}}, "__type__": "1"}, "ebf89484-857f-4198-bf2e-4d03147c9583": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_html\u3010Content\u3011The `pandas.read_html` function is used to read HTML tables into a list of DataFrame objects. It has the following parameters:- `io`: Takes a string, path object, or file-like object. The string can represent a URL or the HTML itself.- `match`: The set of tables containing text matching this regex or string will be returned. Defaults to \u2018.+\u2019 (match any non-empty string).- `flavor`: The parsing engine (or list of parsing engines) to use. Defaults to None, which tries to use lxml to parse and falls back on bs4+html5lib if lxml fails.- `header`: The row (or list of rows for a MultiIndex) to use to make the columns headers. Defaults to None.- `index_col`: The column (or list of columns) to use to create the index. Defaults to None.- `skiprows`: Number of rows to skip after parsing the column integer. 0-based. Defaults to None.- `attrs`: This is a dictionary of attributes that you can pass to use to identify the table in the HTML. Defaults to None.- `parse_dates`: Specifies whether to parse dates in the DataFrame. Defaults to False.- `thousands`: Separator to use to parse thousands. Defaults to ','.- `encoding`: The encoding used to decode the web page. Defaults to None.- `decimal`: Character to recognize as decimal point. Defaults to '.'.- `converters`: Dict of functions for converting values in certain columns. Defaults to None.- `na_values`: Iterable of custom NA values. Defaults to None.- `keep_default_na`: Specifies whether to override default NaN values with custom NA values. Defaults to True.- `displayed_only`: Specifies whether elements with \"display: none\" should be parsed. Defaults to True.- `extract_links`: Specifies the section(s) to extract links from. Defaults to None.- `dtype_backend`: Back-end data type applied to the resulting DataFrame. Defaults to 'numpy_nullable'.- `storage_options`: Extra options that make sense for a particular storage connection. Defaults to None.The function returns a list of DataFrames.Some important notes about using this function:- Before using this function, it is recommended to read the gotchas about the HTML parsing libraries.- Expect to do some cleanup after calling this function, such as manually assigning column names if they are converted to NaN when passing the `header=0` argument.- This function searches for `<table>` elements and only for `<tr>` and `<th>` rows and `<td>` elements within each `<tr>` or `<th>` element in the table.- If the function has a `<thead>` argument, it is used to construct the header, otherwise it attempts to find the header within the body.- The `header` argument is applied after `skiprows` is applied.- This function will always return a list of DataFrame or it will fail, it will not return an empty list.The documentation provides examples of reading HTML tables using the `read_html` function.", "doc_id": "ebf89484-857f-4198-bf2e-4d03147c9583", "embedding": null, "doc_hash": "4150321b0ad5c6f8be9718dc28f53bc71f0ec1580c82dc92579e5cab36ad4bdf", "extra_info": null, "node_info": {"start": 0, "end": 2883, "_node_type": "1"}, "relationships": {"1": "158a2b2a-be10-4cc6-ae00-78a57dcd1505"}}, "__type__": "1"}, "d27b50a4-d2f1-4564-b28d-51935ec380cc": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_html\u3010Content\u3011pandas.DataFrame.to_html#DataFrame.to_html(buf=None,*,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',bold_rows=True,classes=None,escape=True,notebook=False,border=None,table_id=None,render_links=False,encoding=None)[source]#Render a DataFrame as an HTML table.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spacestr or int, list or dict of int or str, optionalThe minimum width of each column in CSS length units. An int is assumed to be px units..headerbool, optionalWhether to print column labels, default True.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default \u2018NaN\u2019String representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columns\u2019 elements by position or\nname.\nThe result of each function must be a unicode string.\nList/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columns\u2019 elements if they are\nfloats. This function must return a unicode string and will be\napplied only to the non-NaNelements, withNaNbeing\nhandled byna_rep.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print\nevery multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from\nthe print configuration (controlled by set_option), \u2018right\u2019 out\nof the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default \u2018.\u2019Character recognized as decimal separator, e.g. \u2018,\u2019 in Europe.bold_rowsbool, default TrueMake the row labels bold in the output.classesstr or list or tuple, default NoneCSS class(es) to apply to the resulting html table.escapebool, default TrueConvert the characters <, >, and & to HTML-safe sequences.notebook{True, False}, default FalseWhether the generated HTML is for IPython Notebook.borderintAborder=borderattribute is included in the opening<table>tag. Defaultpd.options.display.html.border.table_idstr, optionalA css id is included in the opening<table>tag if specified.render_linksbool, default FalseConvert URLs to HTML links.encodingstr, default \u201cutf-8\u201dSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns\nNone.See alsoto_stringConvert DataFrame to a string.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>html_string='''<table border=\"1\" class=\"dataframe\">...<thead>...<tr style=\"text-align:", "doc_id": "d27b50a4-d2f1-4564-b28d-51935ec380cc", "embedding": null, "doc_hash": "d68597e2cc0e9f8f186e94c19a0dc59f288a4e076bd28c84dd3872ffc585d15f", "extra_info": null, "node_info": {"start": 0, "end": 3263, "_node_type": "1"}, "relationships": {"1": "ae01c664-6559-4e6d-99ff-d38c59e3edb8", "3": "e7d1eaae-ed92-43e2-8bdd-5481f7dc6433"}}, "__type__": "1"}, "e7d1eaae-ed92-43e2-8bdd-5481f7dc6433": {"__data__": {"text": "False}, default FalseWhether the generated HTML is for IPython Notebook.borderintAborder=borderattribute is included in the opening<table>tag. Defaultpd.options.display.html.border.table_idstr, optionalA css id is included in the opening<table>tag if specified.render_linksbool, default FalseConvert URLs to HTML links.encodingstr, default \u201cutf-8\u201dSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns\nNone.See alsoto_stringConvert DataFrame to a string.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>html_string='''<table border=\"1\" class=\"dataframe\">...<thead>...<tr style=\"text-align: right;\">...<th></th>...<th>col1</th>...<th>col2</th>...</tr>...</thead>...<tbody>...<tr>...<th>0</th>...<td>1</td>...<td>4</td>...</tr>...<tr>...<th>1</th>...<td>2</td>...<td>3</td>...</tr>...</tbody>...</table>'''>>>asserthtml_string==df.to_html()", "doc_id": "e7d1eaae-ed92-43e2-8bdd-5481f7dc6433", "embedding": null, "doc_hash": "70558bcc88d9f4eb9dd46a3d4928689b4422e966117ac4a9426e366db9a1b9d9", "extra_info": null, "node_info": {"start": 2607, "end": 3512, "_node_type": "1"}, "relationships": {"1": "ae01c664-6559-4e6d-99ff-d38c59e3edb8", "2": "d27b50a4-d2f1-4564-b28d-51935ec380cc"}}, "__type__": "1"}, "b41da508-e92f-4d3f-a2da-23486ab1b7fa": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_xml\u3010Content\u3011The `read_xml` function in the pandas library allows you to read XML documents and convert them into a DataFrame object. It was introduced in version 1.3.0 of pandas.The function has the following parameters:- `path_or_buffer` (required): It is a string, path object, or a file-like object that contains the XML document. It can also be a URL.- `xpath` (optional, default='./*'): The XPath used to parse the XML document. It should return a collection of elements.- `namespaces` (optional): A dictionary that contains the namespaces defined in the XML document.- `elems_only` (optional, default=False): If True, only the child elements at the specified XPath will be parsed.- `attrs_only` (optional, default=False): If True, only the attributes at the specified XPath will be parsed.- `names` (optional): A list-like object that contains the column names for the DataFrame.- `dtype` (optional): The data type for the data or columns. It can be a type name or a dictionary of column -> type.- `converters` (optional): A dictionary of functions for converting values in certain columns.- `parse_dates` (optional): Identifiers to parse the index or columns to datetime.- `encoding` (optional, default='utf-8'): The encoding of the XML document.- `parser` (optional, default='lxml'): The parser module to use for retrieval of data. Only 'lxml' and 'etree' are supported.- `stylesheet` (optional): An XSLT script used to flatten complex, deeply nested XML documents for easier parsing.- `iterparse` (optional): The nodes or attributes to retrieve in iterparsing of the XML document.- `compression` (optional, default='infer'): The compression method to use for on-the-fly decompression of on-disk data.- `storage_options` (optional): Extra options that make sense for a particular storage connection.- `dtype_backend` (optional, default='numpy_nullable'): The back-end data type applied to the resultant DataFrame.The function returns a DataFrame.The examples provided in the documentation show how to use the `read_xml` function with different XML documents. It demonstrates how to parse XML documents with different structures using the `xpath` parameter, how to handle namespaces, and how to specify data types and converters. It also provides examples of using the `parse_dates` parameter to parse dates in the XML document.Overall, the `read_xml` function is a convenient way to import XML documents into pandas DataFrames, particularly for shallow XML documents with a specific structure. However, for more complex XML documents, the function provides the `stylesheet` parameter for temporary redesign of the original document with XSLT.", "doc_id": "b41da508-e92f-4d3f-a2da-23486ab1b7fa", "embedding": null, "doc_hash": "7a7493739f1c796f01faaeb0e32e4f8cffe0f5e4acd86d68184a1a348ddada28", "extra_info": null, "node_info": {"start": 0, "end": 2704, "_node_type": "1"}, "relationships": {"1": "6104effe-e002-4020-94ca-7edc75d64da6"}}, "__type__": "1"}, "ed6cbbc5-d833-4214-a6a4-a702a4d8d5d3": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_xml\u3010Content\u3011The DataFrame.to_xml() function in the pandas library allows you to render a DataFrame into an XML document. This function was introduced in version 1.3.0 of pandas.The function takes several parameters:- path_or_buffer: This parameter specifies the output destination for the XML document. It can be a string representing a file path, a path object, a file-like object with a write() function, or None. If this parameter is None, the result is returned as a string.- index: This boolean parameter determines whether to include the index in the XML document. By default, it is set to True.- root_name: This parameter specifies the name of the root element in the XML document. The default name is 'data'.- row_name: This parameter specifies the name of the row element in the XML document. The default name is 'row'.- na_rep: This optional parameter specifies the representation for missing data. If not specified, missing values will be omitted from the XML document.- attr_cols: This optional parameter is a list-like object that specifies which columns should be written as attributes in the row element. If hierarchical columns are present, they will be flattened with underscores delimiting the different levels.- elem_cols: This optional parameter is a list-like object that specifies which columns should be written as children in the row element. By default, all columns are output as children of the row element. If hierarchical columns are present, they will be flattened with underscores delimiting the different levels.- namespaces: This optional parameter is a dictionary that specifies all namespaces to be defined in the root element. Keys of the dictionary should be prefix names and values should be corresponding URIs. Default namespaces should be given an empty string key.- prefix: This optional parameter specifies the namespace prefix to be used for every element and/or attribute in the document. This should be one of the keys in the namespaces dictionary.- encoding: This parameter specifies the encoding of the resulting document. The default encoding is 'utf-8'.- xml_declaration: This boolean parameter determines whether to include the XML declaration at the start of the document. By default, it is set to True.- pretty_print: This boolean parameter determines whether the output should be pretty printed with indentation and line breaks. By default, it is set to True.- parser: This parameter specifies the parser module to use for building the XML tree. Only 'lxml' and 'etree' are supported. By default, 'lxml' is used.- stylesheet: This optional parameter specifies an XSLT script used to transform the raw XML output. The script should use the layout of elements and attributes from the original output. This parameter requires the 'lxml' parser to be installed. Only XSLT 1.0 scripts are currently supported.- compression: This parameter allows for on-the-fly compression of the output data. The compression method can be automatically detected from the file extension in the path_or_buffer parameter. It can also be specified as a dictionary with additional compression options.- storage_options: This optional parameter allows for additional options that make sense for a particular storage connection, such as host, port, username, password, etc.The function returns None if the path_or_buffer parameter is specified. Otherwise, it returns the resulting XML format as a string.Here are some examples of using the DataFrame.to_xml() function:``` pythondf = pd.DataFrame({'shape': ['square', 'circle', 'triangle'],                   'degrees': [360, 360, 180],                   'sides': [4, np.nan, 3]})df.to_xml()```Output:``` xml<?xml version='1.0'", "doc_id": "ed6cbbc5-d833-4214-a6a4-a702a4d8d5d3", "embedding": null, "doc_hash": "29b689ee2dcd08bc6efe40fd6e6cd2c329b90f152f38b6a17db4c3d7b004b435", "extra_info": null, "node_info": {"start": 0, "end": 3762, "_node_type": "1"}, "relationships": {"1": "dd0e3112-c89c-457d-b957-3d5171c61020", "3": "6f31858b-1ffe-4157-83fb-ef2518ab5f42"}}, "__type__": "1"}, "6f31858b-1ffe-4157-83fb-ef2518ab5f42": {"__data__": {"text": "dictionary with additional compression options.- storage_options: This optional parameter allows for additional options that make sense for a particular storage connection, such as host, port, username, password, etc.The function returns None if the path_or_buffer parameter is specified. Otherwise, it returns the resulting XML format as a string.Here are some examples of using the DataFrame.to_xml() function:``` pythondf = pd.DataFrame({'shape': ['square', 'circle', 'triangle'],                   'degrees': [360, 360, 180],                   'sides': [4, np.nan, 3]})df.to_xml()```Output:``` xml<?xml version='1.0' encoding='utf-8'?><data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data>`````` pythondf.to_xml(attr_cols=['index', 'shape', 'degrees', 'sides'])```Output:``` xml<?xml version='1.0' encoding='utf-8'?><data><row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/><row index=\"1\" shape=\"circle\" degrees=\"360\"/><row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/></data>`````` pythondf.to_xml(namespaces={\"doc\": \"https://example.com\"}, prefix=\"doc\")```Output:``` xml<?xml version='1.0' encoding='utf-8'?><doc:data xmlns:doc=\"https://example.com\"><doc:row><doc:index>0</doc:index><doc:shape>square</doc:shape><doc:degrees>360</doc:degrees><doc:sides>4.0</doc:sides></doc:row><doc:row><doc:index>1</doc:index><doc:shape>circle</doc:shape><doc:degrees>360</doc:degrees><doc:sides/></doc:row><doc:row><doc:index>2</doc:index><doc:shape>triangle</doc:shape><doc:degrees>180</doc:degrees><doc:sides>3.0</doc:sides></doc:row></doc:data>```", "doc_id": "6f31858b-1ffe-4157-83fb-ef2518ab5f42", "embedding": null, "doc_hash": "f85f1923e32b8385f5f84db974a9cef131402a54b9cb21a0e6303744389d707b", "extra_info": null, "node_info": {"start": 3142, "end": 4913, "_node_type": "1"}, "relationships": {"1": "dd0e3112-c89c-457d-b957-3d5171c61020", "2": "ed6cbbc5-d833-4214-a6a4-a702a4d8d5d3"}}, "__type__": "1"}, "66ec1340-36b7-40ac-91a2-985effb605f1": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_hdf\u3010Content\u3011pandas.read_hdf#pandas.read_hdf(path_or_buf,key=None,mode='r',errors='strict',where=None,start=None,stop=None,columns=None,iterator=False,chunksize=None,**kwargs)[source]#Read from the store, close it if we opened it.Retrieve pandas object stored in file, optionally based on where\ncriteria.WarningPandas uses PyTables for reading and writing HDF5 files, which allows\nserializing object-dtype data with pickle when using the \u201cfixed\u201d format.\nLoading pickled data received from untrusted sources can be unsafe.See:https://docs.python.org/3/library/pickle.htmlfor more.Parameters:path_or_bufstr, path object, pandas.HDFStoreAny valid string path is acceptable. Only supports the local file system,\nremote URLs and file-like objects are not supported.If you want to pass in a path object, pandas accepts anyos.PathLike.Alternatively, pandas accepts an openpandas.HDFStoreobject.keyobject, optionalThe group identifier in the store. Can be omitted if the HDF file\ncontains a single pandas object.mode{\u2018r\u2019, \u2018r+\u2019, \u2018a\u2019}, default \u2018r\u2019Mode to use when opening the file. Ignored if path_or_buf is apandas.HDFStore. Default is \u2018r\u2019.errorsstr, default \u2018strict\u2019Specifies how encoding and decoding errors are to be handled.\nSee the errors argument foropen()for a full list\nof options.wherelist, optionalA list of Term (or convertible) objects.startint, optionalRow number to start selection.stopint, optionalRow number to stop selection.columnslist, optionalA list of columns names to return.iteratorbool, optionalReturn an iterator object.chunksizeint, optionalNumber of rows to include in an iteration when using an iterator.**kwargsAdditional keyword arguments passed to HDFStore.Returns:objectThe selected object. Return type depends on the object stored.See alsoDataFrame.to_hdfWrite a HDF file from a DataFrame.HDFStoreLow-level access to HDF files.Examples>>>df=pd.DataFrame([[1,1.0,'a']],columns=['x','y','z'])>>>df.to_hdf('./store.h5','data')>>>reread=pd.read_hdf('./store.h5')", "doc_id": "66ec1340-36b7-40ac-91a2-985effb605f1", "embedding": null, "doc_hash": "70a76be635fc484705d2f26e66e8d19dddd785a03ebfd4be87f6258ea80fd838", "extra_info": null, "node_info": {"start": 0, "end": 2035, "_node_type": "1"}, "relationships": {"1": "2643c806-ff4b-4c17-8eab-f6cb3fbe2fc4"}}, "__type__": "1"}, "de4b5d1d-f2e2-4b1c-ad66-fcab87cc1815": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_feather\u3010Content\u3011pandas.DataFrame.to_feather#DataFrame.to_feather(path,**kwargs)[source]#Write a DataFrame to the binary Feather format.Parameters:pathstr, path object, file-like objectString, path object (implementingos.PathLike[str]), or file-like\nobject implementing a binarywrite()function. If a string or a path,\nit will be used as Root Directory path when writing a partitioned dataset.**kwargsAdditional keywords passed topyarrow.feather.write_feather().\nThis includes thecompression,compression_level,chunksizeandversionkeywords.NotesThis function writes the dataframe as afeather file. Requires a default\nindex. For saving the DataFrame with your custom index use a method that\nsupports custom indices e.g.to_parquet.Examples>>>df=pd.DataFrame([[1,2,3],[4,5,6]])>>>df.to_feather(\"file.feather\")", "doc_id": "de4b5d1d-f2e2-4b1c-ad66-fcab87cc1815", "embedding": null, "doc_hash": "f98f60837c6a5f6069ea42342a63033bf9b94effc4e2b7ae425e1c6494367e0c", "extra_info": null, "node_info": {"start": 0, "end": 864, "_node_type": "1"}, "relationships": {"1": "23d22d2a-9cce-4389-b4b7-79fa3aea94d6"}}, "__type__": "1"}, "637e2577-4fa4-4623-b6f3-befb0b131326": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_parquet\u3010Content\u3011pandas.read_parquet#pandas.read_parquet(path,engine='auto',columns=None,storage_options=None,use_nullable_dtypes=_NoDefault.no_default,dtype_backend=_NoDefault.no_default,filesystem=None,filters=None,**kwargs)[source]#Load a parquet object from the file path, returning a DataFrame.Parameters:pathstr, path object or file-like objectString, path object (implementingos.PathLike[str]), or file-like\nobject implementing a binaryread()function.\nThe string could be a URL. Valid URL schemes include http, ftp, s3,\ngs, and file. For file URLs, a host is expected. A local file could be:file://localhost/path/to/table.parquet.\nA file URL can also be a path to a directory that contains multiple\npartitioned parquet files. Both pyarrow and fastparquet support\npaths to directories as well as file URLs. A directory path could be:file://localhost/path/to/tablesors3://bucket/partition_dir.engine{\u2018auto\u2019, \u2018pyarrow\u2019, \u2018fastparquet\u2019}, default \u2018auto\u2019Parquet library to use. If \u2018auto\u2019, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try \u2018pyarrow\u2019, falling back to \u2018fastparquet\u2019 if\n\u2018pyarrow\u2019 is unavailable.When using the'pyarrow'engine and no storage options are provided\nand a filesystem is implemented by bothpyarrow.fsandfsspec(e.g. \u201cs3://\u201d), then thepyarrow.fsfilesystem is attempted first.\nUse the filesystem keyword with an instantiated fsspec filesystem\nif you wish to use its implementation.columnslist, default=NoneIf not None, only these columns will be read from the file.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\nare forwarded tourllib.request.Requestas header options. For other\nURLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are\nforwarded tofsspec.open. Please seefsspecandurllibfor more\ndetails, and for more examples on storage options referhere.New in version 1.3.0.use_nullable_dtypesbool, default FalseIf True, use dtypes that usepd.NAas missing value indicator\nfor the resulting DataFrame. (only applicable for thepyarrowengine)\nAs new dtypes are added that supportpd.NAin the future, the\noutput with this option will change to use those dtypes.\nNote: this is an experimental option, and behaviour (e.g. additional\nsupport dtypes) may change without notice.Deprecated since version 2.0.dtype_backend{\u2018numpy_nullable\u2019, \u2018pyarrow\u2019}, default \u2018numpy_nullable\u2019Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:\"numpy_nullable\": returns nullable-dtype-backedDataFrame(default).\"pyarrow\": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file. Only implemented\nforengine=\"pyarrow\".New in version 2.1.0.filtersList[Tuple] or List[List[Tuple]], default NoneTo filter out data.\nFilter syntax: [[(column, op, val), \u2026],\u2026]\nwhere op is [==, =, >, >=, <, <=, !=, in, not in]\nThe innermost tuples are transposed into a set of filters applied\nthrough anANDoperation.\nThe outer list combines these sets of filters through anORoperation.\nA single list of tuples can also be used, meaning that", "doc_id": "637e2577-4fa4-4623-b6f3-befb0b131326", "embedding": null, "doc_hash": "3c9aef1029f6728e9f19beb6c1ee6f5b9e67eef58111376603fa5a9fe954d5f7", "extra_info": null, "node_info": {"start": 0, "end": 3310, "_node_type": "1"}, "relationships": {"1": "1ea2926c-7965-496c-bb49-9cb111429f09", "3": "f9cde05f-6afc-47ae-988d-0cdb4d0b0bb6"}}, "__type__": "1"}, "f9cde05f-6afc-47ae-988d-0cdb4d0b0bb6": {"__data__": {"text": "returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file. Only implemented\nforengine=\"pyarrow\".New in version 2.1.0.filtersList[Tuple] or List[List[Tuple]], default NoneTo filter out data.\nFilter syntax: [[(column, op, val), \u2026],\u2026]\nwhere op is [==, =, >, >=, <, <=, !=, in, not in]\nThe innermost tuples are transposed into a set of filters applied\nthrough anANDoperation.\nThe outer list combines these sets of filters through anORoperation.\nA single list of tuples can also be used, meaning that noORoperation between set of filters is to be conducted.Using this argument will NOT result in row-wise filtering of the final\npartitions unlessengine=\"pyarrow\"is also specified. For\nother engines, filtering is only performed at the partition level, that is,\nto prevent the loading of some row-groups and/or files.New in version 2.1.0.**kwargsAny additional kwargs are passed to the engine.Returns:DataFrameSee alsoDataFrame.to_parquetCreate a parquet object that serializes a DataFrame.Examples>>>original_df=pd.DataFrame(...{\"foo\":range(5),\"bar\":range(5,10)}...)>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>df_parquet_bytes=original_df.to_parquet()>>>fromioimportBytesIO>>>restored_df=pd.read_parquet(BytesIO(df_parquet_bytes))>>>restored_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>restored_df.equals(original_df)True>>>restored_bar=pd.read_parquet(BytesIO(df_parquet_bytes),columns=[\"bar\"])>>>restored_barbar0    51    62    73    84    9>>>restored_bar.equals(original_df[['bar']])TrueThe function useskwargsthat are passed directly to the engine.\nIn the following example, we use thefiltersargument of the pyarrow\nengine to filter the rows of the DataFrame.Sincepyarrowis the default engine, we can omit theengineargument.\nNote that thefiltersargument is implemented by thepyarrowengine,\nwhich can benefit from multithreading and also potentially be more\neconomical in terms of memory.>>>sel=[(\"foo\",\">\",2)]>>>restored_part=pd.read_parquet(BytesIO(df_parquet_bytes),filters=sel)>>>restored_partfoo  bar0    3    81    4    9", "doc_id": "f9cde05f-6afc-47ae-988d-0cdb4d0b0bb6", "embedding": null, "doc_hash": "efdc85eadeb70ac2526a8660694dd9daeab3bdd6497faed497039e8fb4c816a5", "extra_info": null, "node_info": {"start": 2696, "end": 4900, "_node_type": "1"}, "relationships": {"1": "1ea2926c-7965-496c-bb49-9cb111429f09", "2": "637e2577-4fa4-4623-b6f3-befb0b131326"}}, "__type__": "1"}, "bc51f29d-9ad7-4b2c-bcd3-b67258cba7c4": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_parquet\u3010Content\u3011pandas.DataFrame.to_parquet#DataFrame.to_parquet(path=None,*,engine='auto',compression='snappy',index=None,partition_cols=None,storage_options=None,**kwargs)[source]#Write a DataFrame to the binary parquet format.This function writes the dataframe as aparquet file. You can choose different parquet\nbackends, and have the option of compression. Seethe user guidefor more details.Parameters:pathstr, path object, file-like object, or None, default NoneString, path object (implementingos.PathLike[str]), or file-like\nobject implementing a binarywrite()function. If None, the result is\nreturned as bytes. If a string or path, it will be used as Root Directory\npath when writing a partitioned dataset.engine{\u2018auto\u2019, \u2018pyarrow\u2019, \u2018fastparquet\u2019}, default \u2018auto\u2019Parquet library to use. If \u2018auto\u2019, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try \u2018pyarrow\u2019, falling back to \u2018fastparquet\u2019 if\n\u2018pyarrow\u2019 is unavailable.compressionstr or None, default \u2018snappy\u2019Name of the compression to use. UseNonefor no compression.\nSupported options: \u2018snappy\u2019, \u2018gzip\u2019, \u2018brotli\u2019, \u2018lz4\u2019, \u2018zstd\u2019.indexbool, default NoneIfTrue, include the dataframe\u2019s index(es) in the file output.\nIfFalse, they will not be written to the file.\nIfNone, similar toTruethe dataframe\u2019s index(es)\nwill be saved. However, instead of being saved as values,\nthe RangeIndex will be stored as a range in the metadata so it\ndoesn\u2019t require much space and is faster. Other indexes will\nbe included as columns in the file output.partition_colslist, optional, default NoneColumn names by which to partition the dataset.\nColumns are partitioned in the order they are given.\nMust be None if path is not a string.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.\nhost, port, username, password, etc. For HTTP(S) URLs the key-value pairs\nare forwarded tourllib.request.Requestas header options. For other\nURLs (e.g. starting with \u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are\nforwarded tofsspec.open. Please seefsspecandurllibfor more\ndetails, and for more examples on storage options referhere.**kwargsAdditional arguments passed to the parquet library. Seepandas iofor more details.Returns:bytes if no path argument is provided else NoneSee alsoread_parquetRead a parquet file.DataFrame.to_orcWrite an orc file.DataFrame.to_csvWrite a csv file.DataFrame.to_sqlWrite to a sql table.DataFrame.to_hdfWrite to hdf.NotesThis function requires either thefastparquetorpyarrowlibrary.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[3,4]})>>>df.to_parquet('df.parquet.gzip',...compression='gzip')>>>pd.read_parquet('df.parquet.gzip')col1  col20     1     31     2     4If you want to get a buffer to the parquet content you can use a io.BytesIO\nobject, as long as you don\u2019t use partition_cols, which creates multiple files.>>>importio>>>f=io.BytesIO()>>>df.to_parquet(f)>>>f.seek(0)0>>>content=f.read()", "doc_id": "bc51f29d-9ad7-4b2c-bcd3-b67258cba7c4", "embedding": null, "doc_hash": "63d3fd2e5bfaeda6df5504087411e260dc694b618d3787c98d779fa28cd2939a", "extra_info": null, "node_info": {"start": 0, "end": 3007, "_node_type": "1"}, "relationships": {"1": "27387f47-10bf-4ffe-8a1b-a4eeb53d110f"}}, "__type__": "1"}, "dfe2a23b-1918-4c22-ad17-47b8aa453736": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_sql_table\u3010Content\u3011pandas.read_sql_table#pandas.read_sql_table(table_name,con,schema=None,index_col=None,coerce_float=True,parse_dates=None,columns=None,chunksize=None,dtype_backend=_NoDefault.no_default)[source]#Read SQL database table into a DataFrame.Given a table name and a SQLAlchemy connectable, returns a DataFrame.\nThis function does not support DBAPI connections.Parameters:table_namestrName of SQL table in database.conSQLAlchemy connectable or strA database URI could be provided as str.\nSQLite DBAPI connection mode not supported.schemastr, default NoneName of SQL schema in database to query (if database flavor\nsupports this). Uses default schema if None (default).index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point. Can result in loss of Precision.parse_dateslist or dict, default NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is\nstrftime compatible in case of parsing string times or is one of\n(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds\nto the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,\nsuch as SQLite.columnslist, default NoneList of column names to select from SQL table.chunksizeint, default NoneIf specified, returns an iterator wherechunksizeis the number of\nrows to include in each chunk.dtype_backend{\u2018numpy_nullable\u2019, \u2018pyarrow\u2019}, default \u2018numpy_nullable\u2019Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:\"numpy_nullable\": returns nullable-dtype-backedDataFrame(default).\"pyarrow\": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or Iterator[DataFrame]A SQL table is returned as two-dimensional data structure with labeled\naxes.See alsoread_sql_queryRead SQL query into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.NotesAny datetime values with time zone information will be converted to UTC.Examples>>>pd.read_sql_table('table_name','postgres:///db_name')", "doc_id": "dfe2a23b-1918-4c22-ad17-47b8aa453736", "embedding": null, "doc_hash": "4b9e2992388fccc5a0d6d1b96401de05525622e9c6d7eb9aa106cf9a195219a8", "extra_info": null, "node_info": {"start": 0, "end": 2306, "_node_type": "1"}, "relationships": {"1": "2d37df91-f945-4d5f-8b93-f08c14930587"}}, "__type__": "1"}, "3fe1a6e1-9104-499b-b141-59610b26b78c": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_sql_query\u3010Content\u3011pandas.read_sql_query#pandas.read_sql_query(sql,con,index_col=None,coerce_float=True,params=None,parse_dates=None,chunksize=None,dtype=None,dtype_backend=_NoDefault.no_default)[source]#Read SQL query into a DataFrame.Returns a DataFrame corresponding to the result set of the query\nstring. Optionally provide anindex_colparameter to use one of the\ncolumns as the index, otherwise default integer index will be used.Parameters:sqlstr SQL query or SQLAlchemy Selectable (select or text object)SQL query to be executed.conSQLAlchemy connectable, str, or sqlite3 connectionUsing SQLAlchemy makes it possible to use any DB supported by that\nlibrary. If a DBAPI2 object, only sqlite3 is supported.index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point. Useful for SQL result sets.paramslist, tuple or mapping, optional, default: NoneList of parameters to pass to execute method. The syntax used\nto pass parameters is database driver dependent. Check your\ndatabase driver documentation for which of the five syntax styles,\ndescribed in PEP 249\u2019s paramstyle, is supported.\nEg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.parse_dateslist or dict, default: NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is\nstrftime compatible in case of parsing string times, or is one of\n(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds\nto the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,\nsuch as SQLite.chunksizeint, default NoneIf specified, return an iterator wherechunksizeis the number of\nrows to include in each chunk.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or\n{\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019}.New in version 1.3.0.dtype_backend{\u2018numpy_nullable\u2019, \u2018pyarrow\u2019}, default \u2018numpy_nullable\u2019Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:\"numpy_nullable\": returns nullable-dtype-backedDataFrame(default).\"pyarrow\": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.NotesAny datetime values with time zone information parsed via theparse_datesparameter will be converted to UTC.Examples>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine(\"sqlite:///database.db\")>>>withengine.connect()asconn,conn.begin():...data=pd.read_sql_table(\"data\",conn)", "doc_id": "3fe1a6e1-9104-499b-b141-59610b26b78c", "embedding": null, "doc_hash": "67ed91e066032d8a13594b5e98b766bf7b3b3f1f39e4f31b59ad95bf3f34134c", "extra_info": null, "node_info": {"start": 0, "end": 2845, "_node_type": "1"}, "relationships": {"1": "e7055960-da50-489a-9f89-becde79d4fa3"}}, "__type__": "1"}, "ac5bb15b-71e4-4c11-82c3-8025abb2e7ce": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_sql\u3010Content\u3011pandas.read_sql#pandas.read_sql(sql,con,index_col=None,coerce_float=True,params=None,parse_dates=None,columns=None,chunksize=None,dtype_backend=_NoDefault.no_default,dtype=None)[source]#Read SQL query or database table into a DataFrame.This function is a convenience wrapper aroundread_sql_tableandread_sql_query(for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed toread_sql_query, while a database table name will\nbe routed toread_sql_table. Note that the delegated function might\nhave more specific notes about their functionality not listed here.Parameters:sqlstr or SQLAlchemy Selectable (select or text object)SQL query to be executed or a table name.conADBC Connection, SQLAlchemy connectable, str, or sqlite3 connectionADBC provides high performance I/O with native type support, where available.\nUsing SQLAlchemy makes it possible to use any DB supported by that\nlibrary. If a DBAPI2 object, only sqlite3 is supported. The user is responsible\nfor engine disposal and connection closure for the ADBC connection and\nSQLAlchemy connectable; str connections are closed automatically. Seehere.index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point, useful for SQL result sets.paramslist, tuple or dict, optional, default: NoneList of parameters to pass to execute method. The syntax used\nto pass parameters is database driver dependent. Check your\ndatabase driver documentation for which of the five syntax styles,\ndescribed in PEP 249\u2019s paramstyle, is supported.\nEg. for psycopg2, uses %(name)s so use params={\u2018name\u2019 : \u2018value\u2019}.parse_dateslist or dict, default: NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is\nstrftime compatible in case of parsing string times, or is one of\n(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds\nto the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,\nsuch as SQLite.columnslist, default: NoneList of column names to select from SQL table (only used when reading\na table).chunksizeint, default NoneIf specified, return an iterator wherechunksizeis the\nnumber of rows to include in each chunk.dtype_backend{\u2018numpy_nullable\u2019, \u2018pyarrow\u2019}, default \u2018numpy_nullable\u2019Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:\"numpy_nullable\": returns nullable-dtype-backedDataFrame(default).\"pyarrow\": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or\n{\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019}.\nThe argument is ignored if a table is passed instead of a query.New in version 2.0.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sql_queryRead SQL query into a DataFrame.ExamplesRead data from SQL via either a SQL query or a SQL tablename.\nWhen using a SQLite database only SQL queries are accepted,\nproviding only the SQL tablename will result in an", "doc_id": "ac5bb15b-71e4-4c11-82c3-8025abb2e7ce", "embedding": null, "doc_hash": "88fcdbec3bcf0e8ad3454db47d9dd730adf36f78845076db0b47e0bd6bdc4f6e", "extra_info": null, "node_info": {"start": 0, "end": 3374, "_node_type": "1"}, "relationships": {"1": "a46267e0-fc16-41db-b435-690d01d8d346", "3": "5a91fddf-4980-41fd-af0f-9e9d54e41d89"}}, "__type__": "1"}, "5a91fddf-4980-41fd-af0f-9e9d54e41d89": {"__data__": {"text": "returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or\n{\u2018a\u2019: np.float64, \u2018b\u2019: np.int32, \u2018c\u2019: \u2018Int64\u2019}.\nThe argument is ignored if a table is passed instead of a query.New in version 2.0.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sql_queryRead SQL query into a DataFrame.ExamplesRead data from SQL via either a SQL query or a SQL tablename.\nWhen using a SQLite database only SQL queries are accepted,\nproviding only the SQL tablename will result in an error.>>>fromsqlite3importconnect>>>conn=connect(':memory:')>>>df=pd.DataFrame(data=[[0,'10/11/12'],[1,'12/11/10']],...columns=['int_column','date_column'])>>>df.to_sql(name='test_data',con=conn)2>>>pd.read_sql('SELECT int_column, date_column FROM test_data',conn)int_column date_column0           0    10/11/121           1    12/11/10>>>pd.read_sql('test_data','postgres:///db_name')Apply date parsing to columns through theparse_datesargument\nTheparse_datesargument callspd.to_datetimeon the provided columns.\nCustom argument values for applyingpd.to_datetimeon a column are specified\nvia a dictionary format:>>>pd.read_sql('SELECT int_column, date_column FROM test_data',...conn,...parse_dates={\"date_column\":{\"format\":\"%d/%m/%y\"}})int_column date_column0           0  2012-11-101           1  2010-11-12New in version 2.2.0:pandas now supports reading via ADBC drivers>>>fromadbc_driver_postgresqlimportdbapi>>>withdbapi.connect('postgres:///db_name')asconn:...pd.read_sql('SELECT int_column FROM test_data',conn)int_column0           01           1", "doc_id": "5a91fddf-4980-41fd-af0f-9e9d54e41d89", "embedding": null, "doc_hash": "72ec95f0d4cff0b9ac4f245c7d1904660d2b4311035c3895a206c2fc5c064965", "extra_info": null, "node_info": {"start": 2759, "end": 4429, "_node_type": "1"}, "relationships": {"1": "a46267e0-fc16-41db-b435-690d01d8d346", "2": "ac5bb15b-71e4-4c11-82c3-8025abb2e7ce"}}, "__type__": "1"}, "e71c834d-c4a0-486f-9d4e-3e3c7fb80bf2": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.DataFrame.to_sql\u3010Content\u3011The text provides a reference for the method `to_sql()` in the pandas library. This method allows users to write records stored in a DataFrame to a SQL database. It supports various databases supported by SQLAlchemy.Here are the key parameters for the `to_sql()` method:1. `name` (str): Specifies the name of the SQL table.2. `con` (SQLAlchemy Engine, Connection, or sqlite3.Connection): Specifies the connection object to the database.3. `schema` (str, optional): Specifies the schema (if applicable).4. `if_exists` (str, default 'fail'): Specifies how to behave if the table already exists ('fail', 'replace', 'append').5. `index` (bool, default True): Specifies whether to write the DataFrame index as a column.6. `index_label` (str or sequence, default None): Specifies the label for the index column(s).7. `chunksize` (int, optional): Specifies the number of rows to be written at a time.8. `dtype` (dict or scalar, optional): Specifies the datatype for columns.9. `method` (None, 'multi', callable, optional): Controls the SQL insertion clause used.The method returns the number of rows affected by the operation or None if the callable passed into the `method` parameter does not return an integer number of rows.The text also provides examples of using the `to_sql()` method. It shows how to create an in-memory SQLite database, create a table from a DataFrame, append data to an existing table, overwrite a table, and use the `method` parameter to define a callable insertion method.Furthermore, the text mentions that timezone aware datetime columns will be written as Timestampwithtimezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone.Overall, the `to_sql()` method in pandas provides a convenient way to write DataFrame records to a SQL database, with support for various databases and customization options.", "doc_id": "e71c834d-c4a0-486f-9d4e-3e3c7fb80bf2", "embedding": null, "doc_hash": "a859933a7d7aa20ebdfd0568724653b3cd55c25c58b873077b118a5b6a301cd4", "extra_info": null, "node_info": {"start": 0, "end": 2002, "_node_type": "1"}, "relationships": {"1": "abffe0d8-2f97-4638-b134-eba5bad092d9"}}, "__type__": "1"}, "f34d4cb8-03a4-45a3-bef8-b8185899d5fc": {"__data__": {"text": "\u3010Name\u3011Pandas\u3010Chapter\u3011Input Output\u3010Section\u3011pandas.read_gbq\u3010Content\u3011pandas.read_gbq#pandas.read_gbq(query,project_id=None,index_col=None,col_order=None,reauth=False,auth_local_webserver=True,dialect=None,location=None,configuration=None,credentials=None,use_bqstorage_api=None,max_results=None,progress_bar_type=None)[source]#Load data from Google BigQuery.Deprecated since version 2.2.0:Please usepandas_gbq.read_gbqinstead.This function requires thepandas-gbq package.See theHow to authenticate with Google BigQueryguide for authentication instructions.Parameters:querystrSQL-Like Query to return data values.project_idstr, optionalGoogle BigQuery Account project ID. Optional when available from\nthe environment.index_colstr, optionalName of result column to use for index in results DataFrame.col_orderlist(str), optionalList of BigQuery column names in the desired order for results\nDataFrame.reauthbool, default FalseForce Google BigQuery to re-authenticate the user. This is useful\nif multiple accounts are used.auth_local_webserverbool, default TrueUse thelocal webserver flowinstead of theconsole flowwhen getting user credentials.New in version 0.2.0 of pandas-gbq.Changed in version 1.5.0:Default value is changed toTrue. Google has deprecated theauth_local_webserver=False\u201cout of band\u201d (copy-paste)\nflow.dialectstr, default \u2018legacy\u2019Note: The default value is changing to \u2018standard\u2019 in a future version.SQL syntax dialect to use. Value can be one of:'legacy'Use BigQuery\u2019s legacy SQL dialect. For more information seeBigQuery Legacy SQL Reference.'standard'Use BigQuery\u2019s standard SQL, which is\ncompliant with the SQL 2011 standard. For more information\nseeBigQuery Standard SQL Reference.locationstr, optionalLocation where the query job should run. See theBigQuery locations\ndocumentationfor a\nlist of available locations. The location must match that of any\ndatasets used in the query.New in version 0.5.0 of pandas-gbq.configurationdict, optionalQuery config parameters for job processing.\nFor example:configuration = {\u2018query\u2019: {\u2018useQueryCache\u2019: False}}For more information seeBigQuery REST API Reference.credentialsgoogle.auth.credentials.Credentials, optionalCredentials for accessing Google APIs. Use this parameter to override\ndefault credentials, such as to use Compute Enginegoogle.auth.compute_engine.Credentialsor Service Accountgoogle.oauth2.service_account.Credentialsdirectly.New in version 0.8.0 of pandas-gbq.use_bqstorage_apibool, default FalseUse theBigQuery Storage APIto\ndownload query results quickly, but at an increased cost. To use this\nAPI, firstenable it in the Cloud Console.\nYou must also have thebigquery.readsessions.createpermission on the project you are billing queries to.This feature requires version 0.10.0 or later of thepandas-gbqpackage. It also requires thegoogle-cloud-bigquery-storageandfastavropackages.max_resultsint, optionalIf set, limit the maximum number of rows to fetch from the query\nresults.progress_bar_typeOptional, strIf set, use thetqdmlibrary to\ndisplay a progress bar while the data downloads. Install thetqdmpackage to use this feature.Possible values ofprogress_bar_typeinclude:NoneNo progress bar.'tqdm'Use thetqdm.tqdm()function to print a progress bar\ntosys.stderr.'tqdm_notebook'Use thetqdm.tqdm_notebook()function to display a\nprogress bar as a Jupyter notebook widget.'tqdm_gui'Use thetqdm.tqdm_gui()function to display a\nprogress bar as a graphical dialog box.Returns:df: DataFrameDataFrame representing results of query.See alsopandas_gbq.read_gbqThis function in the pandas-gbq", "doc_id": "f34d4cb8-03a4-45a3-bef8-b8185899d5fc", "embedding": null, "doc_hash": "d1abffa3a7c2e7939c842b503095801e6ac863b8b134b75858f16f333be5b8f7", "extra_info": null, "node_info": {"start": 0, "end": 3556, "_node_type": "1"}, "relationships": {"1": "d75f968f-8269-4cc8-9d01-8673f954d93b", "3": "3f4c3679-f158-47b8-a304-e507f1e10f1b"}}, "__type__": "1"}, "3f4c3679-f158-47b8-a304-e507f1e10f1b": {"__data__": {"text": "the query\nresults.progress_bar_typeOptional, strIf set, use thetqdmlibrary to\ndisplay a progress bar while the data downloads. Install thetqdmpackage to use this feature.Possible values ofprogress_bar_typeinclude:NoneNo progress bar.'tqdm'Use thetqdm.tqdm()function to print a progress bar\ntosys.stderr.'tqdm_notebook'Use thetqdm.tqdm_notebook()function to display a\nprogress bar as a Jupyter notebook widget.'tqdm_gui'Use thetqdm.tqdm_gui()function to display a\nprogress bar as a graphical dialog box.Returns:df: DataFrameDataFrame representing results of query.See alsopandas_gbq.read_gbqThis function in the pandas-gbq library.DataFrame.to_gbqWrite a DataFrame to Google BigQuery.ExamplesExample taken fromGoogle BigQuery documentation>>>sql=\"SELECT name FROM table_name WHERE state = 'TX' LIMIT 100;\">>>df=pd.read_gbq(sql,dialect=\"standard\")>>>project_id=\"your-project-id\">>>df=pd.read_gbq(sql,...project_id=project_id,...dialect=\"standard\"...)", "doc_id": "3f4c3679-f158-47b8-a304-e507f1e10f1b", "embedding": null, "doc_hash": "ecb2730a4f70e1ba8098b0169b8cebcc60153b094716037036f0180165e75788", "extra_info": null, "node_info": {"start": 2935, "end": 3883, "_node_type": "1"}, "relationships": {"1": "d75f968f-8269-4cc8-9d01-8673f954d93b", "2": "f34d4cb8-03a4-45a3-bef8-b8185899d5fc"}}, "__type__": "1"}}}