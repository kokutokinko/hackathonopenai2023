ÉâÉCÉuÉâÉäñº,èÕ,êﬂ,ì‡óe
Pandas,Combined_csv,pandas.read_csv,"pandas.read_csv#pandas.read_csv(filepath_or_buffer,*,sep=_NoDefault.no_default,delimiter=None,header='infer',names=_NoDefault.no_default,index_col=None,usecols=None,dtype=None,engine=None,converters=None,true_values=None,false_values=None,skipinitialspace=False,skiprows=None,skipfooter=0,nrows=None,na_values=None,keep_default_na=True,na_filter=True,verbose=False,skip_blank_lines=True,parse_dates=None,infer_datetime_format=_NoDefault.no_default,keep_date_col=False,date_parser=_NoDefault.no_default,date_format=None,dayfirst=False,cache_dates=True,iterator=False,chunksize=None,compression='infer',thousands=None,decimal='.',lineterminator=None,quotechar='""',quoting=0,doublequote=True,escapechar=None,comment=None,encoding=None,encoding_errors='strict',dialect=None,on_bad_lines='error',delim_whitespace=False,low_memory=True,memory_map=False,float_precision=None,storage_options=None,dtype_backend=_NoDefault.no_default)[source]#Read a comma-separated values (csv) file into DataFrame.Also supports optionally iterating or breaking of the file
into chunks.Additional help can be found in the online docs forIO Tools.Parameters:filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.csv.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method, such as
a file handle (e.g. via builtinopenfunction) orStringIO.sepstr, default Åe,ÅfCharacter or regex pattern to treat as the delimiter. Ifsep=None, the
C engine cannot automatically detect
the separator, but the Python parsing engine can, meaning the latter will
be used and automatically detect the separator from only the first valid
row of the file by PythonÅfs builtin sniffer tool,csv.Sniffer.
In addition, separators longer than 1 character and different from'\s+'will be interpreted as regular expressions and will also force
the use of the Python parsing engine. Note that regex delimiters are prone
to ignoring quoted data. Regex example:'\r\t'.delimiterstr, optionalAlias forsep.headerint, Sequence of int, ÅeinferÅf or None, default ÅeinferÅfRow number(s) containing column labels and marking the start of the
data (zero-indexed). Default behavior is to infer the column names: if nonamesare passed the behavior is identical toheader=0and column
names are inferred from the first line of the file, if column
names are passed explicitly tonamesthen the behavior is identical toheader=None. Explicitly passheader=0to be able to
replace existing names. The header can be a list of integers that
specify row locations for aMultiIndexon the columns
e.g.[0,1,3]. Intervening rows that are not specified will be
skipped (e.g. 2 in this example is skipped). Note that this
parameter ignores commented lines and empty lines ifskip_blank_lines=True, soheader=0denotes the first line of
data rather than the first line of the file.namesSequence of Hashable, optionalSequence of column labels to apply. If the file contains a header row,
then you should explicitly passheader=0to override the column names.
Duplicates in this list are not allowed.index_colHashable, Sequence of Hashable or False, optionalColumn(s) to use as row label(s), denoted either by column labels or column
indices. If a sequence of labels or indices is given,MultiIndexwill be formed for the row labels.Note:index_col=Falsecan be used to force pandas tonotuse the first
column as the index, e.g., when you have a malformed file with delimiters at
the end of each line.usecolslist of Hashable or Callable, optionalSubset of columns to select, denoted either by column labels or column indices.
If list-like, all elements must either
be positional (i.e. integer indices into the document columns) or strings
that correspond to column names provided either by the user innamesor
inferred from the document header row(s). Ifnamesare given, the document
header row(s) are not taken into account. For example, a valid list-likeusecolsparameter would be[0,1,2]or['foo','bar','baz'].
Element order is ignored, sousecols=[0,1]is the same as[1,0].
To instantiate aDataFramefromdatawith element order
preserved usepd.read_csv(data,usecols=['foo','bar'])[['foo','bar']]for columns in['foo','bar']order orpd.read_csv(data,usecols=['foo','bar'])[['bar','foo']]for['bar','foo']order.If callable, the callable function will be evaluated against the column
names, returning names where the callable function evaluates toTrue. An
example of a valid callable argument would belambdax:x.upper()in['AAA','BBB','DDD']. Using this parameter results in much faster
parsing time and lower memory usage.dtypedtype or dict of {Hashabledtype}, optionalData type(s) to apply to either the whole dataset or individual columns.
E.g.,{'a':np.float64,'b':np.int32,'c':'Int64'}Usestrorobjecttogether with suitablena_valuessettings
to preserve and not interpretdtype.
Ifconvertersare specified, they will be applied INSTEAD
ofdtypeconversion.New in version 1.5.0:Support fordefaultdictwas added. Specify adefaultdictas input where
the default determines thedtypeof the columns which are not explicitly
listed.engine{ÅecÅf, ÅepythonÅf, ÅepyarrowÅf}, optionalParser engine to use. The C and pyarrow engines are faster, while the python engine
is currently more feature-complete. Multithreading is currently only supported by
the pyarrow engine.New in version 1.4.0:The ÅepyarrowÅf engine was added as anexperimentalengine, and some features
are unsupported, or may not work correctly, with this engine.convertersdict of {HashableCallable}, optionalFunctions for converting values in specified columns. Keys can either
be column labels or column indices.true_valueslist, optionalValues to consider asTruein addition to case-insensitive variants of ÅeTrueÅf.false_valueslist, optionalValues to consider asFalsein addition to case-insensitive variants of ÅeFalseÅf.skipinitialspacebool, default FalseSkip spaces after delimiter.skiprowsint, list of int or Callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int)
at the start of the file.If callable, the callable function will be evaluated against the row
indices, returningTrueif the row should be skipped andFalseotherwise.
An example of a valid callable argument would belambdax:xin[0,2].skipfooterint, default 0Number of lines at bottom of file to skip (Unsupported withengine='c').nrowsint, optionalNumber of rows of file to read. Useful for reading pieces of large files.na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optionalAdditional strings to recognize asNA/NaN. Ifdictpassed, specific
per-columnNAvalues. By default the following values are interpreted asNaN: Åg Åg, Åg#N/AÅh, Åg#N/A N/AÅh, Åg#NAÅh, Åg-1.#INDÅh, Åg-1.#QNANÅh, Åg-NaNÅh, Åg-nanÅh,
Åg1.#INDÅh, Åg1.#QNANÅh, Åg<NA>Åh, ÅgN/AÅh, ÅgNAÅh, ÅgNULLÅh, ÅgNaNÅh, ÅgNoneÅh,
Ågn/aÅh, ÅgnanÅh, Ågnull Åg.keep_default_nabool, default TrueWhether or not to include the defaultNaNvalues when parsing the data.
Depending on whetherna_valuesis passed in, the behavior is as follows:Ifkeep_default_naisTrue, andna_valuesare specified,na_valuesis appended to the defaultNaNvalues used for parsing.Ifkeep_default_naisTrue, andna_valuesare not specified, only
the defaultNaNvalues are used for parsing.Ifkeep_default_naisFalse, andna_valuesare specified, only
theNaNvalues specifiedna_valuesare used for parsing.Ifkeep_default_naisFalse, andna_valuesare not specified, no
strings will be parsed asNaN.Note that ifna_filteris passed in asFalse, thekeep_default_naandna_valuesparameters will be ignored.na_filterbool, default TrueDetect missing value markers (empty strings and the value ofna_values). In
data without anyNAvalues, passingna_filter=Falsecan improve the
performance of reading a large file.verbosebool, default FalseIndicate number ofNAvalues placed in non-numeric columns.skip_blank_linesbool, default TrueIfTrue, skip over blank lines rather than interpreting asNaNvalues.parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default FalseThe behavior is as follows:bool. IfTrue-> try parsing the index.listofintor names. e.g. If[1,2,3]-> try parsing columns 1, 2, 3
each as a separate date column.listoflist. e.g. If[[1,3]]-> combine columns 1 and 3 and parse
as a single date column.dict, e.g.{'foo':[1,3]}-> parse columns 1, 3 as date and call
result ÅefooÅfIf a column or index cannot be represented as an array ofdatetime,
say because of an unparsable value or a mixture of timezones, the column
or index will be returned unaltered as anobjectdata type. For
non-standarddatetimeparsing, useto_datetime()afterread_csv().Note: A fast-path exists for iso8601-formatted dates.infer_datetime_formatbool, default FalseIfTrueandparse_datesis enabled, pandas will attempt to infer the
format of thedatetimestrings in the columns, and if it can be inferred,
switch to a faster method of parsing them. In some cases this can increase
the parsing speed by 5-10x.Deprecated since version 2.0.0:A strict version of this argument is now the default, passing it has no effect.keep_date_colbool, default FalseIfTrueandparse_datesspecifies combining multiple columns then
keep the original columns.date_parserCallable, optionalFunction to use for converting a sequence of string columns to an array ofdatetimeinstances. The default usesdateutil.parser.parserto do the
conversion. pandas will try to calldate_parserin three different ways,
advancing to the next if an exception occurs: 1) Pass one or more arrays
(as defined byparse_dates) as arguments; 2) concatenate (row-wise) the
string values from the columns defined byparse_datesinto a single array
and pass that; and 3) calldate_parseronce for each row using one or
more strings (corresponding to the columns defined byparse_dates) as
arguments.Deprecated since version 2.0.0:Usedate_formatinstead, or read in asobjectand then applyto_datetime()as-needed.date_formatstr or dict of column -> format, optionalFormat to use for parsing dates when used in conjunction withparse_dates.
For anything more complex, please read in asobjectand then applyto_datetime()as-needed.New in version 2.0.0.dayfirstbool, default FalseDD/MM format dates, international and European format.cache_datesbool, default TrueIfTrue, use a cache of unique, converted dates to apply thedatetimeconversion. May produce significant speed-up when parsing duplicate
date strings, especially ones with timezone offsets.iteratorbool, default FalseReturnTextFileReaderobject for iteration or getting chunks withget_chunk().Changed in version 1.2:TextFileReaderis a context manager.chunksizeint, optionalNumber of lines to read from the file per chunk. Passing a value will cause the
function to return aTextFileReaderobject for iteration.
See theIO Tools docsfor more information oniteratorandchunksize.Changed in version 1.2:TextFileReaderis a context manager.compressionstr or dict, default ÅeinferÅfFor on-the-fly decompression of on-disk data. If ÅeinferÅf and Åefilepath_or_bufferÅf is
path-like, then detect compression from the following extensions: Åe.gzÅf,
Åe.bz2Åf, Åe.zipÅf, Åe.xzÅf, Åe.zstÅf, Åe.tarÅf, Åe.tar.gzÅf, Åe.tar.xzÅf or Åe.tar.bz2Åf
(otherwise no compression).
If using ÅezipÅf or ÅetarÅf, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.thousandsstr (length 1), optionalCharacter acting as the thousands separator in numerical values.decimalstr (length 1), default Åe.ÅfCharacter to recognize as decimal point (e.g., use Åe,Åf for European data).lineterminatorstr (length 1), optionalCharacter used to denote a line break. Only valid with C parser.quotecharstr (length 1), optionalCharacter used to denote the start and end of a quoted item. Quoted
items can include thedelimiterand it will be ignored.quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMALControl field quoting behavior percsv.QUOTE_*constants. Default iscsv.QUOTE_MINIMAL(i.e., 0) which implies that only fields containing special
characters are quoted (e.g., characters defined inquotechar,delimiter,
orlineterminator.doublequotebool, default TrueWhenquotecharis specified andquotingis notQUOTE_NONE, indicate
whether or not to interpret two consecutivequotecharelements INSIDE a
field as a singlequotecharelement.escapecharstr (length 1), optionalCharacter used to escape other characters.commentstr (length 1), optionalCharacter indicating that the remainder of line should not be parsed.
If found at the beginning
of a line, the line will be ignored altogether. This parameter must be a
single character. Like empty lines (as long asskip_blank_lines=True),
fully commented lines are ignored by the parameterheaderbut not byskiprows. For example, ifcomment='#', parsing#empty\na,b,c\n1,2,3withheader=0will result in'a,b,c'being
treated as the header.encodingstr, optional, default Åeutf-8ÅfEncoding to use for UTF when reading/writing (ex.'utf-8').List of Python
standard encodings.Changed in version 1.2:WhenencodingisNone,errors='replace'is passed toopen(). Otherwise,errors='strict'is passed toopen().
This behavior was previously only the case forengine='python'.Changed in version 1.3.0:encoding_errorsis a new argument.encodinghas no longer an
influence on how encoding errors are handled.encoding_errorsstr, optional, default ÅestrictÅfHow encoding errors are treated.List of possible values.New in version 1.3.0.dialectstr or csv.Dialect, optionalIf provided, this parameter will override values (default or not) for the
following parameters:delimiter,doublequote,escapechar,skipinitialspace,quotechar, andquoting. If it is necessary to
override values, aParserWarningwill be issued. Seecsv.Dialectdocumentation for more details.on_bad_lines{ÅeerrorÅf, ÅewarnÅf, ÅeskipÅf} or Callable, default ÅeerrorÅfSpecifies what to do upon encountering a bad line (a line with too many fields).
Allowed values are :'error', raise an Exception when a bad line is encountered.'warn', raise a warning when a bad line is encountered and skip that line.'skip', skip bad lines without raising or warning when they are encountered.New in version 1.3.0.New in version 1.4.0:Callable, function with signature(bad_line:list[str])->list[str]|Nonethat will process a single
bad line.bad_lineis a list of strings split by thesep.
If the function returnsNone, the bad line will be ignored.
If the function returns a newlistof strings with more elements than
expected, aParserWarningwill be emitted while dropping extra elements.
Only supported whenengine='python'delim_whitespacebool, default FalseSpecifies whether or not whitespace (e.g.''or'\t') will be
used as thesepdelimiter. Equivalent to settingsep='\s+'. If this option
is set toTrue, nothing should be passed in for thedelimiterparameter.low_memorybool, default TrueInternally process the file in chunks, resulting in lower memory use
while parsing, but possibly mixed type inference. To ensure no mixed
types either setFalse, or specify the type with thedtypeparameter.
Note that the entire file is read into a singleDataFrameregardless, use thechunksizeoriteratorparameter to return the data in
chunks. (Only valid with C parser).memory_mapbool, default FalseIf a filepath is provided forfilepath_or_buffer, map the file object
directly onto memory and access the data directly from there. Using this
option can improve performance because there is no longer any I/O overhead.float_precision{ÅehighÅf, ÅelegacyÅf, Åeround_tripÅf}, optionalSpecifies which converter the C engine should use for floating-point
values. The options areNoneor'high'for the ordinary converter,'legacy'for the original lower precision pandas converter, and'round_trip'for the round-trip converter.Changed in version 1.2.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.dtype_backend{Åenumpy_nullableÅf, ÅepyarrowÅf}, default Åenumpy_nullableÅfBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional
data structure with labeled axes.See alsoDataFrame.to_csvWrite DataFrame to a comma-separated values (csv) file.read_tableRead general delimited file into DataFrame.read_fwfRead a table of fixed-width formatted lines into DataFrame.Examples>>>pd.read_csv('data.csv')"
Pandas,Combined_csv,pandas.DataFrame.to_csv,"pandas.DataFrame.to_csv#DataFrame.to_csv(path_or_buf=None,sep=',',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,mode='w',encoding=None,compression='infer',quoting=None,quotechar='""',lineterminator=None,chunksize=None,date_format=None,doublequote=True,escapechar=None,decimal='.',errors='strict',storage_options=None)[source]#Write object to a comma-separated values (csv) file.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string. If a non-binary file object is passed, it should
be opened withnewline=ÅfÅf, disabling universal newlines. If a binary
file object is passed,modemight need to contain aÅebÅf.Changed in version 1.2.0:Support for binary file objects was introduced.sepstr, default Åe,ÅfString of length 1. Field delimiter for the output file.na_repstr, default ÅeÅfMissing data representation.float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes
precedence over other numeric formatting parameters, like decimal.columnssequence, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, andheaderandindexare True, then the index names are used. A
sequence should be given if the object uses MultiIndex. If
False do not print fields for index names. Use index_label=False
for easier importing in R.mode{ÅewÅf, ÅexÅf, ÅeaÅf}, default ÅewÅfForwarded to eitheropen(mode=)orfsspec.open(mode=)to control
the file opening. Typical values include:ÅewÅf, truncate the file first.ÅexÅf, exclusive creation, failing if the file already exists.ÅeaÅf, append to the end of file if it exists.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to Åeutf-8Åf.encodingis not supported ifpath_or_bufis a non-binary file object.compressionstr or dict, default ÅeinferÅfFor on-the-fly compression of the output data. If ÅeinferÅf and Åepath_or_bufÅf is
path-like, then detect compression from the following extensions: Åe.gzÅf,
Åe.bz2Åf, Åe.zipÅf, Åe.xzÅf, Åe.zstÅf, Åe.tarÅf, Åe.tar.gzÅf, Åe.tar.xzÅf or Åe.tar.bz2Åf
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key ÅemethodÅf as compression mode
and other entries as additional compression options if
compression mode is ÅezipÅf.Passing compression options as keys in dict is
supported for compression modes ÅegzipÅf, Åebz2Åf, ÅezstdÅf, and ÅezipÅf.Changed in version 1.2.0:Compression is supported for binary file objects.Changed in version 1.2.0:Previous versions forwarded dict entries for ÅegzipÅf togzip.openinstead ofgzip.GzipFilewhich prevented
settingmtime.quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set afloat_formatthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC
will treat them as non-numeric.quotecharstr, default Åe""ÅfString of length 1. Character used to quote fields.lineterminatorstr, optionalThe newline character or character sequence to use in the output
file. Defaults toos.linesep, which depends on the OS in which
this method is called (Åf\nÅf for linux, Åe\r\nÅf for Windows, i.e.).Changed in version 1.5.0:Previously was line_terminator, changed for consistency with
read_csv and the standard library ÅecsvÅf module.chunksizeint or NoneRows to write at a time.date_formatstr, default NoneFormat string for datetime objects.doublequotebool, default TrueControl quoting ofquotecharinside a field.escapecharstr, default NoneString of length 1. Character used to escapesepandquotecharwhen appropriate.decimalstr, default Åe.ÅfCharacter recognized as decimal separator. E.g. use Åe,Åf for
European data.errorsstr, default ÅestrictÅfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.Returns:None or strIf path_or_buf is None, returns the resulting csv format as a
string. Otherwise returns None.See alsoread_csvLoad a CSV file into a DataFrame.to_excelWrite DataFrame to an Excel file.Examples>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv(index=False)'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'Create Åeout.zipÅf containing Åeout.csvÅf>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first
need to create it using either Pathlib or os:>>>frompathlibimportPath>>>filepath=Path('folder/subfolder/out.csv')>>>filepath.parent.mkdir(parents=True,exist_ok=True)>>>df.to_csv(filepath)>>>importos>>>os.makedirs('folder/subfolder',exist_ok=True)>>>df.to_csv('folder/subfolder/out.csv')"
Pandas,Combined_csv,pandas.read_excel,"pandas.read_excel#pandas.read_excel(io,sheet_name=0,*,header=0,names=None,index_col=None,usecols=None,dtype=None,engine=None,converters=None,true_values=None,false_values=None,skiprows=None,nrows=None,na_values=None,keep_default_na=True,na_filter=True,verbose=False,parse_dates=False,date_parser=_NoDefault.no_default,date_format=None,thousands=None,decimal='.',comment=None,skipfooter=0,storage_options=None,dtype_backend=_NoDefault.no_default,engine_kwargs=None)[source]#Read an Excel file into a pandas DataFrame.Supportsxls,xlsx,xlsm,xlsb,odf,odsandodtfile extensions
read from a local filesystem or URL. Supports an option to read
a single sheet or a list of sheets.Parameters:iostr, bytes, ExcelFile, xlrd.Book, path object, or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.xlsx.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method,
such as a file handle (e.g. via builtinopenfunction)
orStringIO.Deprecated since version 2.1.0:Passing byte strings is deprecated. To read from a
byte string, wrap it in aBytesIOobject.sheet_namestr, int, list, or None, default 0Strings are used for sheet names. Integers are used in zero-indexed
sheet positions (chart sheets do not count as a sheet position).
Lists of strings/integers are used to request multiple sheets.
Specify None to get all worksheets.Available cases:Defaults to0: 1st sheet as aDataFrame1: 2nd sheet as aDataFrame""Sheet1"": Load sheet with name ÅgSheet1Åh[0,1,""Sheet5""]: Load first, second and sheet named ÅgSheet5Åh
as a dict ofDataFrameNone: All worksheets.headerint, list of int, default 0Row (0-indexed) to use for the column labels of the parsed
DataFrame. If a list of integers is passed those row positions will
be combined into aMultiIndex. Use None if there is no header.namesarray-like, default NoneList of column names to use. If file contains no header row,
then you should explicitly pass header=None.index_colint, str, list of int, default NoneColumn (0-indexed) to use as the row labels of the DataFrame.
Pass None if there is no such column. If a list is passed,
those columns will be combined into aMultiIndex. If a
subset of data is selected withusecols, index_col
is based on the subset.Missing values will be forward filled to allow roundtripping withto_excelformerged_cells=True. To avoid forward filling the
missing values useset_indexafter reading the data instead ofindex_col.usecolsstr, list-like, or callable, default NoneIf None, then parse all columns.If str, then indicates comma separated list of Excel column letters
and column ranges (e.g. ÅgA:EÅh or ÅgA,C,E:FÅh). Ranges are inclusive of
both sides.If list of int, then indicates list of column numbers to be parsed
(0-indexed).If list of string, then indicates list of column names to be parsed.If callable, then evaluate each column name against it and parse the
column if the callable returnsTrue.Returns a subset of the columns according to behavior above.dtypeType name or dict of column -> type, default NoneData type for data or columns. E.g. {ÅeaÅf: np.float64, ÅebÅf: np.int32}
Useobjectto preserve data as stored in Excel and not interpret dtype.
If converters are specified, they will be applied INSTEAD
of dtype conversion.enginestr, default NoneIf io is not a buffer or path, this must be set to identify io.
Supported engines: ÅgxlrdÅh, ÅgopenpyxlÅh, ÅgodfÅh, ÅgpyxlsbÅh.
Engine compatibility :ÅgxlrdÅh supports old-style Excel files (.xls).ÅgopenpyxlÅh supports newer Excel file formats.ÅgodfÅh supports OpenDocument file formats (.odf, .ods, .odt).ÅgpyxlsbÅh supports Binary Excel files.Changed in version 1.2.0:The enginexlrdnow only supports old-style.xlsfiles.
Whenengine=None, the following logic will be
used to determine the engine:Ifpath_or_bufferis an OpenDocument format (.odf, .ods, .odt),
thenodfwill be used.Otherwise ifpath_or_bufferis an xls format,xlrdwill be used.Otherwise ifpath_or_bufferis in xlsb format,pyxlsbwill be used.New in version 1.3.0.Otherwiseopenpyxlwill be used.Changed in version 1.3.0.convertersdict, default NoneDict of functions for converting values in certain columns. Keys can
either be integers or column labels, values are functions that take one
input argument, the Excel cell content, and return the transformed
content.true_valueslist, default NoneValues to consider as True.false_valueslist, default NoneValues to consider as False.skiprowslist-like, int, or callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int) at the
start of the file. If callable, the callable function will be evaluated
against the row indices, returning True if the row should be skipped and
False otherwise. An example of a valid callable argument would belambdax:xin[0,2].nrowsint, default NoneNumber of rows to parse.na_valuesscalar, str, list-like, or dict, default NoneAdditional strings to recognize as NA/NaN. If dict passed, specific
per-column NA values. By default the following values are interpreted
as NaN: ÅeÅf, Åe#N/AÅf, Åe#N/A N/AÅf, Åe#NAÅf, Åe-1.#INDÅf, Åe-1.#QNANÅf, Åe-NaNÅf, Åe-nanÅf,
Åe1.#INDÅf, Åe1.#QNANÅf, Åe<NA>Åf, ÅeN/AÅf, ÅeNAÅf, ÅeNULLÅf, ÅeNaNÅf, ÅeNoneÅf,
Åen/aÅf, ÅenanÅf, ÅenullÅf.keep_default_nabool, default TrueWhether or not to include the default NaN values when parsing the data.
Depending on whetherna_valuesis passed in, the behavior is as follows:Ifkeep_default_nais True, andna_valuesare specified,na_valuesis appended to the default NaN values used for parsing.Ifkeep_default_nais True, andna_valuesare not specified, only
the default NaN values are used for parsing.Ifkeep_default_nais False, andna_valuesare specified, only
the NaN values specifiedna_valuesare used for parsing.Ifkeep_default_nais False, andna_valuesare not specified, no
strings will be parsed as NaN.Note that ifna_filteris passed in as False, thekeep_default_naandna_valuesparameters will be ignored.na_filterbool, default TrueDetect missing value markers (empty strings and the value of na_values). In
data without any NAs, passing na_filter=False can improve the performance
of reading a large file.verbosebool, default FalseIndicate number of NA values placed in non-numeric columns.parse_datesbool, list-like, or dict, default FalseThe behavior is as follows:bool. If True -> try parsing the index.list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3
each as a separate date column.list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as
a single date column.dict, e.g. {ÅefooÅf : [1, 3]} -> parse columns 1, 3 as date and call
result ÅefooÅfIf a column or index contains an unparsable date, the entire column or
index will be returned unaltered as an object data type. If you don`t want to
parse some cells as date just change their type in Excel to ÅgTextÅh.
For non-standard datetime parsing, usepd.to_datetimeafterpd.read_excel.Note: A fast-path exists for iso8601-formatted dates.date_parserfunction, optionalFunction to use for converting a sequence of string columns to an array of
datetime instances. The default usesdateutil.parser.parserto do the
conversion. Pandas will try to calldate_parserin three different ways,
advancing to the next if an exception occurs: 1) Pass one or more arrays
(as defined byparse_dates) as arguments; 2) concatenate (row-wise) the
string values from the columns defined byparse_datesinto a single array
and pass that; and 3) calldate_parseronce for each row using one or
more strings (corresponding to the columns defined byparse_dates) as
arguments.Deprecated since version 2.0.0:Usedate_formatinstead, or read in asobjectand then applyto_datetime()as-needed.date_formatstr or dict of column -> format, defaultNoneIf used in conjunction withparse_dates, will parse dates according to this
format. For anything more complex,
please read in asobjectand then applyto_datetime()as-needed.New in version 2.0.0.thousandsstr, default NoneThousands separator for parsing string columns to numeric. Note that
this parameter is only necessary for columns stored as TEXT in Excel,
any numeric columns will automatically be parsed, regardless of display
format.decimalstr, default Åe.ÅfCharacter to recognize as decimal point for parsing string columns to numeric.
Note that this parameter is only necessary for columns stored as TEXT in Excel,
any numeric columns will automatically be parsed, regardless of display
format.(e.g. use Åe,Åf for European data).New in version 1.4.0.commentstr, default NoneComments out remainder of line. Pass a character or characters to this
argument to indicate comments in the input file. Any data between the
comment string and the end of the current line is ignored.skipfooterint, default 0Rows at the end to skip (0-indexed).storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.dtype_backend{Åenumpy_nullableÅf, ÅepyarrowÅf}, default Åenumpy_nullableÅfBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.Returns:DataFrame or dict of DataFramesDataFrame from the passed in Excel file. See notes in sheet_name
argument for more information on when a dict of DataFrames is returned.See alsoDataFrame.to_excelWrite DataFrame to an Excel file.DataFrame.to_csvWrite DataFrame to a comma-separated values (csv) file.read_csvRead a comma-separated values (csv) file into DataFrame.read_fwfRead a table of fixed-width formatted lines into DataFrame.NotesFor specific information on the methods used for each Excel engine, refer to the pandasuser guideExamplesThe file can be read using the file name as string or an open file object:>>>pd.read_excel('tmp.xlsx',index_col=0)Name  Value0   string1      11   string2      22  #Comment      3>>>pd.read_excel(open('tmp.xlsx','rb'),...sheet_name='Sheet3')Unnamed: 0      Name  Value0           0   string1      11           1   string2      22           2  #Comment      3Index and header can be specified via theindex_colandheaderarguments>>>pd.read_excel('tmp.xlsx',index_col=None,header=None)0         1      20  NaN      Name  Value1  0.0   string1      12  1.0   string2      23  2.0  #Comment      3Column types are inferred but can be explicitly specified>>>pd.read_excel('tmp.xlsx',index_col=0,...dtype={'Name':str,'Value':float})Name  Value0   string1    1.01   string2    2.02  #Comment    3.0True, False, and NA values, and thousands separators have defaults,
but can be explicitly specified, too. Supply the values you would like
as strings or lists of strings!>>>pd.read_excel('tmp.xlsx',index_col=0,...na_values=['string1','string2'])Name  Value0       NaN      11       NaN      22  #Comment      3Comment lines in the excel input file can be skipped using thecommentkwarg>>>pd.read_excel('tmp.xlsx',index_col=0,comment='#')Name  Value0  string1    1.01  string2    2.02     None    NaN"
Pandas,Combined_csv,pandas.DataFrame.head,"pandas.DataFrame.head#DataFrame.head(n=5)[source]#Return the firstnrows.This function returns the firstnrows for the object based
on position. It is useful for quickly testing if your object
has the right type of data in it.For negative values ofn, this function returns all rows except
the last|n|rows, equivalent todf[:n].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:same type as callerThe firstnrows of the caller object.See alsoDataFrame.tailReturns the lastnrows.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the first 5 lines>>>df.head()animal0  alligator1        bee2     falcon3       lion4     monkeyViewing the firstnlines (three in this case)>>>df.head(3)animal0  alligator1        bee2     falconFor negative values ofn>>>df.head(-3)animal0  alligator1        bee2     falcon3       lion4     monkey5     parrot"
Pandas,Combined_csv,pandas.DataFrame.tail,"pandas.DataFrame.tail#DataFrame.tail(n=5)[source]#Return the lastnrows.This function returns lastnrows from the object based on
position. It is useful for quickly verifying data, for example,
after sorting or appending rows.For negative values ofn, this function returns all rows except
the first|n|rows, equivalent todf[|n|:].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:type of callerThe lastnrows of the caller object.See alsoDataFrame.headThe firstnrows of the caller object.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the last 5 lines>>>df.tail()animal4  monkey5  parrot6   shark7   whale8   zebraViewing the lastnlines (three in this case)>>>df.tail(3)animal6  shark7  whale8  zebraFor negative values ofn>>>df.tail(-3)animal3    lion4  monkey5  parrot6   shark7   whale8   zebra"
Pandas,Combined_csv,pandas.DataFrame.describe,"pandas.DataFrame.describe#DataFrame.describe(percentiles=None,include=None,exclude=None)[source]#Generate descriptive statistics.Descriptive statistics include those that summarize the central
tendency, dispersion and shape of a
datasetÅfs distribution, excludingNaNvalues.Analyzes both numeric and object series, as well
asDataFramecolumn sets of mixed data types. The output
will vary depending on what is provided. Refer to the notes
below for more detail.Parameters:percentileslist-like of numbers, optionalThe percentiles to include in the output. All should
fall between 0 and 1. The default is[.25,.5,.75], which returns the 25th, 50th, and
75th percentiles.includeÅeallÅf, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored
forSeries. Here are the options:ÅeallÅf : All columns of the input will be included in the output.A list-like of dtypes : Limits the results to the
provided data types.
To limit the result to numeric types submitnumpy.number. To limit it instead to object columns submit
thenumpy.objectdata type. Strings
can also be used in the style ofselect_dtypes(e.g.df.describe(include=['O'])). To
select pandas categorical columns, use'category'None (default) : The result will include all numeric columns.excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored
forSeries. Here are the options:A list-like of dtypes : Excludes the provided data types
from the result. To exclude numeric types submitnumpy.number. To exclude object columns submit the data
typenumpy.object. Strings can also be used in the style ofselect_dtypes(e.g.df.describe(exclude=['O'])). To
exclude pandas categorical columns, use'category'None (default) : The result will exclude nothing.Returns:Series or DataFrameSummary statistics of the Series or Dataframe provided.See alsoDataFrame.countCount number of non-NA/null observations.DataFrame.maxMaximum of the values in the object.DataFrame.minMinimum of the values in the object.DataFrame.meanMean of the values.DataFrame.stdStandard deviation of the observations.DataFrame.select_dtypesSubset of a DataFrame including/excluding columns based on their dtype.NotesFor numeric data, the resultÅfs index will includecount,mean,std,min,maxas well as lower,50and
upper percentiles. By default the lower percentile is25and the
upper percentile is75. The50percentile is the
same as the median.For object data (e.g. strings or timestamps), the resultÅfs index
will includecount,unique,top, andfreq. Thetopis the most common value. Thefreqis the most common valueÅfs
frequency. Timestamps also include thefirstandlastitems.If multiple object values have the highest count, then thecountandtopresults will be arbitrarily chosen from
among those with the highest count.For mixed data types provided via aDataFrame, the default is to
return only an analysis of numeric columns. If the dataframe consists
only of object and categorical data without any numeric columns, the
default is to return an analysis of both the object and categorical
columns. Ifinclude='all'is provided as an option, the result
will include a union of attributes of each type.Theincludeandexcludeparameters can be used to limit
which columns in aDataFrameare analyzed for the output.
The parameters are ignored when analyzing aSeries.ExamplesDescribing a numericSeries.>>>s=pd.Series([1,2,3])>>>s.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0dtype: float64Describing a categoricalSeries.>>>s=pd.Series(['a','a','b','c'])>>>s.describe()count     4unique    3top       afreq      2dtype: objectDescribing a timestampSeries.>>>s=pd.Series([...np.datetime64(""2000-01-01""),...np.datetime64(""2010-01-01""),...np.datetime64(""2010-01-01"")...])>>>s.describe()count                      3mean     2006-09-01 08:00:00min      2000-01-01 00:00:0025%      2004-12-31 12:00:0050%      2010-01-01 00:00:0075%      2010-01-01 00:00:00max      2010-01-01 00:00:00dtype: objectDescribing aDataFrame. By default only numeric fields
are returned.>>>df=pd.DataFrame({'categorical':pd.Categorical(['d','e','f']),...'numeric':[1,2,3],...'object':['a','b','c']...})>>>df.describe()numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Describing all columns of aDataFrameregardless of data type.>>>df.describe(include='all')categorical  numeric objectcount            3      3.0      3unique           3      NaN      3top              f      NaN      afreq             1      NaN      1mean           NaN      2.0    NaNstd            NaN      1.0    NaNmin            NaN      1.0    NaN25%            NaN      1.5    NaN50%            NaN      2.0    NaN75%            NaN      2.5    NaNmax            NaN      3.0    NaNDescribing a column from aDataFrameby accessing it as
an attribute.>>>df.numeric.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0Name: numeric, dtype: float64Including only numeric columns in aDataFramedescription.>>>df.describe(include=[np.number])numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Including only string columns in aDataFramedescription.>>>df.describe(include=[object])objectcount       3unique      3top         afreq        1Including only categorical columns from aDataFramedescription.>>>df.describe(include=['category'])categoricalcount            3unique           3top              dfreq             1Excluding numeric columns from aDataFramedescription.>>>df.describe(exclude=[np.number])categorical objectcount            3      3unique           3      3top              f      afreq             1      1Excluding object columns from aDataFramedescription.>>>df.describe(exclude=[object])categorical  numericcount            3      3.0unique           3      NaNtop              f      NaNfreq             1      NaNmean           NaN      2.0std            NaN      1.0min            NaN      1.025%            NaN      1.550%            NaN      2.075%            NaN      2.5max            NaN      3.0"
Pandas,Combined_csv,pandas.DataFrame.drop,"pandas.DataFrame.drop#DataFrame.drop(labels=None,*,axis=0,index=None,columns=None,level=None,inplace=False,errors='raise')[source]#Drop specified labels from rows or columns.Remove rows or columns by specifying label names and corresponding
axis, or by directly specifying index or column names. When using a
multi-index, labels on different levels can be removed by specifying
the level. See theuser guidefor more information about the now unused levels.Parameters:labelssingle label or list-likeIndex or column labels to drop. A tuple will be used as a single
label and not treated as a list-like.axis{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}, default 0Whether to drop labels from the index (0 or ÅeindexÅf) or
columns (1 or ÅecolumnsÅf).indexsingle label or list-likeAlternative to specifying axis (labels,axis=0is equivalent toindex=labels).columnssingle label or list-likeAlternative to specifying axis (labels,axis=1is equivalent tocolumns=labels).levelint or level name, optionalFor MultiIndex, level from which the labels will be removed.inplacebool, default FalseIf False, return a copy. Otherwise, do operation
in place and return None.errors{ÅeignoreÅf, ÅeraiseÅf}, default ÅeraiseÅfIf ÅeignoreÅf, suppress error and only existing labels are
dropped.Returns:DataFrame or NoneReturns DataFrame or None DataFrame with the specified
index or column labels removed or None if inplace=True.Raises:KeyErrorIf any of the labels is not found in the selected axis.See alsoDataFrame.locLabel-location based indexer for selection by label.DataFrame.dropnaReturn DataFrame with labels on given axis omitted where (all or any) data are missing.DataFrame.drop_duplicatesReturn DataFrame with duplicate rows removed, optionally only considering certain columns.Series.dropReturn Series with specified index labels removed.Examples>>>df=pd.DataFrame(np.arange(12).reshape(3,4),...columns=['A','B','C','D'])>>>dfA  B   C   D0  0  1   2   31  4  5   6   72  8  9  10  11Drop columns>>>df.drop(['B','C'],axis=1)A   D0  0   31  4   72  8  11>>>df.drop(columns=['B','C'])A   D0  0   31  4   72  8  11Drop a row by index>>>df.drop([0,1])A  B   C   D2  8  9  10  11Drop columns and/or rows of MultiIndex DataFrame>>>midx=pd.MultiIndex(levels=[['llama','cow','falcon'],...['speed','weight','length']],...codes=[[0,0,0,1,1,1,2,2,2],...[0,1,2,0,1,2,0,1,2]])>>>df=pd.DataFrame(index=midx,columns=['big','small'],...data=[[45,30],[200,100],[1.5,1],[30,20],...[250,150],[1.5,0.8],[320,250],...[1,0.8],[0.3,0.2]])>>>dfbig     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0weight  1.0     0.8length  0.3     0.2Drop a specific index combination from the MultiIndex
DataFrame, i.e., drop the combination'falcon'and'weight', which deletes only the corresponding row>>>df.drop(index=('falcon','weight'))big     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0length  0.3     0.2>>>df.drop(index='cow',columns='small')bigllama   speed   45.0weight  200.0length  1.5falcon  speed   320.0weight  1.0length  0.3>>>df.drop(index='length',level=1)big     smallllama   speed   45.0    30.0weight  200.0   100.0cow     speed   30.0    20.0weight  250.0   150.0falcon  speed   320.0   250.0weight  1.0     0.8"
Pandas,Combined_csv,pandas.DataFrame.dropna,"pandas.DataFrame.dropna#DataFrame.dropna(*,axis=0,how=_NoDefault.no_default,thresh=_NoDefault.no_default,subset=None,inplace=False,ignore_index=False)[source]#Remove missing values.See theUser Guidefor more on which values are
considered missing, and how to work with missing data.Parameters:axis{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}, default 0Determine if rows or columns which contain missing values are
removed.0, or ÅeindexÅf : Drop rows which contain missing values.1, or ÅecolumnsÅf : Drop columns which contain missing value.Only a single axis is allowed.how{ÅeanyÅf, ÅeallÅf}, default ÅeanyÅfDetermine if row or column is removed from DataFrame, when we have
at least one NA or all NA.ÅeanyÅf : If any NA values are present, drop that row or column.ÅeallÅf : If all values are NA, drop that row or column.threshint, optionalRequire that many non-NA values. Cannot be combined with how.subsetcolumn label or sequence of labels, optionalLabels along other axis to consider, e.g. if you are dropping rows
these would be a list of columns to include.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.ignore_indexbool, defaultFalseIfTrue, the resulting axis will be labeled 0, 1, Åc, n - 1.New in version 2.0.0.Returns:DataFrame or NoneDataFrame with NA entries dropped from it or None ifinplace=True.See alsoDataFrame.isnaIndicate missing values.DataFrame.notnaIndicate existing (non-missing) values.DataFrame.fillnaReplace missing values.Series.dropnaDrop missing values.Index.dropnaDrop missing indices.Examples>>>df=pd.DataFrame({""name"":['Alfred','Batman','Catwoman'],...""toy"":[np.nan,'Batmobile','Bullwhip'],...""born"":[pd.NaT,pd.Timestamp(""1940-04-25""),...pd.NaT]})>>>dfname        toy       born0    Alfred        NaN        NaT1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaTDrop the rows where at least one element is missing.>>>df.dropna()name        toy       born1  Batman  Batmobile 1940-04-25Drop the columns where at least one element is missing.>>>df.dropna(axis='columns')name0    Alfred1    Batman2  CatwomanDrop the rows where all elements are missing.>>>df.dropna(how='all')name        toy       born0    Alfred        NaN        NaT1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaTKeep only the rows with at least 2 non-NA values.>>>df.dropna(thresh=2)name        toy       born1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaTDefine in which columns to look for missing values.>>>df.dropna(subset=['name','toy'])name        toy       born1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaT"
Pandas,Combined_csv,pandas.Series.value_counts,"pandas.Series.value_counts#Series.value_counts(normalize=False,sort=True,ascending=False,bins=None,dropna=True)[source]#Return a Series containing counts of unique values.The resulting object will be in descending order so that the
first element is the most frequently-occurring element.
Excludes NA values by default.Parameters:normalizebool, default FalseIf True then the object returned will contain the relative
frequencies of the unique values.sortbool, default TrueSort by frequencies when True. Preserve the order of the data when False.ascendingbool, default FalseSort in ascending order.binsint, optionalRather than count values, group them into half-open bins,
a convenience forpd.cut, only works with numeric data.dropnabool, default TrueDonÅft include counts of NaN.Returns:SeriesSee alsoSeries.countNumber of non-NA elements in a Series.DataFrame.countNumber of non-NA elements in a DataFrame.DataFrame.value_countsEquivalent method on DataFrames.Examples>>>index=pd.Index([3,1,2,3,4,np.nan])>>>index.value_counts()3.0    21.0    12.0    14.0    1Name: count, dtype: int64Withnormalizeset toTrue, returns the relative frequency by
dividing all values by the sum of values.>>>s=pd.Series([3,1,2,3,4,np.nan])>>>s.value_counts(normalize=True)3.0    0.41.0    0.22.0    0.24.0    0.2Name: proportion, dtype: float64binsBins can be useful for going from a continuous variable to a
categorical variable; instead of counting unique
apparitions of values, divide the index in the specified
number of half-open bins.>>>s.value_counts(bins=3)(0.996, 2.0]    2(2.0, 3.0]      2(3.0, 4.0]      1Name: count, dtype: int64dropnaWithdropnaset toFalsewe can also see NaN index values.>>>s.value_counts(dropna=False)3.0    21.0    12.0    14.0    1NaN    1Name: count, dtype: int64"
Pandas,Combined_csv,pandas.Series.apply,"pandas.Series.apply#Series.apply(func,convert_dtype=_NoDefault.no_default,args=(),*,by_row='compat',**kwargs)[source]#Invoke function on values of Series.Can be ufunc (a NumPy function that applies to the entire Series)
or a Python function that only works on single values.Parameters:funcfunctionPython function or NumPy ufunc to apply.convert_dtypebool, default TrueTry to find better dtype for elementwise function results. If
False, leave as dtype=object. Note that the dtype is always
preserved for some extension array dtypes, such as Categorical.Deprecated since version 2.1.0:convert_dtypehas been deprecated. Doser.astype(object).apply()instead if you wantconvert_dtype=False.argstuplePositional arguments passed to func after the series value.by_rowFalse or ÅgcompatÅh, default ÅgcompatÅhIf""compat""and func is a callable, func will be passed each element of
the Series, likeSeries.map. If func is a list or dict of
callables, will first try to translate each func into pandas methods. If
that doesnÅft work, will try call to apply again withby_row=""compat""and if that fails, will call apply again withby_row=False(backward compatible).
If False, the func will be passed the whole Series at once.by_rowhas no effect whenfuncis a string.New in version 2.1.0.**kwargsAdditional keyword arguments passed to func.Returns:Series or DataFrameIf func returns a Series object the result will be a DataFrame.See alsoSeries.mapFor element-wise operations.Series.aggOnly perform aggregating type operations.Series.transformOnly perform transforming type operations.NotesFunctions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.ExamplesCreate a series with typical summer temperatures for each city.>>>s=pd.Series([20,21,12],...index=['London','New York','Helsinki'])>>>sLondon      20New York    21Helsinki    12dtype: int64Square the values by defining a function and passing it as an
argument toapply().>>>defsquare(x):...returnx**2>>>s.apply(square)London      400New York    441Helsinki    144dtype: int64Square the values by passing an anonymous function as an
argument toapply().>>>s.apply(lambdax:x**2)London      400New York    441Helsinki    144dtype: int64Define a custom function that needs additional positional
arguments and pass these additional arguments using theargskeyword.>>>defsubtract_custom_value(x,custom_value):...returnx-custom_value>>>s.apply(subtract_custom_value,args=(5,))London      15New York    16Helsinki     7dtype: int64Define a custom function that takes keyword arguments
and pass these arguments toapply.>>>defadd_custom_values(x,**kwargs):...formonthinkwargs:...x+=kwargs[month]...returnx>>>s.apply(add_custom_values,june=30,july=20,august=25)London      95New York    96Helsinki    87dtype: int64Use a function from the Numpy library.>>>s.apply(np.log)London      2.995732New York    3.044522Helsinki    2.484907dtype: float64"
Pandas,Combined_csv,pandas.merge,"pandas.merge#pandas.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=None,indicator=False,validate=None)[source]#Merge DataFrame or named Series objects with a database-style join.A named Series object is treated as a DataFrame with a single named column.The join is done on columns or indexes. If joining columns on
columns, the DataFrame indexeswill be ignored. Otherwise if joining indexes
on indexes or indexes on a column or columns, the index will be passed on.
When performing a cross merge, no column specifications to merge on are
allowed.WarningIf both key columns contain rows where the key is a null value, those
rows will be matched against each other. This is different from usual SQL
join behaviour and can lead to unexpected results.Parameters:leftDataFrame or named SeriesrightDataFrame or named SeriesObject to merge with.how{ÅeleftÅf, ÅerightÅf, ÅeouterÅf, ÅeinnerÅf, ÅecrossÅf}, default ÅeinnerÅfType of merge to be performed.left: use only keys from left frame, similar to a SQL left outer join;
preserve key order.right: use only keys from right frame, similar to a SQL right outer join;
preserve key order.outer: use union of keys from both frames, similar to a SQL full outer
join; sort keys lexicographically.inner: use intersection of keys from both frames, similar to a SQL inner
join; preserve the order of the left keys.cross: creates the cartesian product from both frames, preserves the order
of the left keys.New in version 1.2.0.onlabel or listColumn or index level names to join on. These must be found in both
DataFrames. Ifonis None and not merging on indexes then this defaults
to the intersection of the columns in both DataFrames.left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also
be an array or list of arrays of the length of the left DataFrame.
These arrays are treated as if they are columns.right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also
be an array or list of arrays of the length of the right DataFrame.
These arrays are treated as if they are columns.left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a
MultiIndex, the number of keys in the other DataFrame (either the index
or a number of columns) must match the number of levels.right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as
left_index.sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False,
the order of the join keys depends on the join type (how keyword).suffixeslist-like, default is (Åg_xÅh, Åg_yÅh)A length-2 sequence where each element is optionally a string
indicating the suffix to add to overlapping column names inleftandrightrespectively. Pass a value ofNoneinstead
of a string to indicate that the column name fromleftorrightshould be left as-is, with no suffix. At least one of the
values must not be None.copybool, default TrueIf False, avoid copy if possible.indicatorbool or str, default FalseIf True, adds a column to the output DataFrame called Åg_mergeÅh with
information on the source of each row. The column can be given a different
name by providing a string argument. The column will have a Categorical
type with the value of Ågleft_onlyÅh for observations whose merge key only
appears in the left DataFrame, Ågright_onlyÅh for observations
whose merge key only appears in the right DataFrame, and ÅgbothÅh
if the observationÅfs merge key is found in both DataFrames.validatestr, optionalIf specified, checks if merge is of specified type.Ågone_to_oneÅh or Åg1:1Åh: check if merge keys are unique in both
left and right datasets.Ågone_to_manyÅh or Åg1:mÅh: check if merge keys are unique in left
dataset.Ågmany_to_oneÅh or Ågm:1Åh: check if merge keys are unique in right
dataset.Ågmany_to_manyÅh or Ågm:mÅh: allowed, but does not result in checks.Returns:DataFrameA DataFrame of the two merged objects.See alsomerge_orderedMerge with optional filling/interpolation.merge_asofMerge on nearest keys.DataFrame.joinSimilar method using indices.Examples>>>df1=pd.DataFrame({'lkey':['foo','bar','baz','foo'],...'value':[1,2,3,5]})>>>df2=pd.DataFrame({'rkey':['foo','bar','baz','foo'],...'value':[5,6,7,8]})>>>df1lkey value0   foo      11   bar      22   baz      33   foo      5>>>df2rkey value0   foo      51   bar      62   baz      73   foo      8Merge df1 and df2 on the lkey and rkey columns. The value columns have
the default suffixes, _x and _y, appended.>>>df1.merge(df2,left_on='lkey',right_on='rkey')lkey  value_x rkey  value_y0  foo        1  foo        51  foo        1  foo        82  foo        5  foo        53  foo        5  foo        84  bar        2  bar        65  baz        3  baz        7Merge DataFrames df1 and df2 with specified left and right suffixes
appended to any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',...suffixes=('_left','_right'))lkey  value_left rkey  value_right0  foo           1  foo            51  foo           1  foo            82  foo           5  foo            53  foo           5  foo            84  bar           2  bar            65  baz           3  baz            7Merge DataFrames df1 and df2, but raise an exception if the DataFrames have
any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',suffixes=(False,False))Traceback (most recent call last):...ValueError:columns overlap but no suffix specified:Index(['value'], dtype='object')>>>df1=pd.DataFrame({'a':['foo','bar'],'b':[1,2]})>>>df2=pd.DataFrame({'a':['foo','baz'],'c':[3,4]})>>>df1a  b0   foo  11   bar  2>>>df2a  c0   foo  31   baz  4>>>df1.merge(df2,how='inner',on='a')a  b  c0   foo  1  3>>>df1.merge(df2,how='left',on='a')a  b  c0   foo  1  3.01   bar  2  NaN>>>df1=pd.DataFrame({'left':['foo','bar']})>>>df2=pd.DataFrame({'right':[7,8]})>>>df1left0   foo1   bar>>>df2right0   71   8>>>df1.merge(df2,how='cross')left  right0   foo      71   foo      82   bar      73   bar      8"
Pandas,Combined_csv,pandas.concat,"pandas.concat#pandas.concat(objs,*,axis=0,join='outer',ignore_index=False,keys=None,levels=None,names=None,verify_integrity=False,sort=False,copy=None)[source]#Concatenate pandas objects along a particular axis.Allows optional set logic along the other axes.Can also add a layer of hierarchical indexing on the concatenation axis,
which may be useful if the labels are the same (or overlapping) on
the passed axis number.Parameters:objsa sequence or mapping of Series or DataFrame objectsIf a mapping is passed, the sorted keys will be used as thekeysargument, unless it is passed, in which case the values will be
selected (see below). Any None objects will be dropped silently unless
they are all None in which case a ValueError will be raised.axis{0/ÅfindexÅf, 1/ÅfcolumnsÅf}, default 0The axis to concatenate along.join{ÅeinnerÅf, ÅeouterÅf}, default ÅeouterÅfHow to handle indexes on other axis (or axes).ignore_indexbool, default FalseIf True, do not use the index values along the concatenation axis. The
resulting axis will be labeled 0, Åc, n - 1. This is useful if you are
concatenating objects where the concatenation axis does not have
meaningful indexing information. Note the index values on the other
axes are still respected in the join.keyssequence, default NoneIf multiple levels passed, should contain tuples. Construct
hierarchical index using the passed keys as the outermost level.levelslist of sequences, default NoneSpecific levels (unique values) to use for constructing a
MultiIndex. Otherwise they will be inferred from the keys.nameslist, default NoneNames for the levels in the resulting hierarchical index.verify_integritybool, default FalseCheck whether the new concatenated axis contains duplicates. This can
be very expensive relative to the actual data concatenation.sortbool, default FalseSort non-concatenation axis if it is not already aligned.copybool, default TrueIf False, do not copy data unnecessarily.Returns:object, type of objsWhen concatenating allSeriesalong the index (axis=0), aSeriesis returned. Whenobjscontains at least oneDataFrame, aDataFrameis returned. When concatenating along
the columns (axis=1), aDataFrameis returned.See alsoDataFrame.joinJoin DataFrames using indexes.DataFrame.mergeMerge DataFrames by indexes or columns.NotesThe keys, levels, and names arguments are all optional.A walkthrough of how this method fits in with other tools for combining
pandas objects can be foundhere.It is not recommended to build DataFrames by adding single rows in a
for loop. Build a list of rows and make a DataFrame in a single concat.ExamplesCombine twoSeries.>>>s1=pd.Series(['a','b'])>>>s2=pd.Series(['c','d'])>>>pd.concat([s1,s2])0    a1    b0    c1    ddtype: objectClear the existing index and reset it in the result
by setting theignore_indexoption toTrue.>>>pd.concat([s1,s2],ignore_index=True)0    a1    b2    c3    ddtype: objectAdd a hierarchical index at the outermost level of
the data with thekeysoption.>>>pd.concat([s1,s2],keys=['s1','s2'])s1  0    a1    bs2  0    c1    ddtype: objectLabel the index keys you create with thenamesoption.>>>pd.concat([s1,s2],keys=['s1','s2'],...names=['Series name','Row ID'])Series name  Row IDs1           0         a1         bs2           0         c1         ddtype: objectCombine twoDataFrameobjects with identical columns.>>>df1=pd.DataFrame([['a',1],['b',2]],...columns=['letter','number'])>>>df1letter  number0      a       11      b       2>>>df2=pd.DataFrame([['c',3],['d',4]],...columns=['letter','number'])>>>df2letter  number0      c       31      d       4>>>pd.concat([df1,df2])letter  number0      a       11      b       20      c       31      d       4CombineDataFrameobjects with overlapping columns
and return everything. Columns outside the intersection will
be filled withNaNvalues.>>>df3=pd.DataFrame([['c',3,'cat'],['d',4,'dog']],...columns=['letter','number','animal'])>>>df3letter  number animal0      c       3    cat1      d       4    dog>>>pd.concat([df1,df3],sort=False)letter  number animal0      a       1    NaN1      b       2    NaN0      c       3    cat1      d       4    dogCombineDataFrameobjects with overlapping columns
and return only those that are shared by passinginnerto
thejoinkeyword argument.>>>pd.concat([df1,df3],join=""inner"")letter  number0      a       11      b       20      c       31      d       4CombineDataFrameobjects horizontally along the x axis by
passing inaxis=1.>>>df4=pd.DataFrame([['bird','polly'],['monkey','george']],...columns=['animal','name'])>>>pd.concat([df1,df4],axis=1)letter  number  animal    name0      a       1    bird   polly1      b       2  monkey  georgePrevent the result from including duplicate index values with theverify_integrityoption.>>>df5=pd.DataFrame([1],index=['a'])>>>df50a  1>>>df6=pd.DataFrame([2],index=['a'])>>>df60a  2>>>pd.concat([df5,df6],verify_integrity=True)Traceback (most recent call last):...ValueError:Indexes have overlapping values: ['a']Append a single row to the end of aDataFrameobject.>>>df7=pd.DataFrame({'a':1,'b':2},index=[0])>>>df7a   b0   1   2>>>new_row=pd.Series({'a':3,'b':4})>>>new_rowa    3b    4dtype: int64>>>pd.concat([df7,new_row.to_frame().T],ignore_index=True)a   b0   1   21   3   4"
Pandas,Combined_csv,pandas.DataFrame.sort_values,"pandas.DataFrame.sort_values#DataFrame.sort_values(by,*,axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last',ignore_index=False,key=None)[source]#Sort by the values along either axis.Parameters:bystr or list of strName or list of names to sort by.ifaxisis 0 orÅeindexÅfthenbymay contain index
levels and/or column labels.ifaxisis 1 orÅecolumnsÅfthenbymay contain column
levels and/or index labels.axisÅg{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}Åh, default 0Axis to be sorted.ascendingbool or list of bool, default TrueSort ascending vs. descending. Specify list for multiple sort
orders. If this is a list of bools, must match the length of
the by.inplacebool, default FalseIf True, perform operation in-place.kind{ÅequicksortÅf, ÅemergesortÅf, ÅeheapsortÅf, ÅestableÅf}, default ÅequicksortÅfChoice of sorting algorithm. See alsonumpy.sort()for more
information.mergesortandstableare the only stable algorithms. For
DataFrames, this option is only applied when sorting on a single
column or label.na_position{ÅefirstÅf, ÅelastÅf}, default ÅelastÅfPuts NaNs at the beginning iffirst;lastputs NaNs at the
end.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, Åc, n - 1.keycallable, optionalApply the key function to the values
before sorting. This is similar to thekeyargument in the
builtinsorted()function, with the notable difference that
thiskeyfunction should bevectorized. It should expect aSeriesand return a Series with the same shape as the input.
It will be applied to each column inbyindependently.Returns:DataFrame or NoneDataFrame with sorted values or None ifinplace=True.See alsoDataFrame.sort_indexSort a DataFrame by the index.Series.sort_valuesSimilar method for a Series.Examples>>>df=pd.DataFrame({...'col1':['A','A','B',np.nan,'D','C'],...'col2':[2,1,9,8,7,4],...'col3':[0,1,9,4,2,3],...'col4':['a','B','c','D','e','F']...})>>>dfcol1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FSort by col1>>>df.sort_values(by=['col1'])col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort by multiple columns>>>df.sort_values(by=['col1','col2'])col1  col2  col3 col41    A     1     1    B0    A     2     0    a2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort Descending>>>df.sort_values(by='col1',ascending=False)col1  col2  col3 col44    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    B3  NaN     8     4    DPutting NAs first>>>df.sort_values(by='col1',ascending=False,na_position='first')col1  col2  col3 col43  NaN     8     4    D4    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    BSorting with a key function>>>df.sort_values(by='col4',key=lambdacol:col.str.lower())col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FNatural sort with the key argument,
using thenatsort <https://github.com/SethMMorton/natsort>package.>>>df=pd.DataFrame({...""time"":['0hr','128hr','72hr','48hr','96hr'],...""value"":[10,20,30,40,50]...})>>>dftime  value0    0hr     101  128hr     202   72hr     303   48hr     404   96hr     50>>>fromnatsortimportindex_natsorted>>>df.sort_values(...by=""time"",...key=lambdax:np.argsort(index_natsorted(df[""time""]))...)time  value0    0hr     103   48hr     402   72hr     304   96hr     501  128hr     20"
Pandas,Combined_csv,pandas.DataFrame.astype,"pandas.DataFrame.astype#DataFrame.astype(dtype,copy=None,errors='raise')[source]#Cast a pandas object to a specified dtypedtype.Parameters:dtypestr, data type, Series or Mapping of column name -> data typeUse a str, numpy.dtype, pandas.ExtensionDtype or Python type to
cast entire pandas object to the same type. Alternatively, use a
mapping, e.g. {col: dtype, Åc}, where col is a column label and dtype is
a numpy.dtype or Python type to cast one or more of the DataFrameÅfs
columns to column-specific types.copybool, default TrueReturn a copy whencopy=True(be very careful settingcopy=Falseas changes to values then may propagate to other
pandas objects).errors{ÅeraiseÅf, ÅeignoreÅf}, default ÅeraiseÅfControl raising of exceptions on invalid data for provided dtype.raise: allow exceptions to be raisedignore: suppress exceptions. On error return original object.Returns:same type as callerSee alsoto_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to a numeric type.numpy.ndarray.astypeCast a numpy array to a specified type.NotesChanged in version 2.0.0:Usingastypeto convert from timezone-naive dtype to
timezone-aware dtype will raise an exception.
UseSeries.dt.tz_localize()instead.ExamplesCreate a DataFrame:>>>d={'col1':[1,2],'col2':[3,4]}>>>df=pd.DataFrame(data=d)>>>df.dtypescol1    int64col2    int64dtype: objectCast all columns to int32:>>>df.astype('int32').dtypescol1    int32col2    int32dtype: objectCast col1 to int32 using a dictionary:>>>df.astype({'col1':'int32'}).dtypescol1    int32col2    int64dtype: objectCreate a series:>>>ser=pd.Series([1,2],dtype='int32')>>>ser0    11    2dtype: int32>>>ser.astype('int64')0    11    2dtype: int64Convert to categorical type:>>>ser.astype('category')0    11    2dtype: categoryCategories (2, int32): [1, 2]Convert to ordered categorical type with custom ordering:>>>frompandas.api.typesimportCategoricalDtype>>>cat_dtype=CategoricalDtype(...categories=[2,1],ordered=True)>>>ser.astype(cat_dtype)0    11    2dtype: categoryCategories (2, int64): [2 < 1]Create a series of dates:>>>ser_date=pd.Series(pd.date_range('20200101',periods=3))>>>ser_date0   2020-01-011   2020-01-022   2020-01-03dtype: datetime64[ns]"
Pandas,Combined_csv,pandas.to_datetime,"pandas.to_datetime#pandas.to_datetime(arg,errors='raise',dayfirst=False,yearfirst=False,utc=False,format=None,exact=_NoDefault.no_default,unit=None,infer_datetime_format=_NoDefault.no_default,origin='unix',cache=True)[source]#Convert argument to datetime.This function converts a scalar, array-like,SeriesorDataFrame/dict-like to a pandas datetime object.Parameters:argint, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-likeThe object to convert to a datetime. If aDataFrameis provided, the
method expects minimally the following columns:""year"",""month"",""day"". The column ÅgyearÅh
must be specified in 4-digit format.errors{ÅeignoreÅf, ÅeraiseÅf, ÅecoerceÅf}, default ÅeraiseÅfIf'raise', then invalid parsing will raise an exception.If'coerce', then invalid parsing will be set asNaT.If'ignore', then invalid parsing will return the input.dayfirstbool, default FalseSpecify a date parse order ifargis str or is list-like.
IfTrue, parses dates with the day first, e.g.""10/11/12""is parsed as2012-11-10.Warningdayfirst=Trueis not strict, but will prefer to parse
with day first.yearfirstbool, default FalseSpecify a date parse order ifargis str or is list-like.IfTrueparses dates with the year first, e.g.""10/11/12""is parsed as2010-11-12.If bothdayfirstandyearfirstareTrue,yearfirstis
preceded (same asdateutil).Warningyearfirst=Trueis not strict, but will prefer to parse
with year first.utcbool, default FalseControl timezone-related parsing, localization and conversion.IfTrue, the functionalwaysreturns a timezone-aware
UTC-localizedTimestamp,SeriesorDatetimeIndex. To do this, timezone-naive inputs arelocalizedas UTC, while timezone-aware inputs areconvertedto UTC.IfFalse(default), inputs will not be coerced to UTC.
Timezone-naive inputs will remain naive, while timezone-aware ones
will keep their time offsets. Limitations exist for mixed
offsets (typically, daylight savings), seeExamplessection for details.WarningIn a future version of pandas, parsing datetimes with mixed time
zones will raise an error unlessutc=True.
Please specifyutc=Trueto opt in to the new behaviour
and silence this warning. To create aSerieswith mixed offsets andobjectdtype, please useapplyanddatetime.datetime.strptime.See also: pandas general documentation abouttimezone conversion and
localization.formatstr, default NoneThe strftime to parse time, e.g.""%d/%m/%Y"". Seestrftime documentationfor more information on choices, though
note that""%f""will parse all the way up to nanoseconds.
You can also pass:ÅgISO8601Åh, to parse anyISO8601time string (not necessarily in exactly the same format);ÅgmixedÅh, to infer the format for each element individually. This is risky,
and you should probably use it along withdayfirst.NoteIf aDataFrameis passed, thenformathas no effect.exactbool, default TrueControl howformatis used:IfTrue, require an exactformatmatch.IfFalse, allow theformatto match anywhere in the target
string.Cannot be used alongsideformat='ISO8601'orformat='mixed'.unitstr, default ÅensÅfThe unit of the arg (D,s,ms,us,ns) denote the unit, which is an
integer or float number. This will be based off the origin.
Example, withunit='ms'andorigin='unix', this would calculate
the number of milliseconds to the unix epoch start.infer_datetime_formatbool, default FalseIfTrueand noformatis given, attempt to infer the format
of the datetime strings based on the first non-NaN element,
and if it can be inferred, switch to a faster method of parsing them.
In some cases this can increase the parsing speed by ~5-10x.Deprecated since version 2.0.0:A strict version of this argument is now the default, passing it has
no effect.originscalar, default ÅeunixÅfDefine the reference date. The numeric values would be parsed as number
of units (defined byunit) since this reference date.If'unix'(or POSIX) time; origin is set to 1970-01-01.If'julian', unit must be'D', and origin is set to
beginning of Julian Calendar. Julian day number0is assigned
to the day starting at noon on January 1, 4713 BC.If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date
string), origin is set to Timestamp identified by origin.If a float or integer, origin is the millisecond difference
relative to 1970-01-01.cachebool, default TrueIfTrue, use a cache of unique, converted dates to apply the
datetime conversion. May produce significant speed-up when parsing
duplicate date strings, especially ones with timezone offsets. The cache
is only used when there are at least 50 values. The presence of
out-of-bounds values will render the cache unusable and may slow down
parsing.Returns:datetimeIf parsing succeeded.
Return type depends on input (types in parenthesis correspond to
fallback in case of unsuccessful timezone or out-of-range timestamp
parsing):scalar:Timestamp(ordatetime.datetime)array-like:DatetimeIndex(orSerieswithobjectdtype containingdatetime.datetime)Series:Seriesofdatetime64dtype (orSeriesofobjectdtype containingdatetime.datetime)DataFrame:Seriesofdatetime64dtype (orSeriesofobjectdtype containingdatetime.datetime)Raises:ParserErrorWhen parsing a date from string fails.ValueErrorWhen another datetime conversion error happens. For example when one
of ÅeyearÅf, ÅemonthÅf, dayÅf columns is missing in aDataFrame, or
when a Timezone-awaredatetime.datetimeis found in an array-like
of mixed time offsets, andutc=False.See alsoDataFrame.astypeCast argument to a specified dtype.to_timedeltaConvert argument to timedelta.convert_dtypesConvert dtypes.NotesMany input types are supported, and lead to different output types:scalarscan be int, float, str, datetime object (from stdlibdatetimemodule ornumpy). They are converted toTimestampwhen
possible, otherwise they are converted todatetime.datetime.
None/NaN/null scalars are converted toNaT.array-likecan contain int, float, str, datetime objects. They are
converted toDatetimeIndexwhen possible, otherwise they are
converted toIndexwithobjectdtype, containingdatetime.datetime. None/NaN/null entries are converted toNaTin both cases.Seriesare converted toSerieswithdatetime64dtype when possible, otherwise they are converted toSerieswithobjectdtype, containingdatetime.datetime. None/NaN/null
entries are converted toNaTin both cases.DataFrame/dict-likeare converted toSerieswithdatetime64dtype. For each row a datetime is created from assembling
the various dataframe columns. Column keys can be common abbreviations
like [ÅeyearÅf, ÅemonthÅf, ÅedayÅf, ÅeminuteÅf, ÅesecondÅf, ÅemsÅf, ÅeusÅf, ÅensÅf]) or
plurals of the same.The following causes are responsible fordatetime.datetimeobjects
being returned (possibly inside anIndexor aSerieswithobjectdtype) instead of a proper pandas designated type
(Timestamp,DatetimeIndexorSerieswithdatetime64dtype):when any input element is beforeTimestamp.minor afterTimestamp.max, seetimestamp limitations.whenutc=False(default) and the input is an array-like orSeriescontaining mixed naive/aware datetime, or aware with mixed
time offsets. Note that this happens in the (quite frequent) situation when
the timezone has a daylight savings policy. In that case you may wish to
useutc=True.ExamplesHandling various input formatsAssembling a datetime from multiple columns of aDataFrame. The keys
can be common abbreviations like [ÅeyearÅf, ÅemonthÅf, ÅedayÅf, ÅeminuteÅf, ÅesecondÅf,
ÅemsÅf, ÅeusÅf, ÅensÅf]) or plurals of the same>>>df=pd.DataFrame({'year':[2015,2016],...'month':[2,3],...'day':[4,5]})>>>pd.to_datetime(df)0   2015-02-041   2016-03-05dtype: datetime64[ns]Using a unix epoch time>>>pd.to_datetime(1490195805,unit='s')Timestamp('2017-03-22 15:16:45')>>>pd.to_datetime(1490195805433502912,unit='ns')Timestamp('2017-03-22 15:16:45.433502912')WarningFor float arg, precision rounding might happen. To prevent
unexpected behavior use a fixed-width exact type.Using a non-unix epoch origin>>>pd.to_datetime([1,2,3],unit='D',...origin=pd.Timestamp('1960-01-01'))DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],dtype='datetime64[ns]', freq=None)Differences with strptime behavior""%f""will parse all the way up to nanoseconds.>>>pd.to_datetime('2018-10-26 12:00:00.0000000011',...format='%Y-%m-%d%H:%M:%S.%f')Timestamp('2018-10-26 12:00:00.000000001')Non-convertible date/timesIf a date does not meet thetimestamp limitations, passingerrors='ignore'will return the original input instead of raising any exception.Passingerrors='coerce'will force an out-of-bounds date toNaT,
in addition to forcing non-dates (or non-parseable dates) toNaT.>>>pd.to_datetime('13000101',format='%Y%m%d',errors='ignore')'13000101'>>>pd.to_datetime('13000101',format='%Y%m%d',errors='coerce')NaTTimezones and time offsetsThe default behaviour (utc=False) is as follows:Timezone-naive inputs are converted to timezone-naiveDatetimeIndex:>>>pd.to_datetime(['2018-10-26 12:00:00','2018-10-26 13:00:15'])DatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],dtype='datetime64[ns]', freq=None)Timezone-aware inputswith constant time offsetare converted to
timezone-awareDatetimeIndex:>>>pd.to_datetime(['2018-10-26 12:00 -0500','2018-10-26 13:00 -0500'])DatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],dtype='datetime64[ns, UTC-05:00]', freq=None)However, timezone-aware inputswith mixed time offsets(for example
issued from a timezone with daylight savings, such as Europe/Paris)
arenot successfully convertedto aDatetimeIndex.
Parsing datetimes with mixed time zones will show a warning unlessutc=True. If you specifyutc=Falsethe warning below will be shown
and a simpleIndexcontainingdatetime.datetimeobjects will be returned:>>>pd.to_datetime(['2020-10-25 02:00 +0200',...'2020-10-25 04:00 +0100'])FutureWarning: In a future version of pandas, parsing datetimes with mixedtime zones will raise an error unless `utc=True`. Please specify `utc=True`to opt in to the new behaviour and silence this warning. To create a `Series`with mixed offsets and `object` dtype, please use `apply` and`datetime.datetime.strptime`.Index([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],dtype='object')A mix of timezone-aware and timezone-naive inputs is also converted to
a simpleIndexcontainingdatetime.datetimeobjects:>>>fromdatetimeimportdatetime>>>pd.to_datetime([""2020-01-01 01:00:00-01:00"",...datetime(2020,1,1,3,0)])FutureWarning: In a future version of pandas, parsing datetimes with mixedtime zones will raise an error unless `utc=True`. Please specify `utc=True`to opt in to the new behaviour and silence this warning. To create a `Series`with mixed offsets and `object` dtype, please use `apply` and`datetime.datetime.strptime`.Index([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')Settingutc=Truesolves most of the above issues:Timezone-naive inputs arelocalizedas UTC>>>pd.to_datetime(['2018-10-26 12:00','2018-10-26 13:00'],utc=True)DatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],dtype='datetime64[ns, UTC]', freq=None)Timezone-aware inputs areconvertedto UTC (the output represents the
exact same datetime, but viewed from the UTC time offset+00:00).>>>pd.to_datetime(['2018-10-26 12:00 -0530','2018-10-26 12:00 -0500'],...utc=True)DatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],dtype='datetime64[ns, UTC]', freq=None)Inputs can contain both string or datetime, the above
rules still apply>>>pd.to_datetime(['2018-10-26 12:00',datetime(2020,1,1,18)],utc=True)DatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],dtype='datetime64[ns, UTC]', freq=None)"
Pandas,Combined_csv,pandas.DataFrame.set_index,"pandas.DataFrame.set_index#DataFrame.set_index(keys,*,drop=True,append=False,inplace=False,verify_integrity=False)[source]#Set the DataFrame index using existing columns.Set the DataFrame index (row labels) using one or more existing
columns or arrays (of the correct length). The index can replace the
existing index or expand on it.Parameters:keyslabel or array-like or list of labels/arraysThis parameter can be either a single column key, a single array of
the same length as the calling DataFrame, or a list containing an
arbitrary combination of column keys and arrays. Here, ÅgarrayÅh
encompassesSeries,Index,np.ndarray, and
instances ofIterator.dropbool, default TrueDelete columns to be used as the new index.appendbool, default FalseWhether to append columns to existing index.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.verify_integritybool, default FalseCheck the new index for duplicates. Otherwise defer the check until
necessary. Setting to False will improve the performance of this
method.Returns:DataFrame or NoneChanged row labels or None ifinplace=True.See alsoDataFrame.reset_indexOpposite of set_index.DataFrame.reindexChange to new indices or expand indices.DataFrame.reindex_likeChange to same indices as other DataFrame.Examples>>>df=pd.DataFrame({'month':[1,4,7,10],...'year':[2012,2014,2013,2014],...'sale':[55,40,84,31]})>>>dfmonth  year  sale0      1  2012    551      4  2014    402      7  2013    843     10  2014    31Set the index to become the ÅemonthÅf column:>>>df.set_index('month')year  salemonth1      2012    554      2014    407      2013    8410     2014    31Create a MultiIndex using columns ÅeyearÅf and ÅemonthÅf:>>>df.set_index(['year','month'])saleyear  month2012  1     552014  4     402013  7     842014  10    31Create a MultiIndex using an Index and a column:>>>df.set_index([pd.Index([1,2,3,4]),'year'])month  saleyear1  2012  1      552  2014  4      403  2013  7      844  2014  10     31Create a MultiIndex using two Series:>>>s=pd.Series([1,2,3,4])>>>df.set_index([s,s**2])month  year  sale1 1       1  2012    552 4       4  2014    403 9       7  2013    844 16     10  2014    31"
Pandas,Combined_csv,pandas.DataFrame.reset_index,"pandas.DataFrame.reset_index#DataFrame.reset_index(level=None,*,drop=False,inplace=False,col_level=0,col_fill='',allow_duplicates=_NoDefault.no_default,names=None)[source]#Reset the index, or a level of it.Reset the index of the DataFrame, and use the default one instead.
If the DataFrame has a MultiIndex, this method can remove one or more
levels.Parameters:levelint, str, tuple, or list, default NoneOnly remove the given levels from the index. Removes all levels by
default.dropbool, default FalseDo not try to insert index into dataframe columns. This resets
the index to the default integer index.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.col_levelint or str, default 0If the columns have multiple levels, determines which level the
labels are inserted into. By default it is inserted into the first
level.col_fillobject, default ÅeÅfIf the columns have multiple levels, determines how the other
levels are named. If None then the index name is repeated.allow_duplicatesbool, optional, default lib.no_defaultAllow duplicate column labels to be created.New in version 1.5.0.namesint, str or 1-dimensional list, default NoneUsing the given string, rename the DataFrame column which contains the
index data. If the DataFrame has a MultiIndex, this has to be a list or
tuple with length equal to the number of levels.New in version 1.5.0.Returns:DataFrame or NoneDataFrame with the new index or None ifinplace=True.See alsoDataFrame.set_indexOpposite of reset_index.DataFrame.reindexChange to new indices or expand indices.DataFrame.reindex_likeChange to same indices as other DataFrame.Examples>>>df=pd.DataFrame([('bird',389.0),...('bird',24.0),...('mammal',80.5),...('mammal',np.nan)],...index=['falcon','parrot','lion','monkey'],...columns=('class','max_speed'))>>>dfclass  max_speedfalcon    bird      389.0parrot    bird       24.0lion    mammal       80.5monkey  mammal        NaNWhen we reset the index, the old index is added as a column, and a
new sequential index is used:>>>df.reset_index()index   class  max_speed0  falcon    bird      389.01  parrot    bird       24.02    lion  mammal       80.53  monkey  mammal        NaNWe can use thedropparameter to avoid the old index being added as
a column:>>>df.reset_index(drop=True)class  max_speed0    bird      389.01    bird       24.02  mammal       80.53  mammal        NaNYou can also usereset_indexwithMultiIndex.>>>index=pd.MultiIndex.from_tuples([('bird','falcon'),...('bird','parrot'),...('mammal','lion'),...('mammal','monkey')],...names=['class','name'])>>>columns=pd.MultiIndex.from_tuples([('speed','max'),...('species','type')])>>>df=pd.DataFrame([(389.0,'fly'),...(24.0,'fly'),...(80.5,'run'),...(np.nan,'jump')],...index=index,...columns=columns)>>>dfspeed speciesmax    typeclass  namebird   falcon  389.0     flyparrot   24.0     flymammal lion     80.5     runmonkey    NaN    jumpUsing thenamesparameter, choose a name for the index column:>>>df.reset_index(names=['classes','names'])classes   names  speed speciesmax    type0    bird  falcon  389.0     fly1    bird  parrot   24.0     fly2  mammal    lion   80.5     run3  mammal  monkey    NaN    jumpIf the index has multiple levels, we can reset a subset of them:>>>df.reset_index(level='class')class  speed speciesmax    typenamefalcon    bird  389.0     flyparrot    bird   24.0     flylion    mammal   80.5     runmonkey  mammal    NaN    jumpIf we are not dropping the index, by default, it is placed in the top
level. We can place it in another level:>>>df.reset_index(level='class',col_level=1)speed speciesclass    max    typenamefalcon    bird  389.0     flyparrot    bird   24.0     flylion    mammal   80.5     runmonkey  mammal    NaN    jumpWhen the index is inserted under another level, we can specify under
which one with the parametercol_fill:>>>df.reset_index(level='class',col_level=1,col_fill='species')species  speed speciesclass    max    typenamefalcon           bird  389.0     flyparrot           bird   24.0     flylion           mammal   80.5     runmonkey         mammal    NaN    jumpIf we specify a nonexistent level forcol_fill, it is created:>>>df.reset_index(level='class',col_level=1,col_fill='genus')genus  speed speciesclass    max    typenamefalcon           bird  389.0     flyparrot           bird   24.0     flylion           mammal   80.5     runmonkey         mammal    NaN    jump"
Pandas,Combined_csv,pandas.DataFrame.reindex,"pandas.DataFrame.reindex#DataFrame.reindex(labels=None,*,index=None,columns=None,axis=None,method=None,copy=None,level=None,fill_value=nan,limit=None,tolerance=None)[source]#Conform DataFrame to new index with optional filling logic.Places NA/NaN in locations having no value in the previous index. A new object
is produced unless the new index is equivalent to the current one andcopy=False.Parameters:labelsarray-like, optionalNew labels / index to conform the axis specified by ÅeaxisÅf to.indexarray-like, optionalNew labels for the index. Preferably an Index object to avoid
duplicating data.columnsarray-like, optionalNew labels for the columns. Preferably an Index object to avoid
duplicating data.axisint or str, optionalAxis to target. Can be either the axis name (ÅeindexÅf, ÅecolumnsÅf)
or number (0, 1).method{None, ÅebackfillÅf/ÅfbfillÅf, ÅepadÅf/ÅfffillÅf, ÅenearestÅf}Method to use for filling holes in reindexed DataFrame.
Please note: this is only applicable to DataFrames/Series with a
monotonically increasing/decreasing index.None (default): donÅft fill gapspad / ffill: Propagate last valid observation forward to next
valid.backfill / bfill: Use next valid observation to fill gap.nearest: Use nearest valid observations to fill gap.copybool, default TrueReturn a new object, even if the passed indexes are the same.levelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any
ÅgcompatibleÅh value.limitint, default NoneMaximum number of consecutive elements to forward or backward fill.toleranceoptionalMaximum distance between original and new labels for inexact
matches. The values of the index at the matching locations most
satisfy the equationabs(index[indexer]-target)<=tolerance.Tolerance may be a scalar value, which applies the same tolerance
to all values, or list-like, which applies variable tolerance per
element. List-like includes list, tuple, array, Series, and must be
the same size as the index and its dtype must exactly match the
indexÅfs type.Returns:DataFrame with changed index.See alsoDataFrame.set_indexSet row labels.DataFrame.reset_indexRemove row labels or move them to new columns.DataFrame.reindex_likeChange to same indices as other DataFrame.ExamplesDataFrame.reindexsupports two calling conventions(index=index_labels,columns=column_labels,...)(labels,axis={'index','columns'},...)Wehighlyrecommend using keyword arguments to clarify your
intent.Create a dataframe with some fictional data.>>>index=['Firefox','Chrome','Safari','IE10','Konqueror']>>>df=pd.DataFrame({'http_status':[200,200,404,404,301],...'response_time':[0.04,0.02,0.07,0.08,1.0]},...index=index)>>>dfhttp_status  response_timeFirefox            200           0.04Chrome             200           0.02Safari             404           0.07IE10               404           0.08Konqueror          301           1.00Create a new index and reindex the dataframe. By default
values in the new index that do not have corresponding
records in the dataframe are assignedNaN.>>>new_index=['Safari','Iceweasel','Comodo Dragon','IE10',...'Chrome']>>>df.reindex(new_index)http_status  response_timeSafari               404.0           0.07Iceweasel              NaN            NaNComodo Dragon          NaN            NaNIE10                 404.0           0.08Chrome               200.0           0.02We can fill in the missing values by passing a value to
the keywordfill_value. Because the index is not monotonically
increasing or decreasing, we cannot use arguments to the keywordmethodto fill theNaNvalues.>>>df.reindex(new_index,fill_value=0)http_status  response_timeSafari                 404           0.07Iceweasel                0           0.00Comodo Dragon            0           0.00IE10                   404           0.08Chrome                 200           0.02>>>df.reindex(new_index,fill_value='missing')http_status response_timeSafari                404          0.07Iceweasel         missing       missingComodo Dragon     missing       missingIE10                  404          0.08Chrome                200          0.02We can also reindex the columns.>>>df.reindex(columns=['http_status','user_agent'])http_status  user_agentFirefox            200         NaNChrome             200         NaNSafari             404         NaNIE10               404         NaNKonqueror          301         NaNOr we can use Ågaxis-styleÅh keyword arguments>>>df.reindex(['http_status','user_agent'],axis=""columns"")http_status  user_agentFirefox            200         NaNChrome             200         NaNSafari             404         NaNIE10               404         NaNKonqueror          301         NaNTo further illustrate the filling functionality inreindex, we will create a dataframe with a
monotonically increasing index (for example, a sequence
of dates).>>>date_index=pd.date_range('1/1/2010',periods=6,freq='D')>>>df2=pd.DataFrame({""prices"":[100,101,np.nan,100,89,88]},...index=date_index)>>>df2prices2010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.0Suppose we decide to expand the dataframe to cover a wider
date range.>>>date_index2=pd.date_range('12/29/2009',periods=10,freq='D')>>>df2.reindex(date_index2)prices2009-12-29     NaN2009-12-30     NaN2009-12-31     NaN2010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.02010-01-07     NaNThe index entries that did not have a value in the original data frame
(for example, Åe2009-12-29Åf) are by default filled withNaN.
If desired, we can fill in the missing values using one of several
options.For example, to back-propagate the last valid value to fill theNaNvalues, passbfillas an argument to themethodkeyword.>>>df2.reindex(date_index2,method='bfill')prices2009-12-29   100.02009-12-30   100.02009-12-31   100.02010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.02010-01-07     NaNPlease note that theNaNvalue present in the original dataframe
(at index value 2010-01-03) will not be filled by any of the
value propagation schemes. This is because filling while reindexing
does not look at dataframe values, but only compares the original and
desired indexes. If you do want to fill in theNaNvalues present
in the original dataframe, use thefillna()method.See theuser guidefor more."
Pandas,Combined_csv,pandas.DataFrame.groupby,"pandas.DataFrame.groupby#DataFrame.groupby(by=None,axis=_NoDefault.no_default,level=None,as_index=True,sort=True,group_keys=True,observed=_NoDefault.no_default,dropna=True)[source]#Group DataFrame using a mapper or by a Series of columns.A groupby operation involves some combination of splitting the
object, applying a function, and combining the results. This can be
used to group large amounts of data and compute operations on these
groups.Parameters:bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby.
Ifbyis a function, itÅfs called on each value of the objectÅfs
index. If a dict or Series is passed, the Series or dict VALUES
will be used to determine the groups (the SeriesÅf values are first
aligned; see.align()method). If a list or ndarray of length
equal to the selected axis is passed (see thegroupby user guide),
the values are used as-is to determine the groups. A label or list
of labels may be passed to group by the columns inself.
Notice that a tuple is interpreted as a (single) key.axis{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}, default 0Split along rows (0) or columns (1). ForSeriesthis parameter
is unused and defaults to 0.Deprecated since version 2.1.0:Will be removed and behave like axis=0 in a future version.
Foraxis=1, doframe.T.groupby(...)instead.levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular
level or levels. Do not specify bothbyandlevel.as_indexbool, default TrueReturn object with group labels as the
index. Only relevant for DataFrame input. as_index=False is
effectively ÅgSQL-styleÅh grouped output. This argument has no effect
on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).sortbool, default TrueSort group keys. Get better performance by turning this off.
Note this does not influence the order of observations within each
group. Groupby preserves the order of rows within each group. If False,
the groups will appear in the same order as they did in the original DataFrame.
This argument has no effect on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).Changed in version 2.0.0:Specifyingsort=Falsewith an ordered categorical grouper will no
longer sort the values.group_keysbool, default TrueWhen calling apply and thebyargument produces a like-indexed
(i.e.a transform) result, add group keys to
index to identify pieces. By default group keys are not included
when the resultÅfs index (and column) labels match the inputs, and
are included otherwise.Changed in version 1.5.0:Warns thatgroup_keyswill no longer be ignored when the
result fromapplyis a like-indexed Series or DataFrame.
Specifygroup_keysexplicitly to include the group keys or
not.Changed in version 2.0.0:group_keysnow defaults toTrue.observedbool, default FalseThis only applies if any of the groupers are Categoricals.
If True: only show observed values for categorical groupers.
If False: show all values for categorical groupers.Deprecated since version 2.1.0:The default value will change to True in a future version of pandas.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together
with row/column will be dropped.
If False, NA values will also be treated as the key in groups.Returns:pandas.api.typing.DataFrameGroupByReturns a groupby object that contains information about the groups.See alsoresampleConvenience method for frequency conversion and resampling of time series.NotesSee theuser guidefor more
detailed usage and examples, including splitting an object into groups,
iterating through groups, selecting a group, aggregation, and more.Examples>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>dfAnimal  Max Speed0  Falcon      380.01  Falcon      370.02  Parrot       24.03  Parrot       26.0>>>df.groupby(['Animal']).mean()Max SpeedAnimalFalcon      375.0Parrot       25.0Hierarchical IndexesWe can groupby different levels of a hierarchical index
using thelevelparameter:>>>arrays=[['Falcon','Falcon','Parrot','Parrot'],...['Captive','Wild','Captive','Wild']]>>>index=pd.MultiIndex.from_arrays(arrays,names=('Animal','Type'))>>>df=pd.DataFrame({'Max Speed':[390.,350.,30.,20.]},...index=index)>>>dfMax SpeedAnimal TypeFalcon Captive      390.0Wild         350.0Parrot Captive       30.0Wild          20.0>>>df.groupby(level=0).mean()Max SpeedAnimalFalcon      370.0Parrot       25.0>>>df.groupby(level=""Type"").mean()Max SpeedTypeCaptive      210.0Wild         185.0We can also choose to include NA in group keys or not by settingdropnaparameter, the default setting isTrue.>>>l=[[1,2,3],[1,None,4],[2,1,3],[1,2,2]]>>>df=pd.DataFrame(l,columns=[""a"",""b"",""c""])>>>df.groupby(by=[""b""]).sum()a   cb1.0 2   32.0 2   5>>>df.groupby(by=[""b""],dropna=False).sum()a   cb1.0 2   32.0 2   5NaN 1   4>>>l=[[""a"",12,12],[None,12.3,33.],[""b"",12.3,123],[""a"",1,1]]>>>df=pd.DataFrame(l,columns=[""a"",""b"",""c""])>>>df.groupby(by=""a"").sum()b     caa   13.0   13.0b   12.3  123.0>>>df.groupby(by=""a"",dropna=False).sum()b     caa   13.0   13.0b   12.3  123.0NaN 12.3   33.0When using.apply(), usegroup_keysto include or exclude the
group keys. Thegroup_keysargument defaults toTrue(include).>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>df.groupby(""Animal"",group_keys=True).apply(lambdax:x)Animal  Max SpeedAnimalFalcon 0  Falcon      380.01  Falcon      370.0Parrot 2  Parrot       24.03  Parrot       26.0>>>df.groupby(""Animal"",group_keys=False).apply(lambdax:x)Animal  Max Speed0  Falcon      380.01  Falcon      370.02  Parrot       24.03  Parrot       26.0"
Pandas,Combined_csv,pandas.core.groupby.DataFrameGroupBy.sum,"pandas.core.groupby.DataFrameGroupBy.sum#DataFrameGroupBy.sum(numeric_only=False,min_count=0,engine=None,engine_kwargs=None)[source]#Compute sum of group values.Parameters:numeric_onlybool, default FalseInclude only float, int, boolean columns.Changed in version 2.0.0:numeric_only no longer acceptsNone.min_countint, default 0The required number of valid values to perform the operation. If fewer
thanmin_countnon-NA values are present the result will be NA.enginestr, default None None'cython': Runs rolling apply through C-extensions from cython.'numba'Runs rolling apply through JIT compiled code from numba.Only available whenrawis set toTrue.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default None NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to both thefuncand theapplygroupby aggregation.Returns:Series or DataFrameComputed sum of values within each group.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b','b']>>>ser=pd.Series([1,2,3,4],index=lst)>>>sera    1a    2b    3b    4dtype: int64>>>ser.groupby(level=0).sum()a    3b    7dtype: int64For DataFrameGroupBy:>>>data=[[1,8,2],[1,2,5],[2,5,8],[2,6,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""tiger"",""leopard"",""cheetah"",""lion""])>>>dfa  b  ctiger   1  8  2leopard   1  2  5cheetah   2  5  8lion   2  6  9>>>df.groupby(""a"").sum()b   ca1   10   72   11  17"
Pandas,Combined_csv,pandas.core.groupby.DataFrameGroupBy.mean,"pandas.core.groupby.DataFrameGroupBy.mean#DataFrameGroupBy.mean(numeric_only=False,engine=None,engine_kwargs=None)[source]#Compute mean of groups, excluding missing values.Parameters:numeric_onlybool, default FalseInclude only float, int, boolean columns.Changed in version 2.0.0:numeric_only no longer acceptsNoneand defaults toFalse.enginestr, default None'cython': Runs the operation through C-extensions from cython.'numba': Runs the operation through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaNew in version 1.4.0.engine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{{'nopython':True,'nogil':False,'parallel':False}}New in version 1.4.0.Returns:pandas.Series or pandas.DataFrameSee alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.Examples>>>df=pd.DataFrame({'A':[1,1,2,1,2],...'B':[np.nan,2,3,4,5],...'C':[1,2,1,1,2]},columns=['A','B','C'])Groupby one column and return the mean of the remaining columns in
each group.>>>df.groupby('A').mean()B         CA1  3.0  1.3333332  4.0  1.500000Groupby two columns and return the mean of the remaining column.>>>df.groupby(['A','B']).mean()CA B1 2.0  2.04.0  1.02 3.0  1.05.0  2.0Groupby one column and return the mean of only particular column in
the group.>>>df.groupby('A')['B'].mean()A1    3.02    4.0Name: B, dtype: float64"
Pandas,Combined_csv,pandas.core.groupby.DataFrameGroupBy.count,"pandas.core.groupby.DataFrameGroupBy.count#DataFrameGroupBy.count()[source]#Compute count of group, excluding missing values.Returns:Series or DataFrameCount of values within each group.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,np.nan],index=lst)>>>sera    1.0a    2.0b    NaNdtype: float64>>>ser.groupby(level=0).count()a    2b    0dtype: int64For DataFrameGroupBy:>>>data=[[1,np.nan,3],[1,np.nan,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""cow"",""horse"",""bull""])>>>dfa         b     ccow     1       NaN     3horse   1       NaN     6bull    7       8.0     9>>>df.groupby(""a"").count()b   ca1   0   27   1   1For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').count()2023-01-01    22023-02-01    2Freq: MS, dtype: int64"
