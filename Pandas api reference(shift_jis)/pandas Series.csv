ƒ‰ƒCƒuƒ‰ƒŠ–¼,Í,ß,“à—e
Pandas,Series,pandas.Series,"pandas.Series#classpandas.Series(data=None,index=None,dtype=None,name=None,copy=None,fastpath=False)[source]#One-dimensional ndarray with axis labels (including time series).Labels need not be unique but must be a hashable type. The object
supports both integer- and label-based indexing and provides a host of
methods for performing operations involving the index. Statistical
methods from ndarray have been overridden to automatically exclude
missing data (currently represented as NaN).Operations between Series (+, -, /, *, **) align values based on their
associated index values? they need not be the same length. The result
index will be the sorted union of the two indexes.Parameters:dataarray-like, Iterable, dict, or scalar valueContains data stored in Series. If data is a dict, argument order is
maintained.indexarray-like or Index (1d)Values must be hashable and have the same length asdata.
Non-unique index values are allowed. Will default to
RangeIndex (0, 1, 2, c, n) if not provided. If data is dict-like
and index is None, then the keys in the data are used as the index. If the
index is not None, the resulting Series is reindexed with the index values.dtypestr, numpy.dtype, or ExtensionDtype, optionalData type for the output Series. If not specified, this will be
inferred fromdata.
See theuser guidefor more usages.nameHashable, default NoneThe name to give to the Series.copybool, default FalseCopy input data. Only affects Series or 1d ndarray input. See examples.NotesPlease reference theUser Guidefor more information.ExamplesConstructing Series from a dictionary with an Index specified>>>d={'a':1,'b':2,'c':3}>>>ser=pd.Series(data=d,index=['a','b','c'])>>>sera   1b   2c   3dtype: int64The keys of the dictionary match with the Index values, hence the Index
values have no effect.>>>d={'a':1,'b':2,'c':3}>>>ser=pd.Series(data=d,index=['x','y','z'])>>>serx   NaNy   NaNz   NaNdtype: float64Note that the Index is first build with the keys from the dictionary.
After this the Series is reindexed with the given Index values, hence we
get all NaN as a result.Constructing Series from a list withcopy=False.>>>r=[1,2]>>>ser=pd.Series(r,copy=False)>>>ser.iloc[0]=999>>>r[1, 2]>>>ser0    9991      2dtype: int64Due to input data type the Series has acopyof
the original data even thoughcopy=False, so
the data is unchanged.Constructing Series from a 1d ndarray withcopy=False.>>>r=np.array([1,2])>>>ser=pd.Series(r,copy=False)>>>ser.iloc[0]=999>>>rarray([999,   2])>>>ser0    9991      2dtype: int64Due to input data type the Series has aviewon
the original data, so
the data is changed as well.AttributesTReturn the transpose, which is by definition self.arrayThe ExtensionArray of the data backing this Series or Index.atAccess a single value for a row/column label pair.attrsDictionary of global attributes of this dataset.axesReturn a list of the row axis labels.dtypeReturn the dtype object of the underlying data.dtypesReturn the dtype object of the underlying data.flagsGet the properties associated with this pandas object.hasnansReturn True if there are any NaNs.iatAccess a single value for a row/column pair by integer position.ilocPurely integer-location based indexing for selection by position.indexThe index (axis labels) of the Series.is_monotonic_decreasingReturn boolean if values in the object are monotonically decreasing.is_monotonic_increasingReturn boolean if values in the object are monotonically increasing.is_uniqueReturn boolean if values in the object are unique.locAccess a group of rows and columns by label(s) or a boolean array.nameReturn the name of the Series.nbytesReturn the number of bytes in the underlying data.ndimNumber of dimensions of the underlying data, by definition 1.shapeReturn a tuple of the shape of the underlying data.sizeReturn the number of elements in the underlying data.valuesReturn Series as ndarray or ndarray-like depending on the dtype.emptyMethodsabs()Return a Series/DataFrame with absolute numeric value of each element.add(other[,?level,?fill_value,?axis])Return Addition of series and other, element-wise (binary operatoradd).add_prefix(prefix[,?axis])Prefix labels with stringprefix.add_suffix(suffix[,?axis])Suffix labels with stringsuffix.agg([func,?axis])Aggregate using one or more operations over the specified axis.aggregate([func,?axis])Aggregate using one or more operations over the specified axis.align(other[,?join,?axis,?level,?copy,?...])Align two objects on their axes with the specified join method.all([axis,?bool_only,?skipna])Return whether all elements are True, potentially over an axis.any(*[,?axis,?bool_only,?skipna])Return whether any element is True, potentially over an axis.apply(func[,?convert_dtype,?args,?by_row])Invoke function on values of Series.argmax([axis,?skipna])Return int position of the largest value in the Series.argmin([axis,?skipna])Return int position of the smallest value in the Series.argsort([axis,?kind,?order])Return the integer indices that would sort the Series values.asfreq(freq[,?method,?how,?normalize,?...])Convert time series to specified frequency.asof(where[,?subset])Return the last row(s) without any NaNs beforewhere.astype(dtype[,?copy,?errors])Cast a pandas object to a specified dtypedtype.at_time(time[,?asof,?axis])Select values at particular time of day (e.g., 9:30AM).autocorr([lag])Compute the lag-N autocorrelation.backfill(*[,?axis,?inplace,?limit,?downcast])(DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap.between(left,?right[,?inclusive])Return boolean Series equivalent to left <= series <= right.between_time(start_time,?end_time[,?...])Select values between particular times of the day (e.g., 9:00-9:30 AM).bfill(*[,?axis,?inplace,?limit,?downcast])Fill NA/NaN values by using the next valid observation to fill the gap.bool()(DEPRECATED) Return the bool of a single element Series or DataFrame.catalias ofCategoricalAccessorclip([lower,?upper,?axis,?inplace])Trim values at input threshold(s).combine(other,?func[,?fill_value])Combine the Series with a Series or scalar according tofunc.combine_first(other)Update null elements with value in the same location in 'other'.compare(other[,?align_axis,?keep_shape,?...])Compare to another Series and show the differences.convert_dtypes([infer_objects,?...])Convert columns to the best possible dtypes using dtypes supportingpd.NA.copy([deep])Make a copy of this object's indices and data.corr(other[,?method,?min_periods])Compute correlation withotherSeries, excluding missing values.count()Return number of non-NA/null observations in the Series.cov(other[,?min_periods,?ddof])Compute covariance with Series, excluding missing values.cummax([axis,?skipna])Return cumulative maximum over a DataFrame or Series axis.cummin([axis,?skipna])Return cumulative minimum over a DataFrame or Series axis.cumprod([axis,?skipna])Return cumulative product over a DataFrame or Series axis.cumsum([axis,?skipna])Return cumulative sum over a DataFrame or Series axis.describe([percentiles,?include,?exclude])Generate descriptive statistics.diff([periods])First discrete difference of element.div(other[,?level,?fill_value,?axis])Return Floating division of series and other, element-wise (binary operatortruediv).divide(other[,?level,?fill_value,?axis])Return Floating division of series and other, element-wise (binary operatortruediv).divmod(other[,?level,?fill_value,?axis])Return Integer division and modulo of series and other, element-wise (binary operatordivmod).dot(other)Compute the dot product between the Series and the columns of other.drop([labels,?axis,?index,?columns,?level,?...])Return Series with specified index labels removed.drop_duplicates(*[,?keep,?inplace,?ignore_index])Return Series with duplicate values removed.droplevel(level[,?axis])Return Series/DataFrame with requested index / column level(s) removed.dropna(*[,?axis,?inplace,?how,?ignore_index])Return a new Series with missing values removed.duplicated([keep])Indicate duplicate Series values.eq(other[,?level,?fill_value,?axis])Return Equal to of series and other, element-wise (binary operatoreq).equals(other)Test whether two objects contain the same elements.ewm([com,?span,?halflife,?alpha,?...])Provide exponentially weighted (EW) calculations.expanding([min_periods,?axis,?method])Provide expanding window calculations.explode([ignore_index])Transform each element of a list-like to a row.factorize([sort,?use_na_sentinel])Encode the object as an enumerated type or categorical variable.ffill(*[,?axis,?inplace,?limit,?downcast])Fill NA/NaN values by propagating the last valid observation to next valid.fillna([value,?method,?axis,?inplace,?...])Fill NA/NaN values using the specified method.filter([items,?like,?regex,?axis])Subset the dataframe rows or columns according to the specified index labels.first(offset)(DEPRECATED) Select initial periods of time series data based on a date offset.first_valid_index()Return index for first non-NA value or None, if no non-NA value is found.floordiv(other[,?level,?fill_value,?axis])Return Integer division of series and other, element-wise (binary operatorfloordiv).ge(other[,?level,?fill_value,?axis])Return Greater than or equal to of series and other, element-wise (binary operatorge).get(key[,?default])Get item from object for given key (ex: DataFrame column).groupby([by,?axis,?level,?as_index,?sort,?...])Group Series using a mapper or by a Series of columns.gt(other[,?level,?fill_value,?axis])Return Greater than of series and other, element-wise (binary operatorgt).head([n])Return the firstnrows.hist([by,?ax,?grid,?xlabelsize,?xrot,?...])Draw histogram of the input series using matplotlib.idxmax([axis,?skipna])Return the row label of the maximum value.idxmin([axis,?skipna])Return the row label of the minimum value.infer_objects([copy])Attempt to infer better dtypes for object columns.info([verbose,?buf,?max_cols,?memory_usage,?...])Print a concise summary of a Series.interpolate([method,?axis,?limit,?inplace,?...])Fill NaN values using an interpolation method.isin(values)Whether elements in Series are contained invalues.isna()Detect missing values.isnull()Series.isnull is an alias for Series.isna.item()Return the first element of the underlying data as a Python scalar.items()Lazily iterate over (index, value) tuples.keys()Return alias for index.kurt([axis,?skipna,?numeric_only])Return unbiased kurtosis over requested axis.kurtosis([axis,?skipna,?numeric_only])Return unbiased kurtosis over requested axis.last(offset)(DEPRECATED) Select final periods of time series data based on a date offset.last_valid_index()Return index for last non-NA value or None, if no non-NA value is found.le(other[,?level,?fill_value,?axis])Return Less than or equal to of series and other, element-wise (binary operatorle).lt(other[,?level,?fill_value,?axis])Return Less than of series and other, element-wise (binary operatorlt).map(arg[,?na_action])Map values of Series according to an input mapping or function.mask(cond[,?other,?inplace,?axis,?level])Replace values where the condition is True.max([axis,?skipna,?numeric_only])Return the maximum of the values over the requested axis.mean([axis,?skipna,?numeric_only])Return the mean of the values over the requested axis.median([axis,?skipna,?numeric_only])Return the median of the values over the requested axis.memory_usage([index,?deep])Return the memory usage of the Series.min([axis,?skipna,?numeric_only])Return the minimum of the values over the requested axis.mod(other[,?level,?fill_value,?axis])Return Modulo of series and other, element-wise (binary operatormod).mode([dropna])Return the mode(s) of the Series.mul(other[,?level,?fill_value,?axis])Return Multiplication of series and other, element-wise (binary operatormul).multiply(other[,?level,?fill_value,?axis])Return Multiplication of series and other, element-wise (binary operatormul).ne(other[,?level,?fill_value,?axis])Return Not equal to of series and other, element-wise (binary operatorne).nlargest([n,?keep])Return the largestnelements.notna()Detect existing (non-missing) values.notnull()Series.notnull is an alias for Series.notna.nsmallest([n,?keep])Return the smallestnelements.nunique([dropna])Return number of unique elements in the object.pad(*[,?axis,?inplace,?limit,?downcast])(DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid.pct_change([periods,?fill_method,?limit,?freq])Fractional change between the current and a prior element.pipe(func,?*args,?**kwargs)Apply chainable functions that expect Series or DataFrames.plotalias ofPlotAccessorpop(item)Return item and drops from series.pow(other[,?level,?fill_value,?axis])Return Exponential power of series and other, element-wise (binary operatorpow).prod([axis,?skipna,?numeric_only,?min_count])Return the product of the values over the requested axis.product([axis,?skipna,?numeric_only,?min_count])Return the product of the values over the requested axis.quantile([q,?interpolation])Return value at the given quantile.radd(other[,?level,?fill_value,?axis])Return Addition of series and other, element-wise (binary operatorradd).rank([axis,?method,?numeric_only,?...])Compute numerical data ranks (1 through n) along axis.ravel([order])Return the flattened underlying data as an ndarray or ExtensionArray.rdiv(other[,?level,?fill_value,?axis])Return Floating division of series and other, element-wise (binary operatorrtruediv).rdivmod(other[,?level,?fill_value,?axis])Return Integer division and modulo of series and other, element-wise (binary operatorrdivmod).reindex([index,?axis,?method,?copy,?level,?...])Conform Series to new index with optional filling logic.reindex_like(other[,?method,?copy,?limit,?...])Return an object with matching indices as other object.rename([index,?axis,?copy,?inplace,?level,?...])Alter Series index labels or name.rename_axis([mapper,?index,?axis,?copy,?inplace])Set the name of the axis for the index or columns.reorder_levels(order)Rearrange index levels using input order.repeat(repeats[,?axis])Repeat elements of a Series.replace([to_replace,?value,?inplace,?limit,?...])Replace values given into_replacewithvalue.resample(rule[,?axis,?closed,?label,?...])Resample time-series data.reset_index([level,?drop,?name,?inplace,?...])Generate a new DataFrame or Series with the index reset.rfloordiv(other[,?level,?fill_value,?axis])Return Integer division of series and other, element-wise (binary operatorrfloordiv).rmod(other[,?level,?fill_value,?axis])Return Modulo of series and other, element-wise (binary operatorrmod).rmul(other[,?level,?fill_value,?axis])Return Multiplication of series and other, element-wise (binary operatorrmul).rolling(window[,?min_periods,?center,?...])Provide rolling window calculations.round([decimals])Round each value in a Series to the given number of decimals.rpow(other[,?level,?fill_value,?axis])Return Exponential power of series and other, element-wise (binary operatorrpow).rsub(other[,?level,?fill_value,?axis])Return Subtraction of series and other, element-wise (binary operatorrsub).rtruediv(other[,?level,?fill_value,?axis])Return Floating division of series and other, element-wise (binary operatorrtruediv).sample([n,?frac,?replace,?weights,?...])Return a random sample of items from an axis of object.searchsorted(value[,?side,?sorter])Find indices where elements should be inserted to maintain order.sem([axis,?skipna,?ddof,?numeric_only])Return unbiased standard error of the mean over requested axis.set_axis(labels,?*[,?axis,?copy])Assign desired index to given axis.set_flags(*[,?copy,?allows_duplicate_labels])Return a new object with updated flags.shift([periods,?freq,?axis,?fill_value,?suffix])Shift index by desired number of periods with an optional timefreq.skew([axis,?skipna,?numeric_only])Return unbiased skew over requested axis.sort_index(*[,?axis,?level,?ascending,?...])Sort Series by index labels.sort_values(*[,?axis,?ascending,?inplace,?...])Sort by the values.sparsealias ofSparseAccessorsqueeze([axis])Squeeze 1 dimensional axis objects into scalars.std([axis,?skipna,?ddof,?numeric_only])Return sample standard deviation over requested axis.stralias ofStringMethodssub(other[,?level,?fill_value,?axis])Return Subtraction of series and other, element-wise (binary operatorsub).subtract(other[,?level,?fill_value,?axis])Return Subtraction of series and other, element-wise (binary operatorsub).sum([axis,?skipna,?numeric_only,?min_count])Return the sum of the values over the requested axis.swapaxes(axis1,?axis2[,?copy])(DEPRECATED) Interchange axes and swap values axes appropriately.swaplevel([i,?j,?copy])Swap levels i and j in aMultiIndex.tail([n])Return the lastnrows.take(indices[,?axis])Return the elements in the givenpositionalindices along an axis.to_clipboard([excel,?sep])Copy object to the system clipboard.to_csv([path_or_buf,?sep,?na_rep,?...])Write object to a comma-separated values (csv) file.to_dict([into])Convert Series to {label -> value} dict or dict-like object.to_excel(excel_writer[,?sheet_name,?na_rep,?...])Write object to an Excel sheet.to_frame([name])Convert Series to DataFrame.to_hdf(path_or_buf,?key[,?mode,?complevel,?...])Write the contained data to an HDF5 file using HDFStore.to_json([path_or_buf,?orient,?date_format,?...])Convert the object to a JSON string.to_latex([buf,?columns,?header,?index,?...])Render object to a LaTeX tabular, longtable, or nested table.to_list()Return a list of the values.to_markdown([buf,?mode,?index,?storage_options])Print Series in Markdown-friendly format.to_numpy([dtype,?copy,?na_value])A NumPy ndarray representing the values in this Series or Index.to_period([freq,?copy])Convert Series from DatetimeIndex to PeriodIndex.to_pickle(path[,?compression,?protocol,?...])Pickle (serialize) object to file.to_sql(name,?con,?*[,?schema,?if_exists,?...])Write records stored in a DataFrame to a SQL database.to_string([buf,?na_rep,?float_format,?...])Render a string representation of the Series.to_timestamp([freq,?how,?copy])Cast to DatetimeIndex of Timestamps, atbeginningof period.to_xarray()Return an xarray object from the pandas object.tolist()Return a list of the values.transform(func[,?axis])Callfuncon self producing a Series with the same axis shape as self.transpose(*args,?**kwargs)Return the transpose, which is by definition self.truediv(other[,?level,?fill_value,?axis])Return Floating division of series and other, element-wise (binary operatortruediv).truncate([before,?after,?axis,?copy])Truncate a Series or DataFrame before and after some index value.tz_convert(tz[,?axis,?level,?copy])Convert tz-aware axis to target time zone.tz_localize(tz[,?axis,?level,?copy,?...])Localize tz-naive index of a Series or DataFrame to target time zone.unique()Return unique values of Series object.unstack([level,?fill_value,?sort])Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.update(other)Modify Series in place using values from passed Series.value_counts([normalize,?sort,?ascending,?...])Return a Series containing counts of unique values.var([axis,?skipna,?ddof,?numeric_only])Return unbiased variance over requested axis.view([dtype])Create a new view of the Series.where(cond[,?other,?inplace,?axis,?level])Replace values where the condition is False.xs(key[,?axis,?level,?drop_level])Return cross-section from the Series/DataFrame.dt"
Pandas,Series,pandas.Series.index,"pandas.Series.index#Series.index#The index (axis labels) of the Series.The index of a Series is used to label and identify each element of the
underlying data. The index can be thought of as an immutable ordered set
(technically a multi-set, as it may contain duplicate labels), and is
used to index and align data in pandas.Returns:IndexThe index labels of the Series.See alsoSeries.reindexConform Series to new index.Series.set_indexSet Series as DataFrame index.IndexThe base pandas index type.NotesFor more information on pandas indexing, see theindexing user guide.ExamplesTo create a Series with a custom index and view the index labels:>>>cities=['Kolkata','Chicago','Toronto','Lisbon']>>>populations=[14.85,2.71,2.93,0.51]>>>city_series=pd.Series(populations,index=cities)>>>city_series.indexIndex(['Kolkata', 'Chicago', 'Toronto', 'Lisbon'], dtype='object')To change the index labels of an existing Series:>>>city_series.index=['KOL','CHI','TOR','LIS']>>>city_series.indexIndex(['KOL', 'CHI', 'TOR', 'LIS'], dtype='object')"
Pandas,Series,pandas.Series.array,"pandas.Series.array#propertySeries.array[source]#The ExtensionArray of the data backing this Series or Index.Returns:ExtensionArrayAn ExtensionArray of the values stored within. For extension
types, this is the actual array. For NumPy native types, this
is a thin (no copy) wrapper aroundnumpy.ndarray..arraydiffers.valueswhich may require converting the
data to a different form.See alsoIndex.to_numpySimilar method that always returns a NumPy array.Series.to_numpySimilar method that always returns a NumPy array.NotesThis table lays out the different array types for each extension
dtype within pandas.dtypearray typecategoryCategoricalperiodPeriodArrayintervalIntervalArrayIntegerNAIntegerArraystringStringArraybooleanBooleanArraydatetime64[ns, tz]DatetimeArrayFor any 3rd-party extension types, the array type will be an
ExtensionArray.For all remaining dtypes.arraywill be aarrays.NumpyExtensionArraywrapping the actual ndarray
stored within. If you absolutely need a NumPy array (possibly with
copying / coercing data), then useSeries.to_numpy()instead.ExamplesFor regular NumPy types like int, and float, a NumpyExtensionArray
is returned.>>>pd.Series([1,2,3]).array<NumpyExtensionArray>[1, 2, 3]Length: 3, dtype: int64For extension types, like Categorical, the actual ExtensionArray
is returned>>>ser=pd.Series(pd.Categorical(['a','b','a']))>>>ser.array['a', 'b', 'a']Categories (2, object): ['a', 'b']"
Pandas,Series,pandas.Series.values,"pandas.Series.values#propertySeries.values[source]#Return Series as ndarray or ndarray-like depending on the dtype.WarningWe recommend usingSeries.arrayorSeries.to_numpy(), depending on whether you need
a reference to the underlying data or a NumPy array.Returns:numpy.ndarray or ndarray-likeSee alsoSeries.arrayReference to the underlying data.Series.to_numpyA NumPy array representing the underlying data.Examples>>>pd.Series([1,2,3]).valuesarray([1, 2, 3])>>>pd.Series(list('aabc')).valuesarray(['a', 'a', 'b', 'c'], dtype=object)>>>pd.Series(list('aabc')).astype('category').values['a', 'a', 'b', 'c']Categories (3, object): ['a', 'b', 'c']Timezone aware datetime data is converted to UTC:>>>pd.Series(pd.date_range('20130101',periods=3,...tz='US/Eastern')).valuesarray(['2013-01-01T05:00:00.000000000','2013-01-02T05:00:00.000000000','2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')"
Pandas,Series,pandas.Series.dtype,"pandas.Series.dtype#propertySeries.dtype[source]#Return the dtype object of the underlying data.Examples>>>s=pd.Series([1,2,3])>>>s.dtypedtype('int64')"
Pandas,Series,pandas.Series.shape,"pandas.Series.shape#propertySeries.shape[source]#Return a tuple of the shape of the underlying data.Examples>>>s=pd.Series([1,2,3])>>>s.shape(3,)"
Pandas,Series,pandas.Series.nbytes,"pandas.Series.nbytes#propertySeries.nbytes[source]#Return the number of bytes in the underlying data.ExamplesFor Series:>>>s=pd.Series(['Ant','Bear','Cow'])>>>s0     Ant1    Bear2     Cowdtype: object>>>s.nbytes24For Index:>>>idx=pd.Index([1,2,3])>>>idxIndex([1, 2, 3], dtype='int64')>>>idx.nbytes24"
Pandas,Series,pandas.Series.ndim,"pandas.Series.ndim#propertySeries.ndim[source]#Number of dimensions of the underlying data, by definition 1.Examples>>>s=pd.Series(['Ant','Bear','Cow'])>>>s0     Ant1    Bear2     Cowdtype: object>>>s.ndim1For Index:>>>idx=pd.Index([1,2,3])>>>idxIndex([1, 2, 3], dtype='int64')>>>idx.ndim1"
Pandas,Series,pandas.Series.size,"pandas.Series.size#propertySeries.size[source]#Return the number of elements in the underlying data.ExamplesFor Series:>>>s=pd.Series(['Ant','Bear','Cow'])>>>s0     Ant1    Bear2     Cowdtype: object>>>s.size3For Index:>>>idx=pd.Index([1,2,3])>>>idxIndex([1, 2, 3], dtype='int64')>>>idx.size3"
Pandas,Series,pandas.Series.T,"pandas.Series.T#propertySeries.T[source]#Return the transpose, which is by definition self.ExamplesFor Series:>>>s=pd.Series(['Ant','Bear','Cow'])>>>s0     Ant1    Bear2     Cowdtype: object>>>s.T0     Ant1    Bear2     Cowdtype: objectFor Index:>>>idx=pd.Index([1,2,3])>>>idx.TIndex([1, 2, 3], dtype='int64')"
Pandas,Series,pandas.Series.memory_usage,"pandas.Series.memory_usage#Series.memory_usage(index=True,deep=False)[source]#Return the memory usage of the Series.The memory usage can optionally include the contribution of
the index and of elements ofobjectdtype.Parameters:indexbool, default TrueSpecifies whether to include the memory usage of the Series index.deepbool, default FalseIf True, introspect the data deeply by interrogatingobjectdtypes for system-level memory consumption, and include
it in the returned value.Returns:intBytes of memory consumed.See alsonumpy.ndarray.nbytesTotal bytes consumed by the elements of the array.DataFrame.memory_usageBytes consumed by a DataFrame.Examples>>>s=pd.Series(range(3))>>>s.memory_usage()152Not including the index gives the size of the rest of the data, which
is necessarily smaller:>>>s.memory_usage(index=False)24The memory footprint ofobjectvalues is ignored by default:>>>s=pd.Series([""a"",""b""])>>>s.valuesarray(['a', 'b'], dtype=object)>>>s.memory_usage()144>>>s.memory_usage(deep=True)244"
Pandas,Series,pandas.Series.hasnans,"pandas.Series.hasnans#propertySeries.hasnans[source]#Return True if there are any NaNs.Enables various performance speedups.Returns:boolExamples>>>s=pd.Series([1,2,3,None])>>>s0    1.01    2.02    3.03    NaNdtype: float64>>>s.hasnansTrue"
Pandas,Series,pandas.Series.empty,"pandas.Series.empty#propertySeries.empty[source]#Indicator whether Series/DataFrame is empty.True if Series/DataFrame is entirely empty (no items), meaning any of the
axes are of length 0.Returns:boolIf Series/DataFrame is empty, return True, if not return False.See alsoSeries.dropnaReturn series without null values.DataFrame.dropnaReturn DataFrame with labels on given axis omitted where (all or any) data are missing.NotesIf Series/DataFrame contains only NaNs, it is still not considered empty. See
the example below.ExamplesAn example of an actual empty DataFrame. Notice the index is empty:>>>df_empty=pd.DataFrame({'A':[]})>>>df_emptyEmpty DataFrameColumns: [A]Index: []>>>df_empty.emptyTrueIf we only have NaNs in our DataFrame, it is not considered empty! We
will need to drop the NaNs to make the DataFrame empty:>>>df=pd.DataFrame({'A':[np.nan]})>>>dfA0 NaN>>>df.emptyFalse>>>df.dropna().emptyTrue>>>ser_empty=pd.Series({'A':[]})>>>ser_emptyA    []dtype: object>>>ser_empty.emptyFalse>>>ser_empty=pd.Series()>>>ser_empty.emptyTrue"
Pandas,Series,pandas.Series.dtypes,"pandas.Series.dtypes#propertySeries.dtypes[source]#Return the dtype object of the underlying data.Examples>>>s=pd.Series([1,2,3])>>>s.dtypesdtype('int64')"
Pandas,Series,pandas.Series.name,"pandas.Series.name#propertySeries.name[source]#Return the name of the Series.The name of a Series becomes its index or column name if it is used
to form a DataFrame. It is also used whenever displaying the Series
using the interpreter.Returns:label (hashable object)The name of the Series, also the column name if part of a DataFrame.See alsoSeries.renameSets the Series name when given a scalar input.Index.nameCorresponding Index property.ExamplesThe Series name can be set initially when calling the constructor.>>>s=pd.Series([1,2,3],dtype=np.int64,name='Numbers')>>>s0    11    22    3Name: Numbers, dtype: int64>>>s.name=""Integers"">>>s0    11    22    3Name: Integers, dtype: int64The name of a Series within a DataFrame is its column name.>>>df=pd.DataFrame([[1,2],[3,4],[5,6]],...columns=[""Odd Numbers"",""Even Numbers""])>>>dfOdd Numbers  Even Numbers0            1             21            3             42            5             6>>>df[""Even Numbers""].name'Even Numbers'"
Pandas,Series,pandas.Series.flags,"pandas.Series.flags#propertySeries.flags[source]#Get the properties associated with this pandas object.The available flags areFlags.allows_duplicate_labelsSee alsoFlagsFlags that apply to pandas objects.DataFrame.attrsGlobal metadata applying to this dataset.NotesgFlagsh differ from gmetadatah. Flags reflect properties of the
pandas object (the Series or DataFrame). Metadata refer to properties
of the dataset, and should be stored inDataFrame.attrs.Examples>>>df=pd.DataFrame({""A"":[1,2]})>>>df.flags<Flags(allows_duplicate_labels=True)>Flags can be get or set using.>>>df.flags.allows_duplicate_labelsTrue>>>df.flags.allows_duplicate_labels=FalseOr by slicing with a key>>>df.flags[""allows_duplicate_labels""]False>>>df.flags[""allows_duplicate_labels""]=True"
Pandas,Series,pandas.Series.set_flags,"pandas.Series.set_flags#Series.set_flags(*,copy=False,allows_duplicate_labels=None)[source]#Return a new object with updated flags.Parameters:copybool, default FalseSpecify if a copy of the object should be made.allows_duplicate_labelsbool, optionalWhether the returned object allows duplicate labels.Returns:Series or DataFrameThe same type as the caller.See alsoDataFrame.attrsGlobal metadata applying to this dataset.DataFrame.flagsGlobal flags applying to this object.NotesThis method returns a new object thatfs a view on the same data
as the input. Mutating the input or the output values will be reflected
in the other.This method is intended to be used in method chains.gFlagsh differ from gmetadatah. Flags reflect properties of the
pandas object (the Series or DataFrame). Metadata refer to properties
of the dataset, and should be stored inDataFrame.attrs.Examples>>>df=pd.DataFrame({""A"":[1,2]})>>>df.flags.allows_duplicate_labelsTrue>>>df2=df.set_flags(allows_duplicate_labels=False)>>>df2.flags.allows_duplicate_labelsFalse"
Pandas,Series,pandas.Series.astype,"pandas.Series.astype#Series.astype(dtype,copy=None,errors='raise')[source]#Cast a pandas object to a specified dtypedtype.Parameters:dtypestr, data type, Series or Mapping of column name -> data typeUse a str, numpy.dtype, pandas.ExtensionDtype or Python type to
cast entire pandas object to the same type. Alternatively, use a
mapping, e.g. {col: dtype, c}, where col is a column label and dtype is
a numpy.dtype or Python type to cast one or more of the DataFramefs
columns to column-specific types.copybool, default TrueReturn a copy whencopy=True(be very careful settingcopy=Falseas changes to values then may propagate to other
pandas objects).errors{eraisef, eignoref}, default eraisefControl raising of exceptions on invalid data for provided dtype.raise: allow exceptions to be raisedignore: suppress exceptions. On error return original object.Returns:same type as callerSee alsoto_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to a numeric type.numpy.ndarray.astypeCast a numpy array to a specified type.NotesChanged in version 2.0.0:Usingastypeto convert from timezone-naive dtype to
timezone-aware dtype will raise an exception.
UseSeries.dt.tz_localize()instead.ExamplesCreate a DataFrame:>>>d={'col1':[1,2],'col2':[3,4]}>>>df=pd.DataFrame(data=d)>>>df.dtypescol1    int64col2    int64dtype: objectCast all columns to int32:>>>df.astype('int32').dtypescol1    int32col2    int32dtype: objectCast col1 to int32 using a dictionary:>>>df.astype({'col1':'int32'}).dtypescol1    int32col2    int64dtype: objectCreate a series:>>>ser=pd.Series([1,2],dtype='int32')>>>ser0    11    2dtype: int32>>>ser.astype('int64')0    11    2dtype: int64Convert to categorical type:>>>ser.astype('category')0    11    2dtype: categoryCategories (2, int32): [1, 2]Convert to ordered categorical type with custom ordering:>>>frompandas.api.typesimportCategoricalDtype>>>cat_dtype=CategoricalDtype(...categories=[2,1],ordered=True)>>>ser.astype(cat_dtype)0    11    2dtype: categoryCategories (2, int64): [2 < 1]Create a series of dates:>>>ser_date=pd.Series(pd.date_range('20200101',periods=3))>>>ser_date0   2020-01-011   2020-01-022   2020-01-03dtype: datetime64[ns]"
Pandas,Series,pandas.Series.convert_dtypes,"pandas.Series.convert_dtypes#Series.convert_dtypes(infer_objects=True,convert_string=True,convert_integer=True,convert_boolean=True,convert_floating=True,dtype_backend='numpy_nullable')[source]#Convert columns to the best possible dtypes using dtypes supportingpd.NA.Parameters:infer_objectsbool, default TrueWhether object dtypes should be converted to the best possible types.convert_stringbool, default TrueWhether object dtypes should be converted toStringDtype().convert_integerbool, default TrueWhether, if possible, conversion can be done to integer extension types.convert_booleanbool, defaults TrueWhether object dtypes should be converted toBooleanDtypes().convert_floatingbool, defaults TrueWhether, if possible, conversion can be done to floating extension types.
Ifconvert_integeris also True, preference will be give to integer
dtypes if the floats can be faithfully casted to integers.New in version 1.2.0.dtype_backend{enumpy_nullablef, epyarrowf}, default enumpy_nullablefBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:Series or DataFrameCopy of input object with new dtype.See alsoinfer_objectsInfer dtypes of objects.to_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to a numeric type.NotesBy default,convert_dtypeswill attempt to convert a Series (or each
Series in a DataFrame) to dtypes that supportpd.NA. By using the optionsconvert_string,convert_integer,convert_booleanandconvert_floating, it is possible to turn off individual conversions
toStringDtype, the integer extension types,BooleanDtypeor floating extension types, respectively.For object-dtyped columns, ifinfer_objectsisTrue, use the inference
rules as during normal Series/DataFrame construction. Then, if possible,
convert toStringDtype,BooleanDtypeor an appropriate integer
or floating extension type, otherwise leave asobject.If the dtype is integer, convert to an appropriate integer extension type.If the dtype is numeric, and consists of all integers, convert to an
appropriate integer extension type. Otherwise, convert to an
appropriate floating extension type.Changed in version 1.2:Starting with pandas 1.2, this method also converts float columns
to the nullable floating extension type.In the future, as new dtypes are added that supportpd.NA, the results
of this method will change to support those new dtypes.Examples>>>df=pd.DataFrame(...{...""a"":pd.Series([1,2,3],dtype=np.dtype(""int32"")),...""b"":pd.Series([""x"",""y"",""z""],dtype=np.dtype(""O"")),...""c"":pd.Series([True,False,np.nan],dtype=np.dtype(""O"")),...""d"":pd.Series([""h"",""i"",np.nan],dtype=np.dtype(""O"")),...""e"":pd.Series([10,np.nan,20],dtype=np.dtype(""float"")),...""f"":pd.Series([np.nan,100.5,200],dtype=np.dtype(""float"")),...}...)Start with a DataFrame with default dtypes.>>>dfa  b      c    d     e      f0  1  x   True    h  10.0    NaN1  2  y  False    i   NaN  100.52  3  z    NaN  NaN  20.0  200.0>>>df.dtypesa      int32b     objectc     objectd     objecte    float64f    float64dtype: objectConvert the DataFrame to use best possible dtypes.>>>dfn=df.convert_dtypes()>>>dfna  b      c     d     e      f0  1  x   True     h    10   <NA>1  2  y  False     i  <NA>  100.52  3  z   <NA>  <NA>    20  200.0>>>dfn.dtypesa             Int32b    string[python]c           booleand    string[python]e             Int64f           Float64dtype: objectStart with a Series of strings and missing data represented bynp.nan.>>>s=pd.Series([""a"",""b"",np.nan])>>>s0      a1      b2    NaNdtype: objectObtain a Series with dtypeStringDtype.>>>s.convert_dtypes()0       a1       b2    <NA>dtype: string"
Pandas,Series,pandas.Series.infer_objects,"pandas.Series.infer_objects#Series.infer_objects(copy=None)[source]#Attempt to infer better dtypes for object columns.Attempts soft conversion of object-dtyped
columns, leaving non-object and unconvertible
columns unchanged. The inference rules are the
same as during normal Series/DataFrame construction.Parameters:copybool, default TrueWhether to make a copy for non-object or non-inferable columns
or Series.Returns:same type as input objectSee alsoto_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to numeric type.convert_dtypesConvert argument to best possible dtype.Examples>>>df=pd.DataFrame({""A"":[""a"",1,2,3]})>>>df=df.iloc[1:]>>>dfA1  12  23  3>>>df.dtypesA    objectdtype: object>>>df.infer_objects().dtypesA    int64dtype: object"
Pandas,Series,pandas.Series.copy,"pandas.Series.copy#Series.copy(deep=True)[source]#Make a copy of this objectfs indices and data.Whendeep=True(default), a new object will be created with a
copy of the calling objectfs data and indices. Modifications to
the data or indices of the copy will not be reflected in the
original object (see notes below).Whendeep=False, a new object will be created without copying
the calling objectfs data or index (only references to the data
and index are copied). Any changes to the data of the original
will be reflected in the shallow copy (and vice versa).Parameters:deepbool, default TrueMake a deep copy, including a copy of the data and the indices.
Withdeep=Falseneither the indices nor the data are copied.Returns:Series or DataFrameObject type matches caller.NotesWhendeep=True, data is copied but actual Python objects
will not be copied recursively, only the reference to the object.
This is in contrast tocopy.deepcopyin the Standard Library,
which recursively copies object data (see examples below).WhileIndexobjects are copied whendeep=True, the underlying
numpy array is not copied for performance reasons. SinceIndexis
immutable, the underlying data can be safely shared and a copy
is not needed.Since pandas is not thread safe, see thegotchaswhen copying in a threading
environment.Whencopy_on_writein pandas config is set toTrue, thecopy_on_writeconfig takes effect even whendeep=False.
This means that any changes to the copied data would make a new copy
of the data upon write (and vice versa). Changes made to either the
original or copied variable would not be reflected in the counterpart.
SeeCopy_on_Writefor more information.Examples>>>s=pd.Series([1,2],index=[""a"",""b""])>>>sa    1b    2dtype: int64>>>s_copy=s.copy()>>>s_copya    1b    2dtype: int64Shallow copy versus default (deep) copy:>>>s=pd.Series([1,2],index=[""a"",""b""])>>>deep=s.copy()>>>shallow=s.copy(deep=False)Shallow copy shares data and index with original.>>>sisshallowFalse>>>s.valuesisshallow.valuesands.indexisshallow.indexTrueDeep copy has own copy of data and index.>>>sisdeepFalse>>>s.valuesisdeep.valuesors.indexisdeep.indexFalseUpdates to the data shared by shallow copy and original is reflected
in both; deep copy remains unchanged.>>>s.iloc[0]=3>>>shallow.iloc[1]=4>>>sa    3b    4dtype: int64>>>shallowa    3b    4dtype: int64>>>deepa    1b    2dtype: int64Note that when copying an object containing Python objects, a deep copy
will copy the data, but will not do so recursively. Updating a nested
data object will be reflected in the deep copy.>>>s=pd.Series([[1,2],[3,4]])>>>deep=s.copy()>>>s[0][0]=10>>>s0    [10, 2]1     [3, 4]dtype: object>>>deep0    [10, 2]1     [3, 4]dtype: object** Copy-on-Write is set to true: **>>>withpd.option_context(""mode.copy_on_write"",True):...s=pd.Series([1,2],index=[""a"",""b""])...copy=s.copy(deep=False)...s.iloc[0]=100...sa    100b      2dtype: int64>>>copya    1b    2dtype: int64"
Pandas,Series,pandas.Series.bool,"pandas.Series.bool#Series.bool()[source]#Return the bool of a single element Series or DataFrame.Deprecated since version 2.1.0:bool is deprecated and will be removed in future version of pandasThis must be a boolean scalar value, either True or False. It will raise a
ValueError if the Series or DataFrame does not have exactly 1 element, or that
element is not boolean (integer values 0 and 1 will also raise an exception).Returns:boolThe value in the Series or DataFrame.See alsoSeries.astypeChange the data type of a Series, including to boolean.DataFrame.astypeChange the data type of a DataFrame, including to boolean.numpy.bool_NumPy boolean data type, used by pandas for boolean values.ExamplesThe method will only work for single element objects with a boolean value:>>>pd.Series([True]).bool()True>>>pd.Series([False]).bool()False>>>pd.DataFrame({'col':[True]}).bool()True>>>pd.DataFrame({'col':[False]}).bool()False"
Pandas,Series,pandas.Series.to_numpy,"pandas.Series.to_numpy#Series.to_numpy(dtype=None,copy=False,na_value=_NoDefault.no_default,**kwargs)[source]#A NumPy ndarray representing the values in this Series or Index.Parameters:dtypestr or numpy.dtype, optionalThe dtype to pass tonumpy.asarray().copybool, default FalseWhether to ensure that the returned value is not a view on
another array. Note thatcopy=Falsedoes notensurethatto_numpy()is no-copy. Rather,copy=Trueensure that
a copy is made, even if not strictly necessary.na_valueAny, optionalThe value to use for missing values. The default value depends
ondtypeand the type of the array.**kwargsAdditional keywords passed through to theto_numpymethod
of the underlying array (for extension arrays).Returns:numpy.ndarraySee alsoSeries.arrayGet the actual data stored within.Index.arrayGet the actual data stored within.DataFrame.to_numpySimilar method for DataFrame.NotesThe returned array will be the same up to equality (values equal
inselfwill be equal in the returned array; likewise for values
that are not equal). Whenselfcontains an ExtensionArray, the
dtype may be different. For example, for a category-dtype Series,to_numpy()will return a NumPy array and the categorical dtype
will be lost.For NumPy dtypes, this will be a reference to the actual data stored
in this Series or Index (assumingcopy=False). Modifying the result
in place will modify the data stored in the Series or Index (not that
we recommend doing that).For extension types,to_numpy()mayrequire copying data and
coercing the result to a NumPy type (possibly object), which may be
expensive. When you need a no-copy reference to the underlying data,Series.arrayshould be used instead.This table lays out the different dtypes and default return types ofto_numpy()for various dtypes within pandas.dtypearray typecategory[T]ndarray[T] (same dtype as input)periodndarray[object] (Periods)intervalndarray[object] (Intervals)IntegerNAndarray[object]datetime64[ns]datetime64[ns]datetime64[ns, tz]ndarray[object] (Timestamps)Examples>>>ser=pd.Series(pd.Categorical(['a','b','a']))>>>ser.to_numpy()array(['a', 'b', 'a'], dtype=object)Specify thedtypeto control how datetime-aware data is represented.
Usedtype=objectto return an ndarray of pandasTimestampobjects, each with the correcttz.>>>ser=pd.Series(pd.date_range('2000',periods=2,tz=""CET""))>>>ser.to_numpy(dtype=object)array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),Timestamp('2000-01-02 00:00:00+0100', tz='CET')],dtype=object)Ordtype='datetime64[ns]'to return an ndarray of native
datetime64 values. The values are converted to UTC and the timezone
info is dropped.>>>ser.to_numpy(dtype=""datetime64[ns]"")...array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],dtype='datetime64[ns]')"
Pandas,Series,pandas.Series.to_period,"pandas.Series.to_period#Series.to_period(freq=None,copy=None)[source]#Convert Series from DatetimeIndex to PeriodIndex.Parameters:freqstr, default NoneFrequency associated with the PeriodIndex.copybool, default TrueWhether or not to return a copy.Returns:SeriesSeries with index converted to PeriodIndex.Examples>>>idx=pd.DatetimeIndex(['2023','2024','2025'])>>>s=pd.Series([1,2,3],index=idx)>>>s=s.to_period()>>>s2023    12024    22025    3Freq: A-DEC, dtype: int64Viewing the index>>>s.indexPeriodIndex(['2023', '2024', '2025'], dtype='period[A-DEC]')"
Pandas,Series,pandas.Series.to_timestamp,"pandas.Series.to_timestamp#Series.to_timestamp(freq=None,how='start',copy=None)[source]#Cast to DatetimeIndex of Timestamps, atbeginningof period.Parameters:freqstr, default frequency of PeriodIndexDesired frequency.how{esf, eef, estartf, eendf}Convention for converting period to timestamp; start of period
vs. end.copybool, default TrueWhether or not to return a copy.Returns:Series with DatetimeIndexExamples>>>idx=pd.PeriodIndex(['2023','2024','2025'],freq='Y')>>>s1=pd.Series([1,2,3],index=idx)>>>s12023    12024    22025    3Freq: A-DEC, dtype: int64The resulting frequency of the Timestamps isYearBegin>>>s1=s1.to_timestamp()>>>s12023-01-01    12024-01-01    22025-01-01    3Freq: AS-JAN, dtype: int64Usingfreqwhich is the offset that the Timestamps will have>>>s2=pd.Series([1,2,3],index=idx)>>>s2=s2.to_timestamp(freq='M')>>>s22023-01-31    12024-01-31    22025-01-31    3Freq: A-JAN, dtype: int64"
Pandas,Series,pandas.Series.to_list,"pandas.Series.to_list#Series.to_list()[source]#Return a list of the values.These are each a scalar type, which is a Python scalar
(for str, int, float) or a pandas scalar
(for Timestamp/Timedelta/Interval/Period)Returns:listSee alsonumpy.ndarray.tolistReturn the array as an a.ndim-levels deep nested list of Python scalars.ExamplesFor Series>>>s=pd.Series([1,2,3])>>>s.to_list()[1, 2, 3]For Index:>>>idx=pd.Index([1,2,3])>>>idxIndex([1, 2, 3], dtype='int64')>>>idx.to_list()[1, 2, 3]"
Pandas,Series,pandas.Series.__array__,"pandas.Series.__array__#Series.__array__(dtype=None)[source]#Return the values as a NumPy array.Users should not call this directly. Rather, it is invoked bynumpy.array()andnumpy.asarray().Parameters:dtypestr or numpy.dtype, optionalThe dtype to use for the resulting NumPy array. By default,
the dtype is inferred from the data.Returns:numpy.ndarrayThe values in the series converted to anumpy.ndarraywith the specifieddtype.See alsoarrayCreate a new array from data.Series.arrayZero-copy view to the array backing the Series.Series.to_numpySeries method for similar behavior.Examples>>>ser=pd.Series([1,2,3])>>>np.asarray(ser)array([1, 2, 3])For timezone-aware data, the timezones may be retained withdtype='object'>>>tzser=pd.Series(pd.date_range('2000',periods=2,tz=""CET""))>>>np.asarray(tzser,dtype=""object"")array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),Timestamp('2000-01-02 00:00:00+0100', tz='CET')],dtype=object)Or the values may be localized to UTC and the tzinfo discarded withdtype='datetime64[ns]'>>>np.asarray(tzser,dtype=""datetime64[ns]"")array(['1999-12-31T23:00:00.000000000', ...],dtype='datetime64[ns]')"
Pandas,Series,pandas.Series.get,"pandas.Series.get#Series.get(key,default=None)[source]#Get item from object for given key (ex: DataFrame column).Returns default value if not found.Parameters:keyobjectReturns:same type as items contained in objectExamples>>>df=pd.DataFrame(...[...[24.3,75.7,""high""],...[31,87.8,""high""],...[22,71.6,""medium""],...[35,95,""medium""],...],...columns=[""temp_celsius"",""temp_fahrenheit"",""windspeed""],...index=pd.date_range(start=""2014-02-12"",end=""2014-02-15"",freq=""D""),...)>>>dftemp_celsius  temp_fahrenheit windspeed2014-02-12          24.3             75.7      high2014-02-13          31.0             87.8      high2014-02-14          22.0             71.6    medium2014-02-15          35.0             95.0    medium>>>df.get([""temp_celsius"",""windspeed""])temp_celsius windspeed2014-02-12          24.3      high2014-02-13          31.0      high2014-02-14          22.0    medium2014-02-15          35.0    medium>>>ser=df['windspeed']>>>ser.get('2014-02-13')'high'If the key isnft found, the default value will be used.>>>df.get([""temp_celsius"",""temp_kelvin""],default=""default_value"")'default_value'>>>ser.get('2014-02-10','[unknown]')'[unknown]'"
Pandas,Series,pandas.Series.at,"pandas.Series.at#propertySeries.at[source]#Access a single value for a row/column label pair.Similar toloc, in that both provide label-based lookups. Useatif you only need to get or set a single value in a DataFrame
or Series.Raises:KeyErrorIf getting a value and elabelf does not exist in a DataFrame orSeries.ValueErrorIf row/column label pair is not a tuple or if any label fromthe pair is not a scalar for DataFrame.If label is list-like (excludingNamedTuple) for Series.See alsoDataFrame.atAccess a single value for a row/column pair by label.DataFrame.iatAccess a single value for a row/column pair by integer position.DataFrame.locAccess a group of rows and columns by label(s).DataFrame.ilocAccess a group of rows and columns by integer position(s).Series.atAccess a single value by label.Series.iatAccess a single value by integer position.Series.locAccess a group of rows by label(s).Series.ilocAccess a group of rows by integer position(s).NotesSeeFast scalar value getting and settingfor more details.Examples>>>df=pd.DataFrame([[0,2,3],[0,4,1],[10,20,30]],...index=[4,5,6],columns=['A','B','C'])>>>dfA   B   C4   0   2   35   0   4   16  10  20  30Get value at specified row/column pair>>>df.at[4,'B']2Set value at specified row/column pair>>>df.at[4,'B']=10>>>df.at[4,'B']10Get value within a Series>>>df.loc[5].at['B']4"
Pandas,Series,pandas.Series.iat,"pandas.Series.iat#propertySeries.iat[source]#Access a single value for a row/column pair by integer position.Similar toiloc, in that both provide integer-based lookups. Useiatif you only need to get or set a single value in a DataFrame
or Series.Raises:IndexErrorWhen integer position is out of bounds.See alsoDataFrame.atAccess a single value for a row/column label pair.DataFrame.locAccess a group of rows and columns by label(s).DataFrame.ilocAccess a group of rows and columns by integer position(s).Examples>>>df=pd.DataFrame([[0,2,3],[0,4,1],[10,20,30]],...columns=['A','B','C'])>>>dfA   B   C0   0   2   31   0   4   12  10  20  30Get value at specified row/column pair>>>df.iat[1,2]1Set value at specified row/column pair>>>df.iat[1,2]=10>>>df.iat[1,2]10Get value within a series>>>df.loc[0].iat[1]2"
Pandas,Series,pandas.Series.loc,"pandas.Series.loc#propertySeries.loc[source]#Access a group of rows and columns by label(s) or a boolean array..loc[]is primarily label based, but may also be used with a
boolean array.Allowed inputs are:A single label, e.g.5or'a', (note that5is
interpreted as alabelof the index, andneveras an
integer position along the index).A list or array of labels, e.g.['a','b','c'].A slice object with labels, e.g.'a':'f'.WarningNote that contrary to usual python slices,boththe
start and the stop are includedA boolean array of the same length as the axis being sliced,
e.g.[True,False,True].An alignable boolean Series. The index of the key will be aligned before
masking.An alignable Index. The Index of the returned selection will be the input.Acallablefunction with one argument (the calling Series or
DataFrame) and that returns valid output for indexing (one of the above)See more atSelection by Label.Raises:KeyErrorIf any items are not found.IndexingErrorIf an indexed key is passed and its index is unalignable to the frame index.See alsoDataFrame.atAccess a single value for a row/column label pair.DataFrame.ilocAccess group of rows and columns by integer position(s).DataFrame.xsReturns a cross-section (row(s) or column(s)) from the Series/DataFrame.Series.locAccess group of values using labels.ExamplesGetting values>>>df=pd.DataFrame([[1,2],[4,5],[7,8]],...index=['cobra','viper','sidewinder'],...columns=['max_speed','shield'])>>>dfmax_speed  shieldcobra               1       2viper               4       5sidewinder          7       8Single label. Note this returns the row as a Series.>>>df.loc['viper']max_speed    4shield       5Name: viper, dtype: int64List of labels. Note using[[]]returns a DataFrame.>>>df.loc[['viper','sidewinder']]max_speed  shieldviper               4       5sidewinder          7       8Single label for row and column>>>df.loc['cobra','shield']2Slice with labels for row and single label for column. As mentioned
above, note that both the start and stop of the slice are included.>>>df.loc['cobra':'viper','max_speed']cobra    1viper    4Name: max_speed, dtype: int64Boolean list with the same length as the row axis>>>df.loc[[False,False,True]]max_speed  shieldsidewinder          7       8Alignable boolean Series:>>>df.loc[pd.Series([False,True,False],...index=['viper','sidewinder','cobra'])]max_speed  shieldsidewinder          7       8Index (same behavior asdf.reindex)>>>df.loc[pd.Index([""cobra"",""viper""],name=""foo"")]max_speed  shieldfoocobra          1       2viper          4       5Conditional that returns a boolean Series>>>df.loc[df['shield']>6]max_speed  shieldsidewinder          7       8Conditional that returns a boolean Series with column labels specified>>>df.loc[df['shield']>6,['max_speed']]max_speedsidewinder          7Multiple conditional using&that returns a boolean Series>>>df.loc[(df['max_speed']>1)&(df['shield']<8)]max_speed  shieldviper          4       5Multiple conditional using|that returns a boolean Series>>>df.loc[(df['max_speed']>4)|(df['shield']<5)]max_speed  shieldcobra               1       2sidewinder          7       8Please ensure that each condition is wrapped in parentheses().
See theuser guidefor more details and explanations of Boolean indexing.NoteIf you find yourself using 3 or more conditionals in.loc[],
consider usingadvanced indexing.See below for using.loc[]on MultiIndex DataFrames.Callable that returns a boolean Series>>>df.loc[lambdadf:df['shield']==8]max_speed  shieldsidewinder          7       8Setting valuesSet value for all items matching the list of labels>>>df.loc[['viper','sidewinder'],['shield']]=50>>>dfmax_speed  shieldcobra               1       2viper               4      50sidewinder          7      50Set value for an entire row>>>df.loc['cobra']=10>>>dfmax_speed  shieldcobra              10      10viper               4      50sidewinder          7      50Set value for an entire column>>>df.loc[:,'max_speed']=30>>>dfmax_speed  shieldcobra              30      10viper              30      50sidewinder         30      50Set value for rows matching callable condition>>>df.loc[df['shield']>35]=0>>>dfmax_speed  shieldcobra              30      10viper               0       0sidewinder          0       0Add value matching location>>>df.loc[""viper"",""shield""]+=5>>>dfmax_speed  shieldcobra              30      10viper               0       5sidewinder          0       0Setting using aSeriesor aDataFramesets the values matching the
index labels, not the index positions.>>>shuffled_df=df.loc[[""viper"",""cobra"",""sidewinder""]]>>>df.loc[:]+=shuffled_df>>>dfmax_speed  shieldcobra              60      20viper               0      10sidewinder          0       0Getting values on a DataFrame with an index that has integer labelsAnother example using integers for the index>>>df=pd.DataFrame([[1,2],[4,5],[7,8]],...index=[7,8,9],columns=['max_speed','shield'])>>>dfmax_speed  shield7          1       28          4       59          7       8Slice with integer labels for rows. As mentioned above, note that both
the start and stop of the slice are included.>>>df.loc[7:9]max_speed  shield7          1       28          4       59          7       8Getting values with a MultiIndexA number of examples using a DataFrame with a MultiIndex>>>tuples=[...('cobra','mark i'),('cobra','mark ii'),...('sidewinder','mark i'),('sidewinder','mark ii'),...('viper','mark ii'),('viper','mark iii')...]>>>index=pd.MultiIndex.from_tuples(tuples)>>>values=[[12,2],[0,4],[10,20],...[1,4],[7,1],[16,36]]>>>df=pd.DataFrame(values,columns=['max_speed','shield'],index=index)>>>dfmax_speed  shieldcobra      mark i           12       2mark ii           0       4sidewinder mark i           10      20mark ii           1       4viper      mark ii           7       1mark iii         16      36Single label. Note this returns a DataFrame with a single index.>>>df.loc['cobra']max_speed  shieldmark i          12       2mark ii          0       4Single index tuple. Note this returns a Series.>>>df.loc[('cobra','mark ii')]max_speed    0shield       4Name: (cobra, mark ii), dtype: int64Single label for row and column. Similar to passing in a tuple, this
returns a Series.>>>df.loc['cobra','mark i']max_speed    12shield        2Name: (cobra, mark i), dtype: int64Single tuple. Note using[[]]returns a DataFrame.>>>df.loc[[('cobra','mark ii')]]max_speed  shieldcobra mark ii          0       4Single tuple for the index with a single label for the column>>>df.loc[('cobra','mark i'),'shield']2Slice from index tuple to single label>>>df.loc[('cobra','mark i'):'viper']max_speed  shieldcobra      mark i           12       2mark ii           0       4sidewinder mark i           10      20mark ii           1       4viper      mark ii           7       1mark iii         16      36Slice from index tuple to index tuple>>>df.loc[('cobra','mark i'):('viper','mark ii')]max_speed  shieldcobra      mark i          12       2mark ii          0       4sidewinder mark i          10      20mark ii          1       4viper      mark ii          7       1Please see theuser guidefor more details and explanations of advanced indexing."
Pandas,Series,pandas.Series.iloc,"pandas.Series.iloc#propertySeries.iloc[source]#Purely integer-location based indexing for selection by position..iloc[]is primarily integer position based (from0tolength-1of the axis), but may also be used with a boolean
array.Allowed inputs are:An integer, e.g.5.A list or array of integers, e.g.[4,3,0].A slice object with ints, e.g.1:7.A boolean array.Acallablefunction with one argument (the calling Series or
DataFrame) and that returns valid output for indexing (one of the above).
This is useful in method chains, when you donft have a reference to the
calling object, but would like to base your selection on some value.A tuple of row and column indexes. The tuple elements consist of one of the
above inputs, e.g.(0,1)..ilocwill raiseIndexErrorif a requested indexer is
out-of-bounds, exceptsliceindexers which allow out-of-bounds
indexing (this conforms with python/numpyslicesemantics).See more atSelection by Position.See alsoDataFrame.iatFast integer location scalar accessor.DataFrame.locPurely label-location based indexer for selection by label.Series.ilocPurely integer-location based indexing for selection by position.Examples>>>mydict=[{'a':1,'b':2,'c':3,'d':4},...{'a':100,'b':200,'c':300,'d':400},...{'a':1000,'b':2000,'c':3000,'d':4000}]>>>df=pd.DataFrame(mydict)>>>dfa     b     c     d0     1     2     3     41   100   200   300   4002  1000  2000  3000  4000Indexing just the rowsWith a scalar integer.>>>type(df.iloc[0])<class 'pandas.core.series.Series'>>>>df.iloc[0]a    1b    2c    3d    4Name: 0, dtype: int64With a list of integers.>>>df.iloc[[0]]a  b  c  d0  1  2  3  4>>>type(df.iloc[[0]])<class 'pandas.core.frame.DataFrame'>>>>df.iloc[[0,1]]a    b    c    d0    1    2    3    41  100  200  300  400With asliceobject.>>>df.iloc[:3]a     b     c     d0     1     2     3     41   100   200   300   4002  1000  2000  3000  4000With a boolean mask the same length as the index.>>>df.iloc[[True,False,True]]a     b     c     d0     1     2     3     42  1000  2000  3000  4000With a callable, useful in method chains. Thexpassed
to thelambdais the DataFrame being sliced. This selects
the rows whose index label even.>>>df.iloc[lambdax:x.index%2==0]a     b     c     d0     1     2     3     42  1000  2000  3000  4000Indexing both axesYou can mix the indexer types for the index and columns. Use:to
select the entire axis.With scalar integers.>>>df.iloc[0,1]2With lists of integers.>>>df.iloc[[0,2],[1,3]]b     d0     2     42  2000  4000Withsliceobjects.>>>df.iloc[1:3,0:3]a     b     c1   100   200   3002  1000  2000  3000With a boolean array whose length matches the columns.>>>df.iloc[:,[True,False,True,False]]a     c0     1     31   100   3002  1000  3000With a callable function that expects the Series or DataFrame.>>>df.iloc[:,lambdadf:[0,2]]a     c0     1     31   100   3002  1000  3000"
Pandas,Series,pandas.Series.__iter__,"pandas.Series.__iter__#Series.__iter__()[source]#Return an iterator of the values.These are each a scalar type, which is a Python scalar
(for str, int, float) or a pandas scalar
(for Timestamp/Timedelta/Interval/Period)Returns:iteratorExamples>>>s=pd.Series([1,2,3])>>>forxins:...print(x)123"
Pandas,Series,pandas.Series.items,"pandas.Series.items#Series.items()[source]#Lazily iterate over (index, value) tuples.This method returns an iterable tuple (index, value). This is
convenient if you want to create a lazy iterator.Returns:iterableIterable of tuples containing the (index, value) pairs from a
Series.See alsoDataFrame.itemsIterate over (column name, Series) pairs.DataFrame.iterrowsIterate over DataFrame rows as (index, Series) pairs.Examples>>>s=pd.Series(['A','B','C'])>>>forindex,valueins.items():...print(f""Index :{index}, Value :{value}"")Index : 0, Value : AIndex : 1, Value : BIndex : 2, Value : C"
Pandas,Series,pandas.Series.keys,"pandas.Series.keys#Series.keys()[source]#Return alias for index.Returns:IndexIndex of the Series.Examples>>>s=pd.Series([1,2,3],index=[0,1,2])>>>s.keys()Index([0, 1, 2], dtype='int64')"
Pandas,Series,pandas.Series.pop,"pandas.Series.pop#Series.pop(item)[source]#Return item and drops from series. Raise KeyError if not found.Parameters:itemlabelIndex of the element that needs to be removed.Returns:Value that is popped from series.Examples>>>ser=pd.Series([1,2,3])>>>ser.pop(0)1>>>ser1    22    3dtype: int64"
Pandas,Series,pandas.Series.item,"pandas.Series.item#Series.item()[source]#Return the first element of the underlying data as a Python scalar.Returns:scalarThe first element of Series or Index.Raises:ValueErrorIf the data is not length = 1.Examples>>>s=pd.Series([1])>>>s.item()1For an index:>>>s=pd.Series([1],index=['a'])>>>s.index.item()'a'"
Pandas,Series,pandas.Series.xs,"pandas.Series.xs#Series.xs(key,axis=0,level=None,drop_level=True)[source]#Return cross-section from the Series/DataFrame.This method takes akeyargument to select data at a particular
level of a MultiIndex.Parameters:keylabel or tuple of labelLabel contained in the index, or partially in a MultiIndex.axis{0 or eindexf, 1 or ecolumnsf}, default 0Axis to retrieve cross-section on.levelobject, defaults to first n levels (n=1 or len(key))In case of a key partially contained in a MultiIndex, indicate
which levels are used. Levels can be referred by label or position.drop_levelbool, default TrueIf False, returns object with same levels as self.Returns:Series or DataFrameCross-section from the original Series or DataFrame
corresponding to the selected index levels.See alsoDataFrame.locAccess a group of rows and columns by label(s) or a boolean array.DataFrame.ilocPurely integer-location based indexing for selection by position.Notesxscan not be used to set values.MultiIndex Slicers is a generic way to get/set values on
any level or levels.
It is a superset ofxsfunctionality, seeMultiIndex Slicers.Examples>>>d={'num_legs':[4,4,2,2],...'num_wings':[0,0,2,2],...'class':['mammal','mammal','mammal','bird'],...'animal':['cat','dog','bat','penguin'],...'locomotion':['walks','walks','flies','walks']}>>>df=pd.DataFrame(data=d)>>>df=df.set_index(['class','animal','locomotion'])>>>dfnum_legs  num_wingsclass  animal  locomotionmammal cat     walks              4          0dog     walks              4          0bat     flies              2          2bird   penguin walks              2          2Get values at specified index>>>df.xs('mammal')num_legs  num_wingsanimal locomotioncat    walks              4          0dog    walks              4          0bat    flies              2          2Get values at several indexes>>>df.xs(('mammal','dog','walks'))num_legs     4num_wings    0Name: (mammal, dog, walks), dtype: int64Get values at specified index and level>>>df.xs('cat',level=1)num_legs  num_wingsclass  locomotionmammal walks              4          0Get values at several indexes and levels>>>df.xs(('bird','walks'),...level=[0,'locomotion'])num_legs  num_wingsanimalpenguin         2          2Get values at specified column and axis>>>df.xs('num_wings',axis=1)class   animal   locomotionmammal  cat      walks         0dog      walks         0bat      flies         2bird    penguin  walks         2Name: num_wings, dtype: int64"
Pandas,Series,pandas.Series.add,"pandas.Series.add#Series.add(other,level=None,fill_value=None,axis=0)[source]#Return Addition of series and other, element-wise (binary operatoradd).Equivalent toseries+other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.raddReverse of the Addition operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.add(b,fill_value=0)a    2.0b    1.0c    1.0d    1.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.sub,"pandas.Series.sub#Series.sub(other,level=None,fill_value=None,axis=0)[source]#Return Subtraction of series and other, element-wise (binary operatorsub).Equivalent toseries-other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rsubReverse of the Subtraction operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.subtract(b,fill_value=0)a    0.0b    1.0c    1.0d   -1.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.mul,"pandas.Series.mul#Series.mul(other,level=None,fill_value=None,axis=0)[source]#Return Multiplication of series and other, element-wise (binary operatormul).Equivalent toseries*other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rmulReverse of the Multiplication operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.multiply(b,fill_value=0)a    1.0b    0.0c    0.0d    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.div,"pandas.Series.div#Series.div(other,level=None,fill_value=None,axis=0)[source]#Return Floating division of series and other, element-wise (binary operatortruediv).Equivalent toseries/other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rtruedivReverse of the Floating division operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.divide(b,fill_value=0)a    1.0b    infc    infd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.truediv,"pandas.Series.truediv#Series.truediv(other,level=None,fill_value=None,axis=0)[source]#Return Floating division of series and other, element-wise (binary operatortruediv).Equivalent toseries/other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rtruedivReverse of the Floating division operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.divide(b,fill_value=0)a    1.0b    infc    infd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.floordiv,"pandas.Series.floordiv#Series.floordiv(other,level=None,fill_value=None,axis=0)[source]#Return Integer division of series and other, element-wise (binary operatorfloordiv).Equivalent toseries//other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rfloordivReverse of the Integer division operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.floordiv(b,fill_value=0)a    1.0b    infc    infd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.mod,"pandas.Series.mod#Series.mod(other,level=None,fill_value=None,axis=0)[source]#Return Modulo of series and other, element-wise (binary operatormod).Equivalent toseries%other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rmodReverse of the Modulo operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.mod(b,fill_value=0)a    0.0b    NaNc    NaNd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.pow,"pandas.Series.pow#Series.pow(other,level=None,fill_value=None,axis=0)[source]#Return Exponential power of series and other, element-wise (binary operatorpow).Equivalent toseries**other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.rpowReverse of the Exponential power operator, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.pow(b,fill_value=0)a    1.0b    1.0c    1.0d    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.radd,"pandas.Series.radd#Series.radd(other,level=None,fill_value=None,axis=0)[source]#Return Addition of series and other, element-wise (binary operatorradd).Equivalent toother+series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.addElement-wise Addition, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.add(b,fill_value=0)a    2.0b    1.0c    1.0d    1.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rsub,"pandas.Series.rsub#Series.rsub(other,level=None,fill_value=None,axis=0)[source]#Return Subtraction of series and other, element-wise (binary operatorrsub).Equivalent toother-series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.subElement-wise Subtraction, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.subtract(b,fill_value=0)a    0.0b    1.0c    1.0d   -1.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rmul,"pandas.Series.rmul#Series.rmul(other,level=None,fill_value=None,axis=0)[source]#Return Multiplication of series and other, element-wise (binary operatorrmul).Equivalent toother*series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.mulElement-wise Multiplication, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.multiply(b,fill_value=0)a    1.0b    0.0c    0.0d    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rdiv,"pandas.Series.rdiv#Series.rdiv(other,level=None,fill_value=None,axis=0)[source]#Return Floating division of series and other, element-wise (binary operatorrtruediv).Equivalent toother/series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.truedivElement-wise Floating division, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.divide(b,fill_value=0)a    1.0b    infc    infd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rtruediv,"pandas.Series.rtruediv#Series.rtruediv(other,level=None,fill_value=None,axis=0)[source]#Return Floating division of series and other, element-wise (binary operatorrtruediv).Equivalent toother/series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.truedivElement-wise Floating division, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.divide(b,fill_value=0)a    1.0b    infc    infd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rfloordiv,"pandas.Series.rfloordiv#Series.rfloordiv(other,level=None,fill_value=None,axis=0)[source]#Return Integer division of series and other, element-wise (binary operatorrfloordiv).Equivalent toother//series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.floordivElement-wise Integer division, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.floordiv(b,fill_value=0)a    1.0b    infc    infd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rmod,"pandas.Series.rmod#Series.rmod(other,level=None,fill_value=None,axis=0)[source]#Return Modulo of series and other, element-wise (binary operatorrmod).Equivalent toother%series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.modElement-wise Modulo, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.mod(b,fill_value=0)a    0.0b    NaNc    NaNd    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.rpow,"pandas.Series.rpow#Series.rpow(other,level=None,fill_value=None,axis=0)[source]#Return Exponential power of series and other, element-wise (binary operatorrpow).Equivalent toother**series, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.See alsoSeries.powElement-wise Exponential power, seePython documentationfor more details.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.pow(b,fill_value=0)a    1.0b    1.0c    1.0d    0.0e    NaNdtype: float64"
Pandas,Series,pandas.Series.combine,"pandas.Series.combine#Series.combine(other,func,fill_value=None)[source]#Combine the Series with a Series or scalar according tofunc.Combine the Series andotherusingfuncto perform elementwise
selection for combined Series.fill_valueis assumed when value is missing at some index
from one of the two objects being combined.Parameters:otherSeries or scalarThe value(s) to be combined with theSeries.funcfunctionFunction that takes two scalars as inputs and returns an element.fill_valuescalar, optionalThe value to assume when an index is missing from
one Series or the other. The default specifies to use the
appropriate NaN value for the underlying dtype of the Series.Returns:SeriesThe result of combining the Series with the other object.See alsoSeries.combine_firstCombine Series values, choosing the calling Seriesf values first.ExamplesConsider 2 Datasetss1ands2containing
highest clocked speeds of different birds.>>>s1=pd.Series({'falcon':330.0,'eagle':160.0})>>>s1falcon    330.0eagle     160.0dtype: float64>>>s2=pd.Series({'falcon':345.0,'eagle':200.0,'duck':30.0})>>>s2falcon    345.0eagle     200.0duck       30.0dtype: float64Now, to combine the two datasets and view the highest speeds
of the birds across the two datasets>>>s1.combine(s2,max)duck        NaNeagle     200.0falcon    345.0dtype: float64In the previous example, the resulting value for duck is missing,
because the maximum of a NaN and a float is a NaN.
So, in the example, we setfill_value=0,
so the maximum value returned will be the value from some dataset.>>>s1.combine(s2,max,fill_value=0)duck       30.0eagle     200.0falcon    345.0dtype: float64"
Pandas,Series,pandas.Series.combine_first,"pandas.Series.combine_first#Series.combine_first(other)[source]#Update null elements with value in the same location in eotherf.Combine two Series objects by filling null values in one Series with
non-null values from the other Series. Result index will be the union
of the two indexes.Parameters:otherSeriesThe value(s) to be used for filling null values.Returns:SeriesThe result of combining the provided Series with the other object.See alsoSeries.combinePerform element-wise operation on two Series using a given function.Examples>>>s1=pd.Series([1,np.nan])>>>s2=pd.Series([3,4,5])>>>s1.combine_first(s2)0    1.01    4.02    5.0dtype: float64Null values still persist if the location of that null value
does not exist inother>>>s1=pd.Series({'falcon':np.nan,'eagle':160.0})>>>s2=pd.Series({'eagle':200.0,'duck':30.0})>>>s1.combine_first(s2)duck       30.0eagle     160.0falcon      NaNdtype: float64"
Pandas,Series,pandas.Series.round,"pandas.Series.round#Series.round(decimals=0,*args,**kwargs)[source]#Round each value in a Series to the given number of decimals.Parameters:decimalsint, default 0Number of decimal places to round to. If decimals is negative,
it specifies the number of positions to the left of the decimal point.*args, **kwargsAdditional arguments and keywords have no effect but might be
accepted for compatibility with NumPy.Returns:SeriesRounded values of the Series.See alsonumpy.aroundRound values of an np.array.DataFrame.roundRound values of a DataFrame.Examples>>>s=pd.Series([0.1,1.3,2.7])>>>s.round()0    0.01    1.02    3.0dtype: float64"
Pandas,Series,pandas.Series.lt,"pandas.Series.lt#Series.lt(other,level=None,fill_value=None,axis=0)[source]#Return Less than of series and other, element-wise (binary operatorlt).Equivalent toseries<other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.Examples>>>a=pd.Series([1,1,1,np.nan,1],index=['a','b','c','d','e'])>>>aa    1.0b    1.0c    1.0d    NaNe    1.0dtype: float64>>>b=pd.Series([0,1,2,np.nan,1],index=['a','b','c','d','f'])>>>ba    0.0b    1.0c    2.0d    NaNf    1.0dtype: float64>>>a.lt(b,fill_value=0)a    Falseb    Falsec     Trued    Falsee    Falsef     Truedtype: bool"
Pandas,Series,pandas.Series.gt,"pandas.Series.gt#Series.gt(other,level=None,fill_value=None,axis=0)[source]#Return Greater than of series and other, element-wise (binary operatorgt).Equivalent toseries>other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.Examples>>>a=pd.Series([1,1,1,np.nan,1],index=['a','b','c','d','e'])>>>aa    1.0b    1.0c    1.0d    NaNe    1.0dtype: float64>>>b=pd.Series([0,1,2,np.nan,1],index=['a','b','c','d','f'])>>>ba    0.0b    1.0c    2.0d    NaNf    1.0dtype: float64>>>a.gt(b,fill_value=0)a     Trueb    Falsec    Falsed    Falsee     Truef    Falsedtype: bool"
Pandas,Series,pandas.Series.le,"pandas.Series.le#Series.le(other,level=None,fill_value=None,axis=0)[source]#Return Less than or equal to of series and other, element-wise (binary operatorle).Equivalent toseries<=other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.Examples>>>a=pd.Series([1,1,1,np.nan,1],index=['a','b','c','d','e'])>>>aa    1.0b    1.0c    1.0d    NaNe    1.0dtype: float64>>>b=pd.Series([0,1,2,np.nan,1],index=['a','b','c','d','f'])>>>ba    0.0b    1.0c    2.0d    NaNf    1.0dtype: float64>>>a.le(b,fill_value=0)a    Falseb     Truec     Trued    Falsee    Falsef     Truedtype: bool"
Pandas,Series,pandas.Series.ge,"pandas.Series.ge#Series.ge(other,level=None,fill_value=None,axis=0)[source]#Return Greater than or equal to of series and other, element-wise (binary operatorge).Equivalent toseries>=other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.Examples>>>a=pd.Series([1,1,1,np.nan,1],index=['a','b','c','d','e'])>>>aa    1.0b    1.0c    1.0d    NaNe    1.0dtype: float64>>>b=pd.Series([0,1,2,np.nan,1],index=['a','b','c','d','f'])>>>ba    0.0b    1.0c    2.0d    NaNf    1.0dtype: float64>>>a.ge(b,fill_value=0)a     Trueb     Truec    Falsed    Falsee     Truef    Falsedtype: bool"
Pandas,Series,pandas.Series.ne,"pandas.Series.ne#Series.ne(other,level=None,fill_value=None,axis=0)[source]#Return Not equal to of series and other, element-wise (binary operatorne).Equivalent toseries!=other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.ne(b,fill_value=0)a    Falseb     Truec     Trued     Truee     Truedtype: bool"
Pandas,Series,pandas.Series.eq,"pandas.Series.eq#Series.eq(other,level=None,fill_value=None,axis=0)[source]#Return Equal to of series and other, element-wise (binary operatoreq).Equivalent toseries==other, but with support to substitute a fill_value for
missing data in either one of the inputs.Parameters:otherSeries or scalar valuelevelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for
successful Series alignment, with this value before computation.
If data in both corresponding Series locations is missing
the result of filling (at that location) will be missing.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.Returns:SeriesThe result of the operation.Examples>>>a=pd.Series([1,1,1,np.nan],index=['a','b','c','d'])>>>aa    1.0b    1.0c    1.0d    NaNdtype: float64>>>b=pd.Series([1,np.nan,1,np.nan],index=['a','b','d','e'])>>>ba    1.0b    NaNd    1.0e    NaNdtype: float64>>>a.eq(b,fill_value=0)a     Trueb    Falsec    Falsed    Falsee    Falsedtype: bool"
Pandas,Series,pandas.Series.product,"pandas.Series.product#Series.product(axis=None,skipna=True,numeric_only=False,min_count=0,**kwargs)[source]#Return the product of the values over the requested axis.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.min_countint, default 0The required number of valid values to perform the operation. If fewer thanmin_countnon-NA values are present the result will be NA.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.ExamplesBy default, the product of an empty or all-NA Series is1>>>pd.Series([],dtype=""float64"").prod()1.0This can be controlled with themin_countparameter>>>pd.Series([],dtype=""float64"").prod(min_count=1)nanThanks to theskipnaparameter,min_counthandles all-NA and
empty series identically.>>>pd.Series([np.nan]).prod()1.0>>>pd.Series([np.nan]).prod(min_count=1)nan"
Pandas,Series,pandas.Series.dot,"pandas.Series.dot#Series.dot(other)[source]#Compute the dot product between the Series and the columns of other.This method computes the dot product between the Series and another
one, or the Series and each columns of a DataFrame, or the Series and
each columns of an array.It can also be called usingself @ other.Parameters:otherSeries, DataFrame or array-likeThe other object to compute the dot product with its columns.Returns:scalar, Series or numpy.ndarrayReturn the dot product of the Series and other if other is a
Series, the Series of the dot product of Series and each rows of
other if other is a DataFrame or a numpy.ndarray between the Series
and each columns of the numpy array.See alsoDataFrame.dotCompute the matrix product with the DataFrame.Series.mulMultiplication of series and other, element-wise.NotesThe Series and other has to share the same index if other is a Series
or a DataFrame.Examples>>>s=pd.Series([0,1,2,3])>>>other=pd.Series([-1,2,-3,4])>>>s.dot(other)8>>>s@other8>>>df=pd.DataFrame([[0,1],[-2,3],[4,-5],[6,7]])>>>s.dot(df)0    241    14dtype: int64>>>arr=np.array([[0,1],[-2,3],[4,-5],[6,7]])>>>s.dot(arr)array([24, 14])"
Pandas,Series,pandas.Series.apply,"pandas.Series.apply#Series.apply(func,convert_dtype=_NoDefault.no_default,args=(),*,by_row='compat',**kwargs)[source]#Invoke function on values of Series.Can be ufunc (a NumPy function that applies to the entire Series)
or a Python function that only works on single values.Parameters:funcfunctionPython function or NumPy ufunc to apply.convert_dtypebool, default TrueTry to find better dtype for elementwise function results. If
False, leave as dtype=object. Note that the dtype is always
preserved for some extension array dtypes, such as Categorical.Deprecated since version 2.1.0:convert_dtypehas been deprecated. Doser.astype(object).apply()instead if you wantconvert_dtype=False.argstuplePositional arguments passed to func after the series value.by_rowFalse or gcompath, default gcompathIf""compat""and func is a callable, func will be passed each element of
the Series, likeSeries.map. If func is a list or dict of
callables, will first try to translate each func into pandas methods. If
that doesnft work, will try call to apply again withby_row=""compat""and if that fails, will call apply again withby_row=False(backward compatible).
If False, the func will be passed the whole Series at once.by_rowhas no effect whenfuncis a string.New in version 2.1.0.**kwargsAdditional keyword arguments passed to func.Returns:Series or DataFrameIf func returns a Series object the result will be a DataFrame.See alsoSeries.mapFor element-wise operations.Series.aggOnly perform aggregating type operations.Series.transformOnly perform transforming type operations.NotesFunctions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.ExamplesCreate a series with typical summer temperatures for each city.>>>s=pd.Series([20,21,12],...index=['London','New York','Helsinki'])>>>sLondon      20New York    21Helsinki    12dtype: int64Square the values by defining a function and passing it as an
argument toapply().>>>defsquare(x):...returnx**2>>>s.apply(square)London      400New York    441Helsinki    144dtype: int64Square the values by passing an anonymous function as an
argument toapply().>>>s.apply(lambdax:x**2)London      400New York    441Helsinki    144dtype: int64Define a custom function that needs additional positional
arguments and pass these additional arguments using theargskeyword.>>>defsubtract_custom_value(x,custom_value):...returnx-custom_value>>>s.apply(subtract_custom_value,args=(5,))London      15New York    16Helsinki     7dtype: int64Define a custom function that takes keyword arguments
and pass these arguments toapply.>>>defadd_custom_values(x,**kwargs):...formonthinkwargs:...x+=kwargs[month]...returnx>>>s.apply(add_custom_values,june=30,july=20,august=25)London      95New York    96Helsinki    87dtype: int64Use a function from the Numpy library.>>>s.apply(np.log)London      2.995732New York    3.044522Helsinki    2.484907dtype: float64"
Pandas,Series,pandas.Series.agg,"pandas.Series.agg#Series.agg(func=None,axis=0,*args,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either
work when passed a Series or when passed to Series.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.*argsPositional arguments to pass tofunc.**kwargsKeyword arguments to pass tofunc.Returns:scalar, Series or DataFrameThe return can be:scalar : when Series.agg is called with single functionSeries : when DataFrame.agg is called with a single functionDataFrame : when DataFrame.agg is called with several functionsReturn scalar, Series or DataFrame.See alsoSeries.applyInvoke function on a Series.Series.transformTransform function producing a Series with like indexes.NotesThe aggregation operations are always performed over an axis, either the
index (default) or the column axis. This behavior is different fromnumpyaggregation functions (mean,median,prod,sum,std,var), where the default is to compute the aggregation of the flattened
array, e.g.,numpy.mean(arr_2d)as opposed tonumpy.mean(arr_2d,axis=0).aggis an alias foraggregate. Use the alias.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.A passed user-defined-function will be passed a Series for evaluation.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.agg('min')1>>>s.agg(['min','max'])min   1max   4dtype: int64"
Pandas,Series,pandas.Series.aggregate,"pandas.Series.aggregate#Series.aggregate(func=None,axis=0,*args,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either
work when passed a Series or when passed to Series.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.*argsPositional arguments to pass tofunc.**kwargsKeyword arguments to pass tofunc.Returns:scalar, Series or DataFrameThe return can be:scalar : when Series.agg is called with single functionSeries : when DataFrame.agg is called with a single functionDataFrame : when DataFrame.agg is called with several functionsReturn scalar, Series or DataFrame.See alsoSeries.applyInvoke function on a Series.Series.transformTransform function producing a Series with like indexes.NotesThe aggregation operations are always performed over an axis, either the
index (default) or the column axis. This behavior is different fromnumpyaggregation functions (mean,median,prod,sum,std,var), where the default is to compute the aggregation of the flattened
array, e.g.,numpy.mean(arr_2d)as opposed tonumpy.mean(arr_2d,axis=0).aggis an alias foraggregate. Use the alias.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.A passed user-defined-function will be passed a Series for evaluation.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.agg('min')1>>>s.agg(['min','max'])min   1max   4dtype: int64"
Pandas,Series,pandas.Series.transform,"pandas.Series.transform#Series.transform(func,axis=0,*args,**kwargs)[source]#Callfuncon self producing a Series with the same axis shape as self.Parameters:funcfunction, str, list-like or dict-likeFunction to use for transforming the data. If a function, must either
work when passed a Series or when passed to Series.apply. If func
is both list-like and dict-like, dict-like behavior takes precedence.Accepted combinations are:functionstring function namelist-like of functions and/or function names, e.g.[np.exp,'sqrt']dict-like of axis labels -> functions, function names or list-like of such.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.*argsPositional arguments to pass tofunc.**kwargsKeyword arguments to pass tofunc.Returns:SeriesA Series that must have the same length as self.Raises:ValueErrorIf the returned Series has a different length than self.See alsoSeries.aggOnly perform aggregating type operations.Series.applyInvoke function on a Series.NotesFunctions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':range(3),'B':range(1,4)})>>>dfA  B0  0  11  1  22  2  3>>>df.transform(lambdax:x+1)A  B0  1  21  2  32  3  4Even though the resulting Series must have the same length as the
input Series, it is possible to provide several input functions:>>>s=pd.Series(range(3))>>>s0    01    12    2dtype: int64>>>s.transform([np.sqrt,np.exp])sqrt        exp0  0.000000   1.0000001  1.000000   2.7182822  1.414214   7.389056You can call transform on a GroupBy object:>>>df=pd.DataFrame({...""Date"":[...""2015-05-08"",""2015-05-07"",""2015-05-06"",""2015-05-05"",...""2015-05-08"",""2015-05-07"",""2015-05-06"",""2015-05-05""],...""Data"":[5,8,6,1,50,100,60,120],...})>>>dfDate  Data0  2015-05-08     51  2015-05-07     82  2015-05-06     63  2015-05-05     14  2015-05-08    505  2015-05-07   1006  2015-05-06    607  2015-05-05   120>>>df.groupby('Date')['Data'].transform('sum')0     551    1082     663    1214     555    1086     667    121Name: Data, dtype: int64>>>df=pd.DataFrame({...""c"":[1,1,1,2,2,2,2],...""type"":[""m"",""n"",""o"",""m"",""m"",""n"",""n""]...})>>>dfc type0  1    m1  1    n2  1    o3  2    m4  2    m5  2    n6  2    n>>>df['size']=df.groupby('c')['type'].transform(len)>>>dfc type size0  1    m    31  1    n    32  1    o    33  2    m    44  2    m    45  2    n    46  2    n    4"
Pandas,Series,pandas.Series.map,"pandas.Series.map#Series.map(arg,na_action=None)[source]#Map values of Series according to an input mapping or function.Used for substituting each value in a Series with another value,
that may be derived from a function, adictor
aSeries.Parameters:argfunction, collections.abc.Mapping subclass or SeriesMapping correspondence.na_action{None, eignoref}, default NoneIf eignoref, propagate NaN values, without passing them to the
mapping correspondence.Returns:SeriesSame index as caller.See alsoSeries.applyFor applying more complex functions on a Series.Series.replaceReplace values given into_replacewithvalue.DataFrame.applyApply a function row-/column-wise.DataFrame.mapApply a function elementwise on a whole DataFrame.NotesWhenargis a dictionary, values in Series that are not in the
dictionary (as keys) are converted toNaN. However, if the
dictionary is adictsubclass that defines__missing__(i.e.
provides a method for default values), then this default is used
rather thanNaN.Examples>>>s=pd.Series(['cat','dog',np.nan,'rabbit'])>>>s0      cat1      dog2      NaN3   rabbitdtype: objectmapaccepts adictor aSeries. Values that are not found
in thedictare converted toNaN, unless the dict has a default
value (e.g.defaultdict):>>>s.map({'cat':'kitten','dog':'puppy'})0   kitten1    puppy2      NaN3      NaNdtype: objectIt also accepts a function:>>>s.map('I am a{}'.format)0       I am a cat1       I am a dog2       I am a nan3    I am a rabbitdtype: objectTo avoid applying the function to missing values (and keep them asNaN)na_action='ignore'can be used:>>>s.map('I am a{}'.format,na_action='ignore')0     I am a cat1     I am a dog2            NaN3  I am a rabbitdtype: object"
Pandas,Series,pandas.Series.groupby,"pandas.Series.groupby#Series.groupby(by=None,axis=0,level=None,as_index=True,sort=True,group_keys=True,observed=_NoDefault.no_default,dropna=True)[source]#Group Series using a mapper or by a Series of columns.A groupby operation involves some combination of splitting the
object, applying a function, and combining the results. This can be
used to group large amounts of data and compute operations on these
groups.Parameters:bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby.
Ifbyis a function, itfs called on each value of the objectfs
index. If a dict or Series is passed, the Series or dict VALUES
will be used to determine the groups (the Seriesf values are first
aligned; see.align()method). If a list or ndarray of length
equal to the selected axis is passed (see thegroupby user guide),
the values are used as-is to determine the groups. A label or list
of labels may be passed to group by the columns inself.
Notice that a tuple is interpreted as a (single) key.axis{0 or eindexf, 1 or ecolumnsf}, default 0Split along rows (0) or columns (1). ForSeriesthis parameter
is unused and defaults to 0.Deprecated since version 2.1.0:Will be removed and behave like axis=0 in a future version.
Foraxis=1, doframe.T.groupby(...)instead.levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular
level or levels. Do not specify bothbyandlevel.as_indexbool, default TrueReturn object with group labels as the
index. Only relevant for DataFrame input. as_index=False is
effectively gSQL-styleh grouped output. This argument has no effect
on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).sortbool, default TrueSort group keys. Get better performance by turning this off.
Note this does not influence the order of observations within each
group. Groupby preserves the order of rows within each group. If False,
the groups will appear in the same order as they did in the original DataFrame.
This argument has no effect on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).Changed in version 2.0.0:Specifyingsort=Falsewith an ordered categorical grouper will no
longer sort the values.group_keysbool, default TrueWhen calling apply and thebyargument produces a like-indexed
(i.e.a transform) result, add group keys to
index to identify pieces. By default group keys are not included
when the resultfs index (and column) labels match the inputs, and
are included otherwise.Changed in version 1.5.0:Warns thatgroup_keyswill no longer be ignored when the
result fromapplyis a like-indexed Series or DataFrame.
Specifygroup_keysexplicitly to include the group keys or
not.Changed in version 2.0.0:group_keysnow defaults toTrue.observedbool, default FalseThis only applies if any of the groupers are Categoricals.
If True: only show observed values for categorical groupers.
If False: show all values for categorical groupers.Deprecated since version 2.1.0:The default value will change to True in a future version of pandas.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together
with row/column will be dropped.
If False, NA values will also be treated as the key in groups.Returns:pandas.api.typing.SeriesGroupByReturns a groupby object that contains information about the groups.See alsoresampleConvenience method for frequency conversion and resampling of time series.NotesSee theuser guidefor more
detailed usage and examples, including splitting an object into groups,
iterating through groups, selecting a group, aggregation, and more.Examples>>>ser=pd.Series([390.,350.,30.,20.],...index=['Falcon','Falcon','Parrot','Parrot'],...name=""Max Speed"")>>>serFalcon    390.0Falcon    350.0Parrot     30.0Parrot     20.0Name: Max Speed, dtype: float64>>>ser.groupby([""a"",""b"",""a"",""b""]).mean()a    210.0b    185.0Name: Max Speed, dtype: float64>>>ser.groupby(level=0).mean()Falcon    370.0Parrot     25.0Name: Max Speed, dtype: float64>>>ser.groupby(ser>100).mean()Max SpeedFalse     25.0True     370.0Name: Max Speed, dtype: float64Grouping by IndexesWe can groupby different levels of a hierarchical index
using thelevelparameter:>>>arrays=[['Falcon','Falcon','Parrot','Parrot'],...['Captive','Wild','Captive','Wild']]>>>index=pd.MultiIndex.from_arrays(arrays,names=('Animal','Type'))>>>ser=pd.Series([390.,350.,30.,20.],index=index,name=""Max Speed"")>>>serAnimal  TypeFalcon  Captive    390.0Wild       350.0Parrot  Captive     30.0Wild        20.0Name: Max Speed, dtype: float64>>>ser.groupby(level=0).mean()AnimalFalcon    370.0Parrot     25.0Name: Max Speed, dtype: float64>>>ser.groupby(level=""Type"").mean()TypeCaptive    210.0Wild       185.0Name: Max Speed, dtype: float64We can also choose to includeNAin group keys or not by definingdropnaparameter, the default setting isTrue.>>>ser=pd.Series([1,2,3,3],index=[""a"",'a','b',np.nan])>>>ser.groupby(level=0).sum()a    3b    3dtype: int64>>>ser.groupby(level=0,dropna=False).sum()a    3b    3NaN  3dtype: int64>>>arrays=['Falcon','Falcon','Parrot','Parrot']>>>ser=pd.Series([390.,350.,30.,20.],index=arrays,name=""Max Speed"")>>>ser.groupby([""a"",""b"",""a"",np.nan]).mean()a    210.0b    350.0Name: Max Speed, dtype: float64>>>ser.groupby([""a"",""b"",""a"",np.nan],dropna=False).mean()a    210.0b    350.0NaN   20.0Name: Max Speed, dtype: float64"
Pandas,Series,pandas.Series.rolling,"pandas.Series.rolling#Series.rolling(window,min_periods=None,center=False,win_type=None,on=None,axis=_NoDefault.no_default,closed=None,step=None,method='single')[source]#Provide rolling window calculations.Parameters:windowint, timedelta, str, offset, or BaseIndexer subclassSize of the moving window.If an integer, the fixed number of observations used for
each window.If a timedelta, str, or offset, the time period of each window. Each
window will be a variable sized based on the observations included in
the time-period. This is only valid for datetimelike indexes.
To learn more about the offsets & frequency strings, please seethis link.If a BaseIndexer subclass, the window boundaries
based on the definedget_window_boundsmethod. Additional rolling
keyword arguments, namelymin_periods,center,closedandstepwill be passed toget_window_bounds.min_periodsint, default NoneMinimum number of observations in window required to have a value;
otherwise, result isnp.nan.For a window that is specified by an offset,min_periodswill default to 1.For a window that is specified by an integer,min_periodswill default
to the size of the window.centerbool, default FalseIf False, set the window labels as the right edge of the window index.If True, set the window labels as the center of the window index.win_typestr, default NoneIfNone, all points are evenly weighted.If a string, it must be a validscipy.signal window function.Certain Scipy window types require additional parameters to be passed
in the aggregation function. The additional parameters must match
the keywords specified in the Scipy window type method signature.onstr, optionalFor a DataFrame, a column label or Index level on which
to calculate the rolling window, rather than the DataFramefs index.Provided integer column is ignored and excluded from result since
an integer index is not used to calculate the rolling window.axisint or str, default 0If0or'index', roll across the rows.If1or'columns', roll across the columns.ForSeriesthis parameter is unused and defaults to 0.closedstr, default NoneIf'right', the first point in the window is excluded from calculations.If'left', the last point in the window is excluded from calculations.If'both', the no points in the window are excluded from calculations.If'neither', the first and last points in the window are excluded
from calculations.DefaultNone('right').Changed in version 1.2.0:The closed parameter with fixed windows is now supported.stepint, default NoneNew in version 1.5.0.Evaluate the window at everystepresult, equivalent to slicing as[::step].windowmust be an integer. Using a step argument other
than None or 1 will produce a result with a different shape than the input.methodstr {esinglef, etablef}, default esinglefNew in version 1.3.0.Execute the rolling operation per single column or row ('single')
or over the entire object ('table').This argument is only implemented when specifyingengine='numba'in the method call.Returns:pandas.api.typing.Window or pandas.api.typing.RollingAn instance of Window is returned ifwin_typeis passed. Otherwise,
an instance of Rolling is returned.See alsoexpandingProvides expanding transformations.ewmProvides exponential weighted functions.NotesSeeWindowing Operationsfor further usage details
and examples.Examples>>>df=pd.DataFrame({'B':[0,1,2,np.nan,4]})>>>dfB0  0.01  1.02  2.03  NaN4  4.0windowRolling sum with a window length of 2 observations.>>>df.rolling(2).sum()B0  NaN1  1.02  3.03  NaN4  NaNRolling sum with a window span of 2 seconds.>>>df_time=pd.DataFrame({'B':[0,1,2,np.nan,4]},...index=[pd.Timestamp('20130101 09:00:00'),...pd.Timestamp('20130101 09:00:02'),...pd.Timestamp('20130101 09:00:03'),...pd.Timestamp('20130101 09:00:05'),...pd.Timestamp('20130101 09:00:06')])>>>df_timeB2013-01-01 09:00:00  0.02013-01-01 09:00:02  1.02013-01-01 09:00:03  2.02013-01-01 09:00:05  NaN2013-01-01 09:00:06  4.0>>>df_time.rolling('2s').sum()B2013-01-01 09:00:00  0.02013-01-01 09:00:02  1.02013-01-01 09:00:03  3.02013-01-01 09:00:05  NaN2013-01-01 09:00:06  4.0Rolling sum with forward looking windows with 2 observations.>>>indexer=pd.api.indexers.FixedForwardWindowIndexer(window_size=2)>>>df.rolling(window=indexer,min_periods=1).sum()B0  1.01  3.02  2.03  4.04  4.0min_periodsRolling sum with a window length of 2 observations, but only needs a minimum of 1
observation to calculate a value.>>>df.rolling(2,min_periods=1).sum()B0  0.01  1.02  3.03  2.04  4.0centerRolling sum with the result assigned to the center of the window index.>>>df.rolling(3,min_periods=1,center=True).sum()B0  1.01  3.02  3.03  6.04  4.0>>>df.rolling(3,min_periods=1,center=False).sum()B0  0.01  1.02  3.03  3.04  6.0stepRolling sum with a window length of 2 observations, minimum of 1 observation to
calculate a value, and a step of 2.>>>df.rolling(2,min_periods=1,step=2).sum()B0  0.02  3.04  4.0win_typeRolling sum with a window length of 2, using the Scipy'gaussian'window type.stdis required in the aggregation function.>>>df.rolling(2,win_type='gaussian').sum(std=3)B0       NaN1  0.9862072  2.9586213       NaN4       NaNonRolling sum with a window length of 2 days.>>>df=pd.DataFrame({...'A':[pd.to_datetime('2020-01-01'),...pd.to_datetime('2020-01-01'),...pd.to_datetime('2020-01-02'),],...'B':[1,2,3],},...index=pd.date_range('2020',periods=3))>>>dfA  B2020-01-01 2020-01-01  12020-01-02 2020-01-01  22020-01-03 2020-01-02  3>>>df.rolling('2D',on='A').sum()A    B2020-01-01 2020-01-01  1.02020-01-02 2020-01-01  3.02020-01-03 2020-01-02  6.0"
Pandas,Series,pandas.Series.expanding,"pandas.Series.expanding#Series.expanding(min_periods=1,axis=_NoDefault.no_default,method='single')[source]#Provide expanding window calculations.Parameters:min_periodsint, default 1Minimum number of observations in window required to have a value;
otherwise, result isnp.nan.axisint or str, default 0If0or'index', roll across the rows.If1or'columns', roll across the columns.ForSeriesthis parameter is unused and defaults to 0.methodstr {esinglef, etablef}, default esinglefExecute the rolling operation per single column or row ('single')
or over the entire object ('table').This argument is only implemented when specifyingengine='numba'in the method call.New in version 1.3.0.Returns:pandas.api.typing.ExpandingSee alsorollingProvides rolling window calculations.ewmProvides exponential weighted functions.NotesSeeWindowing Operationsfor further usage details
and examples.Examples>>>df=pd.DataFrame({""B"":[0,1,2,np.nan,4]})>>>dfB0  0.01  1.02  2.03  NaN4  4.0min_periodsExpanding sum with 1 vs 3 observations needed to calculate a value.>>>df.expanding(1).sum()B0  0.01  1.02  3.03  3.04  7.0>>>df.expanding(3).sum()B0  NaN1  NaN2  3.03  3.04  7.0"
Pandas,Series,pandas.Series.ewm,"pandas.Series.ewm#Series.ewm(com=None,span=None,halflife=None,alpha=None,min_periods=0,adjust=True,ignore_na=False,axis=_NoDefault.no_default,times=None,method='single')[source]#Provide exponentially weighted (EW) calculations.Exactly one ofcom,span,halflife, oralphamust be
provided iftimesis not provided. Iftimesis provided,halflifeand one ofcom,spanoralphamay be provided.Parameters:comfloat, optionalSpecify decay in terms of center of mass\(\alpha = 1 / (1 + com)\), for\(com \geq 0\).spanfloat, optionalSpecify decay in terms of span\(\alpha = 2 / (span + 1)\), for\(span \geq 1\).halflifefloat, str, timedelta, optionalSpecify decay in terms of half-life\(\alpha = 1 - \exp\left(-\ln(2) / halflife\right)\), for\(halflife > 0\).Iftimesis specified, a timedelta convertible unit over which an
observation decays to half its value. Only applicable tomean(),
and halflife value will not apply to the other functions.alphafloat, optionalSpecify smoothing factor\(\alpha\)directly\(0 < \alpha \leq 1\).min_periodsint, default 0Minimum number of observations in window required to have a value;
otherwise, result isnp.nan.adjustbool, default TrueDivide by decaying adjustment factor in beginning periods to account
for imbalance in relative weightings (viewing EWMA as a moving average).Whenadjust=True(default), the EW function is calculated using weights\(w_i = (1 - \alpha)^i\). For example, the EW moving average of the series
[\(x_0, x_1, ..., x_t\)] would be:\[y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ... + (1 -
\alpha)^t x_0}{1 + (1 - \alpha) + (1 - \alpha)^2 + ... + (1 - \alpha)^t}\]Whenadjust=False, the exponentially weighted function is calculated
recursively:\[\begin{split}\begin{split}
y_0 &= x_0\\
y_t &= (1 - \alpha) y_{t-1} + \alpha x_t,
\end{split}\end{split}\]ignore_nabool, default FalseIgnore missing values when calculating weights.Whenignore_na=False(default), weights are based on absolute positions.
For example, the weights of\(x_0\)and\(x_2\)used in calculating
the final weighted average of [\(x_0\), None,\(x_2\)] are\((1-\alpha)^2\)and\(1\)ifadjust=True, and\((1-\alpha)^2\)and\(\alpha\)ifadjust=False.Whenignore_na=True, weights are based
on relative positions. For example, the weights of\(x_0\)and\(x_2\)used in calculating the final weighted average of
[\(x_0\), None,\(x_2\)] are\(1-\alpha\)and\(1\)ifadjust=True, and\(1-\alpha\)and\(\alpha\)ifadjust=False.axis{0, 1}, default 0If0or'index', calculate across the rows.If1or'columns', calculate across the columns.ForSeriesthis parameter is unused and defaults to 0.timesnp.ndarray, Series, default NoneOnly applicable tomean().Times corresponding to the observations. Must be monotonically increasing anddatetime64[ns]dtype.If 1-D array like, a sequence with the same shape as the observations.methodstr {esinglef, etablef}, default esinglefNew in version 1.4.0.Execute the rolling operation per single column or row ('single')
or over the entire object ('table').This argument is only implemented when specifyingengine='numba'in the method call.Only applicable tomean()Returns:pandas.api.typing.ExponentialMovingWindowSee alsorollingProvides rolling window calculations.expandingProvides expanding transformations.NotesSeeWindowing Operationsfor further usage details and examples.Examples>>>df=pd.DataFrame({'B':[0,1,2,np.nan,4]})>>>dfB0  0.01  1.02  2.03  NaN4  4.0>>>df.ewm(com=0.5).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213>>>df.ewm(alpha=2/3).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213adjust>>>df.ewm(com=0.5,adjust=True).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213>>>df.ewm(com=0.5,adjust=False).mean()B0  0.0000001  0.6666672  1.5555563  1.5555564  3.650794ignore_na>>>df.ewm(com=0.5,ignore_na=True).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.225000>>>df.ewm(com=0.5,ignore_na=False).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213timesExponentially weighted mean with weights calculated with a timedeltahalfliferelative totimes.>>>times=['2020-01-01','2020-01-03','2020-01-10','2020-01-15','2020-01-17']>>>df.ewm(halflife='4 days',times=pd.DatetimeIndex(times)).mean()B0  0.0000001  0.5857862  1.5238893  1.5238894  3.233686"
Pandas,Series,pandas.Series.pipe,"pandas.Series.pipe#Series.pipe(func,*args,**kwargs)[source]#Apply chainable functions that expect Series or DataFrames.Parameters:funcfunctionFunction to apply to the Series/DataFrame.args, andkwargsare passed intofunc.
Alternatively a(callable,data_keyword)tuple wheredata_keywordis a string indicating the keyword ofcallablethat expects the Series/DataFrame.*argsiterable, optionalPositional arguments passed intofunc.**kwargsmapping, optionalA dictionary of keyword arguments passed intofunc.Returns:the return type offunc.See alsoDataFrame.applyApply a function along input axis of DataFrame.DataFrame.mapApply a function elementwise on a whole DataFrame.Series.mapApply a mapping correspondence on aSeries.NotesUse.pipewhen chaining together functions that expect
Series, DataFrames or GroupBy objects.ExamplesConstructing a income DataFrame from a dictionary.>>>data=[[8000,1000],[9500,np.nan],[5000,2000]]>>>df=pd.DataFrame(data,columns=['Salary','Others'])>>>dfSalary  Others0    8000  1000.01    9500     NaN2    5000  2000.0Functions that perform tax reductions on an income DataFrame.>>>defsubtract_federal_tax(df):...returndf*0.9>>>defsubtract_state_tax(df,rate):...returndf*(1-rate)>>>defsubtract_national_insurance(df,rate,rate_increase):...new_rate=rate+rate_increase...returndf*(1-new_rate)Instead of writing>>>subtract_national_insurance(...subtract_state_tax(subtract_federal_tax(df),rate=0.12),...rate=0.05,...rate_increase=0.02)You can write>>>(...df.pipe(subtract_federal_tax)....pipe(subtract_state_tax,rate=0.12)....pipe(subtract_national_insurance,rate=0.05,rate_increase=0.02)...)Salary   Others0  5892.48   736.561  6997.32      NaN2  3682.80  1473.12If you have a function that takes the data as (say) the second
argument, pass a tuple indicating which keyword expects the
data. For example, supposenational_insurancetakes its data asdfin the second argument:>>>defsubtract_national_insurance(rate,df,rate_increase):...new_rate=rate+rate_increase...returndf*(1-new_rate)>>>(...df.pipe(subtract_federal_tax)....pipe(subtract_state_tax,rate=0.12)....pipe(...(subtract_national_insurance,'df'),...rate=0.05,...rate_increase=0.02...)...)Salary   Others0  5892.48   736.561  6997.32      NaN2  3682.80  1473.12"
Pandas,Series,pandas.Series.abs,"pandas.Series.abs#Series.abs()[source]#Return a Series/DataFrame with absolute numeric value of each element.This function only applies to elements that are all numeric.Returns:absSeries/DataFrame containing the absolute value of each element.See alsonumpy.absoluteCalculate the absolute value element-wise.NotesForcomplexinputs,1.2+1j, the absolute value is\(\sqrt{ a^2 + b^2 }\).ExamplesAbsolute numeric values in a Series.>>>s=pd.Series([-1.10,2,-3.33,4])>>>s.abs()0    1.101    2.002    3.333    4.00dtype: float64Absolute numeric values in a Series with complex numbers.>>>s=pd.Series([1.2+1j])>>>s.abs()0    1.56205dtype: float64Absolute numeric values in a Series with a Timedelta element.>>>s=pd.Series([pd.Timedelta('1 days')])>>>s.abs()0   1 daysdtype: timedelta64[ns]Select rows with data closest to certain value using argsort (fromStackOverflow).>>>df=pd.DataFrame({...'a':[4,5,6,7],...'b':[10,20,30,40],...'c':[100,50,-30,-50]...})>>>dfa    b    c0    4   10  1001    5   20   502    6   30  -303    7   40  -50>>>df.loc[(df.c-43).abs().argsort()]a    b    c1    5   20   500    4   10  1002    6   30  -303    7   40  -50"
Pandas,Series,pandas.Series.all,"pandas.Series.all#Series.all(axis=0,bool_only=False,skipna=True,**kwargs)[source]#Return whether all elements are True, potentially over an axis.Returns True unless there at least one element within a series or
along a Dataframe axis that is False or equivalent (e.g. zero or
empty).Parameters:axis{0 or eindexf, 1 or ecolumnsf, None}, default 0Indicate which axis or axes should be reduced. ForSeriesthis parameter
is unused and defaults to 0.0 / eindexf : reduce the index, return a Series whose index is the
original column labels.1 / ecolumnsf : reduce the columns, return a Series whose index is the
original index.None : reduce all axes, return a scalar.bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series.skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is
True, then the result will be True, as for an empty row/column.
If skipna is False, then NA are treated as True, because these are not
equal to zero.**kwargsany, default NoneAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:scalar or SeriesIf level is specified, then, Series is returned; otherwise, scalar
is returned.See alsoSeries.allReturn True if all elements are True.DataFrame.anyReturn True if one (or more) elements are True.ExamplesSeries>>>pd.Series([True,True]).all()True>>>pd.Series([True,False]).all()False>>>pd.Series([],dtype=""float64"").all()True>>>pd.Series([np.nan]).all()True>>>pd.Series([np.nan]).all(skipna=False)TrueDataFramesCreate a dataframe from a dictionary.>>>df=pd.DataFrame({'col1':[True,True],'col2':[True,False]})>>>dfcol1   col20  True   True1  True  FalseDefault behaviour checks if values in each column all return True.>>>df.all()col1     Truecol2    Falsedtype: boolSpecifyaxis='columns'to check if values in each row all return True.>>>df.all(axis='columns')0     True1    Falsedtype: boolOraxis=Nonefor whether every value is True.>>>df.all(axis=None)False"
Pandas,Series,pandas.Series.any,"pandas.Series.any#Series.any(*,axis=0,bool_only=False,skipna=True,**kwargs)[source]#Return whether any element is True, potentially over an axis.Returns False unless there is at least one element within a series or
along a Dataframe axis that is True or equivalent (e.g. non-zero or
non-empty).Parameters:axis{0 or eindexf, 1 or ecolumnsf, None}, default 0Indicate which axis or axes should be reduced. ForSeriesthis parameter
is unused and defaults to 0.0 / eindexf : reduce the index, return a Series whose index is the
original column labels.1 / ecolumnsf : reduce the columns, return a Series whose index is the
original index.None : reduce all axes, return a scalar.bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series.skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is
True, then the result will be False, as for an empty row/column.
If skipna is False, then NA are treated as True, because these are not
equal to zero.**kwargsany, default NoneAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:scalar or SeriesIf level is specified, then, Series is returned; otherwise, scalar
is returned.See alsonumpy.anyNumpy version of this method.Series.anyReturn whether any element is True.Series.allReturn whether all elements are True.DataFrame.anyReturn whether any element is True over requested axis.DataFrame.allReturn whether all elements are True over requested axis.ExamplesSeriesFor Series input, the output is a scalar indicating whether any element
is True.>>>pd.Series([False,False]).any()False>>>pd.Series([True,False]).any()True>>>pd.Series([],dtype=""float64"").any()False>>>pd.Series([np.nan]).any()False>>>pd.Series([np.nan]).any(skipna=False)TrueDataFrameWhether each column contains at least one True element (the default).>>>df=pd.DataFrame({""A"":[1,2],""B"":[0,2],""C"":[0,0]})>>>dfA  B  C0  1  0  01  2  2  0>>>df.any()A     TrueB     TrueC    Falsedtype: boolAggregating over the columns.>>>df=pd.DataFrame({""A"":[True,False],""B"":[1,2]})>>>dfA  B0   True  11  False  2>>>df.any(axis='columns')0    True1    Truedtype: bool>>>df=pd.DataFrame({""A"":[True,False],""B"":[1,0]})>>>dfA  B0   True  11  False  0>>>df.any(axis='columns')0    True1    Falsedtype: boolAggregating over the entire DataFrame withaxis=None.>>>df.any(axis=None)Trueanyfor an empty DataFrame is an empty Series.>>>pd.DataFrame([]).any()Series([], dtype: bool)"
Pandas,Series,pandas.Series.autocorr,"pandas.Series.autocorr#Series.autocorr(lag=1)[source]#Compute the lag-N autocorrelation.This method computes the Pearson correlation between
the Series and its shifted self.Parameters:lagint, default 1Number of lags to apply before performing autocorrelation.Returns:floatThe Pearson correlation between self and self.shift(lag).See alsoSeries.corrCompute the correlation between two Series.Series.shiftShift index by desired number of periods.DataFrame.corrCompute pairwise correlation of columns.DataFrame.corrwithCompute pairwise correlation between rows or columns of two DataFrame objects.NotesIf the Pearson correlation is not well defined return eNaNf.Examples>>>s=pd.Series([0.25,0.5,0.2,-0.05])>>>s.autocorr()0.10355...>>>s.autocorr(lag=2)-0.99999...If the Pearson correlation is not well defined, then eNaNf is returned.>>>s=pd.Series([1,0,0,0])>>>s.autocorr()nan"
Pandas,Series,pandas.Series.between,"pandas.Series.between#Series.between(left,right,inclusive='both')[source]#Return boolean Series equivalent to left <= series <= right.This function returns a boolean vector containingTruewherever the
corresponding Series element is between the boundary valuesleftandright. NA values are treated asFalse.Parameters:leftscalar or list-likeLeft boundary.rightscalar or list-likeRight boundary.inclusive{gbothh, gneitherh, glefth, grighth}Include boundaries. Whether to set each bound as closed or open.Changed in version 1.3.0.Returns:SeriesSeries representing whether each element is between left and
right (inclusive).See alsoSeries.gtGreater than of series and other.Series.ltLess than of series and other.NotesThis function is equivalent to(left<=ser)&(ser<=right)Examples>>>s=pd.Series([2,0,4,8,np.nan])Boundary values are included by default:>>>s.between(1,4)0     True1    False2     True3    False4    Falsedtype: boolWithinclusiveset to""neither""boundary values are excluded:>>>s.between(1,4,inclusive=""neither"")0     True1    False2    False3    False4    Falsedtype: boolleftandrightcan be any scalar value:>>>s=pd.Series(['Alice','Bob','Carol','Eve'])>>>s.between('Anna','Daniel')0    False1     True2     True3    Falsedtype: bool"
Pandas,Series,pandas.Series.clip,"pandas.Series.clip#Series.clip(lower=None,upper=None,*,axis=None,inplace=False,**kwargs)[source]#Trim values at input threshold(s).Assigns values outside boundary to boundary values. Thresholds
can be singular values or array like, and in the latter case
the clipping is performed element-wise in the specified axis.Parameters:lowerfloat or array-like, default NoneMinimum threshold value. All values below this
threshold will be set to it. A missing
threshold (e.gNA) will not clip the value.upperfloat or array-like, default NoneMaximum threshold value. All values above this
threshold will be set to it. A missing
threshold (e.gNA) will not clip the value.axis{{0 or eindexf, 1 or ecolumnsf, None}}, default NoneAlign object with lower and upper along the given axis.
ForSeriesthis parameter is unused and defaults toNone.inplacebool, default FalseWhether to perform the operation in place on the data.*args, **kwargsAdditional keywords have no effect but might be accepted
for compatibility with numpy.Returns:Series or DataFrame or NoneSame type as calling object with the values outside the
clip boundaries replaced or None ifinplace=True.See alsoSeries.clipTrim values at input threshold in series.DataFrame.clipTrim values at input threshold in dataframe.numpy.clipClip (limit) the values in an array.Examples>>>data={'col_0':[9,-3,0,-1,5],'col_1':[-2,-7,6,8,-5]}>>>df=pd.DataFrame(data)>>>dfcol_0  col_10      9     -21     -3     -72      0      63     -1      84      5     -5Clips per column using lower and upper thresholds:>>>df.clip(-4,6)col_0  col_10      6     -21     -3     -42      0      63     -1      64      5     -4Clips using specific lower and upper thresholds per column element:>>>t=pd.Series([2,-4,-1,6,3])>>>t0    21   -42   -13    64    3dtype: int64>>>df.clip(t,t+4,axis=0)col_0  col_10      6      21     -3     -42      0      33      6      84      5      3Clips using specific lower threshold per column element, with missing values:>>>t=pd.Series([2,-4,np.nan,6,3])>>>t0    2.01   -4.02    NaN3    6.04    3.0dtype: float64>>>df.clip(t,axis=0)col_0  col_10      9      21     -3     -42      0      63      6      84      5      3"
Pandas,Series,pandas.Series.corr,"pandas.Series.corr#Series.corr(other,method='pearson',min_periods=None)[source]#Compute correlation withotherSeries, excluding missing values.The twoSeriesobjects are not required to be the same length and will be
aligned internally before the correlation function is applied.Parameters:otherSeriesSeries with which to compute the correlation.method{epearsonf, ekendallf, espearmanf} or callableMethod used to compute correlation:pearson : Standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: Callable with input two 1d ndarrays and returning a float.WarningNote that the returned matrix from corr will have 1 along the
diagonals and will be symmetric regardless of the callablefs
behavior.min_periodsint, optionalMinimum number of observations needed to have a valid result.Returns:floatCorrelation with other.See alsoDataFrame.corrCompute pairwise correlation between columns.DataFrame.corrwithCompute pairwise correlation with another DataFrame or Series.NotesPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.Pearson correlation coefficientKendall rank correlation coefficientSpearmanfs rank correlation coefficientAutomatic data alignment: as with all pandas operations, automatic data alignment is performed for this method.corr()automatically considers values with matching indices.Examples>>>defhistogram_intersection(a,b):...v=np.minimum(a,b).sum().round(decimals=1)...returnv>>>s1=pd.Series([.2,.0,.6,.2])>>>s2=pd.Series([.3,.6,.0,.1])>>>s1.corr(s2,method=histogram_intersection)0.3Pandas auto-aligns the values with matching indices>>>s1=pd.Series([1,2,3],index=[0,1,2])>>>s2=pd.Series([1,2,3],index=[2,1,0])>>>s1.corr(s2)-1.0"
Pandas,Series,pandas.Series.count,"pandas.Series.count#Series.count()[source]#Return number of non-NA/null observations in the Series.Returns:int or Series (if level specified)Number of non-null values in the Series.See alsoDataFrame.countCount non-NA cells for each column or row.Examples>>>s=pd.Series([0.0,1.0,np.nan])>>>s.count()2"
Pandas,Series,pandas.Series.cov,"pandas.Series.cov#Series.cov(other,min_periods=None,ddof=1)[source]#Compute covariance with Series, excluding missing values.The twoSeriesobjects are not required to be the same length and
will be aligned internally before the covariance is calculated.Parameters:otherSeriesSeries with which to compute the covariance.min_periodsint, optionalMinimum number of observations needed to have a valid result.ddofint, default 1Delta degrees of freedom. The divisor used in calculations
isN-ddof, whereNrepresents the number of elements.Returns:floatCovariance between Series and other normalized by N-1
(unbiased estimator).See alsoDataFrame.covCompute pairwise covariance of columns.Examples>>>s1=pd.Series([0.90010907,0.13484424,0.62036035])>>>s2=pd.Series([0.12528585,0.26962463,0.51111198])>>>s1.cov(s2)-0.01685762652715874"
Pandas,Series,pandas.Series.cummax,"pandas.Series.cummax#Series.cummax(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative maximum over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
maximum.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:scalar or SeriesReturn cumulative maximum of scalar or Series.See alsocore.window.expanding.Expanding.maxSimilar functionality but ignoresNaNvalues.Series.maxReturn the maximum over Series axis.Series.cummaxReturn cumulative maximum over Series axis.Series.cumminReturn cumulative minimum over Series axis.Series.cumsumReturn cumulative sum over Series axis.Series.cumprodReturn cumulative product over Series axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cummax()0    2.01    NaN2    5.03    5.04    5.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cummax(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the maximum
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cummax()A    B0  2.0  1.01  3.0  NaN2  3.0  1.0To iterate over columns and find the maximum in each row,
useaxis=1>>>df.cummax(axis=1)A    B0  2.0  2.01  3.0  NaN2  1.0  1.0"
Pandas,Series,pandas.Series.cummin,"pandas.Series.cummin#Series.cummin(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative minimum over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
minimum.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:scalar or SeriesReturn cumulative minimum of scalar or Series.See alsocore.window.expanding.Expanding.minSimilar functionality but ignoresNaNvalues.Series.minReturn the minimum over Series axis.Series.cummaxReturn cumulative maximum over Series axis.Series.cumminReturn cumulative minimum over Series axis.Series.cumsumReturn cumulative sum over Series axis.Series.cumprodReturn cumulative product over Series axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cummin()0    2.01    NaN2    2.03   -1.04   -1.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cummin(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the minimum
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cummin()A    B0  2.0  1.01  2.0  NaN2  1.0  0.0To iterate over columns and find the minimum in each row,
useaxis=1>>>df.cummin(axis=1)A    B0  2.0  1.01  3.0  NaN2  1.0  0.0"
Pandas,Series,pandas.Series.cumprod,"pandas.Series.cumprod#Series.cumprod(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative product over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
product.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:scalar or SeriesReturn cumulative product of scalar or Series.See alsocore.window.expanding.Expanding.prodSimilar functionality but ignoresNaNvalues.Series.prodReturn the product over Series axis.Series.cummaxReturn cumulative maximum over Series axis.Series.cumminReturn cumulative minimum over Series axis.Series.cumsumReturn cumulative sum over Series axis.Series.cumprodReturn cumulative product over Series axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cumprod()0     2.01     NaN2    10.03   -10.04    -0.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cumprod(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the product
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cumprod()A    B0  2.0  1.01  6.0  NaN2  6.0  0.0To iterate over columns and find the product in each row,
useaxis=1>>>df.cumprod(axis=1)A    B0  2.0  2.01  3.0  NaN2  1.0  0.0"
Pandas,Series,pandas.Series.cumsum,"pandas.Series.cumsum#Series.cumsum(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative sum over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
sum.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:scalar or SeriesReturn cumulative sum of scalar or Series.See alsocore.window.expanding.Expanding.sumSimilar functionality but ignoresNaNvalues.Series.sumReturn the sum over Series axis.Series.cummaxReturn cumulative maximum over Series axis.Series.cumminReturn cumulative minimum over Series axis.Series.cumsumReturn cumulative sum over Series axis.Series.cumprodReturn cumulative product over Series axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cumsum()0    2.01    NaN2    7.03    6.04    6.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cumsum(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the sum
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cumsum()A    B0  2.0  1.01  5.0  NaN2  6.0  1.0To iterate over columns and find the sum in each row,
useaxis=1>>>df.cumsum(axis=1)A    B0  2.0  3.01  3.0  NaN2  1.0  1.0"
Pandas,Series,pandas.Series.describe,"pandas.Series.describe#Series.describe(percentiles=None,include=None,exclude=None)[source]#Generate descriptive statistics.Descriptive statistics include those that summarize the central
tendency, dispersion and shape of a
datasetfs distribution, excludingNaNvalues.Analyzes both numeric and object series, as well
asDataFramecolumn sets of mixed data types. The output
will vary depending on what is provided. Refer to the notes
below for more detail.Parameters:percentileslist-like of numbers, optionalThe percentiles to include in the output. All should
fall between 0 and 1. The default is[.25,.5,.75], which returns the 25th, 50th, and
75th percentiles.includeeallf, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored
forSeries. Here are the options:eallf : All columns of the input will be included in the output.A list-like of dtypes : Limits the results to the
provided data types.
To limit the result to numeric types submitnumpy.number. To limit it instead to object columns submit
thenumpy.objectdata type. Strings
can also be used in the style ofselect_dtypes(e.g.df.describe(include=['O'])). To
select pandas categorical columns, use'category'None (default) : The result will include all numeric columns.excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored
forSeries. Here are the options:A list-like of dtypes : Excludes the provided data types
from the result. To exclude numeric types submitnumpy.number. To exclude object columns submit the data
typenumpy.object. Strings can also be used in the style ofselect_dtypes(e.g.df.describe(exclude=['O'])). To
exclude pandas categorical columns, use'category'None (default) : The result will exclude nothing.Returns:Series or DataFrameSummary statistics of the Series or Dataframe provided.See alsoDataFrame.countCount number of non-NA/null observations.DataFrame.maxMaximum of the values in the object.DataFrame.minMinimum of the values in the object.DataFrame.meanMean of the values.DataFrame.stdStandard deviation of the observations.DataFrame.select_dtypesSubset of a DataFrame including/excluding columns based on their dtype.NotesFor numeric data, the resultfs index will includecount,mean,std,min,maxas well as lower,50and
upper percentiles. By default the lower percentile is25and the
upper percentile is75. The50percentile is the
same as the median.For object data (e.g. strings or timestamps), the resultfs index
will includecount,unique,top, andfreq. Thetopis the most common value. Thefreqis the most common valuefs
frequency. Timestamps also include thefirstandlastitems.If multiple object values have the highest count, then thecountandtopresults will be arbitrarily chosen from
among those with the highest count.For mixed data types provided via aDataFrame, the default is to
return only an analysis of numeric columns. If the dataframe consists
only of object and categorical data without any numeric columns, the
default is to return an analysis of both the object and categorical
columns. Ifinclude='all'is provided as an option, the result
will include a union of attributes of each type.Theincludeandexcludeparameters can be used to limit
which columns in aDataFrameare analyzed for the output.
The parameters are ignored when analyzing aSeries.ExamplesDescribing a numericSeries.>>>s=pd.Series([1,2,3])>>>s.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0dtype: float64Describing a categoricalSeries.>>>s=pd.Series(['a','a','b','c'])>>>s.describe()count     4unique    3top       afreq      2dtype: objectDescribing a timestampSeries.>>>s=pd.Series([...np.datetime64(""2000-01-01""),...np.datetime64(""2010-01-01""),...np.datetime64(""2010-01-01"")...])>>>s.describe()count                      3mean     2006-09-01 08:00:00min      2000-01-01 00:00:0025%      2004-12-31 12:00:0050%      2010-01-01 00:00:0075%      2010-01-01 00:00:00max      2010-01-01 00:00:00dtype: objectDescribing aDataFrame. By default only numeric fields
are returned.>>>df=pd.DataFrame({'categorical':pd.Categorical(['d','e','f']),...'numeric':[1,2,3],...'object':['a','b','c']...})>>>df.describe()numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Describing all columns of aDataFrameregardless of data type.>>>df.describe(include='all')categorical  numeric objectcount            3      3.0      3unique           3      NaN      3top              f      NaN      afreq             1      NaN      1mean           NaN      2.0    NaNstd            NaN      1.0    NaNmin            NaN      1.0    NaN25%            NaN      1.5    NaN50%            NaN      2.0    NaN75%            NaN      2.5    NaNmax            NaN      3.0    NaNDescribing a column from aDataFrameby accessing it as
an attribute.>>>df.numeric.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0Name: numeric, dtype: float64Including only numeric columns in aDataFramedescription.>>>df.describe(include=[np.number])numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Including only string columns in aDataFramedescription.>>>df.describe(include=[object])objectcount       3unique      3top         afreq        1Including only categorical columns from aDataFramedescription.>>>df.describe(include=['category'])categoricalcount            3unique           3top              dfreq             1Excluding numeric columns from aDataFramedescription.>>>df.describe(exclude=[np.number])categorical objectcount            3      3unique           3      3top              f      afreq             1      1Excluding object columns from aDataFramedescription.>>>df.describe(exclude=[object])categorical  numericcount            3      3.0unique           3      NaNtop              f      NaNfreq             1      NaNmean           NaN      2.0std            NaN      1.0min            NaN      1.025%            NaN      1.550%            NaN      2.075%            NaN      2.5max            NaN      3.0"
Pandas,Series,pandas.Series.diff,"pandas.Series.diff#Series.diff(periods=1)[source]#First discrete difference of element.Calculates the difference of a Series element compared with another
element in the Series (default is element in previous row).Parameters:periodsint, default 1Periods to shift for calculating difference, accepts negative
values.Returns:SeriesFirst differences of the Series.See alsoSeries.pct_changePercent change over given number of periods.Series.shiftShift index by desired number of periods with an optional time freq.DataFrame.diffFirst discrete difference of object.NotesFor boolean dtypes, this usesoperator.xor()rather thanoperator.sub().
The result is calculated according to current dtype in Series,
however dtype of the result is always float64.ExamplesDifference with previous row>>>s=pd.Series([1,1,2,3,5,8])>>>s.diff()0    NaN1    0.02    1.03    1.04    2.05    3.0dtype: float64Difference with 3rd previous row>>>s.diff(periods=3)0    NaN1    NaN2    NaN3    2.04    4.05    6.0dtype: float64Difference with following row>>>s.diff(periods=-1)0    0.01   -1.02   -1.03   -2.04   -3.05    NaNdtype: float64Overflow in input dtype>>>s=pd.Series([1,0],dtype=np.uint8)>>>s.diff()0      NaN1    255.0dtype: float64"
Pandas,Series,pandas.Series.factorize,"pandas.Series.factorize#Series.factorize(sort=False,use_na_sentinel=True)[source]#Encode the object as an enumerated type or categorical variable.This method is useful for obtaining a numeric representation of an
array when all that matters is identifying distinct values.factorizeis available as both a top-level functionpandas.factorize(),
and as a methodSeries.factorize()andIndex.factorize().Parameters:sortbool, default FalseSortuniquesand shufflecodesto maintain the
relationship.use_na_sentinelbool, default TrueIf True, the sentinel -1 will be used for NaN values. If False,
NaN values will be encoded as non-negative integers and will not drop the
NaN from the uniques of the values.New in version 1.5.0.Returns:codesndarrayAn integer ndarray thatfs an indexer intouniques.uniques.take(codes)will have the same values asvalues.uniquesndarray, Index, or CategoricalThe unique valid values. Whenvaluesis Categorical,uniquesis a Categorical. Whenvaluesis some other pandas object, anIndexis returned. Otherwise, a 1-D ndarray is returned.NoteEven if therefs a missing value invalues,uniqueswillnotcontain an entry for it.See alsocutDiscretize continuous-valued array.uniqueFind the unique value in an array.NotesReferencethe user guidefor more examples.ExamplesThese examples all show factorize as a top-level method likepd.factorize(values). The results are identical for methods likeSeries.factorize().>>>codes,uniques=pd.factorize(np.array(['b','b','a','c','b'],dtype=""O""))>>>codesarray([0, 0, 1, 2, 0])>>>uniquesarray(['b', 'a', 'c'], dtype=object)Withsort=True, theuniqueswill be sorted, andcodeswill be
shuffled so that the relationship is the maintained.>>>codes,uniques=pd.factorize(np.array(['b','b','a','c','b'],dtype=""O""),...sort=True)>>>codesarray([1, 1, 0, 2, 1])>>>uniquesarray(['a', 'b', 'c'], dtype=object)Whenuse_na_sentinel=True(the default), missing values are indicated in
thecodeswith the sentinel value-1and missing values are not
included inuniques.>>>codes,uniques=pd.factorize(np.array(['b',None,'a','c','b'],dtype=""O""))>>>codesarray([ 0, -1,  1,  2,  0])>>>uniquesarray(['b', 'a', 'c'], dtype=object)Thus far, wefve only factorized lists (which are internally coerced to
NumPy arrays). When factorizing pandas objects, the type ofuniqueswill differ. For Categoricals, aCategoricalis returned.>>>cat=pd.Categorical(['a','a','c'],categories=['a','b','c'])>>>codes,uniques=pd.factorize(cat)>>>codesarray([0, 0, 1])>>>uniques['a', 'c']Categories (3, object): ['a', 'b', 'c']Notice that'b'is inuniques.categories, despite not being
present incat.values.For all other pandas objects, an Index of the appropriate type is
returned.>>>cat=pd.Series(['a','a','c'])>>>codes,uniques=pd.factorize(cat)>>>codesarray([0, 0, 1])>>>uniquesIndex(['a', 'c'], dtype='object')If NaN is in the values, and we want to include NaN in the uniques of the
values, it can be achieved by settinguse_na_sentinel=False.>>>values=np.array([1,2,1,np.nan])>>>codes,uniques=pd.factorize(values)# default: use_na_sentinel=True>>>codesarray([ 0,  1,  0, -1])>>>uniquesarray([1., 2.])>>>codes,uniques=pd.factorize(values,use_na_sentinel=False)>>>codesarray([0, 1, 0, 2])>>>uniquesarray([ 1.,  2., nan])"
Pandas,Series,pandas.Series.kurt,"pandas.Series.kurt#Series.kurt(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return unbiased kurtosis over requested axis.Kurtosis obtained using Fisherfs definition of
kurtosis (kurtosis of normal == 0.0). Normalized by N-1.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarExamples>>>s=pd.Series([1,2,2,3],index=['cat','dog','dog','mouse'])>>>scat    1dog    2dog    2mouse  3dtype: int64>>>s.kurt()1.5With a DataFrame>>>df=pd.DataFrame({'a':[1,2,2,3],'b':[3,4,4,4]},...index=['cat','dog','dog','mouse'])>>>dfa   bcat  1   3dog  2   4dog  2   4mouse  3   4>>>df.kurt()a   1.5b   4.0dtype: float64With axis=None>>>df.kurt(axis=None).round(6)-0.988693Using axis=1>>>df=pd.DataFrame({'a':[1,2],'b':[3,4],'c':[3,4],'d':[1,2]},...index=['cat','dog'])>>>df.kurt(axis=1)cat   -6.0dog   -6.0dtype: float64"
Pandas,Series,pandas.Series.max,"pandas.Series.max#Series.max(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the maximum of the values over the requested axis.If you want theindexof the maximum, useidxmax. This is the equivalent of thenumpy.ndarraymethodargmax.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.Examples>>>idx=pd.MultiIndex.from_arrays([...['warm','warm','cold','cold'],...['dog','falcon','fish','spider']],...names=['blooded','animal'])>>>s=pd.Series([4,2,0,8],name='legs',index=idx)>>>sblooded  animalwarm     dog       4falcon    2cold     fish      0spider    8Name: legs, dtype: int64>>>s.max()8"
Pandas,Series,pandas.Series.mean,"pandas.Series.mean#Series.mean(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the mean of the values over the requested axis.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarExamples>>>s=pd.Series([1,2,3])>>>s.mean()2.0With a DataFrame>>>df=pd.DataFrame({'a':[1,2],'b':[2,3]},index=['tiger','zebra'])>>>dfa   btiger  1   2zebra  2   3>>>df.mean()a   1.5b   2.5dtype: float64Using axis=1>>>df.mean(axis=1)tiger   1.5zebra   2.5dtype: float64In this case,numeric_onlyshould be set toTrueto avoid
getting an error.>>>df=pd.DataFrame({'a':[1,2],'b':['T','Z']},...index=['tiger','zebra'])>>>df.mean(numeric_only=True)a   1.5dtype: float64"
Pandas,Series,pandas.Series.median,"pandas.Series.median#Series.median(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the median of the values over the requested axis.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarExamples>>>s=pd.Series([1,2,3])>>>s.median()2.0With a DataFrame>>>df=pd.DataFrame({'a':[1,2],'b':[2,3]},index=['tiger','zebra'])>>>dfa   btiger  1   2zebra  2   3>>>df.median()a   1.5b   2.5dtype: float64Using axis=1>>>df.median(axis=1)tiger   1.5zebra   2.5dtype: float64In this case,numeric_onlyshould be set toTrueto avoid getting an error.>>>df=pd.DataFrame({'a':[1,2],'b':['T','Z']},...index=['tiger','zebra'])>>>df.median(numeric_only=True)a   1.5dtype: float64"
Pandas,Series,pandas.Series.min,"pandas.Series.min#Series.min(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the minimum of the values over the requested axis.If you want theindexof the minimum, useidxmin. This is the equivalent of thenumpy.ndarraymethodargmin.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.Examples>>>idx=pd.MultiIndex.from_arrays([...['warm','warm','cold','cold'],...['dog','falcon','fish','spider']],...names=['blooded','animal'])>>>s=pd.Series([4,2,0,8],name='legs',index=idx)>>>sblooded  animalwarm     dog       4falcon    2cold     fish      0spider    8Name: legs, dtype: int64>>>s.min()0"
Pandas,Series,pandas.Series.mode,"pandas.Series.mode#Series.mode(dropna=True)[source]#Return the mode(s) of the Series.The mode is the value that appears most often. There can be multiple modes.Always returns Series even if only one value is returned.Parameters:dropnabool, default TrueDonft consider counts of NaN/NaT.Returns:SeriesModes of the Series in sorted order.Examples>>>s=pd.Series([2,4,2,2,4,None])>>>s.mode()0    2.0dtype: float64More than one mode:>>>s=pd.Series([2,4,8,2,4,None])>>>s.mode()0    2.01    4.0dtype: float64With and without considering null value:>>>s=pd.Series([2,4,None,None,4,None])>>>s.mode(dropna=False)0   NaNdtype: float64>>>s=pd.Series([2,4,None,None,4,None])>>>s.mode()0    4.0dtype: float64"
Pandas,Series,pandas.Series.nlargest,"pandas.Series.nlargest#Series.nlargest(n=5,keep='first')[source]#Return the largestnelements.Parameters:nint, default 5Return this many descending sorted values.keep{efirstf, elastf, eallf}, default efirstfWhen there are duplicate values that cannot all fit in a
Series ofnelements:first: return the firstnoccurrences in order
of appearance.last: return the lastnoccurrences in reverse
order of appearance.all: keep all occurrences. This can result in a Series of
size larger thann.Returns:SeriesThenlargest values in the Series, sorted in decreasing order.See alsoSeries.nsmallestGet thensmallest elements.Series.sort_valuesSort Series by values.Series.headReturn the firstnrows.NotesFaster than.sort_values(ascending=False).head(n)for smallnrelative to the size of theSeriesobject.Examples>>>countries_population={""Italy"":59000000,""France"":65000000,...""Malta"":434000,""Maldives"":434000,...""Brunei"":434000,""Iceland"":337000,...""Nauru"":11300,""Tuvalu"":11300,...""Anguilla"":11300,""Montserrat"":5200}>>>s=pd.Series(countries_population)>>>sItaly       59000000France      65000000Malta         434000Maldives      434000Brunei        434000Iceland       337000Nauru          11300Tuvalu         11300Anguilla       11300Montserrat      5200dtype: int64Thenlargest elements wheren=5by default.>>>s.nlargest()France      65000000Italy       59000000Malta         434000Maldives      434000Brunei        434000dtype: int64Thenlargest elements wheren=3. Defaultkeepvalue is efirstf
so Malta will be kept.>>>s.nlargest(3)France    65000000Italy     59000000Malta       434000dtype: int64Thenlargest elements wheren=3and keeping the last duplicates.
Brunei will be kept since it is the last with value 434000 based on
the index order.>>>s.nlargest(3,keep='last')France      65000000Italy       59000000Brunei        434000dtype: int64Thenlargest elements wheren=3with all duplicates kept. Note
that the returned Series has five elements due to the three duplicates.>>>s.nlargest(3,keep='all')France      65000000Italy       59000000Malta         434000Maldives      434000Brunei        434000dtype: int64"
Pandas,Series,pandas.Series.nsmallest,"pandas.Series.nsmallest#Series.nsmallest(n=5,keep='first')[source]#Return the smallestnelements.Parameters:nint, default 5Return this many ascending sorted values.keep{efirstf, elastf, eallf}, default efirstfWhen there are duplicate values that cannot all fit in a
Series ofnelements:first: return the firstnoccurrences in order
of appearance.last: return the lastnoccurrences in reverse
order of appearance.all: keep all occurrences. This can result in a Series of
size larger thann.Returns:SeriesThensmallest values in the Series, sorted in increasing order.See alsoSeries.nlargestGet thenlargest elements.Series.sort_valuesSort Series by values.Series.headReturn the firstnrows.NotesFaster than.sort_values().head(n)for smallnrelative to
the size of theSeriesobject.Examples>>>countries_population={""Italy"":59000000,""France"":65000000,...""Brunei"":434000,""Malta"":434000,...""Maldives"":434000,""Iceland"":337000,...""Nauru"":11300,""Tuvalu"":11300,...""Anguilla"":11300,""Montserrat"":5200}>>>s=pd.Series(countries_population)>>>sItaly       59000000France      65000000Brunei        434000Malta         434000Maldives      434000Iceland       337000Nauru          11300Tuvalu         11300Anguilla       11300Montserrat      5200dtype: int64Thensmallest elements wheren=5by default.>>>s.nsmallest()Montserrat    5200Nauru        11300Tuvalu       11300Anguilla     11300Iceland     337000dtype: int64Thensmallest elements wheren=3. Defaultkeepvalue is
efirstf so Nauru and Tuvalu will be kept.>>>s.nsmallest(3)Montserrat   5200Nauru       11300Tuvalu      11300dtype: int64Thensmallest elements wheren=3and keeping the last
duplicates. Anguilla and Tuvalu will be kept since they are the last
with value 11300 based on the index order.>>>s.nsmallest(3,keep='last')Montserrat   5200Anguilla    11300Tuvalu      11300dtype: int64Thensmallest elements wheren=3with all duplicates kept. Note
that the returned Series has four elements due to the three duplicates.>>>s.nsmallest(3,keep='all')Montserrat   5200Nauru       11300Tuvalu      11300Anguilla    11300dtype: int64"
Pandas,Series,pandas.Series.pct_change,"pandas.Series.pct_change#Series.pct_change(periods=1,fill_method=_NoDefault.no_default,limit=_NoDefault.no_default,freq=None,**kwargs)[source]#Fractional change between the current and a prior element.Computes the fractional change from the immediately previous row by
default. This is useful in comparing the fraction of change in a time
series of elements.NoteDespite the name of this method, it calculates fractional change
(also known as per unit change or relative change) and not
percentage change. If you need the percentage change, multiply
these values by 100.Parameters:periodsint, default 1Periods to shift for forming percent change.fill_method{ebackfillf, ebfillf, epadf, effillf, None}, default epadfHow to handle NAsbeforecomputing percent changes.Deprecated since version 2.1:All options offill_methodare deprecated exceptfill_method=None.limitint, default NoneThe number of consecutive NAs to fill before stopping.Deprecated since version 2.1.freqDateOffset, timedelta, or str, optionalIncrement to use from time series API (e.g. eMf or BDay()).**kwargsAdditional keyword arguments are passed intoDataFrame.shiftorSeries.shift.Returns:Series or DataFrameThe same type as the calling object.See alsoSeries.diffCompute the difference of two elements in a Series.DataFrame.diffCompute the difference of two elements in a DataFrame.Series.shiftShift the index by some number of periods.DataFrame.shiftShift the index by some number of periods.ExamplesSeries>>>s=pd.Series([90,91,85])>>>s0    901    912    85dtype: int64>>>s.pct_change()0         NaN1    0.0111112   -0.065934dtype: float64>>>s.pct_change(periods=2)0         NaN1         NaN2   -0.055556dtype: float64See the percentage change in a Series where filling NAs with last
valid observation forward to next valid.>>>s=pd.Series([90,91,None,85])>>>s0    90.01    91.02     NaN3    85.0dtype: float64>>>s.ffill().pct_change()0         NaN1    0.0111112    0.0000003   -0.065934dtype: float64DataFramePercentage change in French franc, Deutsche Mark, and Italian lira from
1980-01-01 to 1980-03-01.>>>df=pd.DataFrame({...'FR':[4.0405,4.0963,4.3149],...'GR':[1.7246,1.7482,1.8519],...'IT':[804.74,810.01,860.13]},...index=['1980-01-01','1980-02-01','1980-03-01'])>>>dfFR      GR      IT1980-01-01  4.0405  1.7246  804.741980-02-01  4.0963  1.7482  810.011980-03-01  4.3149  1.8519  860.13>>>df.pct_change()FR        GR        IT1980-01-01       NaN       NaN       NaN1980-02-01  0.013810  0.013684  0.0065491980-03-01  0.053365  0.059318  0.061876Percentage of change in GOOG and APPL stock volume. Shows computing
the percentage change between columns.>>>df=pd.DataFrame({...'2016':[1769950,30586265],...'2015':[1500923,40912316],...'2014':[1371819,41403351]},...index=['GOOG','APPL'])>>>df2016      2015      2014GOOG   1769950   1500923   1371819APPL  30586265  40912316  41403351>>>df.pct_change(axis='columns',periods=-1)2016      2015  2014GOOG  0.179241  0.094112   NaNAPPL -0.252395 -0.011860   NaN"
Pandas,Series,pandas.Series.prod,"pandas.Series.prod#Series.prod(axis=None,skipna=True,numeric_only=False,min_count=0,**kwargs)[source]#Return the product of the values over the requested axis.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.min_countint, default 0The required number of valid values to perform the operation. If fewer thanmin_countnon-NA values are present the result will be NA.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.ExamplesBy default, the product of an empty or all-NA Series is1>>>pd.Series([],dtype=""float64"").prod()1.0This can be controlled with themin_countparameter>>>pd.Series([],dtype=""float64"").prod(min_count=1)nanThanks to theskipnaparameter,min_counthandles all-NA and
empty series identically.>>>pd.Series([np.nan]).prod()1.0>>>pd.Series([np.nan]).prod(min_count=1)nan"
Pandas,Series,pandas.Series.quantile,"pandas.Series.quantile#Series.quantile(q=0.5,interpolation='linear')[source]#Return value at the given quantile.Parameters:qfloat or array-like, default 0.5 (50% quantile)The quantile(s) to compute, which can lie in range: 0 <= q <= 1.interpolation{elinearf, elowerf, ehigherf, emidpointf, enearestf}This optional parameter specifies the interpolation method to use,
when the desired quantile lies between two data pointsiandj:linear:i + (j - i) * fraction, wherefractionis the
fractional part of the index surrounded byiandj.lower:i.higher:j.nearest:iorjwhichever is nearest.midpoint: (i+j) / 2.Returns:float or SeriesIfqis an array, a Series will be returned where the
index isqand the values are the quantiles, otherwise
a float will be returned.See alsocore.window.Rolling.quantileCalculate the rolling quantile.numpy.percentileReturns the q-th percentile(s) of the array elements.Examples>>>s=pd.Series([1,2,3,4])>>>s.quantile(.5)2.5>>>s.quantile([.25,.5,.75])0.25    1.750.50    2.500.75    3.25dtype: float64"
Pandas,Series,pandas.Series.rank,"pandas.Series.rank#Series.rank(axis=0,method='average',numeric_only=False,na_option='keep',ascending=True,pct=False)[source]#Compute numerical data ranks (1 through n) along axis.By default, equal values are assigned a rank that is the average of the
ranks of those values.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0Index to direct ranking.
ForSeriesthis parameter is unused and defaults to 0.method{eaveragef, eminf, emaxf, efirstf, edensef}, default eaveragefHow to rank the group of records that have the same value (i.e. ties):average: average rank of the groupmin: lowest rank in the groupmax: highest rank in the groupfirst: ranks assigned in order they appear in the arraydense: like eminf, but rank always increases by 1 between groups.numeric_onlybool, default FalseFor DataFrame objects, rank only numeric columns if set to True.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.na_option{ekeepf, etopf, ebottomf}, default ekeepfHow to rank NaN values:keep: assign NaN rank to NaN valuestop: assign lowest rank to NaN valuesbottom: assign highest rank to NaN valuesascendingbool, default TrueWhether or not the elements should be ranked in ascending order.pctbool, default FalseWhether or not to display the returned rankings in percentile
form.Returns:same type as callerReturn a Series or DataFrame with data ranks as values.See alsocore.groupby.DataFrameGroupBy.rankRank of values within each group.core.groupby.SeriesGroupBy.rankRank of values within each group.Examples>>>df=pd.DataFrame(data={'Animal':['cat','penguin','dog',...'spider','snake'],...'Number_legs':[4,2,4,8,np.nan]})>>>dfAnimal  Number_legs0      cat          4.01  penguin          2.02      dog          4.03   spider          8.04    snake          NaNTies are assigned the mean of the ranks (by default) for the group.>>>s=pd.Series(range(5),index=list(""abcde""))>>>s[""d""]=s[""b""]>>>s.rank()a    1.0b    2.5c    4.0d    2.5e    5.0dtype: float64The following example shows how the method behaves with the above
parameters:default_rank: this is the default behaviour obtained without using
any parameter.max_rank: settingmethod='max'the records that have the
same values are ranked using the highest rank (e.g.: since ecatf
and edogf are both in the 2nd and 3rd position, rank 3 is assigned.)NA_bottom: choosingna_option='bottom', if there are records
with NaN values they are placed at the bottom of the ranking.pct_rank: when settingpct=True, the ranking is expressed as
percentile rank.>>>df['default_rank']=df['Number_legs'].rank()>>>df['max_rank']=df['Number_legs'].rank(method='max')>>>df['NA_bottom']=df['Number_legs'].rank(na_option='bottom')>>>df['pct_rank']=df['Number_legs'].rank(pct=True)>>>dfAnimal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank0      cat          4.0           2.5       3.0        2.5     0.6251  penguin          2.0           1.0       1.0        1.0     0.2502      dog          4.0           2.5       3.0        2.5     0.6253   spider          8.0           4.0       4.0        4.0     1.0004    snake          NaN           NaN       NaN        5.0       NaN"
Pandas,Series,pandas.Series.sem,"pandas.Series.sem#Series.sem(axis=None,skipna=True,ddof=1,numeric_only=False,**kwargs)[source]#Return unbiased standard error of the mean over requested axis.Normalized by N-1 by default. This can be changed using the ddof argumentParameters:axis{index (0)}ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
where N represents the number of elements.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.Returns:scalar or Series (if level specified)Examples>>>s=pd.Series([1,2,3])>>>s.sem().round(6)0.57735With a DataFrame>>>df=pd.DataFrame({'a':[1,2],'b':[2,3]},index=['tiger','zebra'])>>>dfa   btiger  1   2zebra  2   3>>>df.sem()a   0.5b   0.5dtype: float64Using axis=1>>>df.sem(axis=1)tiger   0.5zebra   0.5dtype: float64In this case,numeric_onlyshould be set toTrueto avoid getting an error.>>>df=pd.DataFrame({'a':[1,2],'b':['T','Z']},...index=['tiger','zebra'])>>>df.sem(numeric_only=True)a   0.5dtype: float64"
Pandas,Series,pandas.Series.skew,"pandas.Series.skew#Series.skew(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return unbiased skew over requested axis.Normalized by N-1.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarExamples>>>s=pd.Series([1,2,3])>>>s.skew()0.0With a DataFrame>>>df=pd.DataFrame({'a':[1,2,3],'b':[2,3,4],'c':[1,3,5]},...index=['tiger','zebra','cow'])>>>dfa   b   ctiger   1   2   1zebra   2   3   3cow     3   4   5>>>df.skew()a   0.0b   0.0c   0.0dtype: float64Using axis=1>>>df.skew(axis=1)tiger   1.732051zebra  -1.732051cow     0.000000dtype: float64In this case,numeric_onlyshould be set toTrueto avoid
getting an error.>>>df=pd.DataFrame({'a':[1,2,3],'b':['T','Z','X']},...index=['tiger','zebra','cow'])>>>df.skew(numeric_only=True)a   0.0dtype: float64"
Pandas,Series,pandas.Series.std,"pandas.Series.std#Series.std(axis=None,skipna=True,ddof=1,numeric_only=False,**kwargs)[source]#Return sample standard deviation over requested axis.Normalized by N-1 by default. This can be changed using the ddof argument.Parameters:axis{index (0)}ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
where N represents the number of elements.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.Returns:scalar or Series (if level specified)NotesTo have the same behaviour asnumpy.std, useddof=0(instead of the
defaultddof=1)Examples>>>df=pd.DataFrame({'person_id':[0,1,2,3],...'age':[21,25,62,43],...'height':[1.61,1.87,1.49,2.01]}...).set_index('person_id')>>>dfage  heightperson_id0           21    1.611           25    1.872           62    1.493           43    2.01The standard deviation of the columns can be found as follows:>>>df.std()age       18.786076height     0.237417dtype: float64Alternatively,ddof=0can be set to normalize by N instead of N-1:>>>df.std(ddof=0)age       16.269219height     0.205609dtype: float64"
Pandas,Series,pandas.Series.sum,"pandas.Series.sum#Series.sum(axis=None,skipna=True,numeric_only=False,min_count=0,**kwargs)[source]#Return the sum of the values over the requested axis.This is equivalent to the methodnumpy.sum.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.min_countint, default 0The required number of valid values to perform the operation. If fewer thanmin_countnon-NA values are present the result will be NA.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.Examples>>>idx=pd.MultiIndex.from_arrays([...['warm','warm','cold','cold'],...['dog','falcon','fish','spider']],...names=['blooded','animal'])>>>s=pd.Series([4,2,0,8],name='legs',index=idx)>>>sblooded  animalwarm     dog       4falcon    2cold     fish      0spider    8Name: legs, dtype: int64>>>s.sum()14By default, the sum of an empty or all-NA Series is0.>>>pd.Series([],dtype=""float64"").sum()# min_count=0 is the default0.0This can be controlled with themin_countparameter. For example, if
youfd like the sum of an empty series to be NaN, passmin_count=1.>>>pd.Series([],dtype=""float64"").sum(min_count=1)nanThanks to theskipnaparameter,min_counthandles all-NA and
empty series identically.>>>pd.Series([np.nan]).sum()0.0>>>pd.Series([np.nan]).sum(min_count=1)nan"
Pandas,Series,pandas.Series.var,"pandas.Series.var#Series.var(axis=None,skipna=True,ddof=1,numeric_only=False,**kwargs)[source]#Return unbiased variance over requested axis.Normalized by N-1 by default. This can be changed using the ddof argument.Parameters:axis{index (0)}ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
where N represents the number of elements.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.Returns:scalar or Series (if level specified)Examples>>>df=pd.DataFrame({'person_id':[0,1,2,3],...'age':[21,25,62,43],...'height':[1.61,1.87,1.49,2.01]}...).set_index('person_id')>>>dfage  heightperson_id0           21    1.611           25    1.872           62    1.493           43    2.01>>>df.var()age       352.916667height      0.056367dtype: float64Alternatively,ddof=0can be set to normalize by N instead of N-1:>>>df.var(ddof=0)age       264.687500height      0.042275dtype: float64"
Pandas,Series,pandas.Series.kurtosis,"pandas.Series.kurtosis#Series.kurtosis(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return unbiased kurtosis over requested axis.Kurtosis obtained using Fisherfs definition of
kurtosis (kurtosis of normal == 0.0). Normalized by N-1.Parameters:axis{index (0)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:scalar or scalarExamples>>>s=pd.Series([1,2,2,3],index=['cat','dog','dog','mouse'])>>>scat    1dog    2dog    2mouse  3dtype: int64>>>s.kurt()1.5With a DataFrame>>>df=pd.DataFrame({'a':[1,2,2,3],'b':[3,4,4,4]},...index=['cat','dog','dog','mouse'])>>>dfa   bcat  1   3dog  2   4dog  2   4mouse  3   4>>>df.kurt()a   1.5b   4.0dtype: float64With axis=None>>>df.kurt(axis=None).round(6)-0.988693Using axis=1>>>df=pd.DataFrame({'a':[1,2],'b':[3,4],'c':[3,4],'d':[1,2]},...index=['cat','dog'])>>>df.kurt(axis=1)cat   -6.0dog   -6.0dtype: float64"
Pandas,Series,pandas.Series.unique,"pandas.Series.unique#Series.unique()[source]#Return unique values of Series object.Uniques are returned in order of appearance. Hash table-based unique,
therefore does NOT sort.Returns:ndarray or ExtensionArrayThe unique values returned as a NumPy array. See Notes.See alsoSeries.drop_duplicatesReturn Series with duplicate values removed.uniqueTop-level unique method for any 1-d array-like object.Index.uniqueReturn Index with unique values from an Index object.NotesReturns the unique values as a NumPy array. In case of an
extension-array backed Series, a newExtensionArrayof that type with just
the unique values is returned. This includesCategoricalPeriodDatetime with TimezoneDatetime without TimezoneTimedeltaIntervalSparseIntegerNASee Examples section.Examples>>>pd.Series([2,1,3,3],name='A').unique()array([2, 1, 3])>>>pd.Series([pd.Timestamp('2016-01-01')for_inrange(3)]).unique()<DatetimeArray>['2016-01-01 00:00:00']Length: 1, dtype: datetime64[ns]>>>pd.Series([pd.Timestamp('2016-01-01',tz='US/Eastern')...for_inrange(3)]).unique()<DatetimeArray>['2016-01-01 00:00:00-05:00']Length: 1, dtype: datetime64[ns, US/Eastern]An Categorical will return categories in the order of
appearance and with the same dtype.>>>pd.Series(pd.Categorical(list('baabc'))).unique()['b', 'a', 'c']Categories (3, object): ['a', 'b', 'c']>>>pd.Series(pd.Categorical(list('baabc'),categories=list('abc'),...ordered=True)).unique()['b', 'a', 'c']Categories (3, object): ['a' < 'b' < 'c']"
Pandas,Series,pandas.Series.nunique,"pandas.Series.nunique#Series.nunique(dropna=True)[source]#Return number of unique elements in the object.Excludes NA values by default.Parameters:dropnabool, default TrueDonft include NaN in the count.Returns:intSee alsoDataFrame.nuniqueMethod nunique for DataFrame.Series.countCount non-NA/null observations in the Series.Examples>>>s=pd.Series([1,3,5,7,7])>>>s0    11    32    53    74    7dtype: int64>>>s.nunique()4"
Pandas,Series,pandas.Series.is_unique,"pandas.Series.is_unique#propertySeries.is_unique[source]#Return boolean if values in the object are unique.Returns:boolExamples>>>s=pd.Series([1,2,3])>>>s.is_uniqueTrue>>>s=pd.Series([1,2,3,1])>>>s.is_uniqueFalse"
Pandas,Series,pandas.Series.is_monotonic_increasing,"pandas.Series.is_monotonic_increasing#propertySeries.is_monotonic_increasing[source]#Return boolean if values in the object are monotonically increasing.Returns:boolExamples>>>s=pd.Series([1,2,2])>>>s.is_monotonic_increasingTrue>>>s=pd.Series([3,2,1])>>>s.is_monotonic_increasingFalse"
Pandas,Series,pandas.Series.is_monotonic_decreasing,"pandas.Series.is_monotonic_decreasing#propertySeries.is_monotonic_decreasing[source]#Return boolean if values in the object are monotonically decreasing.Returns:boolExamples>>>s=pd.Series([3,2,2,1])>>>s.is_monotonic_decreasingTrue>>>s=pd.Series([1,2,3])>>>s.is_monotonic_decreasingFalse"
Pandas,Series,pandas.Series.value_counts,"pandas.Series.value_counts#Series.value_counts(normalize=False,sort=True,ascending=False,bins=None,dropna=True)[source]#Return a Series containing counts of unique values.The resulting object will be in descending order so that the
first element is the most frequently-occurring element.
Excludes NA values by default.Parameters:normalizebool, default FalseIf True then the object returned will contain the relative
frequencies of the unique values.sortbool, default TrueSort by frequencies when True. Preserve the order of the data when False.ascendingbool, default FalseSort in ascending order.binsint, optionalRather than count values, group them into half-open bins,
a convenience forpd.cut, only works with numeric data.dropnabool, default TrueDonft include counts of NaN.Returns:SeriesSee alsoSeries.countNumber of non-NA elements in a Series.DataFrame.countNumber of non-NA elements in a DataFrame.DataFrame.value_countsEquivalent method on DataFrames.Examples>>>index=pd.Index([3,1,2,3,4,np.nan])>>>index.value_counts()3.0    21.0    12.0    14.0    1Name: count, dtype: int64Withnormalizeset toTrue, returns the relative frequency by
dividing all values by the sum of values.>>>s=pd.Series([3,1,2,3,4,np.nan])>>>s.value_counts(normalize=True)3.0    0.41.0    0.22.0    0.24.0    0.2Name: proportion, dtype: float64binsBins can be useful for going from a continuous variable to a
categorical variable; instead of counting unique
apparitions of values, divide the index in the specified
number of half-open bins.>>>s.value_counts(bins=3)(0.996, 2.0]    2(2.0, 3.0]      2(3.0, 4.0]      1Name: count, dtype: int64dropnaWithdropnaset toFalsewe can also see NaN index values.>>>s.value_counts(dropna=False)3.0    21.0    12.0    14.0    1NaN    1Name: count, dtype: int64"
Pandas,Series,pandas.Series.align,"pandas.Series.align#Series.align(other,join='outer',axis=None,level=None,copy=None,fill_value=None,method=_NoDefault.no_default,limit=_NoDefault.no_default,fill_axis=_NoDefault.no_default,broadcast_axis=_NoDefault.no_default)[source]#Align two objects on their axes with the specified join method.Join method is specified for each axis Index.Parameters:otherDataFrame or Seriesjoin{eouterf, einnerf, eleftf, erightf}, default eouterfType of alignment to be performed.left: use only keys from left frame, preserve key order.right: use only keys from right frame, preserve key order.outer: use union of keys from both frames, sort keys lexicographically.inner: use intersection of keys from both frames,
preserve the order of the left keys.axisallowed axis of the other object, default NoneAlign on index (0), columns (1), or both (None).levelint or level name, default NoneBroadcast across a level, matching Index values on the
passed MultiIndex level.copybool, default TrueAlways returns new objects. If copy=False and no reindexing is
required then original objects are returned.fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any
gcompatibleh value.method{ebackfillf, ebfillf, epadf, effillf, None}, default NoneMethod to use for filling holes in reindexed Series:pad / ffill: propagate last valid observation forward to next valid.backfill / bfill: use NEXT valid observation to fill gap.Deprecated since version 2.1.limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.Deprecated since version 2.1.fill_axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrame, default 0Filling axis, method and limit.Deprecated since version 2.1.broadcast_axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrame, default NoneBroadcast values along this axis, if aligning two objects of
different dimensions.Deprecated since version 2.1.Returns:tuple of (Series/DataFrame, type of other)Aligned objects.Examples>>>df=pd.DataFrame(...[[1,2,3,4],[6,7,8,9]],columns=[""D"",""B"",""E"",""A""],index=[1,2]...)>>>other=pd.DataFrame(...[[10,20,30,40],[60,70,80,90],[600,700,800,900]],...columns=[""A"",""B"",""C"",""D""],...index=[2,3,4],...)>>>dfD  B  E  A1  1  2  3  42  6  7  8  9>>>otherA    B    C    D2   10   20   30   403   60   70   80   904  600  700  800  900Align on columns:>>>left,right=df.align(other,join=""outer"",axis=1)>>>leftA  B   C  D  E1  4  2 NaN  1  32  9  7 NaN  6  8>>>rightA    B    C    D   E2   10   20   30   40 NaN3   60   70   80   90 NaN4  600  700  800  900 NaNWe can also align on the index:>>>left,right=df.align(other,join=""outer"",axis=0)>>>leftD    B    E    A1  1.0  2.0  3.0  4.02  6.0  7.0  8.0  9.03  NaN  NaN  NaN  NaN4  NaN  NaN  NaN  NaN>>>rightA      B      C      D1    NaN    NaN    NaN    NaN2   10.0   20.0   30.0   40.03   60.0   70.0   80.0   90.04  600.0  700.0  800.0  900.0Finally, the defaultaxis=Nonewill align on both index and columns:>>>left,right=df.align(other,join=""outer"",axis=None)>>>leftA    B   C    D    E1  4.0  2.0 NaN  1.0  3.02  9.0  7.0 NaN  6.0  8.03  NaN  NaN NaN  NaN  NaN4  NaN  NaN NaN  NaN  NaN>>>rightA      B      C      D   E1    NaN    NaN    NaN    NaN NaN2   10.0   20.0   30.0   40.0 NaN3   60.0   70.0   80.0   90.0 NaN4  600.0  700.0  800.0  900.0 NaN"
Pandas,Series,pandas.Series.drop,"pandas.Series.drop#Series.drop(labels=None,*,axis=0,index=None,columns=None,level=None,inplace=False,errors='raise')[source]#Return Series with specified index labels removed.Remove elements of a Series based on specifying the index labels.
When using a multi-index, labels on different levels can be removed
by specifying the level.Parameters:labelssingle label or list-likeIndex labels to drop.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.indexsingle label or list-likeRedundant for application on Series, but eindexf can be used instead
of elabelsf.columnssingle label or list-likeNo change is made to the Series; use eindexf or elabelsf instead.levelint or level name, optionalFor MultiIndex, level for which the labels will be removed.inplacebool, default FalseIf True, do operation inplace and return None.errors{eignoref, eraisef}, default eraisefIf eignoref, suppress error and only existing labels are dropped.Returns:Series or NoneSeries with specified index labels removed or None ifinplace=True.Raises:KeyErrorIf none of the labels are found in the index.See alsoSeries.reindexReturn only specified index labels of Series.Series.dropnaReturn series without null values.Series.drop_duplicatesReturn Series with duplicate values removed.DataFrame.dropDrop specified labels from rows or columns.Examples>>>s=pd.Series(data=np.arange(3),index=['A','B','C'])>>>sA  0B  1C  2dtype: int64Drop labels B en C>>>s.drop(labels=['B','C'])A  0dtype: int64Drop 2nd level label in MultiIndex Series>>>midx=pd.MultiIndex(levels=[['llama','cow','falcon'],...['speed','weight','length']],...codes=[[0,0,0,1,1,1,2,2,2],...[0,1,2,0,1,2,0,1,2]])>>>s=pd.Series([45,200,1.2,30,250,1.5,320,1,0.3],...index=midx)>>>sllama   speed      45.0weight    200.0length      1.2cow     speed      30.0weight    250.0length      1.5falcon  speed     320.0weight      1.0length      0.3dtype: float64>>>s.drop(labels='weight',level=1)llama   speed      45.0length      1.2cow     speed      30.0length      1.5falcon  speed     320.0length      0.3dtype: float64"
Pandas,Series,pandas.Series.droplevel,"pandas.Series.droplevel#Series.droplevel(level,axis=0)[source]#Return Series/DataFrame with requested index / column level(s) removed.Parameters:levelint, str, or list-likeIf a string is given, must be the name of a level
If list-like, elements must be names or positional indexes
of levels.axis{0 or eindexf, 1 or ecolumnsf}, default 0Axis along which the level(s) is removed:0 or eindexf: remove level(s) in column.1 or ecolumnsf: remove level(s) in row.ForSeriesthis parameter is unused and defaults to 0.Returns:Series/DataFrameSeries/DataFrame with requested index / column level(s) removed.Examples>>>df=pd.DataFrame([...[1,2,3,4],...[5,6,7,8],...[9,10,11,12]...]).set_index([0,1]).rename_axis(['a','b'])>>>df.columns=pd.MultiIndex.from_tuples([...('c','e'),('d','f')...],names=['level_1','level_2'])>>>dflevel_1   c   dlevel_2   e   fa b1 2      3   45 6      7   89 10    11  12>>>df.droplevel('a')level_1   c   dlevel_2   e   fb2        3   46        7   810      11  12>>>df.droplevel('level_2',axis=1)level_1   c   da b1 2      3   45 6      7   89 10    11  12"
Pandas,Series,pandas.Series.drop_duplicates,"pandas.Series.drop_duplicates#Series.drop_duplicates(*,keep='first',inplace=False,ignore_index=False)[source]#Return Series with duplicate values removed.Parameters:keep{efirstf, elastf,False}, default efirstfMethod to handle dropping duplicates:efirstf : Drop duplicates except for the first occurrence.elastf : Drop duplicates except for the last occurrence.False: Drop all duplicates.inplacebool, defaultFalseIfTrue, performs operation inplace and returns None.ignore_indexbool, defaultFalseIfTrue, the resulting axis will be labeled 0, 1, c, n - 1.New in version 2.0.0.Returns:Series or NoneSeries with duplicates dropped or None ifinplace=True.See alsoIndex.drop_duplicatesEquivalent method on Index.DataFrame.drop_duplicatesEquivalent method on DataFrame.Series.duplicatedRelated method on Series, indicating duplicate Series values.Series.uniqueReturn unique values as an array.ExamplesGenerate a Series with duplicated entries.>>>s=pd.Series(['llama','cow','llama','beetle','llama','hippo'],...name='animal')>>>s0     llama1       cow2     llama3    beetle4     llama5     hippoName: animal, dtype: objectWith the ekeepf parameter, the selection behaviour of duplicated values
can be changed. The value efirstf keeps the first occurrence for each
set of duplicated entries. The default value of keep is efirstf.>>>s.drop_duplicates()0     llama1       cow3    beetle5     hippoName: animal, dtype: objectThe value elastf for parameter ekeepf keeps the last occurrence for
each set of duplicated entries.>>>s.drop_duplicates(keep='last')1       cow3    beetle4     llama5     hippoName: animal, dtype: objectThe valueFalsefor parameter ekeepf discards all sets of
duplicated entries.>>>s.drop_duplicates(keep=False)1       cow3    beetle5     hippoName: animal, dtype: object"
Pandas,Series,pandas.Series.duplicated,"pandas.Series.duplicated#Series.duplicated(keep='first')[source]#Indicate duplicate Series values.Duplicated values are indicated asTruevalues in the resulting
Series. Either all duplicates, all except the first or all except the
last occurrence of duplicates can be indicated.Parameters:keep{efirstf, elastf, False}, default efirstfMethod to handle dropping duplicates:efirstf : Mark duplicates asTrueexcept for the first
occurrence.elastf : Mark duplicates asTrueexcept for the last
occurrence.False: Mark all duplicates asTrue.Returns:Series[bool]Series indicating whether each value has occurred in the
preceding values.See alsoIndex.duplicatedEquivalent method on pandas.Index.DataFrame.duplicatedEquivalent method on pandas.DataFrame.Series.drop_duplicatesRemove duplicate values from Series.ExamplesBy default, for each set of duplicated values, the first occurrence is
set on False and all others on True:>>>animals=pd.Series(['llama','cow','llama','beetle','llama'])>>>animals.duplicated()0    False1    False2     True3    False4     Truedtype: boolwhich is equivalent to>>>animals.duplicated(keep='first')0    False1    False2     True3    False4     Truedtype: boolBy using elastf, the last occurrence of each set of duplicated values
is set on False and all others on True:>>>animals.duplicated(keep='last')0     True1    False2     True3    False4    Falsedtype: boolBy setting keep onFalse, all duplicates are True:>>>animals.duplicated(keep=False)0     True1    False2     True3    False4     Truedtype: bool"
Pandas,Series,pandas.Series.equals,"pandas.Series.equals#Series.equals(other)[source]#Test whether two objects contain the same elements.This function allows two Series or DataFrames to be compared against
each other to see if they have the same shape and elements. NaNs in
the same location are considered equal.The row/column index do not need to have the same type, as long
as the values are considered equal. Corresponding columns must be of
the same dtype.Parameters:otherSeries or DataFrameThe other Series or DataFrame to be compared with the first.Returns:boolTrue if all elements are the same in both objects, False
otherwise.See alsoSeries.eqCompare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise.DataFrame.eqCompare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise.testing.assert_series_equalRaises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others.testing.assert_frame_equalLike assert_series_equal, but targets DataFrames.numpy.array_equalReturn True if two arrays have the same shape and elements, False otherwise.Examples>>>df=pd.DataFrame({1:[10],2:[20]})>>>df1   20  10  20DataFrames df and exactly_equal have the same types and values for
their elements and column labels, which will return True.>>>exactly_equal=pd.DataFrame({1:[10],2:[20]})>>>exactly_equal1   20  10  20>>>df.equals(exactly_equal)TrueDataFrames df and different_column_type have the same element
types and values, but have different types for the column labels,
which will still return True.>>>different_column_type=pd.DataFrame({1.0:[10],2.0:[20]})>>>different_column_type1.0  2.00   10   20>>>df.equals(different_column_type)TrueDataFrames df and different_data_type have different types for the
same values for their elements, and will return False even though
their column labels are the same values and types.>>>different_data_type=pd.DataFrame({1:[10.0],2:[20.0]})>>>different_data_type1     20  10.0  20.0>>>df.equals(different_data_type)False"
Pandas,Series,pandas.Series.first,"pandas.Series.first#Series.first(offset)[source]#Select initial periods of time series data based on a date offset.Deprecated since version 2.1:first()is deprecated and will be removed in a future version.
Please create a mask and filter using.locinstead.For a DataFrame with a sorted DatetimeIndex, this function can
select the first few rows based on a date offset.Parameters:offsetstr, DateOffset or dateutil.relativedeltaThe offset length of the data that will be selected. For instance,
e1Mf will display all the rows having their index within the first month.Returns:Series or DataFrameA subset of the caller.Raises:TypeErrorIf the index is not aDatetimeIndexSee alsolastSelect final periods of time series based on a date offset.at_timeSelect values at a particular time of the day.between_timeSelect values between particular times of the day.Examples>>>i=pd.date_range('2018-04-09',periods=4,freq='2D')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09  12018-04-11  22018-04-13  32018-04-15  4Get the rows for the first 3 days:>>>ts.first('3D')A2018-04-09  12018-04-11  2Notice the data for 3 first calendar days were returned, not the first
3 days observed in the dataset, and therefore data for 2018-04-13 was
not returned."
Pandas,Series,pandas.Series.head,"pandas.Series.head#Series.head(n=5)[source]#Return the firstnrows.This function returns the firstnrows for the object based
on position. It is useful for quickly testing if your object
has the right type of data in it.For negative values ofn, this function returns all rows except
the last|n|rows, equivalent todf[:n].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:same type as callerThe firstnrows of the caller object.See alsoDataFrame.tailReturns the lastnrows.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the first 5 lines>>>df.head()animal0  alligator1        bee2     falcon3       lion4     monkeyViewing the firstnlines (three in this case)>>>df.head(3)animal0  alligator1        bee2     falconFor negative values ofn>>>df.head(-3)animal0  alligator1        bee2     falcon3       lion4     monkey5     parrot"
Pandas,Series,pandas.Series.idxmax,"pandas.Series.idxmax#Series.idxmax(axis=0,skipna=True,*args,**kwargs)[source]#Return the row label of the maximum value.If multiple values equal the maximum, the first row label with that
value is returned.Parameters:axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.skipnabool, default TrueExclude NA/null values. If the entire Series is NA, the result
will be NA.*args, **kwargsAdditional arguments and keywords have no effect but might be
accepted for compatibility with NumPy.Returns:IndexLabel of the maximum value.Raises:ValueErrorIf the Series is empty.See alsonumpy.argmaxReturn indices of the maximum values along the given axis.DataFrame.idxmaxReturn index of first occurrence of maximum over requested axis.Series.idxminReturn indexlabelof the first occurrence of minimum of values.NotesThis method is the Series version ofndarray.argmax. This method
returns the label of the maximum, whilendarray.argmaxreturns
the position. To get the position, useseries.values.argmax().Examples>>>s=pd.Series(data=[1,None,4,3,4],...index=['A','B','C','D','E'])>>>sA    1.0B    NaNC    4.0D    3.0E    4.0dtype: float64>>>s.idxmax()'C'Ifskipnais False and there is an NA value in the data,
the function returnsnan.>>>s.idxmax(skipna=False)nan"
Pandas,Series,pandas.Series.idxmin,"pandas.Series.idxmin#Series.idxmin(axis=0,skipna=True,*args,**kwargs)[source]#Return the row label of the minimum value.If multiple values equal the minimum, the first row label with that
value is returned.Parameters:axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.skipnabool, default TrueExclude NA/null values. If the entire Series is NA, the result
will be NA.*args, **kwargsAdditional arguments and keywords have no effect but might be
accepted for compatibility with NumPy.Returns:IndexLabel of the minimum value.Raises:ValueErrorIf the Series is empty.See alsonumpy.argminReturn indices of the minimum values along the given axis.DataFrame.idxminReturn index of first occurrence of minimum over requested axis.Series.idxmaxReturn indexlabelof the first occurrence of maximum of values.NotesThis method is the Series version ofndarray.argmin. This method
returns the label of the minimum, whilendarray.argminreturns
the position. To get the position, useseries.values.argmin().Examples>>>s=pd.Series(data=[1,None,4,1],...index=['A','B','C','D'])>>>sA    1.0B    NaNC    4.0D    1.0dtype: float64>>>s.idxmin()'A'Ifskipnais False and there is an NA value in the data,
the function returnsnan.>>>s.idxmin(skipna=False)nan"
Pandas,Series,pandas.Series.isin,"pandas.Series.isin#Series.isin(values)[source]#Whether elements in Series are contained invalues.Return a boolean Series showing whether each element in the Series
matches an element in the passed sequence ofvaluesexactly.Parameters:valuesset or list-likeThe sequence of values to test. Passing in a single string will
raise aTypeError. Instead, turn a single string into a
list of one element.Returns:SeriesSeries of booleans indicating if each element is in values.Raises:TypeErrorIfvaluesis a stringSee alsoDataFrame.isinEquivalent method on DataFrame.Examples>>>s=pd.Series(['llama','cow','llama','beetle','llama',...'hippo'],name='animal')>>>s.isin(['cow','llama'])0     True1     True2     True3    False4     True5    FalseName: animal, dtype: boolTo invert the boolean values, use the~operator:>>>~s.isin(['cow','llama'])0    False1    False2    False3     True4    False5     TrueName: animal, dtype: boolPassing a single string ass.isin('llama')will raise an error. Use
a list of one element instead:>>>s.isin(['llama'])0     True1    False2     True3    False4     True5    FalseName: animal, dtype: boolStrings and integers are distinct and are therefore not comparable:>>>pd.Series([1]).isin(['1'])0    Falsedtype: bool>>>pd.Series([1.1]).isin(['1.1'])0    Falsedtype: bool"
Pandas,Series,pandas.Series.last,"pandas.Series.last#Series.last(offset)[source]#Select final periods of time series data based on a date offset.Deprecated since version 2.1:last()is deprecated and will be removed in a future version.
Please create a mask and filter using.locinstead.For a DataFrame with a sorted DatetimeIndex, this function
selects the last few rows based on a date offset.Parameters:offsetstr, DateOffset, dateutil.relativedeltaThe offset length of the data that will be selected. For instance,
e3Df will display all the rows having their index within the last 3 days.Returns:Series or DataFrameA subset of the caller.Raises:TypeErrorIf the index is not aDatetimeIndexSee alsofirstSelect initial periods of time series based on a date offset.at_timeSelect values at a particular time of the day.between_timeSelect values between particular times of the day.NotesDeprecated since version 2.1.0:Please create a mask and filter using.locinsteadExamples>>>i=pd.date_range('2018-04-09',periods=4,freq='2D')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09  12018-04-11  22018-04-13  32018-04-15  4Get the rows for the last 3 days:>>>ts.last('3D')A2018-04-13  32018-04-15  4Notice the data for 3 last calendar days were returned, not the last
3 observed days in the dataset, and therefore data for 2018-04-11 was
not returned."
Pandas,Series,pandas.Series.reindex,"pandas.Series.reindex#Series.reindex(index=None,*,axis=None,method=None,copy=None,level=None,fill_value=None,limit=None,tolerance=None)[source]#Conform Series to new index with optional filling logic.Places NA/NaN in locations having no value in the previous index. A new object
is produced unless the new index is equivalent to the current one andcopy=False.Parameters:indexarray-like, optionalNew labels for the index. Preferably an Index object to avoid
duplicating data.axisint or str, optionalUnused.method{None, ebackfillf/fbfillf, epadf/fffillf, enearestf}Method to use for filling holes in reindexed DataFrame.
Please note: this is only applicable to DataFrames/Series with a
monotonically increasing/decreasing index.None (default): donft fill gapspad / ffill: Propagate last valid observation forward to next
valid.backfill / bfill: Use next valid observation to fill gap.nearest: Use nearest valid observations to fill gap.copybool, default TrueReturn a new object, even if the passed indexes are the same.levelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any
gcompatibleh value.limitint, default NoneMaximum number of consecutive elements to forward or backward fill.toleranceoptionalMaximum distance between original and new labels for inexact
matches. The values of the index at the matching locations most
satisfy the equationabs(index[indexer]-target)<=tolerance.Tolerance may be a scalar value, which applies the same tolerance
to all values, or list-like, which applies variable tolerance per
element. List-like includes list, tuple, array, Series, and must be
the same size as the index and its dtype must exactly match the
indexfs type.Returns:Series with changed index.See alsoDataFrame.set_indexSet row labels.DataFrame.reset_indexRemove row labels or move them to new columns.DataFrame.reindex_likeChange to same indices as other DataFrame.ExamplesDataFrame.reindexsupports two calling conventions(index=index_labels,columns=column_labels,...)(labels,axis={'index','columns'},...)Wehighlyrecommend using keyword arguments to clarify your
intent.Create a dataframe with some fictional data.>>>index=['Firefox','Chrome','Safari','IE10','Konqueror']>>>df=pd.DataFrame({'http_status':[200,200,404,404,301],...'response_time':[0.04,0.02,0.07,0.08,1.0]},...index=index)>>>dfhttp_status  response_timeFirefox            200           0.04Chrome             200           0.02Safari             404           0.07IE10               404           0.08Konqueror          301           1.00Create a new index and reindex the dataframe. By default
values in the new index that do not have corresponding
records in the dataframe are assignedNaN.>>>new_index=['Safari','Iceweasel','Comodo Dragon','IE10',...'Chrome']>>>df.reindex(new_index)http_status  response_timeSafari               404.0           0.07Iceweasel              NaN            NaNComodo Dragon          NaN            NaNIE10                 404.0           0.08Chrome               200.0           0.02We can fill in the missing values by passing a value to
the keywordfill_value. Because the index is not monotonically
increasing or decreasing, we cannot use arguments to the keywordmethodto fill theNaNvalues.>>>df.reindex(new_index,fill_value=0)http_status  response_timeSafari                 404           0.07Iceweasel                0           0.00Comodo Dragon            0           0.00IE10                   404           0.08Chrome                 200           0.02>>>df.reindex(new_index,fill_value='missing')http_status response_timeSafari                404          0.07Iceweasel         missing       missingComodo Dragon     missing       missingIE10                  404          0.08Chrome                200          0.02We can also reindex the columns.>>>df.reindex(columns=['http_status','user_agent'])http_status  user_agentFirefox            200         NaNChrome             200         NaNSafari             404         NaNIE10               404         NaNKonqueror          301         NaNOr we can use gaxis-styleh keyword arguments>>>df.reindex(['http_status','user_agent'],axis=""columns"")http_status  user_agentFirefox            200         NaNChrome             200         NaNSafari             404         NaNIE10               404         NaNKonqueror          301         NaNTo further illustrate the filling functionality inreindex, we will create a dataframe with a
monotonically increasing index (for example, a sequence
of dates).>>>date_index=pd.date_range('1/1/2010',periods=6,freq='D')>>>df2=pd.DataFrame({""prices"":[100,101,np.nan,100,89,88]},...index=date_index)>>>df2prices2010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.0Suppose we decide to expand the dataframe to cover a wider
date range.>>>date_index2=pd.date_range('12/29/2009',periods=10,freq='D')>>>df2.reindex(date_index2)prices2009-12-29     NaN2009-12-30     NaN2009-12-31     NaN2010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.02010-01-07     NaNThe index entries that did not have a value in the original data frame
(for example, e2009-12-29f) are by default filled withNaN.
If desired, we can fill in the missing values using one of several
options.For example, to back-propagate the last valid value to fill theNaNvalues, passbfillas an argument to themethodkeyword.>>>df2.reindex(date_index2,method='bfill')prices2009-12-29   100.02009-12-30   100.02009-12-31   100.02010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.02010-01-07     NaNPlease note that theNaNvalue present in the original dataframe
(at index value 2010-01-03) will not be filled by any of the
value propagation schemes. This is because filling while reindexing
does not look at dataframe values, but only compares the original and
desired indexes. If you do want to fill in theNaNvalues present
in the original dataframe, use thefillna()method.See theuser guidefor more."
Pandas,Series,pandas.Series.reindex_like,"pandas.Series.reindex_like#Series.reindex_like(other,method=None,copy=None,limit=None,tolerance=None)[source]#Return an object with matching indices as other object.Conform the object to the same index on all axes. Optional
filling logic, placing NaN in locations having no value
in the previous index. A new object is produced unless the
new index is equivalent to the current one and copy=False.Parameters:otherObject of the same data typeIts row and column indices are used to define the new indices
of this object.method{None, ebackfillf/fbfillf, epadf/fffillf, enearestf}Method to use for filling holes in reindexed DataFrame.
Please note: this is only applicable to DataFrames/Series with a
monotonically increasing/decreasing index.None (default): donft fill gapspad / ffill: propagate last valid observation forward to next
validbackfill / bfill: use next valid observation to fill gapnearest: use nearest valid observations to fill gap.copybool, default TrueReturn a new object, even if the passed indexes are the same.limitint, default NoneMaximum number of consecutive labels to fill for inexact matches.toleranceoptionalMaximum distance between original and new labels for inexact
matches. The values of the index at the matching locations must
satisfy the equationabs(index[indexer]-target)<=tolerance.Tolerance may be a scalar value, which applies the same tolerance
to all values, or list-like, which applies variable tolerance per
element. List-like includes list, tuple, array, Series, and must be
the same size as the index and its dtype must exactly match the
indexfs type.Returns:Series or DataFrameSame type as caller, but with changed indices on each axis.See alsoDataFrame.set_indexSet row labels.DataFrame.reset_indexRemove row labels or move them to new columns.DataFrame.reindexChange to new indices or expand indices.NotesSame as calling.reindex(index=other.index,columns=other.columns,...).Examples>>>df1=pd.DataFrame([[24.3,75.7,'high'],...[31,87.8,'high'],...[22,71.6,'medium'],...[35,95,'medium']],...columns=['temp_celsius','temp_fahrenheit',...'windspeed'],...index=pd.date_range(start='2014-02-12',...end='2014-02-15',freq='D'))>>>df1temp_celsius  temp_fahrenheit windspeed2014-02-12          24.3             75.7      high2014-02-13          31.0             87.8      high2014-02-14          22.0             71.6    medium2014-02-15          35.0             95.0    medium>>>df2=pd.DataFrame([[28,'low'],...[30,'low'],...[35.1,'medium']],...columns=['temp_celsius','windspeed'],...index=pd.DatetimeIndex(['2014-02-12','2014-02-13',...'2014-02-15']))>>>df2temp_celsius windspeed2014-02-12          28.0       low2014-02-13          30.0       low2014-02-15          35.1    medium>>>df2.reindex_like(df1)temp_celsius  temp_fahrenheit windspeed2014-02-12          28.0              NaN       low2014-02-13          30.0              NaN       low2014-02-14           NaN              NaN       NaN2014-02-15          35.1              NaN    medium"
Pandas,Series,pandas.Series.rename,"pandas.Series.rename#Series.rename(index=None,*,axis=None,copy=None,inplace=False,level=None,errors='ignore')[source]#Alter Series index labels or name.Function / dict values must be unique (1-to-1). Labels not contained in
a dict / Series will be left as-is. Extra labels listed donft throw an
error.Alternatively, changeSeries.namewith a scalar value.See theuser guidefor more.Parameters:indexscalar, hashable sequence, dict-like or function optionalFunctions or dict-like are transformations to apply to
the index.
Scalar or hashable sequence-like will alter theSeries.nameattribute.axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.copybool, default TrueAlso copy underlying data.inplacebool, default FalseWhether to return a new Series. If True the value of copy is ignored.levelint or level name, default NoneIn case of MultiIndex, only rename labels in the specified level.errors{eignoref, eraisef}, default eignorefIf eraisef, raiseKeyErrorwhen adict-like mapperorindexcontains labels that are not present in the index being transformed.
If eignoref, existing keys will be renamed and extra keys will be ignored.Returns:Series or NoneSeries with index labels or name altered or None ifinplace=True.See alsoDataFrame.renameCorresponding DataFrame method.Series.rename_axisSet the name of the axis.Examples>>>s=pd.Series([1,2,3])>>>s0    11    22    3dtype: int64>>>s.rename(""my_name"")# scalar, changes Series.name0    11    22    3Name: my_name, dtype: int64>>>s.rename(lambdax:x**2)# function, changes labels0    11    24    3dtype: int64>>>s.rename({1:3,2:5})# mapping, changes labels0    13    25    3dtype: int64"
Pandas,Series,pandas.Series.rename_axis,"pandas.Series.rename_axis#Series.rename_axis(mapper=_NoDefault.no_default,*,index=_NoDefault.no_default,axis=0,copy=True,inplace=False)[source]#Set the name of the axis for the index or columns.Parameters:mapperscalar, list-like, optionalValue to set the axis name attribute.index, columnsscalar, list-like, dict-like or function, optionalA scalar, list-like, dict-like or functions transformations to
apply to that axisf values.
Note that thecolumnsparameter is not allowed if the
object is a Series. This parameter only apply for DataFrame
type objects.Use eithermapperandaxisto
specify the axis to target withmapper, orindexand/orcolumns.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to rename. ForSeriesthis parameter is unused and defaults to 0.copybool, default NoneAlso copy underlying data.inplacebool, default FalseModifies the object directly, instead of creating a new Series
or DataFrame.Returns:Series, DataFrame, or NoneThe same type as the caller or None ifinplace=True.See alsoSeries.renameAlter Series index labels or name.DataFrame.renameAlter DataFrame index labels or name.Index.renameSet new names on index.NotesDataFrame.rename_axissupports two calling conventions(index=index_mapper,columns=columns_mapper,...)(mapper,axis={'index','columns'},...)The first calling convention will only modify the names of
the index and/or the names of the Index object that is the columns.
In this case, the parametercopyis ignored.The second calling convention will modify the names of the
corresponding index if mapper is a list or a scalar.
However, if mapper is dict-like or a function, it will use the
deprecated behavior of modifying the axislabels.Wehighlyrecommend using keyword arguments to clarify your
intent.ExamplesSeries>>>s=pd.Series([""dog"",""cat"",""monkey""])>>>s0       dog1       cat2    monkeydtype: object>>>s.rename_axis(""animal"")animal0    dog1    cat2    monkeydtype: objectDataFrame>>>df=pd.DataFrame({""num_legs"":[4,4,2],...""num_arms"":[0,0,2]},...[""dog"",""cat"",""monkey""])>>>dfnum_legs  num_armsdog            4         0cat            4         0monkey         2         2>>>df=df.rename_axis(""animal"")>>>dfnum_legs  num_armsanimaldog            4         0cat            4         0monkey         2         2>>>df=df.rename_axis(""limbs"",axis=""columns"")>>>dflimbs   num_legs  num_armsanimaldog            4         0cat            4         0monkey         2         2MultiIndex>>>df.index=pd.MultiIndex.from_product([['mammal'],...['dog','cat','monkey']],...names=['type','name'])>>>dflimbs          num_legs  num_armstype   namemammal dog            4         0cat            4         0monkey         2         2>>>df.rename_axis(index={'type':'class'})limbs          num_legs  num_armsclass  namemammal dog            4         0cat            4         0monkey         2         2>>>df.rename_axis(columns=str.upper)LIMBS          num_legs  num_armstype   namemammal dog            4         0cat            4         0monkey         2         2"
Pandas,Series,pandas.Series.reset_index,"pandas.Series.reset_index#Series.reset_index(level=None,*,drop=False,name=_NoDefault.no_default,inplace=False,allow_duplicates=False)[source]#Generate a new DataFrame or Series with the index reset.This is useful when the index needs to be treated as a column, or
when the index is meaningless and needs to be reset to the default
before another operation.Parameters:levelint, str, tuple, or list, default optionalFor a Series with a MultiIndex, only remove the specified levels
from the index. Removes all levels by default.dropbool, default FalseJust reset the index, without inserting it as a column in
the new DataFrame.nameobject, optionalThe name to use for the column containing the original Series
values. Usesself.nameby default. This argument is ignored
whendropis True.inplacebool, default FalseModify the Series in place (do not create a new object).allow_duplicatesbool, default FalseAllow duplicate column labels to be created.New in version 1.5.0.Returns:Series or DataFrame or NoneWhendropis False (the default), a DataFrame is returned.
The newly created columns will come first in the DataFrame,
followed by the original Series values.
Whendropis True, aSeriesis returned.
In either case, ifinplace=True, no value is returned.See alsoDataFrame.reset_indexAnalogous function for DataFrame.Examples>>>s=pd.Series([1,2,3,4],name='foo',...index=pd.Index(['a','b','c','d'],name='idx'))Generate a DataFrame with default index.>>>s.reset_index()idx  foo0   a    11   b    22   c    33   d    4To specify the name of the new column usename.>>>s.reset_index(name='values')idx  values0   a       11   b       22   c       33   d       4To generate a new Series with the default setdropto True.>>>s.reset_index(drop=True)0    11    22    33    4Name: foo, dtype: int64Thelevelparameter is interesting for Series with a multi-level
index.>>>arrays=[np.array(['bar','bar','baz','baz']),...np.array(['one','two','one','two'])]>>>s2=pd.Series(...range(4),name='foo',...index=pd.MultiIndex.from_arrays(arrays,...names=['a','b']))To remove a specific level from the Index, uselevel.>>>s2.reset_index(level='a')a  foobone  bar    0two  bar    1one  baz    2two  baz    3Iflevelis not set, all levels are removed from the Index.>>>s2.reset_index()a    b  foo0  bar  one    01  bar  two    12  baz  one    23  baz  two    3"
Pandas,Series,pandas.Series.sample,"pandas.Series.sample#Series.sample(n=None,frac=None,replace=False,weights=None,random_state=None,axis=None,ignore_index=False)[source]#Return a random sample of items from an axis of object.You can userandom_statefor reproducibility.Parameters:nint, optionalNumber of items from axis to return. Cannot be used withfrac.
Default = 1 iffrac= None.fracfloat, optionalFraction of axis items to return. Cannot be used withn.replacebool, default FalseAllow or disallow sampling of the same row more than once.weightsstr or ndarray-like, optionalDefault eNonef results in equal probability weighting.
If passed a Series, will align with target object on index. Index
values in weights not found in sampled object will be ignored and
index values in sampled object not in weights will be assigned
weights of zero.
If called on a DataFrame, will accept the name of a column
when axis = 0.
Unless weights are a Series, weights must be same length as axis
being sampled.
If weights do not sum to 1, they will be normalized to sum to 1.
Missing values in the weights column will be treated as zero.
Infinite values not allowed.random_stateint, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optionalIf int, array-like, or BitGenerator, seed for random number generator.
If np.random.RandomState or np.random.Generator, use as given.Changed in version 1.4.0:np.random.Generator objects now acceptedaxis{0 or eindexf, 1 or ecolumnsf, None}, default NoneAxis to sample. Accepts axis number or name. Default is stat axis
for given data type. ForSeriesthis parameter is unused and defaults toNone.ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, c, n - 1.New in version 1.3.0.Returns:Series or DataFrameA new object of same type as caller containingnitems randomly
sampled from the caller object.See alsoDataFrameGroupBy.sampleGenerates random samples from each group of a DataFrame object.SeriesGroupBy.sampleGenerates random samples from each group of a Series object.numpy.random.choiceGenerates a random sample from a given 1-D numpy array.NotesIffrac> 1,replacementshould be set toTrue.Examples>>>df=pd.DataFrame({'num_legs':[2,4,8,0],...'num_wings':[2,0,0,0],...'num_specimen_seen':[10,2,1,8]},...index=['falcon','dog','spider','fish'])>>>dfnum_legs  num_wings  num_specimen_seenfalcon         2          2                 10dog            4          0                  2spider         8          0                  1fish           0          0                  8Extract 3 random elements from theSeriesdf['num_legs']:
Note that we userandom_stateto ensure the reproducibility of
the examples.>>>df['num_legs'].sample(n=3,random_state=1)fish      0spider    8falcon    2Name: num_legs, dtype: int64A random 50% sample of theDataFramewith replacement:>>>df.sample(frac=0.5,replace=True,random_state=1)num_legs  num_wings  num_specimen_seendog          4          0                  2fish         0          0                  8An upsample sample of theDataFramewith replacement:
Note thatreplaceparameter has to beTrueforfracparameter > 1.>>>df.sample(frac=2,replace=True,random_state=1)num_legs  num_wings  num_specimen_seendog            4          0                  2fish           0          0                  8falcon         2          2                 10falcon         2          2                 10fish           0          0                  8dog            4          0                  2fish           0          0                  8dog            4          0                  2Using a DataFrame column as weights. Rows with larger value in thenum_specimen_seencolumn are more likely to be sampled.>>>df.sample(n=2,weights='num_specimen_seen',random_state=1)num_legs  num_wings  num_specimen_seenfalcon         2          2                 10fish           0          0                  8"
Pandas,Series,pandas.Series.set_axis,"pandas.Series.set_axis#Series.set_axis(labels,*,axis=0,copy=None)[source]#Assign desired index to given axis.Indexes for row labels can be changed by assigning
a list-like or Index.Parameters:labelslist-like, IndexThe values for the new index.axis{0 or eindexf}, default 0The axis to update. The value 0 identifies the rows. ForSeriesthis parameter is unused and defaults to 0.copybool, default TrueWhether to make a copy of the underlying data.New in version 1.5.0.Returns:SeriesAn object of type Series.See alsoSeries.rename_axisAlter the name of the index.Examples>>>s=pd.Series([1,2,3])>>>s0    11    22    3dtype: int64>>>s.set_axis(['a','b','c'],axis=0)a    1b    2c    3dtype: int64"
Pandas,Series,pandas.Series.take,"pandas.Series.take#Series.take(indices,axis=0,**kwargs)[source]#Return the elements in the givenpositionalindices along an axis.This means that we are not indexing according to actual values in
the index attribute of the object. We are indexing according to the
actual position of the element in the object.Parameters:indicesarray-likeAn array of ints indicating which positions to take.axis{0 or eindexf, 1 or ecolumnsf, None}, default 0The axis on which to select elements.0means that we are
selecting rows,1means that we are selecting columns.
ForSeriesthis parameter is unused and defaults to 0.**kwargsFor compatibility withnumpy.take(). Has no effect on the
output.Returns:same type as callerAn array-like containing the elements taken from the object.See alsoDataFrame.locSelect a subset of a DataFrame by labels.DataFrame.ilocSelect a subset of a DataFrame by positions.numpy.takeTake elements from an array along an axis.Examples>>>df=pd.DataFrame([('falcon','bird',389.0),...('parrot','bird',24.0),...('lion','mammal',80.5),...('monkey','mammal',np.nan)],...columns=['name','class','max_speed'],...index=[0,2,3,1])>>>dfname   class  max_speed0  falcon    bird      389.02  parrot    bird       24.03    lion  mammal       80.51  monkey  mammal        NaNTake elements at positions 0 and 3 along the axis 0 (default).Note how the actual indices selected (0 and 1) do not correspond to
our selected indices 0 and 3. Thatfs because we are selecting the 0th
and 3rd rows, not rows whose indices equal 0 and 3.>>>df.take([0,3])name   class  max_speed0  falcon    bird      389.01  monkey  mammal        NaNTake elements at indices 1 and 2 along the axis 1 (column selection).>>>df.take([1,2],axis=1)class  max_speed0    bird      389.02    bird       24.03  mammal       80.51  mammal        NaNWe may take elements using negative integers for positive indices,
starting from the end of the object, just like with Python lists.>>>df.take([-1,-2])name   class  max_speed1  monkey  mammal        NaN3    lion  mammal       80.5"
Pandas,Series,pandas.Series.tail,"pandas.Series.tail#Series.tail(n=5)[source]#Return the lastnrows.This function returns lastnrows from the object based on
position. It is useful for quickly verifying data, for example,
after sorting or appending rows.For negative values ofn, this function returns all rows except
the first|n|rows, equivalent todf[|n|:].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:type of callerThe lastnrows of the caller object.See alsoDataFrame.headThe firstnrows of the caller object.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the last 5 lines>>>df.tail()animal4  monkey5  parrot6   shark7   whale8   zebraViewing the lastnlines (three in this case)>>>df.tail(3)animal6  shark7  whale8  zebraFor negative values ofn>>>df.tail(-3)animal3    lion4  monkey5  parrot6   shark7   whale8   zebra"
Pandas,Series,pandas.Series.truncate,"pandas.Series.truncate#Series.truncate(before=None,after=None,axis=None,copy=None)[source]#Truncate a Series or DataFrame before and after some index value.This is a useful shorthand for boolean indexing based on index
values above or below certain thresholds.Parameters:beforedate, str, intTruncate all rows before this index value.afterdate, str, intTruncate all rows after this index value.axis{0 or eindexf, 1 or ecolumnsf}, optionalAxis to truncate. Truncates the index (rows) by default.
ForSeriesthis parameter is unused and defaults to 0.copybool, default is True,Return a copy of the truncated section.Returns:type of callerThe truncated Series or DataFrame.See alsoDataFrame.locSelect a subset of a DataFrame by label.DataFrame.ilocSelect a subset of a DataFrame by position.NotesIf the index being truncated contains only datetime values,beforeandaftermay be specified as strings instead of
Timestamps.Examples>>>df=pd.DataFrame({'A':['a','b','c','d','e'],...'B':['f','g','h','i','j'],...'C':['k','l','m','n','o']},...index=[1,2,3,4,5])>>>dfA  B  C1  a  f  k2  b  g  l3  c  h  m4  d  i  n5  e  j  o>>>df.truncate(before=2,after=4)A  B  C2  b  g  l3  c  h  m4  d  i  nThe columns of a DataFrame can be truncated.>>>df.truncate(before=""A"",after=""B"",axis=""columns"")A  B1  a  f2  b  g3  c  h4  d  i5  e  jFor Series, only rows can be truncated.>>>df['A'].truncate(before=2,after=4)2    b3    c4    dName: A, dtype: objectThe index values intruncatecan be datetimes or string
dates.>>>dates=pd.date_range('2016-01-01','2016-02-01',freq='s')>>>df=pd.DataFrame(index=dates,data={'A':1})>>>df.tail()A2016-01-31 23:59:56  12016-01-31 23:59:57  12016-01-31 23:59:58  12016-01-31 23:59:59  12016-02-01 00:00:00  1>>>df.truncate(before=pd.Timestamp('2016-01-05'),...after=pd.Timestamp('2016-01-10')).tail()A2016-01-09 23:59:56  12016-01-09 23:59:57  12016-01-09 23:59:58  12016-01-09 23:59:59  12016-01-10 00:00:00  1Because the index is a DatetimeIndex containing only dates, we can
specifybeforeandafteras strings. They will be coerced to
Timestamps before truncation.>>>df.truncate('2016-01-05','2016-01-10').tail()A2016-01-09 23:59:56  12016-01-09 23:59:57  12016-01-09 23:59:58  12016-01-09 23:59:59  12016-01-10 00:00:00  1Note thattruncateassumes a 0 value for any unspecified time
component (midnight). This differs from partial string slicing, which
returns any partially matching dates.>>>df.loc['2016-01-05':'2016-01-10',:].tail()A2016-01-10 23:59:55  12016-01-10 23:59:56  12016-01-10 23:59:57  12016-01-10 23:59:58  12016-01-10 23:59:59  1"
Pandas,Series,pandas.Series.where,"pandas.Series.where#Series.where(cond,other=nan,*,inplace=False,axis=None,level=None)[source]#Replace values where the condition is False.Parameters:condbool Series/DataFrame, array-like, or callableWherecondis True, keep the original value. Where
False, replace with corresponding value fromother.
Ifcondis callable, it is computed on the Series/DataFrame and
should return boolean Series/DataFrame or array. The callable must
not change input Series/DataFrame (though pandas doesnft check it).otherscalar, Series/DataFrame, or callableEntries wherecondis False are replaced with
corresponding value fromother.
If other is callable, it is computed on the Series/DataFrame and
should return scalar or Series/DataFrame. The callable must not
change input Series/DataFrame (though pandas doesnft check it).
If not specified, entries will be filled with the corresponding
NULL value (np.nanfor numpy dtypes,pd.NAfor extension
dtypes).inplacebool, default FalseWhether to perform the operation in place on the data.axisint, default NoneAlignment axis if needed. ForSeriesthis parameter is
unused and defaults to 0.levelint, default NoneAlignment level if needed.Returns:Same type as caller or None ifinplace=True.See alsoDataFrame.mask()Return an object of same shape as self.NotesThe where method is an application of the if-then idiom. For each
element in the calling DataFrame, ifcondisTruethe
element is used; otherwise the corresponding element from the DataFrameotheris used. If the axis ofotherdoes not align with axis ofcondSeries/DataFrame, the misaligned index positions will be filled with
False.The signature forDataFrame.where()differs fromnumpy.where(). Roughlydf1.where(m,df2)is equivalent tonp.where(m,df1,df2).For further details and examples see thewheredocumentation inindexing.The dtype of the object takes precedence. The fill value is casted to
the objectfs dtype, if this can be done losslessly.Examples>>>s=pd.Series(range(5))>>>s.where(s>0)0    NaN1    1.02    2.03    3.04    4.0dtype: float64>>>s.mask(s>0)0    0.01    NaN2    NaN3    NaN4    NaNdtype: float64>>>s=pd.Series(range(5))>>>t=pd.Series([True,False])>>>s.where(t,99)0     01    992    993    994    99dtype: int64>>>s.mask(t,99)0    991     12    993    994    99dtype: int64>>>s.where(s>1,10)0    101    102    23    34    4dtype: int64>>>s.mask(s>1,10)0     01     12    103    104    10dtype: int64>>>df=pd.DataFrame(np.arange(10).reshape(-1,2),columns=['A','B'])>>>dfA  B0  0  11  2  32  4  53  6  74  8  9>>>m=df%3==0>>>df.where(m,-df)A  B0  0 -11 -2  32 -4 -53  6 -74 -8  9>>>df.where(m,-df)==np.where(m,df,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True>>>df.where(m,-df)==df.mask(~m,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True"
Pandas,Series,pandas.Series.mask,"pandas.Series.mask#Series.mask(cond,other=_NoDefault.no_default,*,inplace=False,axis=None,level=None)[source]#Replace values where the condition is True.Parameters:condbool Series/DataFrame, array-like, or callableWherecondis False, keep the original value. Where
True, replace with corresponding value fromother.
Ifcondis callable, it is computed on the Series/DataFrame and
should return boolean Series/DataFrame or array. The callable must
not change input Series/DataFrame (though pandas doesnft check it).otherscalar, Series/DataFrame, or callableEntries wherecondis True are replaced with
corresponding value fromother.
If other is callable, it is computed on the Series/DataFrame and
should return scalar or Series/DataFrame. The callable must not
change input Series/DataFrame (though pandas doesnft check it).
If not specified, entries will be filled with the corresponding
NULL value (np.nanfor numpy dtypes,pd.NAfor extension
dtypes).inplacebool, default FalseWhether to perform the operation in place on the data.axisint, default NoneAlignment axis if needed. ForSeriesthis parameter is
unused and defaults to 0.levelint, default NoneAlignment level if needed.Returns:Same type as caller or None ifinplace=True.See alsoDataFrame.where()Return an object of same shape as self.NotesThe mask method is an application of the if-then idiom. For each
element in the calling DataFrame, ifcondisFalsethe
element is used; otherwise the corresponding element from the DataFrameotheris used. If the axis ofotherdoes not align with axis ofcondSeries/DataFrame, the misaligned index positions will be filled with
True.The signature forDataFrame.where()differs fromnumpy.where(). Roughlydf1.where(m,df2)is equivalent tonp.where(m,df1,df2).For further details and examples see themaskdocumentation inindexing.The dtype of the object takes precedence. The fill value is casted to
the objectfs dtype, if this can be done losslessly.Examples>>>s=pd.Series(range(5))>>>s.where(s>0)0    NaN1    1.02    2.03    3.04    4.0dtype: float64>>>s.mask(s>0)0    0.01    NaN2    NaN3    NaN4    NaNdtype: float64>>>s=pd.Series(range(5))>>>t=pd.Series([True,False])>>>s.where(t,99)0     01    992    993    994    99dtype: int64>>>s.mask(t,99)0    991     12    993    994    99dtype: int64>>>s.where(s>1,10)0    101    102    23    34    4dtype: int64>>>s.mask(s>1,10)0     01     12    103    104    10dtype: int64>>>df=pd.DataFrame(np.arange(10).reshape(-1,2),columns=['A','B'])>>>dfA  B0  0  11  2  32  4  53  6  74  8  9>>>m=df%3==0>>>df.where(m,-df)A  B0  0 -11 -2  32 -4 -53  6 -74 -8  9>>>df.where(m,-df)==np.where(m,df,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True>>>df.where(m,-df)==df.mask(~m,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True"
Pandas,Series,pandas.Series.add_prefix,"pandas.Series.add_prefix#Series.add_prefix(prefix,axis=None)[source]#Prefix labels with stringprefix.For Series, the row labels are prefixed.
For DataFrame, the column labels are prefixed.Parameters:prefixstrThe string to add before each label.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneAxis to add prefix onNew in version 2.0.0.Returns:Series or DataFrameNew Series or DataFrame with updated labels.See alsoSeries.add_suffixSuffix row labels with stringsuffix.DataFrame.add_suffixSuffix column labels with stringsuffix.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.add_prefix('item_')item_0    1item_1    2item_2    3item_3    4dtype: int64>>>df=pd.DataFrame({'A':[1,2,3,4],'B':[3,4,5,6]})>>>dfA  B0  1  31  2  42  3  53  4  6>>>df.add_prefix('col_')col_A  col_B0       1       31       2       42       3       53       4       6"
Pandas,Series,pandas.Series.add_suffix,"pandas.Series.add_suffix#Series.add_suffix(suffix,axis=None)[source]#Suffix labels with stringsuffix.For Series, the row labels are suffixed.
For DataFrame, the column labels are suffixed.Parameters:suffixstrThe string to add after each label.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneAxis to add suffix onNew in version 2.0.0.Returns:Series or DataFrameNew Series or DataFrame with updated labels.See alsoSeries.add_prefixPrefix row labels with stringprefix.DataFrame.add_prefixPrefix column labels with stringprefix.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.add_suffix('_item')0_item    11_item    22_item    33_item    4dtype: int64>>>df=pd.DataFrame({'A':[1,2,3,4],'B':[3,4,5,6]})>>>dfA  B0  1  31  2  42  3  53  4  6>>>df.add_suffix('_col')A_col  B_col0       1       31       2       42       3       53       4       6"
Pandas,Series,pandas.Series.filter,"pandas.Series.filter#Series.filter(items=None,like=None,regex=None,axis=None)[source]#Subset the dataframe rows or columns according to the specified index labels.Note that this routine does not filter a dataframe on its
contents. The filter is applied to the labels of the index.Parameters:itemslist-likeKeep labels from axis which are in items.likestrKeep labels from axis for which glike in label == Trueh.regexstr (regular expression)Keep labels from axis for which re.search(regex, label) == True.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneThe axis to filter on, expressed either as an index (int)
or axis name (str). By default this is the info axis, ecolumnsf for
DataFrame. ForSeriesthis parameter is unused and defaults toNone.Returns:same type as input objectSee alsoDataFrame.locAccess a group of rows and columns by label(s) or a boolean array.NotesTheitems,like, andregexparameters are
enforced to be mutually exclusive.axisdefaults to the info axis that is used when indexing
with[].Examples>>>df=pd.DataFrame(np.array(([1,2,3],[4,5,6])),...index=['mouse','rabbit'],...columns=['one','two','three'])>>>dfone  two  threemouse     1    2      3rabbit    4    5      6>>># select columns by name>>>df.filter(items=['one','three'])one  threemouse     1      3rabbit    4      6>>># select columns by regular expression>>>df.filter(regex='e$',axis=1)one  threemouse     1      3rabbit    4      6>>># select rows containing 'bbi'>>>df.filter(like='bbi',axis=0)one  two  threerabbit    4    5      6"
Pandas,Series,pandas.Series.backfill,"pandas.Series.backfill#Series.backfill(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by using the next valid observation to fill the gap.Deprecated since version 2.0:Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead.Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.ExamplesPlease see examples forDataFrame.bfill()orSeries.bfill()."
Pandas,Series,pandas.Series.bfill,"pandas.Series.bfill#Series.bfill(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by using the next valid observation to fill the gap.Parameters:axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string einferf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.ExamplesFor Series:>>>s=pd.Series([1,None,None,2])>>>s.bfill()0    1.01    2.02    2.03    2.0dtype: float64>>>s.bfill(limit=1)0    1.01    NaN2    2.03    2.0dtype: float64With DataFrame:>>>df=pd.DataFrame({'A':[1,None,None,4],'B':[None,5,None,7]})>>>dfA     B0   1.0   NaN1   NaN   5.02   NaN   NaN3   4.0   7.0>>>df.bfill()A     B0   1.0   5.01   4.0   5.02   4.0   7.03   4.0   7.0>>>df.bfill(limit=1)A     B0   1.0   5.01   NaN   5.02   4.0   7.03   4.0   7.0"
Pandas,Series,pandas.Series.dropna,"pandas.Series.dropna#Series.dropna(*,axis=0,inplace=False,how=None,ignore_index=False)[source]#Return a new Series with missing values removed.See theUser Guidefor more on which values are
considered missing, and how to work with missing data.Parameters:axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.inplacebool, default FalseIf True, do operation inplace and return None.howstr, optionalNot in use. Kept for compatibility.ignore_indexbool, defaultFalseIfTrue, the resulting axis will be labeled 0, 1, c, n - 1.New in version 2.0.0.Returns:Series or NoneSeries with NA entries dropped from it or None ifinplace=True.See alsoSeries.isnaIndicate missing values.Series.notnaIndicate existing (non-missing) values.Series.fillnaReplace missing values.DataFrame.dropnaDrop rows or columns which contain NA values.Index.dropnaDrop missing indices.Examples>>>ser=pd.Series([1.,2.,np.nan])>>>ser0    1.01    2.02    NaNdtype: float64Drop NA values from a Series.>>>ser.dropna()0    1.01    2.0dtype: float64Empty strings are not considered NA values.Noneis considered an
NA value.>>>ser=pd.Series([np.nan,2,pd.NaT,'',None,'I stay'])>>>ser0       NaN1         22       NaT34      None5    I staydtype: object>>>ser.dropna()1         235    I staydtype: object"
Pandas,Series,pandas.Series.ffill,"pandas.Series.ffill#Series.ffill(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by propagating the last valid observation to next valid.Parameters:axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string einferf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.Examples>>>df=pd.DataFrame([[np.nan,2,np.nan,0],...[3,4,np.nan,1],...[np.nan,np.nan,np.nan,np.nan],...[np.nan,3,np.nan,4]],...columns=list(""ABCD""))>>>dfA    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  NaN  NaN NaN  NaN3  NaN  3.0 NaN  4.0>>>df.ffill()A    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  3.0  4.0 NaN  1.03  3.0  3.0 NaN  4.0>>>ser=pd.Series([1,np.nan,2,3])>>>ser.ffill()0   1.01   1.02   2.03   3.0dtype: float64"
Pandas,Series,pandas.Series.fillna,"pandas.Series.fillna#Series.fillna(value=None,*,method=None,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values using the specified method.Parameters:valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a
dict/Series/DataFrame of values specifying which value to use for
each index (for a Series) or column (for a DataFrame). Values not
in the dict/Series/DataFrame will not be filled. This value cannot
be a list.method{ebackfillf, ebfillf, effillf, None}, default NoneMethod to use for filling holes in reindexed Series:ffill: propagate last valid observation forward to next valid.backfill / bfill: use next valid observation to fill gap.Deprecated since version 2.1.0:Use ffill or bfill instead.axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string einferf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.See alsoffillFill values by propagating the last valid observation to next valid.bfillFill values by using the next valid observation to fill the gap.interpolateFill NaN values using interpolation.reindexConform object to new index.asfreqConvert TimeSeries to specified frequency.Examples>>>df=pd.DataFrame([[np.nan,2,np.nan,0],...[3,4,np.nan,1],...[np.nan,np.nan,np.nan,np.nan],...[np.nan,3,np.nan,4]],...columns=list(""ABCD""))>>>dfA    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  NaN  NaN NaN  NaN3  NaN  3.0 NaN  4.0Replace all NaN elements with 0s.>>>df.fillna(0)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  0.03  0.0  3.0  0.0  4.0Replace all NaN elements in column eAf, eBf, eCf, and eDf, with 0, 1,
2, and 3 respectively.>>>values={""A"":0,""B"":1,""C"":2,""D"":3}>>>df.fillna(value=values)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  2.0  1.02  0.0  1.0  2.0  3.03  0.0  3.0  2.0  4.0Only replace the first NaN element.>>>df.fillna(value=values,limit=1)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  NaN  1.02  NaN  1.0  NaN  3.03  NaN  3.0  NaN  4.0When filling using a DataFrame, replacement happens along
the same column names and same indices>>>df2=pd.DataFrame(np.zeros((4,4)),columns=list(""ABCE""))>>>df.fillna(df2)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  NaN3  0.0  3.0  0.0  4.0Note that column D is not affected since it is not present in df2."
Pandas,Series,pandas.Series.interpolate,"pandas.Series.interpolate#Series.interpolate(method='linear',*,axis=0,limit=None,inplace=False,limit_direction=None,limit_area=None,downcast=_NoDefault.no_default,**kwargs)[source]#Fill NaN values using an interpolation method.Please note that onlymethod='linear'is supported for
DataFrame/Series with a MultiIndex.Parameters:methodstr, default elinearfInterpolation technique to use. One of:elinearf: Ignore the index and treat the values as equally
spaced. This is the only method supported on MultiIndexes.etimef: Works on daily and higher resolution data to interpolate
given length of interval.eindexf, evaluesf: use the actual numerical values of the index.epadf: Fill in NaNs using existing values.enearestf, ezerof, eslinearf, equadraticf, ecubicf,
ebarycentricf, epolynomialf: Passed toscipy.interpolate.interp1d, whereas esplinef is passed toscipy.interpolate.UnivariateSpline. These methods use the numerical
values of the index. Both epolynomialf and esplinef require that
you also specify anorder(int), e.g.df.interpolate(method='polynomial',order=5). Note that,slinearmethod in Pandas refers to the Scipy first ordersplineinstead of Pandas first orderspline.ekroghf, epiecewise_polynomialf, esplinef, epchipf, eakimaf,
ecubicsplinef: Wrappers around the SciPy interpolation methods of
similar names. SeeNotes.efrom_derivativesf: Refers toscipy.interpolate.BPoly.from_derivatives.axis{{0 or eindexf, 1 or ecolumnsf, None}}, default NoneAxis to interpolate along. ForSeriesthis parameter is unused
and defaults to 0.limitint, optionalMaximum number of consecutive NaNs to fill. Must be greater than
0.inplacebool, default FalseUpdate the data in place if possible.limit_direction{{eforwardf, ebackwardf, ebothf}}, OptionalConsecutive NaNs will be filled in this direction.If limit is specified:If emethodf is epadf or effillf, elimit_directionf must be eforwardf.If emethodf is ebackfillf or ebfillf, elimit_directionf must be
ebackwardsf.If elimitf is not specified:If emethodf is ebackfillf or ebfillf, the default is ebackwardfelse the default is eforwardfraises ValueError iflimit_directionis eforwardf or ebothf andmethod is ebackfillf or ebfillf.raises ValueError iflimit_directionis ebackwardf or ebothf andmethod is epadf or effillf.limit_area{{None, einsidef, eoutsidef}}, default NoneIf limit is specified, consecutive NaNs will be filled with this
restriction.None: No fill restriction.einsidef: Only fill NaNs surrounded by valid values
(interpolate).eoutsidef: Only fill NaNs outside valid values (extrapolate).downcastoptional, einferf or None, defaults to NoneDowncast dtypes if possible.Deprecated since version 2.1.0.``**kwargs``optionalKeyword arguments to pass on to the interpolating function.Returns:Series or DataFrame or NoneReturns the same object type as the caller, interpolated at
some or allNaNvalues or None ifinplace=True.See alsofillnaFill missing values using different methods.scipy.interpolate.Akima1DInterpolatorPiecewise cubic polynomials (Akima interpolator).scipy.interpolate.BPoly.from_derivativesPiecewise polynomial in the Bernstein basis.scipy.interpolate.interp1dInterpolate a 1-D function.scipy.interpolate.KroghInterpolatorInterpolate polynomial (Krogh interpolator).scipy.interpolate.PchipInterpolatorPCHIP 1-d monotonic cubic interpolation.scipy.interpolate.CubicSplineCubic spline data interpolator.NotesThe ekroghf, epiecewise_polynomialf, esplinef, epchipf and eakimaf
methods are wrappers around the respective SciPy implementations of
similar names. These use the actual numerical values of the index.
For more information on their behavior, see theSciPy documentation.ExamplesFilling inNaNin aSeriesvia linear
interpolation.>>>s=pd.Series([0,1,np.nan,3])>>>s0    0.01    1.02    NaN3    3.0dtype: float64>>>s.interpolate()0    0.01    1.02    2.03    3.0dtype: float64Filling inNaNin a Series via polynomial interpolation or splines:
Both epolynomialf and esplinef methods require that you also specify
anorder(int).>>>s=pd.Series([0,2,np.nan,8])>>>s.interpolate(method='polynomial',order=2)0    0.0000001    2.0000002    4.6666673    8.000000dtype: float64Fill the DataFrame forward (that is, going down) along each column
using linear interpolation.Note how the last entry in column eaf is interpolated differently,
because there is no entry after it to use for interpolation.
Note how the first entry in column ebf remainsNaN, because there
is no entry before it to use for interpolation.>>>df=pd.DataFrame([(0.0,np.nan,-1.0,1.0),...(np.nan,2.0,np.nan,np.nan),...(2.0,3.0,np.nan,9.0),...(np.nan,4.0,-4.0,16.0)],...columns=list('abcd'))>>>dfa    b    c     d0  0.0  NaN -1.0   1.01  NaN  2.0  NaN   NaN2  2.0  3.0  NaN   9.03  NaN  4.0 -4.0  16.0>>>df.interpolate(method='linear',limit_direction='forward',axis=0)a    b    c     d0  0.0  NaN -1.0   1.01  1.0  2.0 -2.0   5.02  2.0  3.0 -3.0   9.03  2.0  4.0 -4.0  16.0Using polynomial interpolation.>>>df['d'].interpolate(method='polynomial',order=2)0     1.01     4.02     9.03    16.0Name: d, dtype: float64"
Pandas,Series,pandas.Series.isna,"pandas.Series.isna#Series.isna()[source]#Detect missing values.Return a boolean same-sized object indicating if the values are NA.
NA values, such as None ornumpy.NaN, gets mapped to True
values.
Everything else gets mapped to False values. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).Returns:SeriesMask of bool values for each element in Series that
indicates whether an element is an NA value.See alsoSeries.isnullAlias of isna.Series.notnaBoolean inverse of isna.Series.dropnaOmit axes labels with missing values.isnaTop-level isna.ExamplesShow which entries in a DataFrame are NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.isna()age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  FalseShow which entries in a Series are NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.isna()0    False1    False2     Truedtype: bool"
Pandas,Series,pandas.Series.isnull,"pandas.Series.isnull#Series.isnull()[source]#Series.isnull is an alias for Series.isna.Detect missing values.Return a boolean same-sized object indicating if the values are NA.
NA values, such as None ornumpy.NaN, gets mapped to True
values.
Everything else gets mapped to False values. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).Returns:SeriesMask of bool values for each element in Series that
indicates whether an element is an NA value.See alsoSeries.isnullAlias of isna.Series.notnaBoolean inverse of isna.Series.dropnaOmit axes labels with missing values.isnaTop-level isna.ExamplesShow which entries in a DataFrame are NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.isna()age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  FalseShow which entries in a Series are NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.isna()0    False1    False2     Truedtype: bool"
Pandas,Series,pandas.Series.notna,"pandas.Series.notna#Series.notna()[source]#Detect existing (non-missing) values.Return a boolean same-sized object indicating if the values are not NA.
Non-missing values get mapped to True. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).
NA values, such as None ornumpy.NaN, get mapped to False
values.Returns:SeriesMask of bool values for each element in Series that
indicates whether an element is not an NA value.See alsoSeries.notnullAlias of notna.Series.isnaBoolean inverse of notna.Series.dropnaOmit axes labels with missing values.notnaTop-level notna.ExamplesShow which entries in a DataFrame are not NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.notna()age   born  name    toy0   True  False  True  False1   True   True  True   True2  False   True  True   TrueShow which entries in a Series are not NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.notna()0     True1     True2    Falsedtype: bool"
Pandas,Series,pandas.Series.notnull,"pandas.Series.notnull#Series.notnull()[source]#Series.notnull is an alias for Series.notna.Detect existing (non-missing) values.Return a boolean same-sized object indicating if the values are not NA.
Non-missing values get mapped to True. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).
NA values, such as None ornumpy.NaN, get mapped to False
values.Returns:SeriesMask of bool values for each element in Series that
indicates whether an element is not an NA value.See alsoSeries.notnullAlias of notna.Series.isnaBoolean inverse of notna.Series.dropnaOmit axes labels with missing values.notnaTop-level notna.ExamplesShow which entries in a DataFrame are not NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.notna()age   born  name    toy0   True  False  True  False1   True   True  True   True2  False   True  True   TrueShow which entries in a Series are not NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.notna()0     True1     True2    Falsedtype: bool"
Pandas,Series,pandas.Series.pad,"pandas.Series.pad#Series.pad(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by propagating the last valid observation to next valid.Deprecated since version 2.0:Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead.Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.ExamplesPlease see examples forDataFrame.ffill()orSeries.ffill()."
Pandas,Series,pandas.Series.replace,"pandas.Series.replace#Series.replace(to_replace=None,value=_NoDefault.no_default,*,inplace=False,limit=None,regex=False,method=_NoDefault.no_default)[source]#Replace values given into_replacewithvalue.Values of the Series/DataFrame are replaced with other values dynamically.
This differs from updating with.locor.iloc, which require
you to specify a location to update with some value.Parameters:to_replacestr, regex, list, dict, Series, int, float, or NoneHow to find the values that will be replaced.numeric, str or regex:numeric: numeric values equal toto_replacewill be
replaced withvaluestr: string exactly matchingto_replacewill be replaced
withvalueregex: regexs matchingto_replacewill be replaced withvaluelist of str, regex, or numeric:First, ifto_replaceandvalueare both lists, theymustbe the same length.Second, ifregex=Truethen all of the strings inbothlists will be interpreted as regexs otherwise they will match
directly. This doesnft matter much forvaluesince there
are only a few possible substitution regexes you can use.str, regex and numeric rules apply as above.dict:Dicts can be used to specify different replacement values
for different existing values. For example,{'a':'b','y':'z'}replaces the value eaf with ebf and
eyf with ezf. To use a dict in this way, the optionalvalueparameter should not be given.For a DataFrame a dict can specify that different values
should be replaced in different columns. For example,{'a':1,'b':'z'}looks for the value 1 in column eaf
and the value ezf in column ebf and replaces these values
with whatever is specified invalue. Thevalueparameter
should not beNonein this case. You can treat this as a
special case of passing two lists except that you are
specifying the column to search in.For a DataFrame nested dictionaries, e.g.,{'a':{'b':np.nan}}, are read as follows: look in column
eaf for the value ebf and replace it with NaN. The optionalvalueparameter should not be specified to use a nested dict in this
way. You can nest regular expressions as well. Note that
column names (the top-level dictionary keys in a nested
dictionary)cannotbe regular expressions.None:This means that theregexargument must be a string,
compiled regular expression, or list, dict, ndarray or
Series of such elements. Ifvalueis alsoNonethen
thismustbe a nested dictionary or Series.See the examples section for examples of each of these.valuescalar, dict, list, str, regex, default NoneValue to replace any values matchingto_replacewith.
For a DataFrame a dict of values can be used to specify which
value to use for each column (columns not in the dict will not be
filled). Regular expressions, strings and lists or dicts of such
objects are also allowed.inplacebool, default FalseIf True, performs operation inplace and returns None.limitint, default NoneMaximum size gap to forward or backward fill.Deprecated since version 2.1.0.regexbool or same types asto_replace, default FalseWhether to interpretto_replaceand/orvalueas regular
expressions. If this isTruethento_replacemustbe a
string. Alternatively, this could be a regular expression or a
list, dict, or array of regular expressions in which caseto_replacemust beNone.method{epadf, effillf, ebfillf}The method to use when for replacement, whento_replaceis a
scalar, list or tuple andvalueisNone.Deprecated since version 2.1.0.Returns:Series/DataFrameObject after replacement.Raises:AssertionErrorIfregexis not aboolandto_replaceis notNone.TypeErrorIfto_replaceis not a scalar, array-like,dict, orNoneIfto_replaceis adictandvalueis not alist,dict,ndarray, orSeriesIfto_replaceisNoneandregexis not compilable
into a regular expression or is a list, dict, ndarray, or
Series.When replacing multipleboolordatetime64objects and
the arguments toto_replacedoes not match the type of the
value being replacedValueErrorIf alistor anndarrayis passed toto_replaceandvaluebut they are not the same length.See alsoSeries.fillnaFill NA values.DataFrame.fillnaFill NA values.Series.whereReplace values based on boolean condition.DataFrame.whereReplace values based on boolean condition.DataFrame.mapApply a function to a Dataframe elementwise.Series.mapMap values of Series according to an input mapping or function.Series.str.replaceSimple string replacement.NotesRegex substitution is performed under the hood withre.sub. The
rules for substitution forre.subare the same.Regular expressions will only substitute on strings, meaning you
cannot provide, for example, a regular expression matching floating
point numbers and expect the columns in your frame that have a
numeric dtype to be matched. However, if those floating point
numbersarestrings, then you can do this.This method hasa lotof options. You are encouraged to experiment
and play with this method to gain intuition about how it works.When dict is used as theto_replacevalue, it is like
key(s) in the dict are the to_replace part and
value(s) in the dict are the value parameter.ExamplesScalar `to_replace` and `value`>>>s=pd.Series([1,2,3,4,5])>>>s.replace(1,5)0    51    22    33    44    5dtype: int64>>>df=pd.DataFrame({'A':[0,1,2,3,4],...'B':[5,6,7,8,9],...'C':['a','b','c','d','e']})>>>df.replace(0,5)A  B  C0  5  5  a1  1  6  b2  2  7  c3  3  8  d4  4  9  eList-like `to_replace`>>>df.replace([0,1,2,3],4)A  B  C0  4  5  a1  4  6  b2  4  7  c3  4  8  d4  4  9  e>>>df.replace([0,1,2,3],[4,3,2,1])A  B  C0  4  5  a1  3  6  b2  2  7  c3  1  8  d4  4  9  e>>>s.replace([1,2],method='bfill')0    31    32    33    44    5dtype: int64dict-like `to_replace`>>>df.replace({0:10,1:100})A  B  C0   10  5  a1  100  6  b2    2  7  c3    3  8  d4    4  9  e>>>df.replace({'A':0,'B':5},100)A    B  C0  100  100  a1    1    6  b2    2    7  c3    3    8  d4    4    9  e>>>df.replace({'A':{0:100,4:400}})A  B  C0  100  5  a1    1  6  b2    2  7  c3    3  8  d4  400  9  eRegular expression `to_replace`>>>df=pd.DataFrame({'A':['bat','foo','bait'],...'B':['abc','bar','xyz']})>>>df.replace(to_replace=r'^ba.$',value='new',regex=True)A    B0   new  abc1   foo  new2  bait  xyz>>>df.replace({'A':r'^ba.$'},{'A':'new'},regex=True)A    B0   new  abc1   foo  bar2  bait  xyz>>>df.replace(regex=r'^ba.$',value='new')A    B0   new  abc1   foo  new2  bait  xyz>>>df.replace(regex={r'^ba.$':'new','foo':'xyz'})A    B0   new  abc1   xyz  new2  bait  xyz>>>df.replace(regex=[r'^ba.$','foo'],value='new')A    B0   new  abc1   new  new2  bait  xyzCompare the behavior ofs.replace({'a':None})ands.replace('a',None)to understand the peculiarities
of theto_replaceparameter:>>>s=pd.Series([10,'a','a','b','a'])When one uses a dict as theto_replacevalue, it is like the
value(s) in the dict are equal to thevalueparameter.s.replace({'a':None})is equivalent tos.replace(to_replace={'a':None},value=None,method=None):>>>s.replace({'a':None})0      101    None2    None3       b4    Nonedtype: objectWhenvalueis not explicitly passed andto_replaceis a scalar, list
or tuple,replaceuses the method parameter (default epadf) to do the
replacement. So this is why the eaf values are being replaced by 10
in rows 1 and 2 and ebf in row 4 in this case.>>>s.replace('a')0    101    102    103     b4     bdtype: objectDeprecated since version 2.1.0:The emethodf parameter and padding behavior are deprecated.On the other hand, ifNoneis explicitly passed forvalue, it will
be respected:>>>s.replace('a',None)0      101    None2    None3       b4    Nonedtype: objectChanged in version 1.4.0:Previously the explicitNonewas silently ignored."
Pandas,Series,pandas.Series.argsort,"pandas.Series.argsort#Series.argsort(axis=0,kind='quicksort',order=None)[source]#Return the integer indices that would sort the Series values.Override ndarray.argsort. Argsorts the value, omitting NA/null values,
and places the result in the same locations as the non-NA values.Parameters:axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.kind{emergesortf, equicksortf, eheapsortf, establef}, default equicksortfChoice of sorting algorithm. Seenumpy.sort()for more
information. emergesortf and establef are the only stable algorithms.orderNoneHas no effect but is accepted for compatibility with numpy.Returns:Series[np.intp]Positions of values within the sort order with -1 indicating
nan values.See alsonumpy.ndarray.argsortReturns the indices that would sort this array.Examples>>>s=pd.Series([3,2,1])>>>s.argsort()0    21    12    0dtype: int64"
Pandas,Series,pandas.Series.argmin,"pandas.Series.argmin#Series.argmin(axis=None,skipna=True,*args,**kwargs)[source]#Return int position of the smallest value in the Series.If the minimum is achieved in multiple locations,
the first row position is returned.Parameters:axis{None}Unused. Parameter needed for compatibility with DataFrame.skipnabool, default TrueExclude NA/null values when showing the result.*args, **kwargsAdditional arguments and keywords for compatibility with NumPy.Returns:intRow position of the minimum value.See alsoSeries.argminReturn position of the minimum value.Series.argmaxReturn position of the maximum value.numpy.ndarray.argminEquivalent method for numpy arrays.Series.idxmaxReturn index label of the maximum values.Series.idxminReturn index label of the minimum values.ExamplesConsider dataset containing cereal calories>>>s=pd.Series({'Corn Flakes':100.0,'Almond Delight':110.0,...'Cinnamon Toast Crunch':120.0,'Cocoa Puff':110.0})>>>sCorn Flakes              100.0Almond Delight           110.0Cinnamon Toast Crunch    120.0Cocoa Puff               110.0dtype: float64>>>s.argmax()2>>>s.argmin()0The maximum cereal calories is the third element and
the minimum cereal calories is the first element,
since series is zero-indexed."
Pandas,Series,pandas.Series.argmax,"pandas.Series.argmax#Series.argmax(axis=None,skipna=True,*args,**kwargs)[source]#Return int position of the largest value in the Series.If the maximum is achieved in multiple locations,
the first row position is returned.Parameters:axis{None}Unused. Parameter needed for compatibility with DataFrame.skipnabool, default TrueExclude NA/null values when showing the result.*args, **kwargsAdditional arguments and keywords for compatibility with NumPy.Returns:intRow position of the maximum value.See alsoSeries.argmaxReturn position of the maximum value.Series.argminReturn position of the minimum value.numpy.ndarray.argmaxEquivalent method for numpy arrays.Series.idxmaxReturn index label of the maximum values.Series.idxminReturn index label of the minimum values.ExamplesConsider dataset containing cereal calories>>>s=pd.Series({'Corn Flakes':100.0,'Almond Delight':110.0,...'Cinnamon Toast Crunch':120.0,'Cocoa Puff':110.0})>>>sCorn Flakes              100.0Almond Delight           110.0Cinnamon Toast Crunch    120.0Cocoa Puff               110.0dtype: float64>>>s.argmax()2>>>s.argmin()0The maximum cereal calories is the third element and
the minimum cereal calories is the first element,
since series is zero-indexed."
Pandas,Series,pandas.Series.reorder_levels,"pandas.Series.reorder_levels#Series.reorder_levels(order)[source]#Rearrange index levels using input order.May not drop or duplicate levels.Parameters:orderlist of int representing new level orderReference level by number or key.Returns:type of caller (new object)Examples>>>arrays=[np.array([""dog"",""dog"",""cat"",""cat"",""bird"",""bird""]),...np.array([""white"",""black"",""white"",""black"",""white"",""black""])]>>>s=pd.Series([1,2,3,3,5,2],index=arrays)>>>sdog   white    1black    2cat   white    3black    3bird  white    5black    2dtype: int64>>>s.reorder_levels([1,0])white  dog     1black  dog     2white  cat     3black  cat     3white  bird    5black  bird    2dtype: int64"
Pandas,Series,pandas.Series.sort_values,"pandas.Series.sort_values#Series.sort_values(*,axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last',ignore_index=False,key=None)[source]#Sort by the values.Sort a Series in ascending or descending order by some
criterion.Parameters:axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.ascendingbool or list of bools, default TrueIf True, sort values in ascending order, otherwise descending.inplacebool, default FalseIf True, perform operation in-place.kind{equicksortf, emergesortf, eheapsortf, establef}, default equicksortfChoice of sorting algorithm. See alsonumpy.sort()for more
information. emergesortf and establef are the only stable algorithms.na_position{efirstf or elastf}, default elastfArgument efirstf puts NaNs at the beginning, elastf puts NaNs at
the end.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, c, n - 1.keycallable, optionalIf not None, apply the key function to the series values
before sorting. This is similar to thekeyargument in the
builtinsorted()function, with the notable difference that
thiskeyfunction should bevectorized. It should expect aSeriesand return an array-like.Returns:Series or NoneSeries ordered by values or None ifinplace=True.See alsoSeries.sort_indexSort by the Series indices.DataFrame.sort_valuesSort DataFrame by the values along either axis.DataFrame.sort_indexSort DataFrame by indices.Examples>>>s=pd.Series([np.nan,1,3,10,5])>>>s0     NaN1     1.02     3.03     10.04     5.0dtype: float64Sort values ascending order (default behaviour)>>>s.sort_values(ascending=True)1     1.02     3.04     5.03    10.00     NaNdtype: float64Sort values descending order>>>s.sort_values(ascending=False)3    10.04     5.02     3.01     1.00     NaNdtype: float64Sort values putting NAs first>>>s.sort_values(na_position='first')0     NaN1     1.02     3.04     5.03    10.0dtype: float64Sort a series of strings>>>s=pd.Series(['z','b','d','a','c'])>>>s0    z1    b2    d3    a4    cdtype: object>>>s.sort_values()3    a1    b4    c2    d0    zdtype: objectSort using a key function. Yourkeyfunction will be
given theSeriesof values and should return an array-like.>>>s=pd.Series(['a','B','c','D','e'])>>>s.sort_values()1    B3    D0    a2    c4    edtype: object>>>s.sort_values(key=lambdax:x.str.lower())0    a1    B2    c3    D4    edtype: objectNumPy ufuncs work well here. For example, we can
sort by thesinof the value>>>s=pd.Series([-4,-2,0,2,4])>>>s.sort_values(key=np.sin)1   -24    42    00   -43    2dtype: int64More complicated user-defined functions can be used,
as long as they expect a Series and return an array-like>>>s.sort_values(key=lambdax:(np.tan(x.cumsum())))0   -43    24    41   -22    0dtype: int64"
Pandas,Series,pandas.Series.sort_index,"pandas.Series.sort_index#Series.sort_index(*,axis=0,level=None,ascending=True,inplace=False,kind='quicksort',na_position='last',sort_remaining=True,ignore_index=False,key=None)[source]#Sort Series by index labels.Returns a new Series sorted by label ifinplaceargument isFalse, otherwise updates the original series and returns None.Parameters:axis{0 or eindexf}Unused. Parameter needed for compatibility with DataFrame.levelint, optionalIf not None, sort on values in specified index level(s).ascendingbool or list-like of bools, default TrueSort ascending vs. descending. When the index is a MultiIndex the
sort direction can be controlled for each level individually.inplacebool, default FalseIf True, perform operation in-place.kind{equicksortf, emergesortf, eheapsortf, establef}, default equicksortfChoice of sorting algorithm. See alsonumpy.sort()for more
information. emergesortf and establef are the only stable algorithms. For
DataFrames, this option is only applied when sorting on a single
column or label.na_position{efirstf, elastf}, default elastfIf efirstf puts NaNs at the beginning, elastf puts NaNs at the end.
Not implemented for MultiIndex.sort_remainingbool, default TrueIf True and sorting by level and index is multilevel, sort by other
levels too (in order) after sorting by specified level.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, c, n - 1.keycallable, optionalIf not None, apply the key function to the index values
before sorting. This is similar to thekeyargument in the
builtinsorted()function, with the notable difference that
thiskeyfunction should bevectorized. It should expect anIndexand return anIndexof the same shape.Returns:Series or NoneThe original Series sorted by the labels or None ifinplace=True.See alsoDataFrame.sort_indexSort DataFrame by the index.DataFrame.sort_valuesSort DataFrame by the value.Series.sort_valuesSort Series by the value.Examples>>>s=pd.Series(['a','b','c','d'],index=[3,2,1,4])>>>s.sort_index()1    c2    b3    a4    ddtype: objectSort Descending>>>s.sort_index(ascending=False)4    d3    a2    b1    cdtype: objectBy default NaNs are put at the end, but usena_positionto place
them at the beginning>>>s=pd.Series(['a','b','c','d'],index=[3,2,1,np.nan])>>>s.sort_index(na_position='first')NaN     d1.0    c2.0    b3.0    adtype: objectSpecify index level to sort>>>arrays=[np.array(['qux','qux','foo','foo',...'baz','baz','bar','bar']),...np.array(['two','one','two','one',...'two','one','two','one'])]>>>s=pd.Series([1,2,3,4,5,6,7,8],index=arrays)>>>s.sort_index(level=1)bar  one    8baz  one    6foo  one    4qux  one    2bar  two    7baz  two    5foo  two    3qux  two    1dtype: int64Does not sort by remaining levels when sorting by levels>>>s.sort_index(level=1,sort_remaining=False)qux  one    2foo  one    4baz  one    6bar  one    8qux  two    1foo  two    3baz  two    5bar  two    7dtype: int64Apply a key function before sorting>>>s=pd.Series([1,2,3,4],index=['A','b','C','d'])>>>s.sort_index(key=lambdax:x.str.lower())A    1b    2C    3d    4dtype: int64"
Pandas,Series,pandas.Series.swaplevel,"pandas.Series.swaplevel#Series.swaplevel(i=-2,j=-1,copy=None)[source]#Swap levels i and j in aMultiIndex.Default is to swap the two innermost levels of the index.Parameters:i, jint or strLevels of the indices to be swapped. Can pass level name as string.copybool, default TrueWhether to copy underlying data.Returns:SeriesSeries with levels swapped in MultiIndex.Examples>>>s=pd.Series(...[""A"",""B"",""A"",""C""],...index=[...[""Final exam"",""Final exam"",""Coursework"",""Coursework""],...[""History"",""Geography"",""History"",""Geography""],...[""January"",""February"",""March"",""April""],...],...)>>>sFinal exam  History     January      AGeography   February     BCoursework  History     March        AGeography   April        Cdtype: objectIn the following example, we will swap the levels of the indices.
Here, we will swap the levels column-wise, but levels can be swapped row-wise
in a similar manner. Note that column-wise is the default behaviour.
By not supplying any arguments for i and j, we swap the last and second to
last indices.>>>s.swaplevel()Final exam  January     History         AFebruary    Geography       BCoursework  March       History         AApril       Geography       Cdtype: objectBy supplying one argument, we can choose which index to swap the last
index with. We can for example swap the first index with the last one as
follows.>>>s.swaplevel(0)January     History     Final exam      AFebruary    Geography   Final exam      BMarch       History     Coursework      AApril       Geography   Coursework      Cdtype: objectWe can also define explicitly which indices we want to swap by supplying values
for both i and j. Here, we for example swap the first and second indices.>>>s.swaplevel(0,1)History     Final exam  January         AGeography   Final exam  February        BHistory     Coursework  March           AGeography   Coursework  April           Cdtype: object"
Pandas,Series,pandas.Series.unstack,"pandas.Series.unstack#Series.unstack(level=-1,fill_value=None,sort=True)[source]#Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.Parameters:levelint, str, or list of these, default last levelLevel(s) to unstack, can pass level name.fill_valuescalar value, default NoneValue to use when replacing NaN values.sortbool, default TrueSort the level(s) in the resulting MultiIndex columns.Returns:DataFrameUnstacked Series.NotesReferencethe user guidefor more examples.Examples>>>s=pd.Series([1,2,3,4],...index=pd.MultiIndex.from_product([['one','two'],...['a','b']]))>>>sone  a    1b    2two  a    3b    4dtype: int64>>>s.unstack(level=-1)a  bone  1  2two  3  4>>>s.unstack(level=0)one  twoa    1    3b    2    4"
Pandas,Series,pandas.Series.explode,"pandas.Series.explode#Series.explode(ignore_index=False)[source]#Transform each element of a list-like to a row.Parameters:ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, c, n - 1.Returns:SeriesExploded lists to rows; index will be duplicated for these rows.See alsoSeries.str.splitSplit string values on specified separator.Series.unstackUnstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame.DataFrame.meltUnpivot a DataFrame from wide format to long format.DataFrame.explodeExplode a DataFrame from list-like columns to long format.NotesThis routine will explode list-likes including lists, tuples, sets,
Series, and np.ndarray. The result dtype of the subset rows will
be object. Scalars will be returned unchanged, and empty list-likes will
result in a np.nan for that row. In addition, the ordering of elements in
the output will be non-deterministic when exploding sets.Referencethe user guidefor more examples.Examples>>>s=pd.Series([[1,2,3],'foo',[],[3,4]])>>>s0    [1, 2, 3]1          foo2           []3       [3, 4]dtype: object>>>s.explode()0      10      20      31    foo2    NaN3      33      4dtype: object"
Pandas,Series,pandas.Series.searchsorted,"pandas.Series.searchsorted#Series.searchsorted(value,side='left',sorter=None)[source]#Find indices where elements should be inserted to maintain order.Find the indices into a sorted Seriesselfsuch that, if the
corresponding elements invaluewere inserted before the indices,
the order ofselfwould be preserved.NoteThe Seriesmustbe monotonically sorted, otherwise
wrong locations will likely be returned. Pandas doesnotcheck this for you.Parameters:valuearray-like or scalarValues to insert intoself.side{eleftf, erightf}, optionalIf eleftf, the index of the first suitable location found is given.
If erightf, return the last such index. If there is no suitable
index, return either 0 or N (where N is the length ofself).sorter1-D array-like, optionalOptional array of integer indices that sortselfinto ascending
order. They are typically the result ofnp.argsort.Returns:int or array of intA scalar or array of insertion points with the
same shape asvalue.See alsosort_valuesSort by the values along either axis.numpy.searchsortedSimilar method from NumPy.NotesBinary search is used to find the required insertion points.Examples>>>ser=pd.Series([1,2,3])>>>ser0    11    22    3dtype: int64>>>ser.searchsorted(4)3>>>ser.searchsorted([0,4])array([0, 3])>>>ser.searchsorted([1,3],side='left')array([0, 2])>>>ser.searchsorted([1,3],side='right')array([1, 3])>>>ser=pd.Series(pd.to_datetime(['3/11/2000','3/12/2000','3/13/2000']))>>>ser0   2000-03-111   2000-03-122   2000-03-13dtype: datetime64[ns]>>>ser.searchsorted('3/14/2000')3>>>ser=pd.Categorical(...['apple','bread','bread','cheese','milk'],ordered=True...)>>>ser['apple', 'bread', 'bread', 'cheese', 'milk']Categories (4, object): ['apple' < 'bread' < 'cheese' < 'milk']>>>ser.searchsorted('bread')1>>>ser.searchsorted(['bread'],side='right')array([3])If the values are not monotonically sorted, wrong locations
may be returned:>>>ser=pd.Series([2,1,3])>>>ser0    21    12    3dtype: int64>>>ser.searchsorted(1)0  # wrong result, correct would be 1"
Pandas,Series,pandas.Series.ravel,"pandas.Series.ravel#Series.ravel(order='C')[source]#Return the flattened underlying data as an ndarray or ExtensionArray.Returns:numpy.ndarray or ExtensionArrayFlattened data of the Series.See alsonumpy.ndarray.ravelReturn a flattened array.Examples>>>s=pd.Series([1,2,3])>>>s.ravel()array([1, 2, 3])"
Pandas,Series,pandas.Series.repeat,"pandas.Series.repeat#Series.repeat(repeats,axis=None)[source]#Repeat elements of a Series.Returns a new Series where each element of the current Series
is repeated consecutively a given number of times.Parameters:repeatsint or array of intsThe number of repetitions for each element. This should be a
non-negative integer. Repeating 0 times will return an empty
Series.axisNoneUnused. Parameter needed for compatibility with DataFrame.Returns:SeriesNewly created Series with repeated elements.See alsoIndex.repeatEquivalent function for Index.numpy.repeatSimilar method fornumpy.ndarray.Examples>>>s=pd.Series(['a','b','c'])>>>s0    a1    b2    cdtype: object>>>s.repeat(2)0    a0    a1    b1    b2    c2    cdtype: object>>>s.repeat([1,2,3])0    a1    b1    b2    c2    c2    cdtype: object"
Pandas,Series,pandas.Series.squeeze,"pandas.Series.squeeze#Series.squeeze(axis=None)[source]#Squeeze 1 dimensional axis objects into scalars.Series or DataFrames with a single element are squeezed to a scalar.
DataFrames with a single column or a single row are squeezed to a
Series. Otherwise the object is unchanged.This method is most useful when you donft know if your
object is a Series or DataFrame, but you do know it has just a single
column. In that case you can safely callsqueezeto ensure you have a
Series.Parameters:axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneA specific axis to squeeze. By default, all length-1 axes are
squeezed. ForSeriesthis parameter is unused and defaults toNone.Returns:DataFrame, Series, or scalarThe projection after squeezingaxisor all the axes.See alsoSeries.ilocInteger-location based indexing for selecting scalars.DataFrame.ilocInteger-location based indexing for selecting Series.Series.to_frameInverse of DataFrame.squeeze for a single-column DataFrame.Examples>>>primes=pd.Series([2,3,5,7])Slicing might produce a Series with a single value:>>>even_primes=primes[primes%2==0]>>>even_primes0    2dtype: int64>>>even_primes.squeeze()2Squeezing objects with more than one value in every axis does nothing:>>>odd_primes=primes[primes%2==1]>>>odd_primes1    32    53    7dtype: int64>>>odd_primes.squeeze()1    32    53    7dtype: int64Squeezing is even more effective when used with DataFrames.>>>df=pd.DataFrame([[1,2],[3,4]],columns=['a','b'])>>>dfa  b0  1  21  3  4Slicing a single column will produce a DataFrame with the columns
having only one value:>>>df_a=df[['a']]>>>df_aa0  11  3So the columns can be squeezed down, resulting in a Series:>>>df_a.squeeze('columns')0    11    3Name: a, dtype: int64Slicing a single row from a single column will produce a single
scalar DataFrame:>>>df_0a=df.loc[df.index<1,['a']]>>>df_0aa0  1Squeezing the rows produces a single scalar Series:>>>df_0a.squeeze('rows')a    1Name: 0, dtype: int64Squeezing all axes will project directly into a scalar:>>>df_0a.squeeze()1"
Pandas,Series,pandas.Series.view,"pandas.Series.view#Series.view(dtype=None)[source]#Create a new view of the Series.This function will return a new Series with a view of the same
underlying values in memory, optionally reinterpreted with a new data
type. The new data type must preserve the same size in bytes as to not
cause index misalignment.Parameters:dtypedata typeData type object or one of their string representations.Returns:SeriesA new Series object as a view of the same data in memory.See alsonumpy.ndarray.viewEquivalent numpy function to create a new view of the same data in memory.NotesSeries are instantiated withdtype=float64by default. Whilenumpy.ndarray.view()will return a view with the same data type as
the original array,Series.view()(without specified dtype)
will try usingfloat64and may fail if the original data type size
in bytes is not the same.Examples>>>s=pd.Series([-2,-1,0,1,2],dtype='int8')>>>s0   -21   -12    03    14    2dtype: int8The 8 bit signed integer representation of-1is0b11111111, but
the same bytes represent 255 if read as an 8 bit unsigned integer:>>>us=s.view('uint8')>>>us0    2541    2552      03      14      2dtype: uint8The views share the same underlying values:>>>us[0]=128>>>s0   -1281     -12      03      14      2dtype: int8"
Pandas,Series,pandas.Series.compare,"pandas.Series.compare#Series.compare(other,align_axis=1,keep_shape=False,keep_equal=False,result_names=('self','other'))[source]#Compare to another Series and show the differences.Parameters:otherSeriesObject to compare with.align_axis{0 or eindexf, 1 or ecolumnsf}, default 1Determine which axis to align the comparison on.0, or eindexfResulting differences are stacked verticallywith rows drawn alternately from self and other.1, or ecolumnsfResulting differences are aligned horizontallywith columns drawn alternately from self and other.keep_shapebool, default FalseIf true, all rows and columns are kept.
Otherwise, only the ones with different values are kept.keep_equalbool, default FalseIf true, the result keeps values that are equal.
Otherwise, equal values are shown as NaNs.result_namestuple, default (eselff, eotherf)Set the dataframes names in the comparison.New in version 1.5.0.Returns:Series or DataFrameIf axis is 0 or eindexf the result will be a Series.
The resulting index will be a MultiIndex with eselff and eotherf
stacked alternately at the inner level.If axis is 1 or ecolumnsf the result will be a DataFrame.
It will have two columns namely eselff and eotherf.See alsoDataFrame.compareCompare with another DataFrame and show differences.NotesMatching NaNs will not appear as a difference.Examples>>>s1=pd.Series([""a"",""b"",""c"",""d"",""e""])>>>s2=pd.Series([""a"",""a"",""c"",""b"",""e""])Align the differences on columns>>>s1.compare(s2)self other1    b     a3    d     bStack the differences on indices>>>s1.compare(s2,align_axis=0)1  self     bother    a3  self     dother    bdtype: objectKeep all original rows>>>s1.compare(s2,keep_shape=True)self other0  NaN   NaN1    b     a2  NaN   NaN3    d     b4  NaN   NaNKeep all original rows and also all original values>>>s1.compare(s2,keep_shape=True,keep_equal=True)self other0    a     a1    b     a2    c     c3    d     b4    e     e"
Pandas,Series,pandas.Series.update,"pandas.Series.update#Series.update(other)[source]#Modify Series in place using values from passed Series.Uses non-NA values from passed Series to make updates. Aligns
on index.Parameters:otherSeries, or object coercible into SeriesExamples>>>s=pd.Series([1,2,3])>>>s.update(pd.Series([4,5,6]))>>>s0    41    52    6dtype: int64>>>s=pd.Series(['a','b','c'])>>>s.update(pd.Series(['d','e'],index=[0,2]))>>>s0    d1    b2    edtype: object>>>s=pd.Series([1,2,3])>>>s.update(pd.Series([4,5,6,7,8]))>>>s0    41    52    6dtype: int64Ifothercontains NaNs the corresponding values are not updated
in the original Series.>>>s=pd.Series([1,2,3])>>>s.update(pd.Series([4,np.nan,6]))>>>s0    41    22    6dtype: int64othercan also be a non-Series object type
that is coercible into a Series>>>s=pd.Series([1,2,3])>>>s.update([4,np.nan,6])>>>s0    41    22    6dtype: int64>>>s=pd.Series([1,2,3])>>>s.update({1:9})>>>s0    11    92    3dtype: int64"
Pandas,Series,pandas.Series.asfreq,"pandas.Series.asfreq#Series.asfreq(freq,method=None,how=None,normalize=False,fill_value=None)[source]#Convert time series to specified frequency.Returns the original data conformed to a new index with the specified
frequency.If the index of this Series/DataFrame is aPeriodIndex, the new index
is the result of transforming the original index withPeriodIndex.asfreq(so the original index
will map one-to-one to the new index).Otherwise, the new index will be equivalent topd.date_range(start,end,freq=freq)wherestartandendare, respectively, the first and
last entries in the original index (seepandas.date_range()). The
values corresponding to any timesteps in the new index which were not present
in the original index will be null (NaN), unless a method for filling
such unknowns is provided (see themethodparameter below).Theresample()method is more appropriate if an operation on each group of
timesteps (such as an aggregate) is necessary to represent the data at the new
frequency.Parameters:freqDateOffset or strFrequency DateOffset or string.method{ebackfillf/fbfillf, epadf/fffillf}, default NoneMethod to use for filling holes in reindexed Series (note this
does not fill NaNs that already were present):epadf / effillf: propagate last valid observation forward to next
validebackfillf / ebfillf: use NEXT valid observation to fill.how{estartf, eendf}, default endFor PeriodIndex only (see PeriodIndex.asfreq).normalizebool, default FalseWhether to reset output index to midnight.fill_valuescalar, optionalValue to use for missing values, applied during upsampling (note
this does not fill NaNs that already were present).Returns:Series/DataFrameSeries/DataFrame object reindexed to the specified frequency.See alsoreindexConform DataFrame to new index with optional filling logic.NotesTo learn more about the frequency strings, please seethis link.ExamplesStart by creating a series with 4 one minute timestamps.>>>index=pd.date_range('1/1/2000',periods=4,freq='T')>>>series=pd.Series([0.0,None,2.0,3.0],index=index)>>>df=pd.DataFrame({'s':series})>>>dfs2000-01-01 00:00:00    0.02000-01-01 00:01:00    NaN2000-01-01 00:02:00    2.02000-01-01 00:03:00    3.0Upsample the series into 30 second bins.>>>df.asfreq(freq='30S')s2000-01-01 00:00:00    0.02000-01-01 00:00:30    NaN2000-01-01 00:01:00    NaN2000-01-01 00:01:30    NaN2000-01-01 00:02:00    2.02000-01-01 00:02:30    NaN2000-01-01 00:03:00    3.0Upsample again, providing afillvalue.>>>df.asfreq(freq='30S',fill_value=9.0)s2000-01-01 00:00:00    0.02000-01-01 00:00:30    9.02000-01-01 00:01:00    NaN2000-01-01 00:01:30    9.02000-01-01 00:02:00    2.02000-01-01 00:02:30    9.02000-01-01 00:03:00    3.0Upsample again, providing amethod.>>>df.asfreq(freq='30S',method='bfill')s2000-01-01 00:00:00    0.02000-01-01 00:00:30    NaN2000-01-01 00:01:00    NaN2000-01-01 00:01:30    2.02000-01-01 00:02:00    2.02000-01-01 00:02:30    3.02000-01-01 00:03:00    3.0"
Pandas,Series,pandas.Series.asof,"pandas.Series.asof#Series.asof(where,subset=None)[source]#Return the last row(s) without any NaNs beforewhere.The last row (for each element inwhere, if list) without any
NaN is taken.
In case of aDataFrame, the last row without NaN
considering only the subset of columns (if notNone)If there is no good value, NaN is returned for a Series or
a Series of NaN values for a DataFrameParameters:wheredate or array-like of datesDate(s) before which the last row(s) are returned.subsetstr or array-like of str, defaultNoneFor DataFrame, if notNone, only use these columns to
check for NaNs.Returns:scalar, Series, or DataFrameThe return can be:scalar : whenselfis a Series andwhereis a scalarSeries: whenselfis a Series andwhereis an array-like,
or whenselfis a DataFrame andwhereis a scalarDataFrame : whenselfis a DataFrame andwhereis an
array-likeReturn scalar, Series, or DataFrame.See alsomerge_asofPerform an asof merge. Similar to left join.NotesDates are assumed to be sorted. Raises if this is not the case.ExamplesA Series and a scalarwhere.>>>s=pd.Series([1,2,np.nan,4],index=[10,20,30,40])>>>s10    1.020    2.030    NaN40    4.0dtype: float64>>>s.asof(20)2.0For a sequencewhere, a Series is returned. The first value is
NaN, because the first element ofwhereis before the first
index value.>>>s.asof([5,20])5     NaN20    2.0dtype: float64Missing values are not considered. The following is2.0, not
NaN, even though NaN is at the index location for30.>>>s.asof(30)2.0Take all columns into consideration>>>df=pd.DataFrame({'a':[10.,20.,30.,40.,50.],...'b':[None,None,None,None,500]},...index=pd.DatetimeIndex(['2018-02-27 09:01:00',...'2018-02-27 09:02:00',...'2018-02-27 09:03:00',...'2018-02-27 09:04:00',...'2018-02-27 09:05:00']))>>>df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',...'2018-02-27 09:04:30']))a   b2018-02-27 09:03:30 NaN NaN2018-02-27 09:04:30 NaN NaNTake a single column into consideration>>>df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',...'2018-02-27 09:04:30']),...subset=['a'])a   b2018-02-27 09:03:30  30.0 NaN2018-02-27 09:04:30  40.0 NaN"
Pandas,Series,pandas.Series.shift,"pandas.Series.shift#Series.shift(periods=1,freq=None,axis=0,fill_value=_NoDefault.no_default,suffix=None)[source]#Shift index by desired number of periods with an optional timefreq.Whenfreqis not passed, shift the index without realigning the data.
Iffreqis passed (in this case, the index must be date or datetime,
or it will raise aNotImplementedError), the index will be
increased using the periods and thefreq.freqcan be inferred
when specified as ginferh as long as either freq or inferred_freq
attribute is set in the index.Parameters:periodsint or SequenceNumber of periods to shift. Can be positive or negative.
If an iterable of ints, the data will be shifted once by each int.
This is equivalent to shifting by one value at a time and
concatenating all resulting frames. The resulting columns will have
the shift suffixed to their column names. For multiple periods,
axis must not be 1.freqDateOffset, tseries.offsets, timedelta, or str, optionalOffset to use from the tseries module or time rule (e.g. eEOMf).
Iffreqis specified then the index values are shifted but the
data is not realigned. That is, usefreqif you would like to
extend the index when shifting and preserve the original data.
Iffreqis specified as ginferh then it will be inferred from
the freq or inferred_freq attributes of the index. If neither of
those attributes exist, a ValueError is thrown.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneShift direction. ForSeriesthis parameter is unused and defaults to 0.fill_valueobject, optionalThe scalar value to use for newly introduced missing values.
the default depends on the dtype ofself.
For numeric data,np.nanis used.
For datetime, timedelta, or period data, etc.NaTis used.
For extension dtypes,self.dtype.na_valueis used.suffixstr, optionalIf str and periods is an iterable, this is added after the column
name and before the shift value for each shifted column name.Returns:Series/DataFrameCopy of input object, shifted.See alsoIndex.shiftShift values of Index.DatetimeIndex.shiftShift values of DatetimeIndex.PeriodIndex.shiftShift values of PeriodIndex.Examples>>>df=pd.DataFrame({""Col1"":[10,20,15,30,45],...""Col2"":[13,23,18,33,48],...""Col3"":[17,27,22,37,52]},...index=pd.date_range(""2020-01-01"",""2020-01-05""))>>>dfCol1  Col2  Col32020-01-01    10    13    172020-01-02    20    23    272020-01-03    15    18    222020-01-04    30    33    372020-01-05    45    48    52>>>df.shift(periods=3)Col1  Col2  Col32020-01-01   NaN   NaN   NaN2020-01-02   NaN   NaN   NaN2020-01-03   NaN   NaN   NaN2020-01-04  10.0  13.0  17.02020-01-05  20.0  23.0  27.0>>>df.shift(periods=1,axis=""columns"")Col1  Col2  Col32020-01-01   NaN    10    132020-01-02   NaN    20    232020-01-03   NaN    15    182020-01-04   NaN    30    332020-01-05   NaN    45    48>>>df.shift(periods=3,fill_value=0)Col1  Col2  Col32020-01-01     0     0     02020-01-02     0     0     02020-01-03     0     0     02020-01-04    10    13    172020-01-05    20    23    27>>>df.shift(periods=3,freq=""D"")Col1  Col2  Col32020-01-04    10    13    172020-01-05    20    23    272020-01-06    15    18    222020-01-07    30    33    372020-01-08    45    48    52>>>df.shift(periods=3,freq=""infer"")Col1  Col2  Col32020-01-04    10    13    172020-01-05    20    23    272020-01-06    15    18    222020-01-07    30    33    372020-01-08    45    48    52>>>df['Col1'].shift(periods=[0,1,2])Col1_0  Col1_1  Col1_22020-01-01      10     NaN     NaN2020-01-02      20    10.0     NaN2020-01-03      15    20.0    10.02020-01-04      30    15.0    20.02020-01-05      45    30.0    15.0"
Pandas,Series,pandas.Series.first_valid_index,"pandas.Series.first_valid_index#Series.first_valid_index()[source]#Return index for first non-NA value or None, if no non-NA value is found.Returns:type of indexNotesIf all elements are non-NA/null, returns None.
Also returns None for empty Series/DataFrame.ExamplesFor Series:>>>s=pd.Series([None,3,4])>>>s.first_valid_index()1>>>s.last_valid_index()2For DataFrame:>>>df=pd.DataFrame({'A':[None,None,2],'B':[None,3,4]})>>>dfA      B0  NaN    NaN1  NaN    3.02  2.0    4.0>>>df.first_valid_index()1>>>df.last_valid_index()2"
Pandas,Series,pandas.Series.last_valid_index,"pandas.Series.last_valid_index#Series.last_valid_index()[source]#Return index for last non-NA value or None, if no non-NA value is found.Returns:type of indexNotesIf all elements are non-NA/null, returns None.
Also returns None for empty Series/DataFrame.ExamplesFor Series:>>>s=pd.Series([None,3,4])>>>s.first_valid_index()1>>>s.last_valid_index()2For DataFrame:>>>df=pd.DataFrame({'A':[None,None,2],'B':[None,3,4]})>>>dfA      B0  NaN    NaN1  NaN    3.02  2.0    4.0>>>df.first_valid_index()1>>>df.last_valid_index()2"
Pandas,Series,pandas.Series.resample,"pandas.Series.resample#Series.resample(rule,axis=_NoDefault.no_default,closed=None,label=None,convention='start',kind=None,on=None,level=None,origin='start_day',offset=None,group_keys=False)[source]#Resample time-series data.Convenience method for frequency conversion and resampling of time series.
The object must have a datetime-like index (DatetimeIndex,PeriodIndex,
orTimedeltaIndex), or the caller must pass the label of a datetime-like
series/index to theon/levelkeyword parameter.Parameters:ruleDateOffset, Timedelta or strThe offset string or object representing target conversion.axis{0 or eindexf, 1 or ecolumnsf}, default 0Which axis to use for up- or down-sampling. ForSeriesthis parameter
is unused and defaults to 0. Must beDatetimeIndex,TimedeltaIndexorPeriodIndex.Deprecated since version 2.0.0:Use frame.T.resample(c) instead.closed{erightf, eleftf}, default NoneWhich side of bin interval is closed. The default is eleftf
for all frequency offsets except for eMf, eAf, eQf, eBMf,
eBAf, eBQf, and eWf which all have a default of erightf.label{erightf, eleftf}, default NoneWhich bin edge label to label bucket with. The default is eleftf
for all frequency offsets except for eMf, eAf, eQf, eBMf,
eBAf, eBQf, and eWf which all have a default of erightf.convention{estartf, eendf, esf, eef}, default estartfForPeriodIndexonly, controls whether to use the start or
end ofrule.kind{etimestampf, eperiodf}, optional, default NonePass etimestampf to convert the resulting index to aDateTimeIndexor eperiodf to convert it to aPeriodIndex.
By default the input representation is retained.onstr, optionalFor a DataFrame, column to use instead of index for resampling.
Column must be datetime-like.levelstr or int, optionalFor a MultiIndex, level (name or number) to use for
resampling.levelmust be datetime-like.originTimestamp or str, default estart_dayfThe timestamp on which to adjust the grouping. The timezone of origin
must match the timezone of the index.
If string, must be one of the following:eepochf:originis 1970-01-01estartf:originis the first value of the timeseriesestart_dayf:originis the first day at midnight of the timeserieseendf:originis the last value of the timeserieseend_dayf:originis the ceiling midnight of the last dayNew in version 1.3.0.NoteOnly takes effect for Tick-frequencies (i.e. fixed frequencies like
days, hours, and minutes, rather than months or quarters).offsetTimedelta or str, default is NoneAn offset timedelta added to the origin.group_keysbool, default FalseWhether to include the group keys in the result index when using.apply()on the resampled object.New in version 1.5.0:Not specifyinggroup_keyswill retain values-dependent behavior
from pandas 1.4 and earlier (seepandas 1.5.0 Release notesfor examples).Changed in version 2.0.0:group_keysnow defaults toFalse.Returns:pandas.api.typing.ResamplerResamplerobject.See alsoSeries.resampleResample a Series.DataFrame.resampleResample a DataFrame.groupbyGroup Series/DataFrame by mapping, function, label, or list of labels.asfreqReindex a Series/DataFrame with the given frequency without grouping.NotesSee theuser guidefor more.To learn more about the offset strings, please seethis link.ExamplesStart by creating a series with 9 one minute timestamps.>>>index=pd.date_range('1/1/2000',periods=9,freq='T')>>>series=pd.Series(range(9),index=index)>>>series2000-01-01 00:00:00    02000-01-01 00:01:00    12000-01-01 00:02:00    22000-01-01 00:03:00    32000-01-01 00:04:00    42000-01-01 00:05:00    52000-01-01 00:06:00    62000-01-01 00:07:00    72000-01-01 00:08:00    8Freq: T, dtype: int64Downsample the series into 3 minute bins and sum the values
of the timestamps falling into a bin.>>>series.resample('3T').sum()2000-01-01 00:00:00     32000-01-01 00:03:00    122000-01-01 00:06:00    21Freq: 3T, dtype: int64Downsample the series into 3 minute bins as above, but label each
bin using the right edge instead of the left. Please note that the
value in the bucket used as the label is not included in the bucket,
which it labels. For example, in the original series the
bucket2000-01-0100:03:00contains the value 3, but the summed
value in the resampled bucket with the label2000-01-0100:03:00does not include 3 (if it did, the summed value would be 6, not 3).
To include this value close the right side of the bin interval as
illustrated in the example below this one.>>>series.resample('3T',label='right').sum()2000-01-01 00:03:00     32000-01-01 00:06:00    122000-01-01 00:09:00    21Freq: 3T, dtype: int64Downsample the series into 3 minute bins as above, but close the right
side of the bin interval.>>>series.resample('3T',label='right',closed='right').sum()2000-01-01 00:00:00     02000-01-01 00:03:00     62000-01-01 00:06:00    152000-01-01 00:09:00    15Freq: 3T, dtype: int64Upsample the series into 30 second bins.>>>series.resample('30S').asfreq()[0:5]# Select first 5 rows2000-01-01 00:00:00   0.02000-01-01 00:00:30   NaN2000-01-01 00:01:00   1.02000-01-01 00:01:30   NaN2000-01-01 00:02:00   2.0Freq: 30S, dtype: float64Upsample the series into 30 second bins and fill theNaNvalues using theffillmethod.>>>series.resample('30S').ffill()[0:5]2000-01-01 00:00:00    02000-01-01 00:00:30    02000-01-01 00:01:00    12000-01-01 00:01:30    12000-01-01 00:02:00    2Freq: 30S, dtype: int64Upsample the series into 30 second bins and fill theNaNvalues using thebfillmethod.>>>series.resample('30S').bfill()[0:5]2000-01-01 00:00:00    02000-01-01 00:00:30    12000-01-01 00:01:00    12000-01-01 00:01:30    22000-01-01 00:02:00    2Freq: 30S, dtype: int64Pass a custom function viaapply>>>defcustom_resampler(arraylike):...returnnp.sum(arraylike)+5...>>>series.resample('3T').apply(custom_resampler)2000-01-01 00:00:00     82000-01-01 00:03:00    172000-01-01 00:06:00    26Freq: 3T, dtype: int64For a Series with a PeriodIndex, the keywordconventioncan be
used to control whether to use the start or end ofrule.Resample a year by quarter using estartfconvention. Values are
assigned to the first quarter of the period.>>>s=pd.Series([1,2],index=pd.period_range('2012-01-01',...freq='A',...periods=2))>>>s2012    12013    2Freq: A-DEC, dtype: int64>>>s.resample('Q',convention='start').asfreq()2012Q1    1.02012Q2    NaN2012Q3    NaN2012Q4    NaN2013Q1    2.02013Q2    NaN2013Q3    NaN2013Q4    NaNFreq: Q-DEC, dtype: float64Resample quarters by month using eendfconvention. Values are
assigned to the last month of the period.>>>q=pd.Series([1,2,3,4],index=pd.period_range('2018-01-01',...freq='Q',...periods=4))>>>q2018Q1    12018Q2    22018Q3    32018Q4    4Freq: Q-DEC, dtype: int64>>>q.resample('M',convention='end').asfreq()2018-03    1.02018-04    NaN2018-05    NaN2018-06    2.02018-07    NaN2018-08    NaN2018-09    3.02018-10    NaN2018-11    NaN2018-12    4.0Freq: M, dtype: float64For DataFrame objects, the keywordoncan be used to specify the
column instead of the index for resampling.>>>d={'price':[10,11,9,13,14,18,17,19],...'volume':[50,60,40,100,50,100,40,50]}>>>df=pd.DataFrame(d)>>>df['week_starting']=pd.date_range('01/01/2018',...periods=8,...freq='W')>>>dfprice  volume week_starting0     10      50    2018-01-071     11      60    2018-01-142      9      40    2018-01-213     13     100    2018-01-284     14      50    2018-02-045     18     100    2018-02-116     17      40    2018-02-187     19      50    2018-02-25>>>df.resample('M',on='week_starting').mean()price  volumeweek_starting2018-01-31     10.75    62.52018-02-28     17.00    60.0For a DataFrame with MultiIndex, the keywordlevelcan be used to
specify on which level the resampling needs to take place.>>>days=pd.date_range('1/1/2000',periods=4,freq='D')>>>d2={'price':[10,11,9,13,14,18,17,19],...'volume':[50,60,40,100,50,100,40,50]}>>>df2=pd.DataFrame(...d2,...index=pd.MultiIndex.from_product(...[days,['morning','afternoon']]...)...)>>>df2price  volume2000-01-01 morning       10      50afternoon     11      602000-01-02 morning        9      40afternoon     13     1002000-01-03 morning       14      50afternoon     18     1002000-01-04 morning       17      40afternoon     19      50>>>df2.resample('D',level=0).sum()price  volume2000-01-01     21     1102000-01-02     22     1402000-01-03     32     1502000-01-04     36      90If you want to adjust the start of the bins based on a fixed timestamp:>>>start,end='2000-10-01 23:30:00','2000-10-02 00:30:00'>>>rng=pd.date_range(start,end,freq='7min')>>>ts=pd.Series(np.arange(len(rng))*3,index=rng)>>>ts2000-10-01 23:30:00     02000-10-01 23:37:00     32000-10-01 23:44:00     62000-10-01 23:51:00     92000-10-01 23:58:00    122000-10-02 00:05:00    152000-10-02 00:12:00    182000-10-02 00:19:00    212000-10-02 00:26:00    24Freq: 7T, dtype: int64>>>ts.resample('17min').sum()2000-10-01 23:14:00     02000-10-01 23:31:00     92000-10-01 23:48:00    212000-10-02 00:05:00    542000-10-02 00:22:00    24Freq: 17T, dtype: int64>>>ts.resample('17min',origin='epoch').sum()2000-10-01 23:18:00     02000-10-01 23:35:00    182000-10-01 23:52:00    272000-10-02 00:09:00    392000-10-02 00:26:00    24Freq: 17T, dtype: int64>>>ts.resample('17min',origin='2000-01-01').sum()2000-10-01 23:24:00     32000-10-01 23:41:00    152000-10-01 23:58:00    452000-10-02 00:15:00    45Freq: 17T, dtype: int64If you want to adjust the start of the bins with anoffsetTimedelta, the two
following lines are equivalent:>>>ts.resample('17min',origin='start').sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17T, dtype: int64>>>ts.resample('17min',offset='23h30min').sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17T, dtype: int64If you want to take the largest Timestamp as the end of the bins:>>>ts.resample('17min',origin='end').sum()2000-10-01 23:35:00     02000-10-01 23:52:00    182000-10-02 00:09:00    272000-10-02 00:26:00    63Freq: 17T, dtype: int64In contrast with thestart_day, you can useend_dayto take the ceiling
midnight of the largest Timestamp as the end of the bins and drop the bins
not containing data:>>>ts.resample('17min',origin='end_day').sum()2000-10-01 23:38:00     32000-10-01 23:55:00    152000-10-02 00:12:00    452000-10-02 00:29:00    45Freq: 17T, dtype: int64"
Pandas,Series,pandas.Series.tz_convert,"pandas.Series.tz_convert#Series.tz_convert(tz,axis=0,level=None,copy=None)[source]#Convert tz-aware axis to target time zone.Parameters:tzstr or tzinfo object or NoneTarget time zone. PassingNonewill convert to
UTC and remove the timezone information.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to convertlevelint, str, default NoneIf axis is a MultiIndex, convert a specific level. Otherwise
must be None.copybool, default TrueAlso make a copy of the underlying data.Returns:Series/DataFrameObject with time zone converted axis.Raises:TypeErrorIf the axis is tz-naive.ExamplesChange to another time zone:>>>s=pd.Series(...[1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']),...)>>>s.tz_convert('Asia/Shanghai')2018-09-15 07:30:00+08:00    1dtype: int64Pass None to convert to UTC and get a tz-naive index:>>>s=pd.Series([1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))>>>s.tz_convert(None)2018-09-14 23:30:00    1dtype: int64"
Pandas,Series,pandas.Series.tz_localize,"pandas.Series.tz_localize#Series.tz_localize(tz,axis=0,level=None,copy=None,ambiguous='raise',nonexistent='raise')[source]#Localize tz-naive index of a Series or DataFrame to target time zone.This operation localizes the Index. To localize the values in a
timezone-naive Series, useSeries.dt.tz_localize().Parameters:tzstr or tzinfo or NoneTime zone to localize. PassingNonewill remove the
time zone information and preserve local time.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to localizelevelint, str, default NoneIf axis ia a MultiIndex, localize a specific level. Otherwise
must be None.copybool, default TrueAlso make a copy of the underlying data.ambiguouseinferf, bool-ndarray, eNaTf, default eraisefWhen clocks moved backward due to DST, ambiguous times may arise.
For example in Central European Time (UTC+01), when going from
03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at
00:30:00 UTC and at 01:30:00 UTC. In such a situation, theambiguousparameter dictates how ambiguous times should be
handled.einferf will attempt to infer fall dst-transition hours based on
orderbool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)eNaTf will return NaT where there are ambiguous timeseraisef will raise an AmbiguousTimeError if there are ambiguous
times.nonexistentstr, default eraisefA nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST. Valid values are:eshift_forwardf will shift the nonexistent time forward to the
closest existing timeeshift_backwardf will shift the nonexistent time backward to the
closest existing timeeNaTf will return NaT where there are nonexistent timestimedelta objects will shift nonexistent times by the timedeltaeraisef will raise an NonExistentTimeError if there are
nonexistent times.Returns:Series/DataFrameSame type as the input.Raises:TypeErrorIf the TimeSeries is tz-aware and tz is not None.ExamplesLocalize local times:>>>s=pd.Series(...[1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00']),...)>>>s.tz_localize('CET')2018-09-15 01:30:00+02:00    1dtype: int64Pass None to convert to tz-naive index and preserve local time:>>>s=pd.Series([1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))>>>s.tz_localize(None)2018-09-15 01:30:00    1dtype: int64Be careful with DST changes. When there is sequential data, pandas
can infer the DST time:>>>s=pd.Series(range(7),...index=pd.DatetimeIndex(['2018-10-28 01:30:00',...'2018-10-28 02:00:00',...'2018-10-28 02:30:00',...'2018-10-28 02:00:00',...'2018-10-28 02:30:00',...'2018-10-28 03:00:00',...'2018-10-28 03:30:00']))>>>s.tz_localize('CET',ambiguous='infer')2018-10-28 01:30:00+02:00    02018-10-28 02:00:00+02:00    12018-10-28 02:30:00+02:00    22018-10-28 02:00:00+01:00    32018-10-28 02:30:00+01:00    42018-10-28 03:00:00+01:00    52018-10-28 03:30:00+01:00    6dtype: int64In some cases, inferring the DST is impossible. In such cases, you can
pass an ndarray to the ambiguous parameter to set the DST explicitly>>>s=pd.Series(range(3),...index=pd.DatetimeIndex(['2018-10-28 01:20:00',...'2018-10-28 02:36:00',...'2018-10-28 03:46:00']))>>>s.tz_localize('CET',ambiguous=np.array([True,True,False]))2018-10-28 01:20:00+02:00    02018-10-28 02:36:00+02:00    12018-10-28 03:46:00+01:00    2dtype: int64If the DST transition causes nonexistent times, you can shift these
dates forward or backward with a timedelta object oreshift_forwardforeshift_backwardf.>>>s=pd.Series(range(2),...index=pd.DatetimeIndex(['2015-03-29 02:30:00',...'2015-03-29 03:30:00']))>>>s.tz_localize('Europe/Warsaw',nonexistent='shift_forward')2015-03-29 03:00:00+02:00    02015-03-29 03:30:00+02:00    1dtype: int64>>>s.tz_localize('Europe/Warsaw',nonexistent='shift_backward')2015-03-29 01:59:59.999999999+01:00    02015-03-29 03:30:00+02:00              1dtype: int64>>>s.tz_localize('Europe/Warsaw',nonexistent=pd.Timedelta('1H'))2015-03-29 03:30:00+02:00    02015-03-29 03:30:00+02:00    1dtype: int64"
Pandas,Series,pandas.Series.at_time,"pandas.Series.at_time#Series.at_time(time,asof=False,axis=None)[source]#Select values at particular time of day (e.g., 9:30AM).Parameters:timedatetime.time or strThe values to select.axis{0 or eindexf, 1 or ecolumnsf}, default 0ForSeriesthis parameter is unused and defaults to 0.Returns:Series or DataFrameRaises:TypeErrorIf the index is not aDatetimeIndexSee alsobetween_timeSelect values between particular times of the day.firstSelect initial periods of time series based on a date offset.lastSelect final periods of time series based on a date offset.DatetimeIndex.indexer_at_timeGet just the index locations for values at particular time of the day.Examples>>>i=pd.date_range('2018-04-09',periods=4,freq='12H')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09 00:00:00  12018-04-09 12:00:00  22018-04-10 00:00:00  32018-04-10 12:00:00  4>>>ts.at_time('12:00')A2018-04-09 12:00:00  22018-04-10 12:00:00  4"
Pandas,Series,pandas.Series.between_time,"pandas.Series.between_time#Series.between_time(start_time,end_time,inclusive='both',axis=None)[source]#Select values between particular times of the day (e.g., 9:00-9:30 AM).By settingstart_timeto be later thanend_time,
you can get the times that arenotbetween the two times.Parameters:start_timedatetime.time or strInitial time as a time filter limit.end_timedatetime.time or strEnd time as a time filter limit.inclusive{gbothh, gneitherh, glefth, grighth}, default gbothhInclude boundaries; whether to set each bound as closed or open.axis{0 or eindexf, 1 or ecolumnsf}, default 0Determine range time on index or columns value.
ForSeriesthis parameter is unused and defaults to 0.Returns:Series or DataFrameData from the original object filtered to the specified dates range.Raises:TypeErrorIf the index is not aDatetimeIndexSee alsoat_timeSelect values at a particular time of the day.firstSelect initial periods of time series based on a date offset.lastSelect final periods of time series based on a date offset.DatetimeIndex.indexer_between_timeGet just the index locations for values between particular times of the day.Examples>>>i=pd.date_range('2018-04-09',periods=4,freq='1D20min')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09 00:00:00  12018-04-10 00:20:00  22018-04-11 00:40:00  32018-04-12 01:00:00  4>>>ts.between_time('0:15','0:45')A2018-04-10 00:20:00  22018-04-11 00:40:00  3You get the times that arenotbetween two times by settingstart_timelater thanend_time:>>>ts.between_time('0:45','0:15')A2018-04-09 00:00:00  12018-04-12 01:00:00  4"
Pandas,Series,pandas.Series.dt.date,"pandas.Series.dt.date#Series.dt.date[source]#Returns numpy array of pythondatetime.dateobjects.Namely, the date part of Timestamps without time and
timezone information.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.date0    2020-01-011    2020-02-01dtype: objectFor DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.datearray([datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)], dtype=object)"
Pandas,Series,pandas.Series.dt.time,"pandas.Series.dt.time#Series.dt.time[source]#Returns numpy array ofdatetime.timeobjects.The time part of the Timestamps.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.time0    10:00:001    11:00:00dtype: objectFor DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.timearray([datetime.time(10, 0), datetime.time(11, 0)], dtype=object)"
Pandas,Series,pandas.Series.dt.timetz,"pandas.Series.dt.timetz#Series.dt.timetz[source]#Returns numpy array ofdatetime.timeobjects with timezones.The time part of the Timestamps.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.timetz0    10:00:00+00:001    11:00:00+00:00dtype: objectFor DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.timetzarray([datetime.time(10, 0, tzinfo=datetime.timezone.utc),datetime.time(11, 0, tzinfo=datetime.timezone.utc)], dtype=object)"
Pandas,Series,pandas.Series.dt.year,"pandas.Series.dt.year#Series.dt.year[source]#The year of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""Y"")...)>>>datetime_series0   2000-12-311   2001-12-312   2002-12-31dtype: datetime64[ns]>>>datetime_series.dt.year0    20001    20012    2002dtype: int32"
Pandas,Series,pandas.Series.dt.month,"pandas.Series.dt.month#Series.dt.month[source]#The month as January=1, December=12.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""M"")...)>>>datetime_series0   2000-01-311   2000-02-292   2000-03-31dtype: datetime64[ns]>>>datetime_series.dt.month0    11    22    3dtype: int32"
Pandas,Series,pandas.Series.dt.day,"pandas.Series.dt.day#Series.dt.day[source]#The day of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""D"")...)>>>datetime_series0   2000-01-011   2000-01-022   2000-01-03dtype: datetime64[ns]>>>datetime_series.dt.day0    11    22    3dtype: int32"
Pandas,Series,pandas.Series.dt.hour,"pandas.Series.dt.hour#Series.dt.hour[source]#The hours of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""h"")...)>>>datetime_series0   2000-01-01 00:00:001   2000-01-01 01:00:002   2000-01-01 02:00:00dtype: datetime64[ns]>>>datetime_series.dt.hour0    01    12    2dtype: int32"
Pandas,Series,pandas.Series.dt.minute,"pandas.Series.dt.minute#Series.dt.minute[source]#The minutes of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""T"")...)>>>datetime_series0   2000-01-01 00:00:001   2000-01-01 00:01:002   2000-01-01 00:02:00dtype: datetime64[ns]>>>datetime_series.dt.minute0    01    12    2dtype: int32"
Pandas,Series,pandas.Series.dt.second,"pandas.Series.dt.second#Series.dt.second[source]#The seconds of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""s"")...)>>>datetime_series0   2000-01-01 00:00:001   2000-01-01 00:00:012   2000-01-01 00:00:02dtype: datetime64[ns]>>>datetime_series.dt.second0    01    12    2dtype: int32"
Pandas,Series,pandas.Series.dt.microsecond,"pandas.Series.dt.microsecond#Series.dt.microsecond[source]#The microseconds of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""us"")...)>>>datetime_series0   2000-01-01 00:00:00.0000001   2000-01-01 00:00:00.0000012   2000-01-01 00:00:00.000002dtype: datetime64[ns]>>>datetime_series.dt.microsecond0       01       12       2dtype: int32"
Pandas,Series,pandas.Series.dt.nanosecond,"pandas.Series.dt.nanosecond#Series.dt.nanosecond[source]#The nanoseconds of the datetime.Examples>>>datetime_series=pd.Series(...pd.date_range(""2000-01-01"",periods=3,freq=""ns"")...)>>>datetime_series0   2000-01-01 00:00:00.0000000001   2000-01-01 00:00:00.0000000012   2000-01-01 00:00:00.000000002dtype: datetime64[ns]>>>datetime_series.dt.nanosecond0       01       12       2dtype: int32"
Pandas,Series,pandas.Series.dt.dayofweek,"pandas.Series.dt.dayofweek#Series.dt.dayofweek[source]#The day of the week with Monday=0, Sunday=6.Return the day of the week. It is assumed the week starts on
Monday, which is denoted by 0 and ends on Sunday which is denoted
by 6. This method is available on both Series with datetime
values (using thedtaccessor) or DatetimeIndex.Returns:Series or IndexContaining integers indicating the day number.See alsoSeries.dt.dayofweekAlias.Series.dt.weekdayAlias.Series.dt.day_nameReturns the name of the day of the week.Examples>>>s=pd.date_range('2016-12-31','2017-01-08',freq='D').to_series()>>>s.dt.dayofweek2016-12-31    52017-01-01    62017-01-02    02017-01-03    12017-01-04    22017-01-05    32017-01-06    42017-01-07    52017-01-08    6Freq: D, dtype: int32"
Pandas,Series,pandas.Series.dt.day_of_week,"pandas.Series.dt.day_of_week#Series.dt.day_of_week[source]#The day of the week with Monday=0, Sunday=6.Return the day of the week. It is assumed the week starts on
Monday, which is denoted by 0 and ends on Sunday which is denoted
by 6. This method is available on both Series with datetime
values (using thedtaccessor) or DatetimeIndex.Returns:Series or IndexContaining integers indicating the day number.See alsoSeries.dt.dayofweekAlias.Series.dt.weekdayAlias.Series.dt.day_nameReturns the name of the day of the week.Examples>>>s=pd.date_range('2016-12-31','2017-01-08',freq='D').to_series()>>>s.dt.dayofweek2016-12-31    52017-01-01    62017-01-02    02017-01-03    12017-01-04    22017-01-05    32017-01-06    42017-01-07    52017-01-08    6Freq: D, dtype: int32"
Pandas,Series,pandas.Series.dt.weekday,"pandas.Series.dt.weekday#Series.dt.weekday[source]#The day of the week with Monday=0, Sunday=6.Return the day of the week. It is assumed the week starts on
Monday, which is denoted by 0 and ends on Sunday which is denoted
by 6. This method is available on both Series with datetime
values (using thedtaccessor) or DatetimeIndex.Returns:Series or IndexContaining integers indicating the day number.See alsoSeries.dt.dayofweekAlias.Series.dt.weekdayAlias.Series.dt.day_nameReturns the name of the day of the week.Examples>>>s=pd.date_range('2016-12-31','2017-01-08',freq='D').to_series()>>>s.dt.dayofweek2016-12-31    52017-01-01    62017-01-02    02017-01-03    12017-01-04    22017-01-05    32017-01-06    42017-01-07    52017-01-08    6Freq: D, dtype: int32"
Pandas,Series,pandas.Series.dt.dayofyear,"pandas.Series.dt.dayofyear#Series.dt.dayofyear[source]#The ordinal day of the year.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.dayofyear0    11   32dtype: int32For DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.dayofyearIndex([1, 32], dtype='int32')"
Pandas,Series,pandas.Series.dt.day_of_year,"pandas.Series.dt.day_of_year#Series.dt.day_of_year[source]#The ordinal day of the year.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.dayofyear0    11   32dtype: int32For DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.dayofyearIndex([1, 32], dtype='int32')"
Pandas,Series,pandas.Series.dt.days_in_month,"pandas.Series.dt.days_in_month#Series.dt.days_in_month[source]#The number of days in the month.Examples>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.daysinmonth0    311    29dtype: int32"
Pandas,Series,pandas.Series.dt.quarter,"pandas.Series.dt.quarter#Series.dt.quarter[source]#The quarter of the date.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""4/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-04-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.quarter0    11    2dtype: int32For DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.quarterIndex([1, 1], dtype='int32')"
Pandas,Series,pandas.Series.dt.is_month_start,"pandas.Series.dt.is_month_start#Series.dt.is_month_start[source]#Indicates whether the date is the first day of the month.Returns:Series or arrayFor Series, returns a Series with boolean values.
For DatetimeIndex, returns a boolean array.See alsois_month_startReturn a boolean indicating whether the date is the first day of the month.is_month_endReturn a boolean indicating whether the date is the last day of the month.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>s=pd.Series(pd.date_range(""2018-02-27"",periods=3))>>>s0   2018-02-271   2018-02-282   2018-03-01dtype: datetime64[ns]>>>s.dt.is_month_start0    False1    False2    Truedtype: bool>>>s.dt.is_month_end0    False1    True2    Falsedtype: bool>>>idx=pd.date_range(""2018-02-27"",periods=3)>>>idx.is_month_startarray([False, False, True])>>>idx.is_month_endarray([False, True, False])"
Pandas,Series,pandas.Series.dt.is_month_end,"pandas.Series.dt.is_month_end#Series.dt.is_month_end[source]#Indicates whether the date is the last day of the month.Returns:Series or arrayFor Series, returns a Series with boolean values.
For DatetimeIndex, returns a boolean array.See alsois_month_startReturn a boolean indicating whether the date is the first day of the month.is_month_endReturn a boolean indicating whether the date is the last day of the month.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>s=pd.Series(pd.date_range(""2018-02-27"",periods=3))>>>s0   2018-02-271   2018-02-282   2018-03-01dtype: datetime64[ns]>>>s.dt.is_month_start0    False1    False2    Truedtype: bool>>>s.dt.is_month_end0    False1    True2    Falsedtype: bool>>>idx=pd.date_range(""2018-02-27"",periods=3)>>>idx.is_month_startarray([False, False, True])>>>idx.is_month_endarray([False, True, False])"
Pandas,Series,pandas.Series.dt.is_quarter_start,"pandas.Series.dt.is_quarter_start#Series.dt.is_quarter_start[source]#Indicator for whether the date is the first day of a quarter.Returns:is_quarter_startSeries or DatetimeIndexThe same type as the original data with boolean values. Series will
have the same name and index. DatetimeIndex will have the same
name.See alsoquarterReturn the quarter of the date.is_quarter_endSimilar property for indicating the quarter end.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>df=pd.DataFrame({'dates':pd.date_range(""2017-03-30"",...periods=4)})>>>df.assign(quarter=df.dates.dt.quarter,...is_quarter_start=df.dates.dt.is_quarter_start)dates  quarter  is_quarter_start0 2017-03-30        1             False1 2017-03-31        1             False2 2017-04-01        2              True3 2017-04-02        2             False>>>idx=pd.date_range('2017-03-30',periods=4)>>>idxDatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],dtype='datetime64[ns]', freq='D')>>>idx.is_quarter_startarray([False, False,  True, False])"
Pandas,Series,pandas.Series.dt.is_quarter_end,"pandas.Series.dt.is_quarter_end#Series.dt.is_quarter_end[source]#Indicator for whether the date is the last day of a quarter.Returns:is_quarter_endSeries or DatetimeIndexThe same type as the original data with boolean values. Series will
have the same name and index. DatetimeIndex will have the same
name.See alsoquarterReturn the quarter of the date.is_quarter_startSimilar property indicating the quarter start.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>df=pd.DataFrame({'dates':pd.date_range(""2017-03-30"",...periods=4)})>>>df.assign(quarter=df.dates.dt.quarter,...is_quarter_end=df.dates.dt.is_quarter_end)dates  quarter    is_quarter_end0 2017-03-30        1             False1 2017-03-31        1              True2 2017-04-01        2             False3 2017-04-02        2             False>>>idx=pd.date_range('2017-03-30',periods=4)>>>idxDatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],dtype='datetime64[ns]', freq='D')>>>idx.is_quarter_endarray([False,  True, False, False])"
Pandas,Series,pandas.Series.dt.is_year_start,"pandas.Series.dt.is_year_start#Series.dt.is_year_start[source]#Indicate whether the date is the first day of a year.Returns:Series or DatetimeIndexThe same type as the original data with boolean values. Series will
have the same name and index. DatetimeIndex will have the same
name.See alsois_year_endSimilar property indicating the last day of the year.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>dates=pd.Series(pd.date_range(""2017-12-30"",periods=3))>>>dates0   2017-12-301   2017-12-312   2018-01-01dtype: datetime64[ns]>>>dates.dt.is_year_start0    False1    False2    Truedtype: bool>>>idx=pd.date_range(""2017-12-30"",periods=3)>>>idxDatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],dtype='datetime64[ns]', freq='D')>>>idx.is_year_startarray([False, False,  True])"
Pandas,Series,pandas.Series.dt.is_year_end,"pandas.Series.dt.is_year_end#Series.dt.is_year_end[source]#Indicate whether the date is the last day of the year.Returns:Series or DatetimeIndexThe same type as the original data with boolean values. Series will
have the same name and index. DatetimeIndex will have the same
name.See alsois_year_startSimilar property indicating the start of the year.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>dates=pd.Series(pd.date_range(""2017-12-30"",periods=3))>>>dates0   2017-12-301   2017-12-312   2018-01-01dtype: datetime64[ns]>>>dates.dt.is_year_end0    False1     True2    Falsedtype: bool>>>idx=pd.date_range(""2017-12-30"",periods=3)>>>idxDatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],dtype='datetime64[ns]', freq='D')>>>idx.is_year_endarray([False,  True, False])"
Pandas,Series,pandas.Series.dt.is_leap_year,"pandas.Series.dt.is_leap_year#Series.dt.is_leap_year[source]#Boolean indicator if the date belongs to a leap year.A leap year is a year, which has 366 days (instead of 365) including
29th of February as an intercalary day.
Leap years are years which are multiples of four with the exception
of years divisible by 100 but not by 400.Returns:Series or ndarrayBooleans indicating if dates belong to a leap year.ExamplesThis method is available on Series with datetime values under
the.dtaccessor, and directly on DatetimeIndex.>>>idx=pd.date_range(""2012-01-01"",""2015-01-01"",freq=""Y"")>>>idxDatetimeIndex(['2012-12-31', '2013-12-31', '2014-12-31'],dtype='datetime64[ns]', freq='A-DEC')>>>idx.is_leap_yeararray([ True, False, False])>>>dates_series=pd.Series(idx)>>>dates_series0   2012-12-311   2013-12-312   2014-12-31dtype: datetime64[ns]>>>dates_series.dt.is_leap_year0     True1    False2    Falsedtype: bool"
Pandas,Series,pandas.Series.dt.daysinmonth,"pandas.Series.dt.daysinmonth#Series.dt.daysinmonth[source]#The number of days in the month.Examples>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.daysinmonth0    311    29dtype: int32"
Pandas,Series,pandas.Series.dt.days_in_month,"pandas.Series.dt.days_in_month#Series.dt.days_in_month[source]#The number of days in the month.Examples>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.daysinmonth0    311    29dtype: int32"
Pandas,Series,pandas.Series.dt.tz,"pandas.Series.dt.tz#Series.dt.tz[source]#Return the timezone.Returns:datetime.tzinfo, pytz.tzinfo.BaseTZInfo, dateutil.tz.tz.tzfile, or NoneReturns None when the array is tz-naive.ExamplesFor Series:>>>s=pd.Series([""1/1/2020 10:00:00+00:00"",""2/1/2020 11:00:00+00:00""])>>>s=pd.to_datetime(s)>>>s0   2020-01-01 10:00:00+00:001   2020-02-01 11:00:00+00:00dtype: datetime64[ns, UTC]>>>s.dt.tzdatetime.timezone.utcFor DatetimeIndex:>>>idx=pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",...""2/1/2020 11:00:00+00:00""])>>>idx.tzdatetime.timezone.utc"
Pandas,Series,pandas.Series.dt.freq,pandas.Series.dt.freq#Series.dt.freq[source]#
Pandas,Series,pandas.Series.dt.unit,pandas.Series.dt.unit#Series.dt.unit[source]#
Pandas,Series,pandas.Series.dt.normalize,"pandas.Series.dt.normalize#Series.dt.normalize(*args,**kwargs)[source]#Convert times to midnight.The time component of the date-time is converted to midnight i.e.
00:00:00. This is useful in cases, when the time does not matter.
Length is unaltered. The timezones are unaffected.This method is available on Series with datetime values under
the.dtaccessor, and directly on Datetime Array/Index.Returns:DatetimeArray, DatetimeIndex or SeriesThe same type as the original data. Series will have the same
name and index. DatetimeIndex will have the same name.See alsofloorFloor the datetimes to the specified freq.ceilCeil the datetimes to the specified freq.roundRound the datetimes to the specified freq.Examples>>>idx=pd.date_range(start='2014-08-01 10:00',freq='H',...periods=3,tz='Asia/Calcutta')>>>idxDatetimeIndex(['2014-08-01 10:00:00+05:30','2014-08-01 11:00:00+05:30','2014-08-01 12:00:00+05:30'],dtype='datetime64[ns, Asia/Calcutta]', freq='H')>>>idx.normalize()DatetimeIndex(['2014-08-01 00:00:00+05:30','2014-08-01 00:00:00+05:30','2014-08-01 00:00:00+05:30'],dtype='datetime64[ns, Asia/Calcutta]', freq=None)"
Pandas,Series,pandas.Series.dt.isocalendar,"pandas.Series.dt.isocalendar#Series.dt.isocalendar()[source]#Calculate year, week, and day according to the ISO 8601 standard.Returns:DataFrameWith columns year, week and day.See alsoTimestamp.isocalendarFunction return a 3-tuple containing ISO year, week number, and weekday for the given Timestamp object.datetime.date.isocalendarReturn a named tuple object with three components: year, week and weekday.Examples>>>ser=pd.to_datetime(pd.Series([""2010-01-01"",pd.NaT]))>>>ser.dt.isocalendar()year  week  day0  2009    53     51  <NA>  <NA>  <NA>>>>ser.dt.isocalendar().week0      531    <NA>Name: week, dtype: UInt32"
Pandas,Series,pandas.Series.dt.to_period,"pandas.Series.dt.to_period#Series.dt.to_period(*args,**kwargs)[source]#Cast to PeriodArray/PeriodIndex at a particular frequency.Converts DatetimeArray/Index to PeriodArray/PeriodIndex.Parameters:freqstr or Period, optionalOne of pandasfperiod aliasesor an Period object. Will be inferred by default.Returns:PeriodArray/PeriodIndexRaises:ValueErrorWhen converting a DatetimeArray/Index with non-regular values,
so that a frequency cannot be inferred.See alsoPeriodIndexImmutable ndarray holding ordinal values.DatetimeIndex.to_pydatetimeReturn DatetimeIndex as object.Examples>>>df=pd.DataFrame({""y"":[1,2,3]},...index=pd.to_datetime([""2000-03-31 00:00:00"",...""2000-05-31 00:00:00"",...""2000-08-31 00:00:00""]))>>>df.index.to_period(""M"")PeriodIndex(['2000-03', '2000-05', '2000-08'],dtype='period[M]')Infer the daily frequency>>>idx=pd.date_range(""2017-01-01"",periods=2)>>>idx.to_period()PeriodIndex(['2017-01-01', '2017-01-02'],dtype='period[D]')"
Pandas,Series,pandas.Series.dt.to_pydatetime,"pandas.Series.dt.to_pydatetime#Series.dt.to_pydatetime()[source]#Return the data as an array ofdatetime.datetimeobjects.Deprecated since version 2.1.0:The current behavior of dt.to_pydatetime is deprecated.
In a future version this will return a Series containing python
datetime objects instead of a ndarray.Timezone information is retained if present.WarningPythonfs datetime uses microsecond resolution, which is lower than
pandas (nanosecond). The values are truncated.Returns:numpy.ndarrayObject dtype array containing native Python datetime objects.See alsodatetime.datetimeStandard library value for a datetime.Examples>>>s=pd.Series(pd.date_range('20180310',periods=2))>>>s0   2018-03-101   2018-03-11dtype: datetime64[ns]>>>s.dt.to_pydatetime()array([datetime.datetime(2018, 3, 10, 0, 0),datetime.datetime(2018, 3, 11, 0, 0)], dtype=object)pandasf nanosecond precision is truncated to microseconds.>>>s=pd.Series(pd.date_range('20180310',periods=2,freq='ns'))>>>s0   2018-03-10 00:00:00.0000000001   2018-03-10 00:00:00.000000001dtype: datetime64[ns]>>>s.dt.to_pydatetime()array([datetime.datetime(2018, 3, 10, 0, 0),datetime.datetime(2018, 3, 10, 0, 0)], dtype=object)"
Pandas,Series,pandas.Series.dt.tz_localize,"pandas.Series.dt.tz_localize#Series.dt.tz_localize(*args,**kwargs)[source]#Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.This method takes a time zone (tz) naive Datetime Array/Index object
and makes this time zone aware. It does not move the time to another
time zone.This method can also be used to do the inverse ? to create a time
zone unaware object from an aware object. To that end, passtz=None.Parameters:tzstr, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or NoneTime zone to convert timestamps to. PassingNonewill
remove the time zone information preserving local time.ambiguouseinferf, eNaTf, bool array, default eraisefWhen clocks moved backward due to DST, ambiguous times may arise.
For example in Central European Time (UTC+01), when going from
03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at
00:30:00 UTC and at 01:30:00 UTC. In such a situation, theambiguousparameter dictates how ambiguous times should be
handled.einferf will attempt to infer fall dst-transition hours based on
orderbool-ndarray where True signifies a DST time, False signifies a
non-DST time (note that this flag is only applicable for
ambiguous times)eNaTf will return NaT where there are ambiguous timeseraisef will raise an AmbiguousTimeError if there are ambiguous
times.nonexistenteshift_forwardf, eshift_backward, eNaTf, timedelta, default eraisefA nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.eshift_forwardf will shift the nonexistent time forward to the
closest existing timeeshift_backwardf will shift the nonexistent time backward to the
closest existing timeeNaTf will return NaT where there are nonexistent timestimedelta objects will shift nonexistent times by the timedeltaeraisef will raise an NonExistentTimeError if there are
nonexistent times.Returns:Same type as selfArray/Index converted to the specified time zone.Raises:TypeErrorIf the Datetime Array/Index is tz-aware and tz is not None.See alsoDatetimeIndex.tz_convertConvert tz-aware DatetimeIndex from one time zone to another.Examples>>>tz_naive=pd.date_range('2018-03-01 09:00',periods=3)>>>tz_naiveDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00','2018-03-03 09:00:00'],dtype='datetime64[ns]', freq='D')Localize DatetimeIndex in US/Eastern time zone:>>>tz_aware=tz_naive.tz_localize(tz='US/Eastern')>>>tz_awareDatetimeIndex(['2018-03-01 09:00:00-05:00','2018-03-02 09:00:00-05:00','2018-03-03 09:00:00-05:00'],dtype='datetime64[ns, US/Eastern]', freq=None)With thetz=None, we can remove the time zone information
while keeping the local time (not converted to UTC):>>>tz_aware.tz_localize(None)DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00','2018-03-03 09:00:00'],dtype='datetime64[ns]', freq=None)Be careful with DST changes. When there is sequential data, pandas can
infer the DST time:>>>s=pd.to_datetime(pd.Series(['2018-10-28 01:30:00',...'2018-10-28 02:00:00',...'2018-10-28 02:30:00',...'2018-10-28 02:00:00',...'2018-10-28 02:30:00',...'2018-10-28 03:00:00',...'2018-10-28 03:30:00']))>>>s.dt.tz_localize('CET',ambiguous='infer')0   2018-10-28 01:30:00+02:001   2018-10-28 02:00:00+02:002   2018-10-28 02:30:00+02:003   2018-10-28 02:00:00+01:004   2018-10-28 02:30:00+01:005   2018-10-28 03:00:00+01:006   2018-10-28 03:30:00+01:00dtype: datetime64[ns, CET]In some cases, inferring the DST is impossible. In such cases, you can
pass an ndarray to the ambiguous parameter to set the DST explicitly>>>s=pd.to_datetime(pd.Series(['2018-10-28 01:20:00',...'2018-10-28 02:36:00',...'2018-10-28 03:46:00']))>>>s.dt.tz_localize('CET',ambiguous=np.array([True,True,False]))0   2018-10-28 01:20:00+02:001   2018-10-28 02:36:00+02:002   2018-10-28 03:46:00+01:00dtype: datetime64[ns, CET]If the DST transition causes nonexistent times, you can shift these
dates forward or backwards with a timedelta object oreshift_forwardforeshift_backwardsf.>>>s=pd.to_datetime(pd.Series(['2015-03-29 02:30:00',...'2015-03-29 03:30:00']))>>>s.dt.tz_localize('Europe/Warsaw',nonexistent='shift_forward')0   2015-03-29 03:00:00+02:001   2015-03-29 03:30:00+02:00dtype: datetime64[ns, Europe/Warsaw]>>>s.dt.tz_localize('Europe/Warsaw',nonexistent='shift_backward')0   2015-03-29 01:59:59.999999999+01:001   2015-03-29 03:30:00+02:00dtype: datetime64[ns, Europe/Warsaw]>>>s.dt.tz_localize('Europe/Warsaw',nonexistent=pd.Timedelta('1H'))0   2015-03-29 03:30:00+02:001   2015-03-29 03:30:00+02:00dtype: datetime64[ns, Europe/Warsaw]"
Pandas,Series,pandas.Series.dt.tz_convert,"pandas.Series.dt.tz_convert#Series.dt.tz_convert(*args,**kwargs)[source]#Convert tz-aware Datetime Array/Index from one time zone to another.Parameters:tzstr, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or NoneTime zone for time. Corresponding timestamps would be converted
to this time zone of the Datetime Array/Index. Atzof None will
convert to UTC and remove the timezone information.Returns:Array or IndexRaises:TypeErrorIf Datetime Array/Index is tz-naive.See alsoDatetimeIndex.tzA timezone that has a variable offset from UTC.DatetimeIndex.tz_localizeLocalize tz-naive DatetimeIndex to a given time zone, or remove timezone from a tz-aware DatetimeIndex.ExamplesWith thetzparameter, we can change the DatetimeIndex
to other time zones:>>>dti=pd.date_range(start='2014-08-01 09:00',...freq='H',periods=3,tz='Europe/Berlin')>>>dtiDatetimeIndex(['2014-08-01 09:00:00+02:00','2014-08-01 10:00:00+02:00','2014-08-01 11:00:00+02:00'],dtype='datetime64[ns, Europe/Berlin]', freq='H')>>>dti.tz_convert('US/Central')DatetimeIndex(['2014-08-01 02:00:00-05:00','2014-08-01 03:00:00-05:00','2014-08-01 04:00:00-05:00'],dtype='datetime64[ns, US/Central]', freq='H')With thetz=None, we can remove the timezone (after converting
to UTC if necessary):>>>dti=pd.date_range(start='2014-08-01 09:00',freq='H',...periods=3,tz='Europe/Berlin')>>>dtiDatetimeIndex(['2014-08-01 09:00:00+02:00','2014-08-01 10:00:00+02:00','2014-08-01 11:00:00+02:00'],dtype='datetime64[ns, Europe/Berlin]', freq='H')>>>dti.tz_convert(None)DatetimeIndex(['2014-08-01 07:00:00','2014-08-01 08:00:00','2014-08-01 09:00:00'],dtype='datetime64[ns]', freq='H')"
Pandas,Series,pandas.Series.dt.normalize,"pandas.Series.dt.normalize#Series.dt.normalize(*args,**kwargs)[source]#Convert times to midnight.The time component of the date-time is converted to midnight i.e.
00:00:00. This is useful in cases, when the time does not matter.
Length is unaltered. The timezones are unaffected.This method is available on Series with datetime values under
the.dtaccessor, and directly on Datetime Array/Index.Returns:DatetimeArray, DatetimeIndex or SeriesThe same type as the original data. Series will have the same
name and index. DatetimeIndex will have the same name.See alsofloorFloor the datetimes to the specified freq.ceilCeil the datetimes to the specified freq.roundRound the datetimes to the specified freq.Examples>>>idx=pd.date_range(start='2014-08-01 10:00',freq='H',...periods=3,tz='Asia/Calcutta')>>>idxDatetimeIndex(['2014-08-01 10:00:00+05:30','2014-08-01 11:00:00+05:30','2014-08-01 12:00:00+05:30'],dtype='datetime64[ns, Asia/Calcutta]', freq='H')>>>idx.normalize()DatetimeIndex(['2014-08-01 00:00:00+05:30','2014-08-01 00:00:00+05:30','2014-08-01 00:00:00+05:30'],dtype='datetime64[ns, Asia/Calcutta]', freq=None)"
Pandas,Series,pandas.Series.dt.strftime,"pandas.Series.dt.strftime#Series.dt.strftime(*args,**kwargs)[source]#Convert to Index using specified date_format.Return an Index of formatted strings specified by date_format, which
supports the same string format as the python standard library. Details
of the string format can be found inpython string format
doc.Formats supported by the CstrftimeAPI but not by the python string format
doc (such asg%Rh,g%rh) are not officially supported and should be
preferably replaced with their supported equivalents (such asg%H:%Mh,g%I:%M:%S %ph).Note thatPeriodIndexsupport additional directives, detailed inPeriod.strftime.Parameters:date_formatstrDate format string (e.g. g%Y-%m-%dh).Returns:ndarray[object]NumPy ndarray of formatted strings.See alsoto_datetimeConvert the given argument to datetime.DatetimeIndex.normalizeReturn DatetimeIndex with times to midnight.DatetimeIndex.roundRound the DatetimeIndex to the specified freq.DatetimeIndex.floorFloor the DatetimeIndex to the specified freq.Timestamp.strftimeFormat a single Timestamp.Period.strftimeFormat a single Period.Examples>>>rng=pd.date_range(pd.Timestamp(""2018-03-10 09:00""),...periods=3,freq='s')>>>rng.strftime('%B%d, %Y,%r')Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM','March 10, 2018, 09:00:02 AM'],dtype='object')"
Pandas,Series,pandas.Series.dt.round,"pandas.Series.dt.round#Series.dt.round(*args,**kwargs)[source]#Perform round operation on the data to the specifiedfreq.Parameters:freqstr or OffsetThe frequency level to round the index to. Must be a fixed
frequency like eSf (second) not eMEf (month end). Seefrequency aliasesfor
a list of possiblefreqvalues.ambiguouseinferf, bool-ndarray, eNaTf, default eraisefOnly relevant for DatetimeIndex:einferf will attempt to infer fall dst-transition hours based on
orderbool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)eNaTf will return NaT where there are ambiguous timeseraisef will raise an AmbiguousTimeError if there are ambiguous
times.nonexistenteshift_forwardf, eshift_backwardf, eNaTf, timedelta, default eraisefA nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.eshift_forwardf will shift the nonexistent time forward to the
closest existing timeeshift_backwardf will shift the nonexistent time backward to the
closest existing timeeNaTf will return NaT where there are nonexistent timestimedelta objects will shift nonexistent times by the timedeltaeraisef will raise an NonExistentTimeError if there are
nonexistent times.Returns:DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex,
or a Series with the same index for a Series.Raises:ValueError if thefreqcannot be converted.NotesIf the timestamps have a timezone, rounding will take place relative to the
local (gwallh) time and re-localized to the same timezone. When rounding
near daylight savings time, usenonexistentandambiguousto
control the re-localization behavior.ExamplesDatetimeIndex>>>rng=pd.date_range('1/1/2018 11:59:00',periods=3,freq='min')>>>rngDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00','2018-01-01 12:01:00'],dtype='datetime64[ns]', freq='T')>>>rng.round('H')DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00','2018-01-01 12:00:00'],dtype='datetime64[ns]', freq=None)Series>>>pd.Series(rng).dt.round(""H"")0   2018-01-01 12:00:001   2018-01-01 12:00:002   2018-01-01 12:00:00dtype: datetime64[ns]When rounding near a daylight savings time transition, useambiguousornonexistentto control how the timestamp should be re-localized.>>>rng_tz=pd.DatetimeIndex([""2021-10-31 03:30:00""],tz=""Europe/Amsterdam"")>>>rng_tz.floor(""2H"",ambiguous=False)DatetimeIndex(['2021-10-31 02:00:00+01:00'],dtype='datetime64[ns, Europe/Amsterdam]', freq=None)>>>rng_tz.floor(""2H"",ambiguous=True)DatetimeIndex(['2021-10-31 02:00:00+02:00'],dtype='datetime64[ns, Europe/Amsterdam]', freq=None)"
Pandas,Series,pandas.Series.dt.floor,"pandas.Series.dt.floor#Series.dt.floor(*args,**kwargs)[source]#Perform floor operation on the data to the specifiedfreq.Parameters:freqstr or OffsetThe frequency level to floor the index to. Must be a fixed
frequency like eSf (second) not eMEf (month end). Seefrequency aliasesfor
a list of possiblefreqvalues.ambiguouseinferf, bool-ndarray, eNaTf, default eraisefOnly relevant for DatetimeIndex:einferf will attempt to infer fall dst-transition hours based on
orderbool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)eNaTf will return NaT where there are ambiguous timeseraisef will raise an AmbiguousTimeError if there are ambiguous
times.nonexistenteshift_forwardf, eshift_backwardf, eNaTf, timedelta, default eraisefA nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.eshift_forwardf will shift the nonexistent time forward to the
closest existing timeeshift_backwardf will shift the nonexistent time backward to the
closest existing timeeNaTf will return NaT where there are nonexistent timestimedelta objects will shift nonexistent times by the timedeltaeraisef will raise an NonExistentTimeError if there are
nonexistent times.Returns:DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex,
or a Series with the same index for a Series.Raises:ValueError if thefreqcannot be converted.NotesIf the timestamps have a timezone, flooring will take place relative to the
local (gwallh) time and re-localized to the same timezone. When flooring
near daylight savings time, usenonexistentandambiguousto
control the re-localization behavior.ExamplesDatetimeIndex>>>rng=pd.date_range('1/1/2018 11:59:00',periods=3,freq='min')>>>rngDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00','2018-01-01 12:01:00'],dtype='datetime64[ns]', freq='T')>>>rng.floor('H')DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00','2018-01-01 12:00:00'],dtype='datetime64[ns]', freq=None)Series>>>pd.Series(rng).dt.floor(""H"")0   2018-01-01 11:00:001   2018-01-01 12:00:002   2018-01-01 12:00:00dtype: datetime64[ns]When rounding near a daylight savings time transition, useambiguousornonexistentto control how the timestamp should be re-localized.>>>rng_tz=pd.DatetimeIndex([""2021-10-31 03:30:00""],tz=""Europe/Amsterdam"")>>>rng_tz.floor(""2H"",ambiguous=False)DatetimeIndex(['2021-10-31 02:00:00+01:00'],dtype='datetime64[ns, Europe/Amsterdam]', freq=None)>>>rng_tz.floor(""2H"",ambiguous=True)DatetimeIndex(['2021-10-31 02:00:00+02:00'],dtype='datetime64[ns, Europe/Amsterdam]', freq=None)"
Pandas,Series,pandas.Series.dt.ceil,"pandas.Series.dt.ceil#Series.dt.ceil(*args,**kwargs)[source]#Perform ceil operation on the data to the specifiedfreq.Parameters:freqstr or OffsetThe frequency level to ceil the index to. Must be a fixed
frequency like eSf (second) not eMEf (month end). Seefrequency aliasesfor
a list of possiblefreqvalues.ambiguouseinferf, bool-ndarray, eNaTf, default eraisefOnly relevant for DatetimeIndex:einferf will attempt to infer fall dst-transition hours based on
orderbool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)eNaTf will return NaT where there are ambiguous timeseraisef will raise an AmbiguousTimeError if there are ambiguous
times.nonexistenteshift_forwardf, eshift_backwardf, eNaTf, timedelta, default eraisefA nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST.eshift_forwardf will shift the nonexistent time forward to the
closest existing timeeshift_backwardf will shift the nonexistent time backward to the
closest existing timeeNaTf will return NaT where there are nonexistent timestimedelta objects will shift nonexistent times by the timedeltaeraisef will raise an NonExistentTimeError if there are
nonexistent times.Returns:DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex,
or a Series with the same index for a Series.Raises:ValueError if thefreqcannot be converted.NotesIf the timestamps have a timezone, ceiling will take place relative to the
local (gwallh) time and re-localized to the same timezone. When ceiling
near daylight savings time, usenonexistentandambiguousto
control the re-localization behavior.ExamplesDatetimeIndex>>>rng=pd.date_range('1/1/2018 11:59:00',periods=3,freq='min')>>>rngDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00','2018-01-01 12:01:00'],dtype='datetime64[ns]', freq='T')>>>rng.ceil('H')DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00','2018-01-01 13:00:00'],dtype='datetime64[ns]', freq=None)Series>>>pd.Series(rng).dt.ceil(""H"")0   2018-01-01 12:00:001   2018-01-01 12:00:002   2018-01-01 13:00:00dtype: datetime64[ns]When rounding near a daylight savings time transition, useambiguousornonexistentto control how the timestamp should be re-localized.>>>rng_tz=pd.DatetimeIndex([""2021-10-31 01:30:00""],tz=""Europe/Amsterdam"")>>>rng_tz.ceil(""H"",ambiguous=False)DatetimeIndex(['2021-10-31 02:00:00+01:00'],dtype='datetime64[ns, Europe/Amsterdam]', freq=None)>>>rng_tz.ceil(""H"",ambiguous=True)DatetimeIndex(['2021-10-31 02:00:00+02:00'],dtype='datetime64[ns, Europe/Amsterdam]', freq=None)"
Pandas,Series,pandas.Series.dt.month_name,"pandas.Series.dt.month_name#Series.dt.month_name(*args,**kwargs)[source]#Return the month names with specified locale.Parameters:localestr, optionalLocale determining the language in which to return the month name.
Default is English locale ('en_US.utf8'). Use the commandlocale-aon your terminal on Unix systems to find your locale
language code.Returns:Series or IndexSeries or Index of month names.Examples>>>s=pd.Series(pd.date_range(start='2018-01',freq='M',periods=3))>>>s0   2018-01-311   2018-02-282   2018-03-31dtype: datetime64[ns]>>>s.dt.month_name()0     January1    February2       Marchdtype: object>>>idx=pd.date_range(start='2018-01',freq='M',periods=3)>>>idxDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],dtype='datetime64[ns]', freq='M')>>>idx.month_name()Index(['January', 'February', 'March'], dtype='object')Using thelocaleparameter you can set a different locale language,
for example:idx.month_name(locale='pt_BR.utf8')will return month
names in Brazilian Portuguese language.>>>idx=pd.date_range(start='2018-01',freq='M',periods=3)>>>idxDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],dtype='datetime64[ns]', freq='M')>>>idx.month_name(locale='pt_BR.utf8')Index(['Janeiro', 'Fevereiro', 'Mar?o'], dtype='object')"
Pandas,Series,pandas.Series.dt.day_name,"pandas.Series.dt.day_name#Series.dt.day_name(*args,**kwargs)[source]#Return the day names with specified locale.Parameters:localestr, optionalLocale determining the language in which to return the day name.
Default is English locale ('en_US.utf8'). Use the commandlocale-aon your terminal on Unix systems to find your locale
language code.Returns:Series or IndexSeries or Index of day names.Examples>>>s=pd.Series(pd.date_range(start='2018-01-01',freq='D',periods=3))>>>s0   2018-01-011   2018-01-022   2018-01-03dtype: datetime64[ns]>>>s.dt.day_name()0       Monday1      Tuesday2    Wednesdaydtype: object>>>idx=pd.date_range(start='2018-01-01',freq='D',periods=3)>>>idxDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],dtype='datetime64[ns]', freq='D')>>>idx.day_name()Index(['Monday', 'Tuesday', 'Wednesday'], dtype='object')Using thelocaleparameter you can set a different locale language,
for example:idx.day_name(locale='pt_BR.utf8')will return day
names in Brazilian Portuguese language.>>>idx=pd.date_range(start='2018-01-01',freq='D',periods=3)>>>idxDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],dtype='datetime64[ns]', freq='D')>>>idx.day_name(locale='pt_BR.utf8')Index(['Segunda', 'Ter?a', 'Quarta'], dtype='object')"
Pandas,Series,pandas.Series.dt.as_unit,"pandas.Series.dt.as_unit#Series.dt.as_unit(*args,**kwargs)[source]#"
Pandas,Series,pandas.Series.dt.qyear,pandas.Series.dt.qyear#Series.dt.qyear[source]#
Pandas,Series,pandas.Series.dt.start_time,"pandas.Series.dt.start_time#Series.dt.start_time[source]#Get the Timestamp for the start of the period.Returns:TimestampSee alsoPeriod.end_timeReturn the end Timestamp.Period.dayofyearReturn the day of year.Period.daysinmonthReturn the days in that month.Period.dayofweekReturn the day of the week.Examples>>>period=pd.Period('2012-1-1',freq='D')>>>periodPeriod('2012-01-01', 'D')>>>period.start_timeTimestamp('2012-01-01 00:00:00')>>>period.end_timeTimestamp('2012-01-01 23:59:59.999999999')"
Pandas,Series,pandas.Series.dt.end_time,"pandas.Series.dt.end_time#Series.dt.end_time[source]#Get the Timestamp for the end of the period.Returns:TimestampSee alsoPeriod.start_timeReturn the start Timestamp.Period.dayofyearReturn the day of year.Period.daysinmonthReturn the days in that month.Period.dayofweekReturn the day of the week.ExamplesFor Period:>>>pd.Period('2020-01','D').end_timeTimestamp('2020-01-01 23:59:59.999999999')For Series:>>>period_index=pd.period_range('2020-1-1 00:00','2020-3-1 00:00',freq='M')>>>s=pd.Series(period_index)>>>s0   2020-011   2020-022   2020-03dtype: period[M]>>>s.dt.end_time0   2020-01-31 23:59:59.9999999991   2020-02-29 23:59:59.9999999992   2020-03-31 23:59:59.999999999dtype: datetime64[ns]For PeriodIndex:>>>idx=pd.PeriodIndex([""2023-01"",""2023-02"",""2023-03""],freq=""M"")>>>idx.end_timeDatetimeIndex(['2023-01-31 23:59:59.999999999','2023-02-28 23:59:59.999999999','2023-03-31 23:59:59.999999999'],dtype='datetime64[ns]', freq=None)"
Pandas,Series,pandas.Series.dt.days,"pandas.Series.dt.days#Series.dt.days[source]#Number of days for each element.ExamplesFor Series:>>>ser=pd.Series(pd.to_timedelta([1,2,3],unit='d'))>>>ser0   1 days1   2 days2   3 daysdtype: timedelta64[ns]>>>ser.dt.days0    11    22    3dtype: int64For TimedeltaIndex:>>>tdelta_idx=pd.to_timedelta([""0 days"",""10 days"",""20 days""])>>>tdelta_idxTimedeltaIndex(['0 days', '10 days', '20 days'],dtype='timedelta64[ns]', freq=None)>>>tdelta_idx.daysIndex([0, 10, 20], dtype='int64')"
Pandas,Series,pandas.Series.dt.seconds,"pandas.Series.dt.seconds#Series.dt.seconds[source]#Number of seconds (>= 0 and less than 1 day) for each element.ExamplesFor Series:>>>ser=pd.Series(pd.to_timedelta([1,2,3],unit='S'))>>>ser0   0 days 00:00:011   0 days 00:00:022   0 days 00:00:03dtype: timedelta64[ns]>>>ser.dt.seconds0    11    22    3dtype: int32For TimedeltaIndex:>>>tdelta_idx=pd.to_timedelta([1,2,3],unit='S')>>>tdelta_idxTimedeltaIndex(['0 days 00:00:01', '0 days 00:00:02', '0 days 00:00:03'],dtype='timedelta64[ns]', freq=None)>>>tdelta_idx.secondsIndex([1, 2, 3], dtype='int32')"
Pandas,Series,pandas.Series.dt.microseconds,"pandas.Series.dt.microseconds#Series.dt.microseconds[source]#Number of microseconds (>= 0 and less than 1 second) for each element.ExamplesFor Series:>>>ser=pd.Series(pd.to_timedelta([1,2,3],unit='U'))>>>ser0   0 days 00:00:00.0000011   0 days 00:00:00.0000022   0 days 00:00:00.000003dtype: timedelta64[ns]>>>ser.dt.microseconds0    11    22    3dtype: int32For TimedeltaIndex:>>>tdelta_idx=pd.to_timedelta([1,2,3],unit='U')>>>tdelta_idxTimedeltaIndex(['0 days 00:00:00.000001', '0 days 00:00:00.000002','0 days 00:00:00.000003'],dtype='timedelta64[ns]', freq=None)>>>tdelta_idx.microsecondsIndex([1, 2, 3], dtype='int32')"
Pandas,Series,pandas.Series.dt.nanoseconds,"pandas.Series.dt.nanoseconds#Series.dt.nanoseconds[source]#Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.ExamplesFor Series:>>>ser=pd.Series(pd.to_timedelta([1,2,3],unit='N'))>>>ser0   0 days 00:00:00.0000000011   0 days 00:00:00.0000000022   0 days 00:00:00.000000003dtype: timedelta64[ns]>>>ser.dt.nanoseconds0    11    22    3dtype: int32For TimedeltaIndex:>>>tdelta_idx=pd.to_timedelta([1,2,3],unit='N')>>>tdelta_idxTimedeltaIndex(['0 days 00:00:00.000000001', '0 days 00:00:00.000000002','0 days 00:00:00.000000003'],dtype='timedelta64[ns]', freq=None)>>>tdelta_idx.nanosecondsIndex([1, 2, 3], dtype='int32')"
Pandas,Series,pandas.Series.dt.components,"pandas.Series.dt.components#Series.dt.components[source]#Return a Dataframe of the components of the Timedeltas.Returns:DataFrameExamples>>>s=pd.Series(pd.to_timedelta(np.arange(5),unit='s'))>>>s0   0 days 00:00:001   0 days 00:00:012   0 days 00:00:023   0 days 00:00:034   0 days 00:00:04dtype: timedelta64[ns]>>>s.dt.componentsdays  hours  minutes  seconds  milliseconds  microseconds  nanoseconds0     0      0        0        0             0             0            01     0      0        0        1             0             0            02     0      0        0        2             0             0            03     0      0        0        3             0             0            04     0      0        0        4             0             0            0"
Pandas,Series,pandas.Series.dt.unit,pandas.Series.dt.unit#Series.dt.unit[source]#
Pandas,Series,pandas.Series.dt.to_pytimedelta,"pandas.Series.dt.to_pytimedelta#Series.dt.to_pytimedelta()[source]#Return an array of nativedatetime.timedeltaobjects.Pythonfs standarddatetimelibrary uses a different representation
timedeltafs. This method converts a Series of pandas Timedeltas
todatetime.timedeltaformat with the same length as the original
Series.Returns:numpy.ndarrayArray of 1D containing data withdatetime.timedeltatype.See alsodatetime.timedeltaA duration expressing the difference between two date, time, or datetime.Examples>>>s=pd.Series(pd.to_timedelta(np.arange(5),unit=""d""))>>>s0   0 days1   1 days2   2 days3   3 days4   4 daysdtype: timedelta64[ns]>>>s.dt.to_pytimedelta()array([datetime.timedelta(0), datetime.timedelta(days=1),datetime.timedelta(days=2), datetime.timedelta(days=3),datetime.timedelta(days=4)], dtype=object)"
Pandas,Series,pandas.Series.dt.total_seconds,"pandas.Series.dt.total_seconds#Series.dt.total_seconds(*args,**kwargs)[source]#Return total duration of each element expressed in seconds.This method is available directly on TimedeltaArray, TimedeltaIndex
and on Series containing timedelta values under the.dtnamespace.Returns:ndarray, Index or SeriesWhen the calling object is a TimedeltaArray, the return type
is ndarray. When the calling object is a TimedeltaIndex,
the return type is an Index with a float64 dtype. When the calling object
is a Series, the return type is Series of typefloat64whose
index is the same as the original.See alsodatetime.timedelta.total_secondsStandard library version of this method.TimedeltaIndex.componentsReturn a DataFrame with components of each Timedelta.ExamplesSeries>>>s=pd.Series(pd.to_timedelta(np.arange(5),unit='d'))>>>s0   0 days1   1 days2   2 days3   3 days4   4 daysdtype: timedelta64[ns]>>>s.dt.total_seconds()0         0.01     86400.02    172800.03    259200.04    345600.0dtype: float64TimedeltaIndex>>>idx=pd.to_timedelta(np.arange(5),unit='d')>>>idxTimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],dtype='timedelta64[ns]', freq=None)>>>idx.total_seconds()Index([0.0, 86400.0, 172800.0, 259200.0, 345600.0], dtype='float64')"
Pandas,Series,pandas.Series.dt.as_unit,"pandas.Series.dt.as_unit#Series.dt.as_unit(*args,**kwargs)[source]#"
Pandas,Series,pandas.Series.str.capitalize,"pandas.Series.str.capitalize#Series.str.capitalize()[source]#Convert strings in the Series/Index to be capitalized.Equivalent tostr.capitalize().Returns:Series or Index of objectSee alsoSeries.str.lowerConverts all characters to lowercase.Series.str.upperConverts all characters to uppercase.Series.str.titleConverts first character of each word to uppercase and remaining to lowercase.Series.str.capitalizeConverts first character to uppercase and remaining to lowercase.Series.str.swapcaseConverts uppercase to lowercase and lowercase to uppercase.Series.str.casefoldRemoves all case distinctions in the string.Examples>>>s=pd.Series(['lower','CAPITALS','this is a sentence','SwApCaSe'])>>>s0                 lower1              CAPITALS2    this is a sentence3              SwApCaSedtype: object>>>s.str.lower()0                 lower1              capitals2    this is a sentence3              swapcasedtype: object>>>s.str.upper()0                 LOWER1              CAPITALS2    THIS IS A SENTENCE3              SWAPCASEdtype: object>>>s.str.title()0                 Lower1              Capitals2    This Is A Sentence3              Swapcasedtype: object>>>s.str.capitalize()0                 Lower1              Capitals2    This is a sentence3              Swapcasedtype: object>>>s.str.swapcase()0                 LOWER1              capitals2    THIS IS A SENTENCE3              sWaPcAsEdtype: object"
Pandas,Series,pandas.Series.str.casefold,"pandas.Series.str.casefold#Series.str.casefold()[source]#Convert strings in the Series/Index to be casefolded.Equivalent tostr.casefold().Returns:Series or Index of objectSee alsoSeries.str.lowerConverts all characters to lowercase.Series.str.upperConverts all characters to uppercase.Series.str.titleConverts first character of each word to uppercase and remaining to lowercase.Series.str.capitalizeConverts first character to uppercase and remaining to lowercase.Series.str.swapcaseConverts uppercase to lowercase and lowercase to uppercase.Series.str.casefoldRemoves all case distinctions in the string.Examples>>>s=pd.Series(['lower','CAPITALS','this is a sentence','SwApCaSe'])>>>s0                 lower1              CAPITALS2    this is a sentence3              SwApCaSedtype: object>>>s.str.lower()0                 lower1              capitals2    this is a sentence3              swapcasedtype: object>>>s.str.upper()0                 LOWER1              CAPITALS2    THIS IS A SENTENCE3              SWAPCASEdtype: object>>>s.str.title()0                 Lower1              Capitals2    This Is A Sentence3              Swapcasedtype: object>>>s.str.capitalize()0                 Lower1              Capitals2    This is a sentence3              Swapcasedtype: object>>>s.str.swapcase()0                 LOWER1              capitals2    THIS IS A SENTENCE3              sWaPcAsEdtype: object"
Pandas,Series,pandas.Series.str.cat,"pandas.Series.str.cat#Series.str.cat(others=None,sep=None,na_rep=None,join='left')[source]#Concatenate strings in the Series/Index with given separator.Ifothersis specified, this function concatenates the Series/Index
and elements ofotherselement-wise.
Ifothersis not passed, then all values in the Series/Index are
concatenated into a single string with a givensep.Parameters:othersSeries, Index, DataFrame, np.ndarray or list-likeSeries, Index, DataFrame, np.ndarray (one- or two-dimensional) and
other list-likes of strings must have the same length as the
calling Series/Index, with the exception of indexed objects (i.e.
Series/Index/DataFrame) ifjoinis not None.If others is a list-like that contains a combination of Series,
Index or np.ndarray (1-dim), then all elements will be unpacked and
must satisfy the above criteria individually.If others is None, the method returns the concatenation of all
strings in the calling Series/Index.sepstr, default efThe separator between the different elements/columns. By default
the empty stringefis used.na_repstr or None, default NoneRepresentation that is inserted for all missing values:Ifna_repis None, andothersis None, missing values in the
Series/Index are omitted from the result.Ifna_repis None, andothersis not None, a row containing a
missing value in any of the columns (before concatenation) will
have a missing value in the result.join{eleftf, erightf, eouterf, einnerf}, default eleftfDetermines the join-style between the calling Series/Index and any
Series/Index/DataFrame inothers(objects without an index need
to match the length of the calling Series/Index). To disable
alignment, use.valueson any Series/Index/DataFrame inothers.Returns:str, Series or IndexIfothersis None,stris returned, otherwise aSeries/Index(same type as caller) of objects is returned.See alsosplitSplit each string in the Series/Index.joinJoin lists contained as elements in the Series/Index.ExamplesWhen not passingothers, all values are concatenated into a single
string:>>>s=pd.Series(['a','b',np.nan,'d'])>>>s.str.cat(sep=' ')'a b d'By default, NA values in the Series are ignored. Usingna_rep, they
can be given a representation:>>>s.str.cat(sep=' ',na_rep='?')'a b ? d'Ifothersis specified, corresponding values are concatenated with
the separator. Result will be a Series of strings.>>>s.str.cat(['A','B','C','D'],sep=',')0    a,A1    b,B2    NaN3    d,Ddtype: objectMissing values will remain missing in the result, but can again be
represented usingna_rep>>>s.str.cat(['A','B','C','D'],sep=',',na_rep='-')0    a,A1    b,B2    -,C3    d,Ddtype: objectIfsepis not specified, the values are concatenated without
separation.>>>s.str.cat(['A','B','C','D'],na_rep='-')0    aA1    bB2    -C3    dDdtype: objectSeries with different indexes can be aligned before concatenation. Thejoin-keyword works as in other methods.>>>t=pd.Series(['d','a','e','c'],index=[3,0,4,2])>>>s.str.cat(t,join='left',na_rep='-')0    aa1    b-2    -c3    dddtype: object>>>>>>s.str.cat(t,join='outer',na_rep='-')0    aa1    b-2    -c3    dd4    -edtype: object>>>>>>s.str.cat(t,join='inner',na_rep='-')0    aa2    -c3    dddtype: object>>>>>>s.str.cat(t,join='right',na_rep='-')3    dd0    aa4    -e2    -cdtype: objectFor more examples, seehere."
Pandas,Series,pandas.Series.str.center,"pandas.Series.str.center#Series.str.center(width,fillchar='')[source]#Pad left and right side of strings in the Series/Index.Equivalent tostr.center().Parameters:widthintMinimum width of resulting string; additional characters will be filled
withfillchar.fillcharstrAdditional character for filling, default is whitespace.Returns:Series/Index of objects.ExamplesFor Series.str.center:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.center(8,fillchar='.')0   ..dog...1   ..bird..2   .mouse..dtype: objectFor Series.str.ljust:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.ljust(8,fillchar='.')0   dog.....1   bird....2   mouse...dtype: objectFor Series.str.rjust:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.rjust(8,fillchar='.')0   .....dog1   ....bird2   ...mousedtype: object"
Pandas,Series,pandas.Series.str.contains,"pandas.Series.str.contains#Series.str.contains(pat,case=True,flags=0,na=None,regex=True)[source]#Test if pattern or regex is contained within a string of a Series or Index.Return boolean Series or Index based on whether a given pattern or regex is
contained within a string of a Series or Index.Parameters:patstrCharacter sequence or regular expression.casebool, default TrueIf True, case sensitive.flagsint, default 0 (no flags)Flags to pass through to the re module, e.g. re.IGNORECASE.nascalar, optionalFill value for missing values. The default depends on dtype of the
array. For object-dtype,numpy.nanis used. ForStringDtype,pandas.NAis used.regexbool, default TrueIf True, assumes the pat is a regular expression.If False, treats the pat as a literal string.Returns:Series or Index of boolean valuesA Series or Index of boolean values indicating whether the
given pattern is contained within the string of each element
of the Series or Index.See alsomatchAnalogous, but stricter, relying on re.match instead of re.search.Series.str.startswithTest if the start of each string element matches a pattern.Series.str.endswithSame as startswith, but tests the end of string.ExamplesReturning a Series of booleans using only a literal pattern.>>>s1=pd.Series(['Mouse','dog','house and parrot','23',np.nan])>>>s1.str.contains('og',regex=False)0    False1     True2    False3    False4      NaNdtype: objectReturning an Index of booleans using only a literal pattern.>>>ind=pd.Index(['Mouse','dog','house and parrot','23.0',np.nan])>>>ind.str.contains('23',regex=False)Index([False, False, False, True, nan], dtype='object')Specifying case sensitivity usingcase.>>>s1.str.contains('oG',case=True,regex=True)0    False1    False2    False3    False4      NaNdtype: objectSpecifyingnato beFalseinstead ofNaNreplaces NaN values
withFalse. If Series or Index does not contain NaN values
the resultant dtype will bebool, otherwise, anobjectdtype.>>>s1.str.contains('og',na=False,regex=True)0    False1     True2    False3    False4    Falsedtype: boolReturning ehousef or edogf when either expression occurs in a string.>>>s1.str.contains('house|dog',regex=True)0    False1     True2     True3    False4      NaNdtype: objectIgnoring case sensitivity usingflagswith regex.>>>importre>>>s1.str.contains('PARROT',flags=re.IGNORECASE,regex=True)0    False1    False2     True3    False4      NaNdtype: objectReturning any digit using regular expression.>>>s1.str.contains('\\d',regex=True)0    False1    False2    False3     True4      NaNdtype: objectEnsurepatis a not a literal pattern whenregexis set to True.
Note in the following example one might expect onlys2[1]ands2[3]to
returnTrue. However, e.0f as a regex matches any character
followed by a 0.>>>s2=pd.Series(['40','40.0','41','41.0','35'])>>>s2.str.contains('.0',regex=True)0     True1     True2    False3     True4    Falsedtype: bool"
Pandas,Series,pandas.Series.str.count,"pandas.Series.str.count#Series.str.count(pat,flags=0)[source]#Count occurrences of pattern in each string of the Series/Index.This function is used to count the number of times a particular regex
pattern is repeated in each of the string elements of theSeries.Parameters:patstrValid regular expression.flagsint, default 0, meaning no flagsFlags for theremodule. For a complete list,see here.**kwargsFor compatibility with other string methods. Not used.Returns:Series or IndexSame type as the calling object containing the integer counts.See alsoreStandard library module for regular expressions.str.countStandard library version, without regular expression support.NotesSome characters need to be escaped when passing inpat.
eg.'$'has a special meaning in regex and must be escaped when
finding this literal character.Examples>>>s=pd.Series(['A','B','Aaba','Baca',np.nan,'CABA','cat'])>>>s.str.count('a')0    0.01    0.02    2.03    2.04    NaN5    0.06    1.0dtype: float64Escape'$'to find the literal dollar sign.>>>s=pd.Series(['$','B','Aab$','$$ca','C$B$','cat'])>>>s.str.count('\\$')0    11    02    13    24    25    0dtype: int64This is also available on Index>>>pd.Index(['A','A','Aaba','cat']).str.count('a')Index([0, 0, 2, 1], dtype='int64')"
Pandas,Series,pandas.Series.str.decode,"pandas.Series.str.decode#Series.str.decode(encoding,errors='strict')[source]#Decode character string in the Series/Index using indicated encoding.Equivalent tostr.decode()in python2 andbytes.decode()in
python3.Parameters:encodingstrerrorsstr, optionalReturns:Series or IndexExamplesFor Series:>>>ser=pd.Series([b'cow',b'123',b'()'])>>>ser.str.decode('ascii')0   cow1   1232   ()dtype: object"
Pandas,Series,pandas.Series.str.encode,"pandas.Series.str.encode#Series.str.encode(encoding,errors='strict')[source]#Encode character string in the Series/Index using indicated encoding.Equivalent tostr.encode().Parameters:encodingstrerrorsstr, optionalReturns:Series/Index of objectsExamples>>>ser=pd.Series(['cow','123','()'])>>>ser.str.encode(encoding='ascii')0     b'cow'1     b'123'2      b'()'dtype: object"
Pandas,Series,pandas.Series.str.endswith,"pandas.Series.str.endswith#Series.str.endswith(pat,na=None)[source]#Test if the end of each string element matches a pattern.Equivalent tostr.endswith().Parameters:patstr or tuple[str, c]Character sequence or tuple of strings. Regular expressions are not
accepted.naobject, default NaNObject shown if element tested is not a string. The default depends
on dtype of the array. For object-dtype,numpy.nanis used.
ForStringDtype,pandas.NAis used.Returns:Series or Index of boolA Series of booleans indicating whether the given pattern matches
the end of each string element.See alsostr.endswithPython standard library string method.Series.str.startswithSame as endswith, but tests the start of string.Series.str.containsTests if string element contains a pattern.Examples>>>s=pd.Series(['bat','bear','caT',np.nan])>>>s0     bat1    bear2     caT3     NaNdtype: object>>>s.str.endswith('t')0     True1    False2    False3      NaNdtype: object>>>s.str.endswith(('t','T'))0     True1    False2     True3      NaNdtype: objectSpecifyingnato beFalseinstead ofNaN.>>>s.str.endswith('t',na=False)0     True1    False2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.extract,"pandas.Series.str.extract#Series.str.extract(pat,flags=0,expand=True)[source]#Extract capture groups in the regexpatas columns in a DataFrame.For each subject string in the Series, extract groups from the
first match of regular expressionpat.Parameters:patstrRegular expression pattern with capturing groups.flagsint, default 0 (no flags)Flags from theremodule, e.g.re.IGNORECASE, that
modify regular expression matching for things like case,
spaces, etc. For more details, seere.expandbool, default TrueIf True, return DataFrame with one column per capture group.
If False, return a Series/Index if there is one capture group
or DataFrame if there are multiple capture groups.Returns:DataFrame or Series or IndexA DataFrame with one row for each subject string, and one
column for each group. Any capture group names in regular
expression pat will be used for column names; otherwise
capture group numbers will be used. The dtype of each result
column is always object, even when no match is found. Ifexpand=Falseand pat has only one capture group, then
return a Series (if subject is a Series) or Index (if subject
is an Index).See alsoextractallReturns all matches (not just the first match).ExamplesA pattern with two groups will return a DataFrame with two columns.
Non-matches will be NaN.>>>s=pd.Series(['a1','b2','c3'])>>>s.str.extract(r'([ab])(\d)')0    10    a    11    b    22  NaN  NaNA pattern may contain optional groups.>>>s.str.extract(r'([ab])?(\d)')0  10    a  11    b  22  NaN  3Named groups will become column names in the result.>>>s.str.extract(r'(?P<letter>[ab])(?P<digit>\d)')letter digit0      a     11      b     22    NaN   NaNA pattern with one group will return a DataFrame with one column
if expand=True.>>>s.str.extract(r'[ab](\d)',expand=True)00    11    22  NaNA pattern with one group will return a Series if expand=False.>>>s.str.extract(r'[ab](\d)',expand=False)0      11      22    NaNdtype: object"
Pandas,Series,pandas.Series.str.extractall,"pandas.Series.str.extractall#Series.str.extractall(pat,flags=0)[source]#Extract capture groups in the regexpatas columns in DataFrame.For each subject string in the Series, extract groups from all
matches of regular expression pat. When each subject string in the
Series has exactly one match, extractall(pat).xs(0, level=fmatchf)
is the same as extract(pat).Parameters:patstrRegular expression pattern with capturing groups.flagsint, default 0 (no flags)Aremodule flag, for examplere.IGNORECASE. These allow
to modify regular expression matching for things like case, spaces,
etc. Multiple flags can be combined with the bitwise OR operator,
for examplere.IGNORECASE|re.MULTILINE.Returns:DataFrameADataFramewith one row for each match, and one column for each
group. Its rows have aMultiIndexwith first levels that come from
the subjectSeries. The last level is named ematchf and indexes the
matches in each item of theSeries. Any capture group names in
regular expression pat will be used for column names; otherwise capture
group numbers will be used.See alsoextractReturns first match only (not all matches).ExamplesA pattern with one group will return a DataFrame with one column.
Indices with no matches will not appear in the result.>>>s=pd.Series([""a1a2"",""b1"",""c1""],index=[""A"",""B"",""C""])>>>s.str.extractall(r""[ab](\d)"")0matchA 0      11      2B 0      1Capture group names are used for column names of the result.>>>s.str.extractall(r""[ab](?P<digit>\d)"")digitmatchA 0         11         2B 0         1A pattern with two groups will return a DataFrame with two columns.>>>s.str.extractall(r""(?P<letter>[ab])(?P<digit>\d)"")letter digitmatchA 0          a     11          a     2B 0          b     1Optional groups that do not match are NaN in the result.>>>s.str.extractall(r""(?P<letter>[ab])?(?P<digit>\d)"")letter digitmatchA 0          a     11          a     2B 0          b     1C 0        NaN     1"
Pandas,Series,pandas.Series.str.find,"pandas.Series.str.find#Series.str.find(sub,start=0,end=None)[source]#Return lowest indexes in each strings in the Series/Index.Each of returned indexes corresponds to the position where the
substring is fully contained between [start:end]. Return -1 on
failure. Equivalent to standardstr.find().Parameters:substrSubstring being searched.startintLeft edge index.endintRight edge index.Returns:Series or Index of int.See alsorfindReturn highest indexes in each strings.ExamplesFor Series.str.find:>>>ser=pd.Series([""cow_"",""duck_"",""do_ve""])>>>ser.str.find(""_"")0   31   42   2dtype: int64For Series.str.rfind:>>>ser=pd.Series([""_cow_"",""duck_"",""do_v_e""])>>>ser.str.rfind(""_"")0   41   42   4dtype: int64"
Pandas,Series,pandas.Series.str.findall,"pandas.Series.str.findall#Series.str.findall(pat,flags=0)[source]#Find all occurrences of pattern or regular expression in the Series/Index.Equivalent to applyingre.findall()to all the elements in the
Series/Index.Parameters:patstrPattern or regular expression.flagsint, default 0Flags fromremodule, e.g.re.IGNORECASE(default is 0, which
means no flags).Returns:Series/Index of lists of stringsAll non-overlapping matches of pattern or regular expression in each
string of this Series/Index.See alsocountCount occurrences of pattern or regular expression in each string of the Series/Index.extractallFor each string in the Series, extract groups from all matches of regular expression and return a DataFrame with one row for each match and one column for each group.re.findallThe equivalentrefunction to all non-overlapping matches of pattern or regular expression in string, as a list of strings.Examples>>>s=pd.Series(['Lion','Monkey','Rabbit'])The search for the pattern eMonkeyf returns one match:>>>s.str.findall('Monkey')0          []1    [Monkey]2          []dtype: objectOn the other hand, the search for the pattern eMONKEYf doesnft return any
match:>>>s.str.findall('MONKEY')0    []1    []2    []dtype: objectFlags can be added to the pattern or regular expression. For instance,
to find the pattern eMONKEYf ignoring the case:>>>importre>>>s.str.findall('MONKEY',flags=re.IGNORECASE)0          []1    [Monkey]2          []dtype: objectWhen the pattern matches more than one string in the Series, all matches
are returned:>>>s.str.findall('on')0    [on]1    [on]2      []dtype: objectRegular expressions are supported too. For instance, the search for all the
strings ending with the word eonf is shown next:>>>s.str.findall('on$')0    [on]1      []2      []dtype: objectIf the pattern is found more than once in the same string, then a list of
multiple strings is returned:>>>s.str.findall('b')0        []1        []2    [b, b]dtype: object"
Pandas,Series,pandas.Series.str.fullmatch,"pandas.Series.str.fullmatch#Series.str.fullmatch(pat,case=True,flags=0,na=None)[source]#Determine if each string entirely matches a regular expression.Parameters:patstrCharacter sequence or regular expression.casebool, default TrueIf True, case sensitive.flagsint, default 0 (no flags)Regex module flags, e.g. re.IGNORECASE.nascalar, optionalFill value for missing values. The default depends on dtype of the
array. For object-dtype,numpy.nanis used. ForStringDtype,pandas.NAis used.Returns:Series/Index/array of boolean valuesSee alsomatchSimilar, but also returnsTruewhen only aprefixof the string matches the regular expression.extractExtract matched groups.Examples>>>ser=pd.Series([""cat"",""duck"",""dove""])>>>ser.str.fullmatch(r'd.+')0   False1    True2    Truedtype: bool"
Pandas,Series,pandas.Series.str.get,"pandas.Series.str.get#Series.str.get(i)[source]#Extract element from each component at specified position or with specified key.Extract element from lists, tuples, dict, or strings in each element in the
Series/Index.Parameters:iint or hashable dict labelPosition or key of element to extract.Returns:Series or IndexExamples>>>s=pd.Series([""String"",...(1,2,3),...[""a"",""b"",""c""],...123,...-456,...{1:""Hello"",""2"":""World""}])>>>s0                        String1                     (1, 2, 3)2                     [a, b, c]3                           1234                          -4565    {1: 'Hello', '2': 'World'}dtype: object>>>s.str.get(1)0        t1        22        b3      NaN4      NaN5    Hellodtype: object>>>s.str.get(-1)0      g1      32      c3    NaN4    NaN5    Nonedtype: objectReturn element with given key>>>s=pd.Series([{""name"":""Hello"",""value"":""World""},...{""name"":""Goodbye"",""value"":""Planet""}])>>>s.str.get('name')0      Hello1    Goodbyedtype: object"
Pandas,Series,pandas.Series.str.index,"pandas.Series.str.index#Series.str.index(sub,start=0,end=None)[source]#Return lowest indexes in each string in Series/Index.Each of the returned indexes corresponds to the position where the
substring is fully contained between [start:end]. This is the same
asstr.findexcept instead of returning -1, it raises a
ValueError when the substring is not found. Equivalent to standardstr.index.Parameters:substrSubstring being searched.startintLeft edge index.endintRight edge index.Returns:Series or Index of objectSee alsorindexReturn highest indexes in each strings.ExamplesFor Series.str.index:>>>ser=pd.Series([""horse"",""eagle"",""donkey""])>>>ser.str.index(""e"")0   41   02   4dtype: int64For Series.str.rindex:>>>ser=pd.Series([""Deer"",""eagle"",""Sheep""])>>>ser.str.rindex(""e"")0   21   42   3dtype: int64"
Pandas,Series,pandas.Series.str.join,"pandas.Series.str.join#Series.str.join(sep)[source]#Join lists contained as elements in the Series/Index with passed delimiter.If the elements of a Series are lists themselves, join the content of these
lists using the delimiter passed to the function.
This function is an equivalent tostr.join().Parameters:sepstrDelimiter to use between list entries.Returns:Series/Index: objectThe list entries concatenated by intervening occurrences of the
delimiter.Raises:AttributeErrorIf the supplied Series contains neither strings nor lists.See alsostr.joinStandard library version of this method.Series.str.splitSplit strings around given separator/delimiter.NotesIf any of the list items is not a string object, the result of the join
will beNaN.ExamplesExample with a list that contains non-string elements.>>>s=pd.Series([['lion','elephant','zebra'],...[1.1,2.2,3.3],...['cat',np.nan,'dog'],...['cow',4.5,'goat'],...['duck',['swan','fish'],'guppy']])>>>s0        [lion, elephant, zebra]1                [1.1, 2.2, 3.3]2                [cat, nan, dog]3               [cow, 4.5, goat]4    [duck, [swan, fish], guppy]dtype: objectJoin all lists using a e-f. The lists containing object(s) of types other
than str will produce a NaN.>>>s.str.join('-')0    lion-elephant-zebra1                    NaN2                    NaN3                    NaN4                    NaNdtype: object"
Pandas,Series,pandas.Series.str.len,"pandas.Series.str.len#Series.str.len()[source]#Compute the length of each element in the Series/Index.The element may be a sequence (such as a string, tuple or list) or a collection
(such as a dictionary).Returns:Series or Index of intA Series or Index of integer values indicating the length of each
element in the Series or Index.See alsostr.lenPython built-in function returning the length of an object.Series.sizeReturns the length of the Series.ExamplesReturns the length (number of characters) in a string. Returns the
number of entries for dictionaries, lists or tuples.>>>s=pd.Series(['dog',...'',...5,...{'foo':'bar'},...[2,3,5,7],...('one','two','three')])>>>s0                  dog12                    53       {'foo': 'bar'}4         [2, 3, 5, 7]5    (one, two, three)dtype: object>>>s.str.len()0    3.01    0.02    NaN3    1.04    4.05    3.0dtype: float64"
Pandas,Series,pandas.Series.str.ljust,"pandas.Series.str.ljust#Series.str.ljust(width,fillchar='')[source]#Pad right side of strings in the Series/Index.Equivalent tostr.ljust().Parameters:widthintMinimum width of resulting string; additional characters will be filled
withfillchar.fillcharstrAdditional character for filling, default is whitespace.Returns:Series/Index of objects.ExamplesFor Series.str.center:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.center(8,fillchar='.')0   ..dog...1   ..bird..2   .mouse..dtype: objectFor Series.str.ljust:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.ljust(8,fillchar='.')0   dog.....1   bird....2   mouse...dtype: objectFor Series.str.rjust:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.rjust(8,fillchar='.')0   .....dog1   ....bird2   ...mousedtype: object"
Pandas,Series,pandas.Series.str.lower,"pandas.Series.str.lower#Series.str.lower()[source]#Convert strings in the Series/Index to lowercase.Equivalent tostr.lower().Returns:Series or Index of objectSee alsoSeries.str.lowerConverts all characters to lowercase.Series.str.upperConverts all characters to uppercase.Series.str.titleConverts first character of each word to uppercase and remaining to lowercase.Series.str.capitalizeConverts first character to uppercase and remaining to lowercase.Series.str.swapcaseConverts uppercase to lowercase and lowercase to uppercase.Series.str.casefoldRemoves all case distinctions in the string.Examples>>>s=pd.Series(['lower','CAPITALS','this is a sentence','SwApCaSe'])>>>s0                 lower1              CAPITALS2    this is a sentence3              SwApCaSedtype: object>>>s.str.lower()0                 lower1              capitals2    this is a sentence3              swapcasedtype: object>>>s.str.upper()0                 LOWER1              CAPITALS2    THIS IS A SENTENCE3              SWAPCASEdtype: object>>>s.str.title()0                 Lower1              Capitals2    This Is A Sentence3              Swapcasedtype: object>>>s.str.capitalize()0                 Lower1              Capitals2    This is a sentence3              Swapcasedtype: object>>>s.str.swapcase()0                 LOWER1              capitals2    THIS IS A SENTENCE3              sWaPcAsEdtype: object"
Pandas,Series,pandas.Series.str.lstrip,"pandas.Series.str.lstrip#Series.str.lstrip(to_strip=None)[source]#Remove leading characters.Strip whitespaces (including newlines) or a set of specified characters
from each string in the Series/Index from left side.
Replaces any non-strings in Series with NaNs.
Equivalent tostr.lstrip().Parameters:to_stripstr or None, default NoneSpecifying the set of characters to be removed.
All combinations of this set of characters will be stripped.
If None then whitespaces are removed.Returns:Series or Index of objectSee alsoSeries.str.stripRemove leading and trailing characters in Series/Index.Series.str.lstripRemove leading characters in Series/Index.Series.str.rstripRemove trailing characters in Series/Index.Examples>>>s=pd.Series(['1. Ant.  ','2. Bee!\n','3. Cat?\t',np.nan,10,True])>>>s0    1. Ant.1    2. Bee!\n2    3. Cat?\t3          NaN4           105         Truedtype: object>>>s.str.strip()0    1. Ant.1    2. Bee!2    3. Cat?3        NaN4        NaN5        NaNdtype: object>>>s.str.lstrip('123.')0    Ant.1    Bee!\n2    Cat?\t3       NaN4       NaN5       NaNdtype: object>>>s.str.rstrip('.!?\n\t')0    1. Ant1    2. Bee2    3. Cat3       NaN4       NaN5       NaNdtype: object>>>s.str.strip('123.!?\n\t')0    Ant1    Bee2    Cat3    NaN4    NaN5    NaNdtype: object"
Pandas,Series,pandas.Series.str.match,"pandas.Series.str.match#Series.str.match(pat,case=True,flags=0,na=None)[source]#Determine if each string starts with a match of a regular expression.Parameters:patstrCharacter sequence or regular expression.casebool, default TrueIf True, case sensitive.flagsint, default 0 (no flags)Regex module flags, e.g. re.IGNORECASE.nascalar, optionalFill value for missing values. The default depends on dtype of the
array. For object-dtype,numpy.nanis used. ForStringDtype,pandas.NAis used.Returns:Series/Index/array of boolean valuesSee alsofullmatchStricter matching that requires the entire string to match.containsAnalogous, but less strict, relying on re.search instead of re.match.extractExtract matched groups.Examples>>>ser=pd.Series([""horse"",""eagle"",""donkey""])>>>ser.str.match(""e"")0   False1   True2   Falsedtype: bool"
Pandas,Series,pandas.Series.str.normalize,"pandas.Series.str.normalize#Series.str.normalize(form)[source]#Return the Unicode normal form for the strings in the Series/Index.For more information on the forms, see theunicodedata.normalize().Parameters:form{eNFCf, eNFKCf, eNFDf, eNFKDf}Unicode form.Returns:Series/Index of objectsExamples>>>ser=pd.Series(['?'])>>>ser.str.normalize('NFC')==ser.str.normalize('NFD')0   Falsedtype: bool"
Pandas,Series,pandas.Series.str.pad,"pandas.Series.str.pad#Series.str.pad(width,side='left',fillchar='')[source]#Pad strings in the Series/Index up to width.Parameters:widthintMinimum width of resulting string; additional characters will be filled
with character defined infillchar.side{eleftf, erightf, ebothf}, default eleftfSide from which to fill resulting string.fillcharstr, default e eAdditional character for filling, default is whitespace.Returns:Series or Index of objectReturns Series or Index with minimum number of char in object.See alsoSeries.str.rjustFills the left side of strings with an arbitrary character. Equivalent toSeries.str.pad(side='left').Series.str.ljustFills the right side of strings with an arbitrary character. Equivalent toSeries.str.pad(side='right').Series.str.centerFills both sides of strings with an arbitrary character. Equivalent toSeries.str.pad(side='both').Series.str.zfillPad strings in the Series/Index by prepending e0f character. Equivalent toSeries.str.pad(side='left',fillchar='0').Examples>>>s=pd.Series([""caribou"",""tiger""])>>>s0    caribou1      tigerdtype: object>>>s.str.pad(width=10)0       caribou1         tigerdtype: object>>>s.str.pad(width=10,side='right',fillchar='-')0    caribou---1    tiger-----dtype: object>>>s.str.pad(width=10,side='both',fillchar='-')0    -caribou--1    --tiger---dtype: object"
Pandas,Series,pandas.Series.str.partition,"pandas.Series.str.partition#Series.str.partition(sep='',expand=True)[source]#Split the string at the first occurrence ofsep.This method splits the string at the first occurrence ofsep,
and returns 3 elements containing the part before the separator,
the separator itself, and the part after the separator.
If the separator is not found, return 3 elements containing the string itself, followed by two empty strings.Parameters:sepstr, default whitespaceString to split on.expandbool, default TrueIf True, return DataFrame/MultiIndex expanding dimensionality.
If False, return Series/Index.Returns:DataFrame/MultiIndex or Series/Index of objectsSee alsorpartitionSplit the string at the last occurrence ofsep.Series.str.splitSplit strings around given separators.str.partitionStandard library version.Examples>>>s=pd.Series(['Linda van der Berg','George Pitt-Rivers'])>>>s0    Linda van der Berg1    George Pitt-Riversdtype: object>>>s.str.partition()0  1             20   Linda     van der Berg1  George      Pitt-RiversTo partition by the last space instead of the first one:>>>s.str.rpartition()0  1            20  Linda van der            Berg1         George     Pitt-RiversTo partition by something different than a space:>>>s.str.partition('-')0  1       20  Linda van der Berg1         George Pitt  -  RiversTo return a Series containing tuples instead of a DataFrame:>>>s.str.partition('-',expand=False)0    (Linda van der Berg, , )1    (George Pitt, -, Rivers)dtype: objectAlso available on indices:>>>idx=pd.Index(['X 123','Y 999'])>>>idxIndex(['X 123', 'Y 999'], dtype='object')Which will create a MultiIndex:>>>idx.str.partition()MultiIndex([('X', ' ', '123'),('Y', ' ', '999')],)Or an index with tuples withexpand=False:>>>idx.str.partition(expand=False)Index([('X', ' ', '123'), ('Y', ' ', '999')], dtype='object')"
Pandas,Series,pandas.Series.str.removeprefix,"pandas.Series.str.removeprefix#Series.str.removeprefix(prefix)[source]#Remove a prefix from an object series.If the prefix is not present, the original string will be returned.Parameters:prefixstrRemove the prefix of the string.Returns:Series/Index: objectThe Series or Index with given prefix removed.See alsoSeries.str.removesuffixRemove a suffix from an object series.Examples>>>s=pd.Series([""str_foo"",""str_bar"",""no_prefix""])>>>s0    str_foo1    str_bar2    no_prefixdtype: object>>>s.str.removeprefix(""str_"")0    foo1    bar2    no_prefixdtype: object>>>s=pd.Series([""foo_str"",""bar_str"",""no_suffix""])>>>s0    foo_str1    bar_str2    no_suffixdtype: object>>>s.str.removesuffix(""_str"")0    foo1    bar2    no_suffixdtype: object"
Pandas,Series,pandas.Series.str.removesuffix,"pandas.Series.str.removesuffix#Series.str.removesuffix(suffix)[source]#Remove a suffix from an object series.If the suffix is not present, the original string will be returned.Parameters:suffixstrRemove the suffix of the string.Returns:Series/Index: objectThe Series or Index with given suffix removed.See alsoSeries.str.removeprefixRemove a prefix from an object series.Examples>>>s=pd.Series([""str_foo"",""str_bar"",""no_prefix""])>>>s0    str_foo1    str_bar2    no_prefixdtype: object>>>s.str.removeprefix(""str_"")0    foo1    bar2    no_prefixdtype: object>>>s=pd.Series([""foo_str"",""bar_str"",""no_suffix""])>>>s0    foo_str1    bar_str2    no_suffixdtype: object>>>s.str.removesuffix(""_str"")0    foo1    bar2    no_suffixdtype: object"
Pandas,Series,pandas.Series.str.repeat,"pandas.Series.str.repeat#Series.str.repeat(repeats)[source]#Duplicate each string in the Series or Index.Parameters:repeatsint or sequence of intSame value for all (int) or different value per (sequence).Returns:Series or pandas.IndexSeries or Index of repeated string objects specified by
input parameter repeats.Examples>>>s=pd.Series(['a','b','c'])>>>s0    a1    b2    cdtype: objectSingle int repeats string in Series>>>s.str.repeat(repeats=2)0    aa1    bb2    ccdtype: objectSequence of int repeats corresponding string in Series>>>s.str.repeat(repeats=[1,2,3])0      a1     bb2    cccdtype: object"
Pandas,Series,pandas.Series.str.replace,"pandas.Series.str.replace#Series.str.replace(pat,repl,n=-1,case=None,flags=0,regex=False)[source]#Replace each occurrence of pattern/regex in the Series/Index.Equivalent tostr.replace()orre.sub(), depending on
the regex value.Parameters:patstr or compiled regexString can be a character sequence or regular expression.replstr or callableReplacement string or a callable. The callable is passed the regex
match object and must return a replacement string to be used.
Seere.sub().nint, default -1 (all)Number of replacements to make from start.casebool, default NoneDetermines if replace is case sensitive:If True, case sensitive (the default ifpatis a string)Set to False for case insensitiveCannot be set ifpatis a compiled regex.flagsint, default 0 (no flags)Regex module flags, e.g. re.IGNORECASE. Cannot be set ifpatis a compiled
regex.regexbool, default FalseDetermines if the passed-in pattern is a regular expression:If True, assumes the passed-in pattern is a regular expression.If False, treats the pattern as a literal stringCannot be set to False ifpatis a compiled regex orreplis
a callable.Returns:Series or Index of objectA copy of the object with all matching occurrences ofpatreplaced byrepl.Raises:ValueErrorifregexis False andreplis a callable orpatis a compiled
regexifpatis a compiled regex andcaseorflagsis setNotesWhenpatis a compiled regex, all flags should be included in the
compiled regex. Use ofcase,flags, orregex=Falsewith a compiled
regex will raise an error.ExamplesWhenpatis a string andregexis True, the givenpatis compiled as a regex. Whenreplis a string, it replaces matching
regex patterns as withre.sub(). NaN value(s) in the Series are
left as is:>>>pd.Series(['foo','fuz',np.nan]).str.replace('f.','ba',regex=True)0    bao1    baz2    NaNdtype: objectWhenpatis a string andregexis False, everypatis replaced withreplas withstr.replace():>>>pd.Series(['f.o','fuz',np.nan]).str.replace('f.','ba',regex=False)0    bao1    fuz2    NaNdtype: objectWhenreplis a callable, it is called on everypatusingre.sub(). The callable should expect one positional argument
(a regex object) and return a string.To get the idea:>>>pd.Series(['foo','fuz',np.nan]).str.replace('f',repr,regex=True)0    <re.Match object; span=(0, 1), match='f'>oo1    <re.Match object; span=(0, 1), match='f'>uz2                                            NaNdtype: objectReverse every lowercase alphabetic word:>>>repl=lambdam:m.group(0)[::-1]>>>ser=pd.Series(['foo 123','bar baz',np.nan])>>>ser.str.replace(r'[a-z]+',repl,regex=True)0    oof 1231    rab zab2        NaNdtype: objectUsing regex groups (extract second group and swap case):>>>pat=r""(?P<one>\w+) (?P<two>\w+) (?P<three>\w+)"">>>repl=lambdam:m.group('two').swapcase()>>>ser=pd.Series(['One Two Three','Foo Bar Baz'])>>>ser.str.replace(pat,repl,regex=True)0    tWO1    bARdtype: objectUsing a compiled regex with flags>>>importre>>>regex_pat=re.compile(r'FUZ',flags=re.IGNORECASE)>>>pd.Series(['foo','fuz',np.nan]).str.replace(regex_pat,'bar',regex=True)0    foo1    bar2    NaNdtype: object"
Pandas,Series,pandas.Series.str.rfind,"pandas.Series.str.rfind#Series.str.rfind(sub,start=0,end=None)[source]#Return highest indexes in each strings in the Series/Index.Each of returned indexes corresponds to the position where the
substring is fully contained between [start:end]. Return -1 on
failure. Equivalent to standardstr.rfind().Parameters:substrSubstring being searched.startintLeft edge index.endintRight edge index.Returns:Series or Index of int.See alsofindReturn lowest indexes in each strings.ExamplesFor Series.str.find:>>>ser=pd.Series([""cow_"",""duck_"",""do_ve""])>>>ser.str.find(""_"")0   31   42   2dtype: int64For Series.str.rfind:>>>ser=pd.Series([""_cow_"",""duck_"",""do_v_e""])>>>ser.str.rfind(""_"")0   41   42   4dtype: int64"
Pandas,Series,pandas.Series.str.rindex,"pandas.Series.str.rindex#Series.str.rindex(sub,start=0,end=None)[source]#Return highest indexes in each string in Series/Index.Each of the returned indexes corresponds to the position where the
substring is fully contained between [start:end]. This is the same
asstr.rfindexcept instead of returning -1, it raises a
ValueError when the substring is not found. Equivalent to standardstr.rindex.Parameters:substrSubstring being searched.startintLeft edge index.endintRight edge index.Returns:Series or Index of objectSee alsoindexReturn lowest indexes in each strings.ExamplesFor Series.str.index:>>>ser=pd.Series([""horse"",""eagle"",""donkey""])>>>ser.str.index(""e"")0   41   02   4dtype: int64For Series.str.rindex:>>>ser=pd.Series([""Deer"",""eagle"",""Sheep""])>>>ser.str.rindex(""e"")0   21   42   3dtype: int64"
Pandas,Series,pandas.Series.str.rjust,"pandas.Series.str.rjust#Series.str.rjust(width,fillchar='')[source]#Pad left side of strings in the Series/Index.Equivalent tostr.rjust().Parameters:widthintMinimum width of resulting string; additional characters will be filled
withfillchar.fillcharstrAdditional character for filling, default is whitespace.Returns:Series/Index of objects.ExamplesFor Series.str.center:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.center(8,fillchar='.')0   ..dog...1   ..bird..2   .mouse..dtype: objectFor Series.str.ljust:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.ljust(8,fillchar='.')0   dog.....1   bird....2   mouse...dtype: objectFor Series.str.rjust:>>>ser=pd.Series(['dog','bird','mouse'])>>>ser.str.rjust(8,fillchar='.')0   .....dog1   ....bird2   ...mousedtype: object"
Pandas,Series,pandas.Series.str.rpartition,"pandas.Series.str.rpartition#Series.str.rpartition(sep='',expand=True)[source]#Split the string at the last occurrence ofsep.This method splits the string at the last occurrence ofsep,
and returns 3 elements containing the part before the separator,
the separator itself, and the part after the separator.
If the separator is not found, return 3 elements containing two empty strings, followed by the string itself.Parameters:sepstr, default whitespaceString to split on.expandbool, default TrueIf True, return DataFrame/MultiIndex expanding dimensionality.
If False, return Series/Index.Returns:DataFrame/MultiIndex or Series/Index of objectsSee alsopartitionSplit the string at the first occurrence ofsep.Series.str.splitSplit strings around given separators.str.partitionStandard library version.Examples>>>s=pd.Series(['Linda van der Berg','George Pitt-Rivers'])>>>s0    Linda van der Berg1    George Pitt-Riversdtype: object>>>s.str.partition()0  1             20   Linda     van der Berg1  George      Pitt-RiversTo partition by the last space instead of the first one:>>>s.str.rpartition()0  1            20  Linda van der            Berg1         George     Pitt-RiversTo partition by something different than a space:>>>s.str.partition('-')0  1       20  Linda van der Berg1         George Pitt  -  RiversTo return a Series containing tuples instead of a DataFrame:>>>s.str.partition('-',expand=False)0    (Linda van der Berg, , )1    (George Pitt, -, Rivers)dtype: objectAlso available on indices:>>>idx=pd.Index(['X 123','Y 999'])>>>idxIndex(['X 123', 'Y 999'], dtype='object')Which will create a MultiIndex:>>>idx.str.partition()MultiIndex([('X', ' ', '123'),('Y', ' ', '999')],)Or an index with tuples withexpand=False:>>>idx.str.partition(expand=False)Index([('X', ' ', '123'), ('Y', ' ', '999')], dtype='object')"
Pandas,Series,pandas.Series.str.rstrip,"pandas.Series.str.rstrip#Series.str.rstrip(to_strip=None)[source]#Remove trailing characters.Strip whitespaces (including newlines) or a set of specified characters
from each string in the Series/Index from right side.
Replaces any non-strings in Series with NaNs.
Equivalent tostr.rstrip().Parameters:to_stripstr or None, default NoneSpecifying the set of characters to be removed.
All combinations of this set of characters will be stripped.
If None then whitespaces are removed.Returns:Series or Index of objectSee alsoSeries.str.stripRemove leading and trailing characters in Series/Index.Series.str.lstripRemove leading characters in Series/Index.Series.str.rstripRemove trailing characters in Series/Index.Examples>>>s=pd.Series(['1. Ant.  ','2. Bee!\n','3. Cat?\t',np.nan,10,True])>>>s0    1. Ant.1    2. Bee!\n2    3. Cat?\t3          NaN4           105         Truedtype: object>>>s.str.strip()0    1. Ant.1    2. Bee!2    3. Cat?3        NaN4        NaN5        NaNdtype: object>>>s.str.lstrip('123.')0    Ant.1    Bee!\n2    Cat?\t3       NaN4       NaN5       NaNdtype: object>>>s.str.rstrip('.!?\n\t')0    1. Ant1    2. Bee2    3. Cat3       NaN4       NaN5       NaNdtype: object>>>s.str.strip('123.!?\n\t')0    Ant1    Bee2    Cat3    NaN4    NaN5    NaNdtype: object"
Pandas,Series,pandas.Series.str.slice,"pandas.Series.str.slice#Series.str.slice(start=None,stop=None,step=None)[source]#Slice substrings from each element in the Series or Index.Parameters:startint, optionalStart position for slice operation.stopint, optionalStop position for slice operation.stepint, optionalStep size for slice operation.Returns:Series or Index of objectSeries or Index from sliced substring from original string object.See alsoSeries.str.slice_replaceReplace a slice with a string.Series.str.getReturn element at position. Equivalent toSeries.str.slice(start=i, stop=i+1)withibeing the position.Examples>>>s=pd.Series([""koala"",""dog"",""chameleon""])>>>s0        koala1          dog2    chameleondtype: object>>>s.str.slice(start=1)0        oala1          og2    hameleondtype: object>>>s.str.slice(start=-1)0           a1           g2           ndtype: object>>>s.str.slice(stop=2)0    ko1    do2    chdtype: object>>>s.str.slice(step=2)0      kaa1       dg2    caeendtype: object>>>s.str.slice(start=0,stop=5,step=3)0    kl1     d2    cmdtype: objectEquivalent behaviour to:>>>s.str[0:5:3]0    kl1     d2    cmdtype: object"
Pandas,Series,pandas.Series.str.slice_replace,"pandas.Series.str.slice_replace#Series.str.slice_replace(start=None,stop=None,repl=None)[source]#Replace a positional slice of a string with another value.Parameters:startint, optionalLeft index position to use for the slice. If not specified (None),
the slice is unbounded on the left, i.e. slice from the start
of the string.stopint, optionalRight index position to use for the slice. If not specified (None),
the slice is unbounded on the right, i.e. slice until the
end of the string.replstr, optionalString for replacement. If not specified (None), the sliced region
is replaced with an empty string.Returns:Series or IndexSame type as the original object.See alsoSeries.str.sliceJust slicing without replacement.Examples>>>s=pd.Series(['a','ab','abc','abdc','abcde'])>>>s0        a1       ab2      abc3     abdc4    abcdedtype: objectSpecify juststart, meaning replacestartuntil the end of the
string withrepl.>>>s.str.slice_replace(1,repl='X')0    aX1    aX2    aX3    aX4    aXdtype: objectSpecify juststop, meaning the start of the string tostopis replaced
withrepl, and the rest of the string is included.>>>s.str.slice_replace(stop=2,repl='X')0       X1       X2      Xc3     Xdc4    Xcdedtype: objectSpecifystartandstop, meaning the slice fromstarttostopis
replaced withrepl. Everything before or afterstartandstopis
included as is.>>>s.str.slice_replace(start=1,stop=3,repl='X')0      aX1      aX2      aX3     aXc4    aXdedtype: object"
Pandas,Series,pandas.Series.str.split,"pandas.Series.str.split#Series.str.split(pat=None,*,n=-1,expand=False,regex=None)[source]#Split strings around given separator/delimiter.Splits the string in the Series/Index from the beginning,
at the specified delimiter string.Parameters:patstr or compiled regex, optionalString or regular expression to split on.
If not specified, split on whitespace.nint, default -1 (all)Limit number of splits in output.None, 0 and -1 will be interpreted as return all splits.expandbool, default FalseExpand the split strings into separate columns.IfTrue, return DataFrame/MultiIndex expanding dimensionality.IfFalse, return Series/Index, containing lists of strings.regexbool, default NoneDetermines if the passed-in pattern is a regular expression:IfTrue, assumes the passed-in pattern is a regular expressionIfFalse, treats the pattern as a literal string.IfNoneandpatlength is 1, treatspatas a literal string.IfNoneandpatlength is not 1, treatspatas a regular expression.Cannot be set to False ifpatis a compiled regexNew in version 1.4.0.Returns:Series, Index, DataFrame or MultiIndexType matches caller unlessexpand=True(see Notes).Raises:ValueErrorifregexis False andpatis a compiled regexSee alsoSeries.str.splitSplit strings around given separator/delimiter.Series.str.rsplitSplits string around given separator/delimiter, starting from the right.Series.str.joinJoin lists contained as elements in the Series/Index with passed delimiter.str.splitStandard library version for split.str.rsplitStandard library version for rsplit.NotesThe handling of thenkeyword depends on the number of found splits:If found splits >n, make firstnsplits onlyIf found splits <=n, make all splitsIf for a certain row the number of found splits <n,
appendNonefor padding up tonifexpand=TrueIf usingexpand=True, Series and Index callers return DataFrame and
MultiIndex objects, respectively.Use ofregex =Falsewith apatas a compiled regex will raise an error.Examples>>>s=pd.Series(...[...""this is a regular sentence"",...""https://docs.python.org/3/tutorial/index.html"",...np.nan...]...)>>>s0                       this is a regular sentence1    https://docs.python.org/3/tutorial/index.html2                                              NaNdtype: objectIn the default setting, the string is split by whitespace.>>>s.str.split()0                   [this, is, a, regular, sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: objectWithout thenparameter, the outputs ofrsplitandsplitare identical.>>>s.str.rsplit()0                   [this, is, a, regular, sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: objectThenparameter can be used to limit the number of splits on the
delimiter. The outputs ofsplitandrsplitare different.>>>s.str.split(n=2)0                     [this, is, a regular sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: object>>>s.str.rsplit(n=2)0                     [this is a, regular, sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: objectThepatparameter can be used to split by other characters.>>>s.str.split(pat=""/"")0                         [this is a regular sentence]1    [https:, , docs.python.org, 3, tutorial, index...2                                                  NaNdtype: objectWhen usingexpand=True, the split elements will expand out into
separate columns. If NaN is present, it is propagated throughout
the columns during the split.>>>s.str.split(expand=True)0     1     2        3         40                                           this    is     a  regular  sentence1  https://docs.python.org/3/tutorial/index.html  None  None     None      None2                                            NaN   NaN   NaN      NaN       NaNFor slightly more complex use cases like splitting the html document name
from a url, a combination of parameter settings can be used.>>>s.str.rsplit(""/"",n=1,expand=True)0           10          this is a regular sentence        None1  https://docs.python.org/3/tutorial  index.html2                                 NaN         NaNRemember to escape special characters when explicitly using regular expressions.>>>s=pd.Series([""foo and bar plus baz""])>>>s.str.split(r""and|plus"",expand=True)0   1   20 foo bar bazRegular expressions can be used to handle urls or file names.
Whenpatis a string andregex=None(the default), the givenpatis compiled
as a regex only iflen(pat)!=1.>>>s=pd.Series(['foojpgbar.jpg'])>>>s.str.split(r""."",expand=True)0    10  foojpgbar  jpg>>>s.str.split(r""\.jpg"",expand=True)0 10  foojpgbarWhenregex=True,patis interpreted as a regex>>>s.str.split(r""\.jpg"",regex=True,expand=True)0 10  foojpgbarA compiled regex can be passed aspat>>>importre>>>s.str.split(re.compile(r""\.jpg""),expand=True)0 10  foojpgbarWhenregex=False,patis interpreted as the string itself>>>s.str.split(r""\.jpg"",regex=False,expand=True)00  foojpgbar.jpg"
Pandas,Series,pandas.Series.str.rsplit,"pandas.Series.str.rsplit#Series.str.rsplit(pat=None,*,n=-1,expand=False)[source]#Split strings around given separator/delimiter.Splits the string in the Series/Index from the end,
at the specified delimiter string.Parameters:patstr, optionalString to split on.
If not specified, split on whitespace.nint, default -1 (all)Limit number of splits in output.None, 0 and -1 will be interpreted as return all splits.expandbool, default FalseExpand the split strings into separate columns.IfTrue, return DataFrame/MultiIndex expanding dimensionality.IfFalse, return Series/Index, containing lists of strings.Returns:Series, Index, DataFrame or MultiIndexType matches caller unlessexpand=True(see Notes).See alsoSeries.str.splitSplit strings around given separator/delimiter.Series.str.rsplitSplits string around given separator/delimiter, starting from the right.Series.str.joinJoin lists contained as elements in the Series/Index with passed delimiter.str.splitStandard library version for split.str.rsplitStandard library version for rsplit.NotesThe handling of thenkeyword depends on the number of found splits:If found splits >n, make firstnsplits onlyIf found splits <=n, make all splitsIf for a certain row the number of found splits <n,
appendNonefor padding up tonifexpand=TrueIf usingexpand=True, Series and Index callers return DataFrame and
MultiIndex objects, respectively.Examples>>>s=pd.Series(...[...""this is a regular sentence"",...""https://docs.python.org/3/tutorial/index.html"",...np.nan...]...)>>>s0                       this is a regular sentence1    https://docs.python.org/3/tutorial/index.html2                                              NaNdtype: objectIn the default setting, the string is split by whitespace.>>>s.str.split()0                   [this, is, a, regular, sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: objectWithout thenparameter, the outputs ofrsplitandsplitare identical.>>>s.str.rsplit()0                   [this, is, a, regular, sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: objectThenparameter can be used to limit the number of splits on the
delimiter. The outputs ofsplitandrsplitare different.>>>s.str.split(n=2)0                     [this, is, a regular sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: object>>>s.str.rsplit(n=2)0                     [this is a, regular, sentence]1    [https://docs.python.org/3/tutorial/index.html]2                                                NaNdtype: objectThepatparameter can be used to split by other characters.>>>s.str.split(pat=""/"")0                         [this is a regular sentence]1    [https:, , docs.python.org, 3, tutorial, index...2                                                  NaNdtype: objectWhen usingexpand=True, the split elements will expand out into
separate columns. If NaN is present, it is propagated throughout
the columns during the split.>>>s.str.split(expand=True)0     1     2        3         40                                           this    is     a  regular  sentence1  https://docs.python.org/3/tutorial/index.html  None  None     None      None2                                            NaN   NaN   NaN      NaN       NaNFor slightly more complex use cases like splitting the html document name
from a url, a combination of parameter settings can be used.>>>s.str.rsplit(""/"",n=1,expand=True)0           10          this is a regular sentence        None1  https://docs.python.org/3/tutorial  index.html2                                 NaN         NaN"
Pandas,Series,pandas.Series.str.startswith,"pandas.Series.str.startswith#Series.str.startswith(pat,na=None)[source]#Test if the start of each string element matches a pattern.Equivalent tostr.startswith().Parameters:patstr or tuple[str, c]Character sequence or tuple of strings. Regular expressions are not
accepted.naobject, default NaNObject shown if element tested is not a string. The default depends
on dtype of the array. For object-dtype,numpy.nanis used.
ForStringDtype,pandas.NAis used.Returns:Series or Index of boolA Series of booleans indicating whether the given pattern matches
the start of each string element.See alsostr.startswithPython standard library string method.Series.str.endswithSame as startswith, but tests the end of string.Series.str.containsTests if string element contains a pattern.Examples>>>s=pd.Series(['bat','Bear','cat',np.nan])>>>s0     bat1    Bear2     cat3     NaNdtype: object>>>s.str.startswith('b')0     True1    False2    False3      NaNdtype: object>>>s.str.startswith(('b','B'))0     True1     True2    False3      NaNdtype: objectSpecifyingnato beFalseinstead ofNaN.>>>s.str.startswith('b',na=False)0     True1    False2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.strip,"pandas.Series.str.strip#Series.str.strip(to_strip=None)[source]#Remove leading and trailing characters.Strip whitespaces (including newlines) or a set of specified characters
from each string in the Series/Index from left and right sides.
Replaces any non-strings in Series with NaNs.
Equivalent tostr.strip().Parameters:to_stripstr or None, default NoneSpecifying the set of characters to be removed.
All combinations of this set of characters will be stripped.
If None then whitespaces are removed.Returns:Series or Index of objectSee alsoSeries.str.stripRemove leading and trailing characters in Series/Index.Series.str.lstripRemove leading characters in Series/Index.Series.str.rstripRemove trailing characters in Series/Index.Examples>>>s=pd.Series(['1. Ant.  ','2. Bee!\n','3. Cat?\t',np.nan,10,True])>>>s0    1. Ant.1    2. Bee!\n2    3. Cat?\t3          NaN4           105         Truedtype: object>>>s.str.strip()0    1. Ant.1    2. Bee!2    3. Cat?3        NaN4        NaN5        NaNdtype: object>>>s.str.lstrip('123.')0    Ant.1    Bee!\n2    Cat?\t3       NaN4       NaN5       NaNdtype: object>>>s.str.rstrip('.!?\n\t')0    1. Ant1    2. Bee2    3. Cat3       NaN4       NaN5       NaNdtype: object>>>s.str.strip('123.!?\n\t')0    Ant1    Bee2    Cat3    NaN4    NaN5    NaNdtype: object"
Pandas,Series,pandas.Series.str.swapcase,"pandas.Series.str.swapcase#Series.str.swapcase()[source]#Convert strings in the Series/Index to be swapcased.Equivalent tostr.swapcase().Returns:Series or Index of objectSee alsoSeries.str.lowerConverts all characters to lowercase.Series.str.upperConverts all characters to uppercase.Series.str.titleConverts first character of each word to uppercase and remaining to lowercase.Series.str.capitalizeConverts first character to uppercase and remaining to lowercase.Series.str.swapcaseConverts uppercase to lowercase and lowercase to uppercase.Series.str.casefoldRemoves all case distinctions in the string.Examples>>>s=pd.Series(['lower','CAPITALS','this is a sentence','SwApCaSe'])>>>s0                 lower1              CAPITALS2    this is a sentence3              SwApCaSedtype: object>>>s.str.lower()0                 lower1              capitals2    this is a sentence3              swapcasedtype: object>>>s.str.upper()0                 LOWER1              CAPITALS2    THIS IS A SENTENCE3              SWAPCASEdtype: object>>>s.str.title()0                 Lower1              Capitals2    This Is A Sentence3              Swapcasedtype: object>>>s.str.capitalize()0                 Lower1              Capitals2    This is a sentence3              Swapcasedtype: object>>>s.str.swapcase()0                 LOWER1              capitals2    THIS IS A SENTENCE3              sWaPcAsEdtype: object"
Pandas,Series,pandas.Series.str.title,"pandas.Series.str.title#Series.str.title()[source]#Convert strings in the Series/Index to titlecase.Equivalent tostr.title().Returns:Series or Index of objectSee alsoSeries.str.lowerConverts all characters to lowercase.Series.str.upperConverts all characters to uppercase.Series.str.titleConverts first character of each word to uppercase and remaining to lowercase.Series.str.capitalizeConverts first character to uppercase and remaining to lowercase.Series.str.swapcaseConverts uppercase to lowercase and lowercase to uppercase.Series.str.casefoldRemoves all case distinctions in the string.Examples>>>s=pd.Series(['lower','CAPITALS','this is a sentence','SwApCaSe'])>>>s0                 lower1              CAPITALS2    this is a sentence3              SwApCaSedtype: object>>>s.str.lower()0                 lower1              capitals2    this is a sentence3              swapcasedtype: object>>>s.str.upper()0                 LOWER1              CAPITALS2    THIS IS A SENTENCE3              SWAPCASEdtype: object>>>s.str.title()0                 Lower1              Capitals2    This Is A Sentence3              Swapcasedtype: object>>>s.str.capitalize()0                 Lower1              Capitals2    This is a sentence3              Swapcasedtype: object>>>s.str.swapcase()0                 LOWER1              capitals2    THIS IS A SENTENCE3              sWaPcAsEdtype: object"
Pandas,Series,pandas.Series.str.translate,"pandas.Series.str.translate#Series.str.translate(table)[source]#Map all characters in the string through the given mapping table.Equivalent to standardstr.translate().Parameters:tabledictTable is a mapping of Unicode ordinals to Unicode ordinals, strings, or
None. Unmapped characters are left untouched.
Characters mapped to None are deleted.str.maketrans()is a
helper function for making translation tables.Returns:Series or IndexExamples>>>ser=pd.Series([""El ni?o"",""Fran?oise""])>>>mytable=str.maketrans({'?':'n','?':'c'})>>>ser.str.translate(mytable)0   El nino1   Francoisedtype: object"
Pandas,Series,pandas.Series.str.upper,"pandas.Series.str.upper#Series.str.upper()[source]#Convert strings in the Series/Index to uppercase.Equivalent tostr.upper().Returns:Series or Index of objectSee alsoSeries.str.lowerConverts all characters to lowercase.Series.str.upperConverts all characters to uppercase.Series.str.titleConverts first character of each word to uppercase and remaining to lowercase.Series.str.capitalizeConverts first character to uppercase and remaining to lowercase.Series.str.swapcaseConverts uppercase to lowercase and lowercase to uppercase.Series.str.casefoldRemoves all case distinctions in the string.Examples>>>s=pd.Series(['lower','CAPITALS','this is a sentence','SwApCaSe'])>>>s0                 lower1              CAPITALS2    this is a sentence3              SwApCaSedtype: object>>>s.str.lower()0                 lower1              capitals2    this is a sentence3              swapcasedtype: object>>>s.str.upper()0                 LOWER1              CAPITALS2    THIS IS A SENTENCE3              SWAPCASEdtype: object>>>s.str.title()0                 Lower1              Capitals2    This Is A Sentence3              Swapcasedtype: object>>>s.str.capitalize()0                 Lower1              Capitals2    This is a sentence3              Swapcasedtype: object>>>s.str.swapcase()0                 LOWER1              capitals2    THIS IS A SENTENCE3              sWaPcAsEdtype: object"
Pandas,Series,pandas.Series.str.wrap,"pandas.Series.str.wrap#Series.str.wrap(width,**kwargs)[source]#Wrap strings in Series/Index at specified line width.This method has the same keyword parameters and defaults astextwrap.TextWrapper.Parameters:widthintMaximum line width.expand_tabsbool, optionalIf True, tab characters will be expanded to spaces (default: True).replace_whitespacebool, optionalIf True, each whitespace character (as defined by string.whitespace)
remaining after tab expansion will be replaced by a single space
(default: True).drop_whitespacebool, optionalIf True, whitespace that, after wrapping, happens to end up at the
beginning or end of a line is dropped (default: True).break_long_wordsbool, optionalIf True, then words longer than width will be broken in order to ensure
that no lines are longer than width. If it is false, long words will
not be broken, and some lines may be longer than width (default: True).break_on_hyphensbool, optionalIf True, wrapping will occur preferably on whitespace and right after
hyphens in compound words, as it is customary in English. If false,
only whitespaces will be considered as potentially good places for line
breaks, but you need to set break_long_words to false if you want truly
insecable words (default: True).Returns:Series or IndexNotesInternally, this method uses atextwrap.TextWrapperinstance with
default settings. To achieve behavior matching Rfs stringr library str_wrap
function, use the arguments:expand_tabs = Falsereplace_whitespace = Truedrop_whitespace = Truebreak_long_words = Falsebreak_on_hyphens = FalseExamples>>>s=pd.Series(['line to be wrapped','another line to be wrapped'])>>>s.str.wrap(12)0             line to be\nwrapped1    another line\nto be\nwrappeddtype: object"
Pandas,Series,pandas.Series.str.zfill,"pandas.Series.str.zfill#Series.str.zfill(width)[source]#Pad strings in the Series/Index by prepending e0f characters.Strings in the Series/Index are padded with e0f characters on the
left of the string to reach a total string lengthwidth. Strings
in the Series/Index with length greater or equal towidthare
unchanged.Parameters:widthintMinimum length of resulting string; strings with length less
thanwidthbe prepended with e0f characters.Returns:Series/Index of objects.See alsoSeries.str.rjustFills the left side of strings with an arbitrary character.Series.str.ljustFills the right side of strings with an arbitrary character.Series.str.padFills the specified sides of strings with an arbitrary character.Series.str.centerFills both sides of strings with an arbitrary character.NotesDiffers fromstr.zfill()which has special handling
for e+f/f-f in the string.Examples>>>s=pd.Series(['-1','1','1000',10,np.nan])>>>s0      -11       12    10003      104     NaNdtype: objectNote that10andNaNare not strings, therefore they are
converted toNaN. The minus sign in'-1'is treated as a
special character and the zero is added to the right of it
(str.zfill()would have moved it to the left).1000remains unchanged as it is longer thanwidth.>>>s.str.zfill(3)0     -011     0012    10003     NaN4     NaNdtype: object"
Pandas,Series,pandas.Series.str.isalnum,"pandas.Series.str.isalnum#Series.str.isalnum()[source]#Check whether all characters in each string are alphanumeric.This is equivalent to running the Python string methodstr.isalnum()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.isalpha,"pandas.Series.str.isalpha#Series.str.isalpha()[source]#Check whether all characters in each string are alphabetic.This is equivalent to running the Python string methodstr.isalpha()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.isdigit,"pandas.Series.str.isdigit#Series.str.isdigit()[source]#Check whether all characters in each string are digits.This is equivalent to running the Python string methodstr.isdigit()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.isspace,"pandas.Series.str.isspace#Series.str.isspace()[source]#Check whether all characters in each string are whitespace.This is equivalent to running the Python string methodstr.isspace()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.islower,"pandas.Series.str.islower#Series.str.islower()[source]#Check whether all characters in each string are lowercase.This is equivalent to running the Python string methodstr.islower()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.isupper,"pandas.Series.str.isupper#Series.str.isupper()[source]#Check whether all characters in each string are uppercase.This is equivalent to running the Python string methodstr.isupper()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.istitle,"pandas.Series.str.istitle#Series.str.istitle()[source]#Check whether all characters in each string are titlecase.This is equivalent to running the Python string methodstr.istitle()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.isnumeric,"pandas.Series.str.isnumeric#Series.str.isnumeric()[source]#Check whether all characters in each string are numeric.This is equivalent to running the Python string methodstr.isnumeric()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.isdecimal,"pandas.Series.str.isdecimal#Series.str.isdecimal()[source]#Check whether all characters in each string are decimal.This is equivalent to running the Python string methodstr.isdecimal()for each element of the Series/Index. If a string
has zero characters,Falseis returned for that check.Returns:Series or Index of boolSeries or Index of boolean values with the same length as the original
Series/Index.See alsoSeries.str.isalphaCheck whether all characters are alphabetic.Series.str.isnumericCheck whether all characters are numeric.Series.str.isalnumCheck whether all characters are alphanumeric.Series.str.isdigitCheck whether all characters are digits.Series.str.isdecimalCheck whether all characters are decimal.Series.str.isspaceCheck whether all characters are whitespace.Series.str.islowerCheck whether all characters are lowercase.Series.str.isupperCheck whether all characters are uppercase.Series.str.istitleCheck whether all characters are titlecase.ExamplesChecks for Alphabetic and Numeric Characters>>>s1=pd.Series(['one','one1','1',''])>>>s1.str.isalpha()0     True1    False2    False3    Falsedtype: bool>>>s1.str.isnumeric()0    False1    False2     True3    Falsedtype: bool>>>s1.str.isalnum()0     True1     True2     True3    Falsedtype: boolNote that checks against characters mixed with any additional punctuation
or whitespace will evaluate to false for an alphanumeric check.>>>s2=pd.Series(['A B','1.5','3,000'])>>>s2.str.isalnum()0    False1    False2    Falsedtype: boolMore Detailed Checks for Numeric CharactersThere are several different but overlapping sets of numeric characters that
can be checked for.>>>s3=pd.Series(['23','?','?',''])Thes3.str.isdecimalmethod checks for characters used to form numbers
in base 10.>>>s3.str.isdecimal()0     True1    False2    False3    Falsedtype: boolThes.str.isdigitmethod is the same ass3.str.isdecimalbut also
includes special digits, like superscripted and subscripted digits in
unicode.>>>s3.str.isdigit()0     True1     True2    False3    Falsedtype: boolThes.str.isnumericmethod is the same ass3.str.isdigitbut also
includes other characters that can represent quantities such as unicode
fractions.>>>s3.str.isnumeric()0     True1     True2     True3    Falsedtype: boolChecks for Whitespace>>>s4=pd.Series([' ','\t\r\n',''])>>>s4.str.isspace()0     True1     True2    Falsedtype: boolChecks for Character Case>>>s5=pd.Series(['leopard','Golden Eagle','SNAKE',''])>>>s5.str.islower()0     True1    False2    False3    Falsedtype: bool>>>s5.str.isupper()0    False1    False2     True3    Falsedtype: boolThes5.str.istitlemethod checks for whether all words are in title
case (whether only the first letter of each word is capitalized). Words are
assumed to be as any sequence of non-numeric characters separated by
whitespace characters.>>>s5.str.istitle()0    False1     True2    False3    Falsedtype: bool"
Pandas,Series,pandas.Series.str.get_dummies,"pandas.Series.str.get_dummies#Series.str.get_dummies(sep='|')[source]#Return DataFrame of dummy/indicator variables for Series.Each string in Series is split by sep and returned as a DataFrame
of dummy/indicator variables.Parameters:sepstr, default g|hString to split on.Returns:DataFrameDummy variables corresponding to values of the Series.See alsoget_dummiesConvert categorical variable into dummy/indicator variables.Examples>>>pd.Series(['a|b','a','a|c']).str.get_dummies()a  b  c0  1  1  01  1  0  02  1  0  1>>>pd.Series(['a|b',np.nan,'a|c']).str.get_dummies()a  b  c0  1  1  01  0  0  02  1  0  1"
Pandas,Series,pandas.Series.cat.categories,"pandas.Series.cat.categories#Series.cat.categories[source]#The categories of this categorical.Setting assigns new values to each category (effectively a rename of
each individual category).The assigned value has to be a list-like object. All items must be
unique and the number of items in the new categories must be the same
as the number of items in the old categories.Raises:ValueErrorIf the new categories do not validate as categories or if the
number of new categories is unequal the number of old categoriesSee alsorename_categoriesRename categories.reorder_categoriesReorder categories.add_categoriesAdd new categories.remove_categoriesRemove the specified categories.remove_unused_categoriesRemove categories which are not used.set_categoriesSet the categories to the specified ones.ExamplesForpandas.Series:>>>ser=pd.Series(['a','b','c','a'],dtype='category')>>>ser.cat.categoriesIndex(['a', 'b', 'c'], dtype='object')>>>raw_cat=pd.Categorical(['a','b','c','a'],categories=['b','c','d'])>>>ser=pd.Series(raw_cat)>>>ser.cat.categoriesIndex(['b', 'c', 'd'], dtype='object')Forpandas.Categorical:>>>cat=pd.Categorical(['a','b'],ordered=True)>>>cat.categoriesIndex(['a', 'b'], dtype='object')Forpandas.CategoricalIndex:>>>ci=pd.CategoricalIndex(['a','c','b','a','c','b'])>>>ci.categoriesIndex(['a', 'b', 'c'], dtype='object')>>>ci=pd.CategoricalIndex(['a','c'],categories=['c','b','a'])>>>ci.categoriesIndex(['c', 'b', 'a'], dtype='object')"
Pandas,Series,pandas.Series.cat.ordered,"pandas.Series.cat.ordered#Series.cat.ordered[source]#Whether the categories have an ordered relationship.ExamplesForpandas.Series:>>>ser=pd.Series(['a','b','c','a'],dtype='category')>>>ser.cat.orderedFalse>>>raw_cat=pd.Categorical(['a','b','c','a'],ordered=True)>>>ser=pd.Series(raw_cat)>>>ser.cat.orderedTrueForpandas.Categorical:>>>cat=pd.Categorical(['a','b'],ordered=True)>>>cat.orderedTrue>>>cat=pd.Categorical(['a','b'],ordered=False)>>>cat.orderedFalseForpandas.CategoricalIndex:>>>ci=pd.CategoricalIndex(['a','b'],ordered=True)>>>ci.orderedTrue>>>ci=pd.CategoricalIndex(['a','b'],ordered=False)>>>ci.orderedFalse"
Pandas,Series,pandas.Series.cat.codes,"pandas.Series.cat.codes#Series.cat.codes[source]#Return Series of codes as well as the index.Examples>>>raw_cate=pd.Categorical([""a"",""b"",""c"",""a""],categories=[""a"",""b""])>>>ser=pd.Series(raw_cate)>>>ser.cat.codes0   01   12  -13   0dtype: int8"
Pandas,Series,pandas.Series.cat.rename_categories,"pandas.Series.cat.rename_categories#Series.cat.rename_categories(*args,**kwargs)[source]#Rename categories.Parameters:new_categorieslist-like, dict-like or callableNew categories which will replace old categories.list-like: all items must be unique and the number of items in
the new categories must match the existing number of categories.dict-like: specifies a mapping from
old categories to new. Categories not contained in the mapping
are passed through and extra categories in the mapping are
ignored.callable : a callable that is called on all items in the old
categories and whose return values comprise the new categories.Returns:CategoricalCategorical with renamed categories.Raises:ValueErrorIf new categories are list-like and do not have the same number of
items than the current categories or do not validate as categoriesSee alsoreorder_categoriesReorder categories.add_categoriesAdd new categories.remove_categoriesRemove the specified categories.remove_unused_categoriesRemove categories which are not used.set_categoriesSet the categories to the specified ones.Examples>>>c=pd.Categorical(['a','a','b'])>>>c.rename_categories([0,1])[0, 0, 1]Categories (2, int64): [0, 1]For dict-likenew_categories, extra keys are ignored and
categories not in the dictionary are passed through>>>c.rename_categories({'a':'A','c':'C'})['A', 'A', 'b']Categories (2, object): ['A', 'b']You may also provide a callable to create the new categories>>>c.rename_categories(lambdax:x.upper())['A', 'A', 'B']Categories (2, object): ['A', 'B']"
Pandas,Series,pandas.Series.cat.reorder_categories,"pandas.Series.cat.reorder_categories#Series.cat.reorder_categories(*args,**kwargs)[source]#Reorder categories as specified in new_categories.new_categoriesneed to include all old categories and no new category
items.Parameters:new_categoriesIndex-likeThe categories in new order.orderedbool, optionalWhether or not the categorical is treated as a ordered categorical.
If not given, do not change the ordered information.Returns:CategoricalCategorical with reordered categories.Raises:ValueErrorIf the new categories do not contain all old category items or any
new onesSee alsorename_categoriesRename categories.add_categoriesAdd new categories.remove_categoriesRemove the specified categories.remove_unused_categoriesRemove categories which are not used.set_categoriesSet the categories to the specified ones.ExamplesForpandas.Series:>>>ser=pd.Series(['a','b','c','a'],dtype='category')>>>ser=ser.cat.reorder_categories(['c','b','a'],ordered=True)>>>ser0   a1   b2   c3   adtype: categoryCategories (3, object): ['c' < 'b' < 'a']>>>ser.sort_values()2   c1   b0   a3   adtype: categoryCategories (3, object): ['c' < 'b' < 'a']Forpandas.CategoricalIndex:>>>ci=pd.CategoricalIndex(['a','b','c','a'])>>>ciCategoricalIndex(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c'],ordered=False, dtype='category')>>>ci.reorder_categories(['c','b','a'],ordered=True)CategoricalIndex(['a', 'b', 'c', 'a'], categories=['c', 'b', 'a'],ordered=True, dtype='category')"
Pandas,Series,pandas.Series.cat.add_categories,"pandas.Series.cat.add_categories#Series.cat.add_categories(*args,**kwargs)[source]#Add new categories.new_categorieswill be included at the last/highest place in the
categories and will be unused directly after this call.Parameters:new_categoriescategory or list-like of categoryThe new categories to be included.Returns:CategoricalCategorical with new categories added.Raises:ValueErrorIf the new categories include old categories or do not validate as
categoriesSee alsorename_categoriesRename categories.reorder_categoriesReorder categories.remove_categoriesRemove the specified categories.remove_unused_categoriesRemove categories which are not used.set_categoriesSet the categories to the specified ones.Examples>>>c=pd.Categorical(['c','b','c'])>>>c['c', 'b', 'c']Categories (2, object): ['b', 'c']>>>c.add_categories(['d','a'])['c', 'b', 'c']Categories (4, object): ['b', 'c', 'd', 'a']"
Pandas,Series,pandas.Series.cat.remove_categories,"pandas.Series.cat.remove_categories#Series.cat.remove_categories(*args,**kwargs)[source]#Remove the specified categories.removalsmust be included in the old categories. Values which were in
the removed categories will be set to NaNParameters:removalscategory or list of categoriesThe categories which should be removed.Returns:CategoricalCategorical with removed categories.Raises:ValueErrorIf the removals are not contained in the categoriesSee alsorename_categoriesRename categories.reorder_categoriesReorder categories.add_categoriesAdd new categories.remove_unused_categoriesRemove categories which are not used.set_categoriesSet the categories to the specified ones.Examples>>>c=pd.Categorical(['a','c','b','c','d'])>>>c['a', 'c', 'b', 'c', 'd']Categories (4, object): ['a', 'b', 'c', 'd']>>>c.remove_categories(['d','a'])[NaN, 'c', 'b', 'c', NaN]Categories (2, object): ['b', 'c']"
Pandas,Series,pandas.Series.cat.remove_unused_categories,"pandas.Series.cat.remove_unused_categories#Series.cat.remove_unused_categories(*args,**kwargs)[source]#Remove categories which are not used.Returns:CategoricalCategorical with unused categories dropped.See alsorename_categoriesRename categories.reorder_categoriesReorder categories.add_categoriesAdd new categories.remove_categoriesRemove the specified categories.set_categoriesSet the categories to the specified ones.Examples>>>c=pd.Categorical(['a','c','b','c','d'])>>>c['a', 'c', 'b', 'c', 'd']Categories (4, object): ['a', 'b', 'c', 'd']>>>c[2]='a'>>>c[4]='c'>>>c['a', 'c', 'a', 'c', 'c']Categories (4, object): ['a', 'b', 'c', 'd']>>>c.remove_unused_categories()['a', 'c', 'a', 'c', 'c']Categories (2, object): ['a', 'c']"
Pandas,Series,pandas.Series.cat.set_categories,"pandas.Series.cat.set_categories#Series.cat.set_categories(*args,**kwargs)[source]#Set the categories to the specified new categories.new_categoriescan include new categories (which will result in
unused categories) or remove old categories (which results in values
set toNaN). Ifrename=True, the categories will simply be renamed
(less or more items than in old categories will result in values set toNaNor in unused categories respectively).This method can be used to perform more than one action of adding,
removing, and reordering simultaneously and is therefore faster than
performing the individual steps via the more specialised methods.On the other hand this methods does not do checks (e.g., whether the
old categories are included in the new categories on a reorder), which
can result in surprising changes, for example when using special string
dtypes, which does not considers a S1 string equal to a single char
python string.Parameters:new_categoriesIndex-likeThe categories in new order.orderedbool, default FalseWhether or not the categorical is treated as a ordered categorical.
If not given, do not change the ordered information.renamebool, default FalseWhether or not the new_categories should be considered as a rename
of the old categories or as reordered categories.Returns:Categorical with reordered categories.Raises:ValueErrorIf new_categories does not validate as categoriesSee alsorename_categoriesRename categories.reorder_categoriesReorder categories.add_categoriesAdd new categories.remove_categoriesRemove the specified categories.remove_unused_categoriesRemove categories which are not used.ExamplesForpandas.Series:>>>raw_cat=pd.Categorical(['a','b','c','A'],...categories=['a','b','c'],ordered=True)>>>ser=pd.Series(raw_cat)>>>ser0   a1   b2   c3   NaNdtype: categoryCategories (3, object): ['a' < 'b' < 'c']>>>ser.cat.set_categories(['A','B','C'],rename=True)0   A1   B2   C3   NaNdtype: categoryCategories (3, object): ['A' < 'B' < 'C']Forpandas.CategoricalIndex:>>>ci=pd.CategoricalIndex(['a','b','c','A'],...categories=['a','b','c'],ordered=True)>>>ciCategoricalIndex(['a', 'b', 'c', nan], categories=['a', 'b', 'c'],ordered=True, dtype='category')>>>ci.set_categories(['A','b','c'])CategoricalIndex([nan, 'b', 'c', nan], categories=['A', 'b', 'c'],ordered=True, dtype='category')>>>ci.set_categories(['A','b','c'],rename=True)CategoricalIndex(['A', 'b', 'c', nan], categories=['A', 'b', 'c'],ordered=True, dtype='category')"
Pandas,Series,pandas.Series.cat.as_ordered,"pandas.Series.cat.as_ordered#Series.cat.as_ordered(*args,**kwargs)[source]#Set the Categorical to be ordered.Returns:CategoricalOrdered Categorical.ExamplesForpandas.Series:>>>ser=pd.Series(['a','b','c','a'],dtype='category')>>>ser.cat.orderedFalse>>>ser=ser.cat.as_ordered()>>>ser.cat.orderedTrueForpandas.CategoricalIndex:>>>ci=pd.CategoricalIndex(['a','b','c','a'])>>>ci.orderedFalse>>>ci=ci.as_ordered()>>>ci.orderedTrue"
Pandas,Series,pandas.Series.cat.as_unordered,"pandas.Series.cat.as_unordered#Series.cat.as_unordered(*args,**kwargs)[source]#Set the Categorical to be unordered.Returns:CategoricalUnordered Categorical.ExamplesForpandas.Series:>>>raw_cat=pd.Categorical(['a','b','c','a'],ordered=True)>>>ser=pd.Series(raw_cat)>>>ser.cat.orderedTrue>>>ser=ser.cat.as_unordered()>>>ser.cat.orderedFalseForpandas.CategoricalIndex:>>>ci=pd.CategoricalIndex(['a','b','c','a'],ordered=True)>>>ci.orderedTrue>>>ci=ci.as_unordered()>>>ci.orderedFalse"
Pandas,Series,pandas.Series.sparse.npoints,"pandas.Series.sparse.npoints#Series.sparse.npoints[source]#The number of non-fill_valuepoints.Examples>>>frompandas.arraysimportSparseArray>>>s=SparseArray([0,0,1,1,1],fill_value=0)>>>s.npoints3"
Pandas,Series,pandas.Series.sparse.density,"pandas.Series.sparse.density#Series.sparse.density[source]#The percent of non-fill_valuepoints, as decimal.Examples>>>frompandas.arraysimportSparseArray>>>s=SparseArray([0,0,1,1,1],fill_value=0)>>>s.density0.6"
Pandas,Series,pandas.Series.sparse.fill_value,"pandas.Series.sparse.fill_value#Series.sparse.fill_value[source]#Elements indatathat arefill_valueare not stored.For memory savings, this should be the most common value in the array.Examples>>>ser=pd.Series([0,0,2,2,2],dtype=""Sparse[int]"")>>>ser.sparse.fill_value0>>>spa_dtype=pd.SparseDtype(dtype=np.int32,fill_value=2)>>>ser=pd.Series([0,0,2,2,2],dtype=spa_dtype)>>>ser.sparse.fill_value2"
Pandas,Series,pandas.Series.sparse.sp_values,"pandas.Series.sparse.sp_values#Series.sparse.sp_values[source]#An ndarray containing the non-fill_valuevalues.Examples>>>frompandas.arraysimportSparseArray>>>s=SparseArray([0,0,1,0,2],fill_value=0)>>>s.sp_valuesarray([1, 2])"
Pandas,Series,pandas.Series.sparse.from_coo,"pandas.Series.sparse.from_coo#classmethodSeries.sparse.from_coo(A,dense_index=False)[source]#Create a Series with sparse values from a scipy.sparse.coo_matrix.Parameters:Ascipy.sparse.coo_matrixdense_indexbool, default FalseIf False (default), the index consists of only the
coords of the non-null entries of the original coo_matrix.
If True, the index consists of the full sorted
(row, col) coordinates of the coo_matrix.Returns:sSeriesA Series with sparse values.Examples>>>fromscipyimportsparse>>>A=sparse.coo_matrix(...([3.0,1.0,2.0],([1,0,0],[0,2,3])),shape=(3,4)...)>>>A<3x4 sparse matrix of type '<class 'numpy.float64'>'with 3 stored elements in COOrdinate format>>>>A.todense()matrix([[0., 0., 1., 2.],[3., 0., 0., 0.],[0., 0., 0., 0.]])>>>ss=pd.Series.sparse.from_coo(A)>>>ss0  2    1.03    2.01  0    3.0dtype: Sparse[float64, nan]"
Pandas,Series,pandas.Series.sparse.to_coo,"pandas.Series.sparse.to_coo#Series.sparse.to_coo(row_levels=(0,),column_levels=(1,),sort_labels=False)[source]#Create a scipy.sparse.coo_matrix from a Series with MultiIndex.Use row_levels and column_levels to determine the row and column
coordinates respectively. row_levels and column_levels are the names
(labels) or numbers of the levels. {row_levels, column_levels} must be
a partition of the MultiIndex level names (or numbers).Parameters:row_levelstuple/listcolumn_levelstuple/listsort_labelsbool, default FalseSort the row and column labels before forming the sparse matrix.
Whenrow_levelsand/orcolumn_levelsrefer to a single level,
set toTruefor a faster execution.Returns:yscipy.sparse.coo_matrixrowslist (row labels)columnslist (column labels)Examples>>>s=pd.Series([3.0,np.nan,1.0,3.0,np.nan,np.nan])>>>s.index=pd.MultiIndex.from_tuples(...[...(1,2,""a"",0),...(1,2,""a"",1),...(1,1,""b"",0),...(1,1,""b"",1),...(2,1,""b"",0),...(2,1,""b"",1)...],...names=[""A"",""B"",""C"",""D""],...)>>>sA  B  C  D1  2  a  0    3.01    NaN1  b  0    1.01    3.02  1  b  0    NaN1    NaNdtype: float64>>>ss=s.astype(""Sparse"")>>>ssA  B  C  D1  2  a  0    3.01    NaN1  b  0    1.01    3.02  1  b  0    NaN1    NaNdtype: Sparse[float64, nan]>>>A,rows,columns=ss.sparse.to_coo(...row_levels=[""A"",""B""],column_levels=[""C"",""D""],sort_labels=True...)>>>A<3x4 sparse matrix of type '<class 'numpy.float64'>'with 3 stored elements in COOrdinate format>>>>A.todense()matrix([[0., 0., 1., 3.],[3., 0., 0., 0.],[0., 0., 0., 0.]])>>>rows[(1, 1), (1, 2), (2, 1)]>>>columns[('a', 0), ('a', 1), ('b', 0), ('b', 1)]"
Pandas,Series,pandas.Flags,"pandas.Flags#classpandas.Flags(obj,*,allows_duplicate_labels)[source]#Flags that apply to pandas objects.New in version 1.2.0.Parameters:objSeries or DataFrameThe object these flags are associated with.allows_duplicate_labelsbool, default TrueWhether to allow duplicate labels in this object. By default,
duplicate labels are permitted. Setting this toFalsewill
cause anerrors.DuplicateLabelErrorto be raised whenindex(or columns for DataFrame) is not unique, or any
subsequent operation on introduces duplicates.
SeeDisallowing Duplicate Labelsfor more.WarningThis is an experimental feature. Currently, many methods fail to
propagate theallows_duplicate_labelsvalue. In future versions
it is expected that every method taking or returning one or more
DataFrame or Series objects will propagateallows_duplicate_labels.ExamplesAttributes can be set in two ways:>>>df=pd.DataFrame()>>>df.flags<Flags(allows_duplicate_labels=True)>>>>df.flags.allows_duplicate_labels=False>>>df.flags<Flags(allows_duplicate_labels=False)>>>>df.flags['allows_duplicate_labels']=True>>>df.flags<Flags(allows_duplicate_labels=True)>Attributesallows_duplicate_labelsWhether this object allows duplicate labels."
Pandas,Series,pandas.Series.attrs,"pandas.Series.attrs#propertySeries.attrs[source]#Dictionary of global attributes of this dataset.Warningattrs is experimental and may change without warning.See alsoDataFrame.flagsGlobal flags applying to this object.ExamplesFor Series:>>>ser=pd.Series([1,2,3])>>>ser.attrs={""A"":[10,20,30]}>>>ser.attrs{'A': [10, 20, 30]}For DataFrame:>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>df.attrs={""A"":[10,20,30]}>>>df.attrs{'A': [10, 20, 30]}"
Pandas,Series,pandas.Series.plot,"pandas.Series.plot#Series.plot(*args,**kwargs)[source]#Make plots of Series or DataFrame.Uses the backend specified by the
optionplotting.backend. By default, matplotlib is used.Parameters:dataSeries or DataFrameThe object for which the method is called.xlabel or position, default NoneOnly used if data is a DataFrame.ylabel, position or list of label, positions, default NoneAllows plotting of one column versus another. Only used if data is a
DataFrame.kindstrThe kind of plot to produce:elinef : line plot (default)ebarf : vertical bar plotebarhf : horizontal bar plotehistf : histogrameboxf : boxplotekdef : Kernel Density Estimation plotedensityf : same as ekdefeareaf : area plotepief : pie plotescatterf : scatter plot (DataFrame only)ehexbinf : hexbin plot (DataFrame only)axmatplotlib axes object, default NoneAn axes of the current figure.subplotsbool or sequence of iterables, default FalseWhether to group columns into subplots:False: No subplots will be usedTrue: Make separate subplots for each column.sequence of iterables of column labels: Create a subplot for each
group of columns. For example[(eaf, ecf), (ebf, edf)]will
create 2 subplots: one with columns eaf and ecf, and one
with columns ebf and edf. Remaining columns that arenft specified
will be plotted in additional subplots (one per column).New in version 1.5.0.sharexbool, default True if ax is None else FalseIn casesubplots=True, share x axis and set some x axis labels
to invisible; defaults to True if ax is None otherwise False if
an ax is passed in; Be aware, that passing in both an ax andsharex=Truewill alter all x axis labels for all axis in a figure.shareybool, default FalseIn casesubplots=True, share y axis and set some y axis labels to invisible.layouttuple, optional(rows, columns) for the layout of subplots.figsizea tuple (width, height) in inchesSize of a figure object.use_indexbool, default TrueUse index as ticks for x axis.titlestr or listTitle to use for the plot. If a string is passed, print the string
at the top of the figure. If a list is passed andsubplotsis
True, print each item in the list above the corresponding subplot.gridbool, default None (matlab style default)Axis grid lines.legendbool or {ereversef}Place legend on axis subplots.stylelist or dictThe matplotlib line style per column.logxbool or esymf, default FalseUse log scaling or symlog scaling on x axis.logybool or esymf default FalseUse log scaling or symlog scaling on y axis.loglogbool or esymf, default FalseUse log scaling or symlog scaling on both x and y axes.xtickssequenceValues to use for the xticks.ytickssequenceValues to use for the yticks.xlim2-tuple/listSet the x limits of the current axes.ylim2-tuple/listSet the y limits of the current axes.xlabellabel, optionalName to use for the xlabel on x-axis. Default uses index name as xlabel, or the
x-column name for planar plots.Changed in version 1.2.0:Now applicable to planar plots (scatter,hexbin).Changed in version 2.0.0:Now applicable to histograms.ylabellabel, optionalName to use for the ylabel on y-axis. Default will show no ylabel, or the
y-column name for planar plots.Changed in version 1.2.0:Now applicable to planar plots (scatter,hexbin).Changed in version 2.0.0:Now applicable to histograms.rotfloat, default NoneRotation for ticks (xticks for vertical, yticks for horizontal
plots).fontsizefloat, default NoneFont size for xticks and yticks.colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If string, load colormap with that
name from matplotlib.colorbarbool, optionalIf True, plot colorbar (only relevant for escatterf and ehexbinf
plots).positionfloatSpecify relative alignments for bar plot layout.
From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5
(center).tablebool, Series or DataFrame, default FalseIf True, draw a table using the data in the DataFrame and the data
will be transposed to meet matplotlibfs default layout.
If a Series or DataFrame is passed, use passed data to draw a
table.yerrDataFrame, Series, array-like, dict and strSeePlotting with Error Barsfor
detail.xerrDataFrame, Series, array-like, dict and strEquivalent to yerr.stackedbool, default False in line and bar plots, and True in area plotIf True, create stacked plot.secondary_ybool or sequence, default FalseWhether to plot on the secondary y-axis if a list/tuple, which
columns to plot on secondary y-axis.mark_rightbool, default TrueWhen using a secondary_y axis, automatically mark the column
labels with g(right)h in the legend.include_boolbool, default is FalseIf True, boolean values can be plotted.backendstr, default NoneBackend to use instead of the backend specified in the optionplotting.backend. For instance, ematplotlibf. Alternatively, to
specify theplotting.backendfor the whole session, setpd.options.plotting.backend.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.Axesor numpy.ndarray of themIf the backend is not the default matplotlib one, the return value
will be the object returned by the backend.NotesSee matplotlib documentation online for more on this subjectIfkind= ebarf or ebarhf, you can specify relative alignments
for bar plot layout bypositionkeyword.
From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5
(center)ExamplesFor Series:>>>ser=pd.Series([1,2,3,3])>>>plot=ser.plot(kind='hist',title=""My plot"")For DataFrame:>>>df=pd.DataFrame({'length':[1.5,0.5,1.2,0.9,3],...'width':[0.7,0.2,0.15,0.2,1.1]},...index=['pig','rabbit','duck','chicken','horse'])>>>plot=df.plot(title=""DataFrame Plot"")For SeriesGroupBy:>>>lst=[-1,-2,-3,1,2,3]>>>ser=pd.Series([1,2,2,4,6,6],index=lst)>>>plot=ser.groupby(lambdax:x>0).plot(title=""SeriesGroupBy Plot"")For DataFrameGroupBy:>>>df=pd.DataFrame({""col1"":[1,2,3,4],...""col2"":[""A"",""B"",""A"",""B""]})>>>plot=df.groupby(""col2"").plot(kind=""bar"",title=""DataFrameGroupBy Plot"")"
Pandas,Series,pandas.Series.plot.area,"pandas.Series.plot.area#Series.plot.area(x=None,y=None,stacked=True,**kwargs)[source]#Draw a stacked area plot.An area plot displays quantitative data visually.
This function wraps the matplotlib area function.Parameters:xlabel or position, optionalCoordinates for the X axis. By default uses the index.ylabel or position, optionalColumn to plot. By default uses all columns.stackedbool, default TrueArea plots are stacked by default. Set to False to create a
unstacked plot.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or numpy.ndarrayArea plot, or array of area plots if subplots is True.See alsoDataFrame.plotMake plots of DataFrame using matplotlib / pylab.ExamplesDraw an area plot based on basic business metrics:>>>df=pd.DataFrame({...'sales':[3,2,3,9,10,6],...'signups':[5,5,6,12,14,13],...'visits':[20,42,28,62,81,50],...},index=pd.date_range(start='2018/01/01',end='2018/07/01',...freq='M'))>>>ax=df.plot.area()Area plots are stacked by default. To produce an unstacked plot,
passstacked=False:>>>ax=df.plot.area(stacked=False)Draw an area plot for a single column:>>>ax=df.plot.area(y='sales')Draw with a differentx:>>>df=pd.DataFrame({...'sales':[3,2,3],...'visits':[20,42,28],...'day':[1,2,3],...})>>>ax=df.plot.area(x='day')"
Pandas,Series,pandas.Series.plot.bar,"pandas.Series.plot.bar#Series.plot.bar(x=None,y=None,**kwargs)[source]#Vertical bar plot.A bar plot is a plot that presents categorical data with
rectangular bars with lengths proportional to the values that they
represent. A bar plot shows comparisons among discrete categories. One
axis of the plot shows the specific categories being compared, and the
other axis represents a measured value.Parameters:xlabel or position, optionalAllows plotting of one column versus another. If not specified,
the index of the DataFrame is used.ylabel or position, optionalAllows plotting of one column versus another. If not specified,
all numerical columns are used.colorstr, array-like, or dict, optionalThe color for each of the DataFramefs columns. Possible values are:A single color string referred to by name, RGB or RGBA code,for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For
instance [egreenf,fyellowf] each columnfs bar will be filled in
green or yellow, alternatively. If there is only a single column to
be plotted, then only the first color from the color list will be
used.A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are calledaandb, then passing {eaf: egreenf, ebf: eredf} will color bars for
columnain green and bars for columnbin red.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with onematplotlib.axes.Axesper column whensubplots=True.See alsoDataFrame.plot.barhHorizontal bar plot.DataFrame.plotMake plots of a DataFrame.matplotlib.pyplot.barMake a bar plot with matplotlib.ExamplesBasic plot.>>>df=pd.DataFrame({'lab':['A','B','C'],'val':[10,30,20]})>>>ax=df.plot.bar(x='lab',y='val',rot=0)Plot a whole dataframe to a bar plot. Each column is assigned a
distinct color, and each row is nested in a group along the
horizontal axis.>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.bar(rot=0)Plot stacked bar charts for the DataFrame>>>ax=df.plot.bar(stacked=True)Instead of nesting, the figure can be split by column withsubplots=True. In this case, anumpy.ndarrayofmatplotlib.axes.Axesare returned.>>>axes=df.plot.bar(rot=0,subplots=True)>>>axes[1].legend(loc=2)If you donft like the default colours, you can specify how youfd
like each column to be colored.>>>axes=df.plot.bar(...rot=0,subplots=True,color={""speed"":""red"",""lifespan"":""green""}...)>>>axes[1].legend(loc=2)Plot a single column.>>>ax=df.plot.bar(y='speed',rot=0)Plot only selected categories for the DataFrame.>>>ax=df.plot.bar(x='lifespan',rot=0)"
Pandas,Series,pandas.Series.plot.barh,"pandas.Series.plot.barh#Series.plot.barh(x=None,y=None,**kwargs)[source]#Make a horizontal bar plot.A horizontal bar plot is a plot that presents quantitative data with
rectangular bars with lengths proportional to the values that they
represent. A bar plot shows comparisons among discrete categories. One
axis of the plot shows the specific categories being compared, and the
other axis represents a measured value.Parameters:xlabel or position, optionalAllows plotting of one column versus another. If not specified,
the index of the DataFrame is used.ylabel or position, optionalAllows plotting of one column versus another. If not specified,
all numerical columns are used.colorstr, array-like, or dict, optionalThe color for each of the DataFramefs columns. Possible values are:A single color string referred to by name, RGB or RGBA code,for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For
instance [egreenf,fyellowf] each columnfs bar will be filled in
green or yellow, alternatively. If there is only a single column to
be plotted, then only the first color from the color list will be
used.A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are calledaandb, then passing {eaf: egreenf, ebf: eredf} will color bars for
columnain green and bars for columnbin red.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with onematplotlib.axes.Axesper column whensubplots=True.See alsoDataFrame.plot.barVertical bar plot.DataFrame.plotMake plots of DataFrame using matplotlib.matplotlib.axes.Axes.barPlot a vertical bar plot using matplotlib.ExamplesBasic example>>>df=pd.DataFrame({'lab':['A','B','C'],'val':[10,30,20]})>>>ax=df.plot.barh(x='lab',y='val')Plot a whole DataFrame to a horizontal bar plot>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.barh()Plot stacked barh charts for the DataFrame>>>ax=df.plot.barh(stacked=True)We can specify colors for each column>>>ax=df.plot.barh(color={""speed"":""red"",""lifespan"":""green""})Plot a column of the DataFrame to a horizontal bar plot>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.barh(y='speed')Plot DataFrame versus the desired column>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.barh(x='lifespan')"
Pandas,Series,pandas.Series.plot.box,"pandas.Series.plot.box#Series.plot.box(by=None,**kwargs)[source]#Make a box plot of the DataFrame columns.A box plot is a method for graphically depicting groups of numerical
data through their quartiles.
The box extends from the Q1 to Q3 quartile values of the data,
with a line at the median (Q2). The whiskers extend from the edges
of box to show the range of the data. The position of the whiskers
is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the
box. Outlier points are those past the end of the whiskers.For further details see Wikipediafs
entry forboxplot.A consideration when using this chart is that the box and the whiskers
can overlap, which is very common when plotting small sets of data.Parameters:bystr or sequenceColumn in the DataFrame to group by.Changed in version 1.4.0:Previously,byis silently ignore and makes no groupings**kwargsAdditional keywords are documented inDataFrame.plot().Returns:matplotlib.axes.Axesor numpy.ndarray of themSee alsoDataFrame.boxplotAnother method to draw a box plot.Series.plot.boxDraw a box plot from a Series object.matplotlib.pyplot.boxplotDraw a box plot in matplotlib.ExamplesDraw a box plot from a DataFrame with four columns of randomly
generated data.>>>data=np.random.randn(25,4)>>>df=pd.DataFrame(data,columns=list('ABCD'))>>>ax=df.plot.box()You can also generate groupings if you specify thebyparameter (which
can take a column name, or a list or tuple of column names):Changed in version 1.4.0.>>>age_list=[8,10,12,14,72,74,76,78,20,25,30,35,60,85]>>>df=pd.DataFrame({""gender"":list(""MMMMMMMMFFFFFF""),""age"":age_list})>>>ax=df.plot.box(column=""age"",by=""gender"",figsize=(10,8))"
Pandas,Series,pandas.Series.plot.density,"pandas.Series.plot.density#Series.plot.density(bw_method=None,ind=None,**kwargs)[source]#Generate Kernel Density Estimate plot using Gaussian kernels.In statistics,kernel density estimation(KDE) is a non-parametric
way to estimate the probability density function (PDF) of a random
variable. This function uses Gaussian kernels and includes automatic
bandwidth determination.Parameters:bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be
escottf, esilvermanf, a scalar constant or a callable.
If None (default), escottf is used.
Seescipy.stats.gaussian_kdefor more information.indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default),
1000 equally spaced points are used. Ifindis a NumPy array, the
KDE is evaluated at the points passed. Ifindis an integer,indnumber of equally spaced points are used.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or numpy.ndarray of themSee alsoscipy.stats.gaussian_kdeRepresentation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.ExamplesGiven a Series of points randomly sampled from an unknown
distribution, estimate its PDF using KDE with automatic
bandwidth determination and plot the results, evaluating them at
1000 equally spaced points (default):>>>s=pd.Series([1,2,2.5,3,3.5,4,5])>>>ax=s.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=s.plot.kde(bw_method=0.3)>>>ax=s.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=s.plot.kde(ind=[1,2,3,4,5])For DataFrame, it works in the same way:>>>df=pd.DataFrame({...'x':[1,2,2.5,3,3.5,4,5],...'y':[4,4,4.5,5,5.5,6,6],...})>>>ax=df.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=df.plot.kde(bw_method=0.3)>>>ax=df.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=df.plot.kde(ind=[1,2,3,4,5,6])"
Pandas,Series,pandas.Series.plot.hist,"pandas.Series.plot.hist#Series.plot.hist(by=None,bins=10,**kwargs)[source]#Draw one histogram of the DataFramefs columns.A histogram is a representation of the distribution of data.
This function groups the values of all given Series in the DataFrame
into bins and draws all bins in onematplotlib.axes.Axes.
This is useful when the DataFramefs Series are in a similar scale.Parameters:bystr or sequence, optionalColumn in the DataFrame to group by.Changed in version 1.4.0:Previously,byis silently ignore and makes no groupingsbinsint, default 10Number of histogram bins to be used.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:class:matplotlib.AxesSubplotReturn a histogram plot.See alsoDataFrame.histDraw histograms per DataFramefs Series.Series.histDraw a histogram with Seriesf data.ExamplesWhen we roll a die 6000 times, we expect to get each value around 1000
times. But when we roll two dice and sum the result, the distribution
is going to be quite different. A histogram illustrates those
distributions.>>>df=pd.DataFrame(...np.random.randint(1,7,6000),...columns=['one'])>>>df['two']=df['one']+np.random.randint(1,7,6000)>>>ax=df.plot.hist(bins=12,alpha=0.5)A grouped histogram can be generated by providing the parameterby(which
can be a column name, or a list of column names):>>>age_list=[8,10,12,14,72,74,76,78,20,25,30,35,60,85]>>>df=pd.DataFrame({""gender"":list(""MMMMMMMMFFFFFF""),""age"":age_list})>>>ax=df.plot.hist(column=[""age""],by=""gender"",figsize=(10,8))"
Pandas,Series,pandas.Series.plot.kde,"pandas.Series.plot.kde#Series.plot.kde(bw_method=None,ind=None,**kwargs)[source]#Generate Kernel Density Estimate plot using Gaussian kernels.In statistics,kernel density estimation(KDE) is a non-parametric
way to estimate the probability density function (PDF) of a random
variable. This function uses Gaussian kernels and includes automatic
bandwidth determination.Parameters:bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be
escottf, esilvermanf, a scalar constant or a callable.
If None (default), escottf is used.
Seescipy.stats.gaussian_kdefor more information.indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default),
1000 equally spaced points are used. Ifindis a NumPy array, the
KDE is evaluated at the points passed. Ifindis an integer,indnumber of equally spaced points are used.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or numpy.ndarray of themSee alsoscipy.stats.gaussian_kdeRepresentation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.ExamplesGiven a Series of points randomly sampled from an unknown
distribution, estimate its PDF using KDE with automatic
bandwidth determination and plot the results, evaluating them at
1000 equally spaced points (default):>>>s=pd.Series([1,2,2.5,3,3.5,4,5])>>>ax=s.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=s.plot.kde(bw_method=0.3)>>>ax=s.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=s.plot.kde(ind=[1,2,3,4,5])For DataFrame, it works in the same way:>>>df=pd.DataFrame({...'x':[1,2,2.5,3,3.5,4,5],...'y':[4,4,4.5,5,5.5,6,6],...})>>>ax=df.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=df.plot.kde(bw_method=0.3)>>>ax=df.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=df.plot.kde(ind=[1,2,3,4,5,6])"
Pandas,Series,pandas.Series.plot.line,"pandas.Series.plot.line#Series.plot.line(x=None,y=None,**kwargs)[source]#Plot Series or DataFrame as lines.This function is useful to plot lines using DataFramefs values
as coordinates.Parameters:xlabel or position, optionalAllows plotting of one column versus another. If not specified,
the index of the DataFrame is used.ylabel or position, optionalAllows plotting of one column versus another. If not specified,
all numerical columns are used.colorstr, array-like, or dict, optionalThe color for each of the DataFramefs columns. Possible values are:A single color string referred to by name, RGB or RGBA code,for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For
instance [egreenf,fyellowf] each columnfs line will be filled in
green or yellow, alternatively. If there is only a single column to
be plotted, then only the first color from the color list will be
used.A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are calledaandb, then passing {eaf: egreenf, ebf: eredf} will color lines for
columnain green and lines for columnbin red.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with onematplotlib.axes.Axesper column whensubplots=True.See alsomatplotlib.pyplot.plotPlot y versus x as lines and/or markers.Examples>>>s=pd.Series([1,3,2])>>>s.plot.line()The following example shows the populations for some animals
over the years.>>>df=pd.DataFrame({...'pig':[20,18,489,675,1776],...'horse':[4,25,281,600,1900]...},index=[1990,1997,2003,2009,2014])>>>lines=df.plot.line()An example with subplots, so an array of axes is returned.>>>axes=df.plot.line(subplots=True)>>>type(axes)<class 'numpy.ndarray'>Letfs repeat the same example, but specifying colors for
each column (in this case, for each animal).>>>axes=df.plot.line(...subplots=True,color={""pig"":""pink"",""horse"":""#742802""}...)The following example shows the relationship between both
populations.>>>lines=df.plot.line(x='pig',y='horse')"
Pandas,Series,pandas.Series.plot.pie,"pandas.Series.plot.pie#Series.plot.pie(**kwargs)[source]#Generate a pie plot.A pie plot is a proportional representation of the numerical data in a
column. This function wrapsmatplotlib.pyplot.pie()for the
specified column. If no column reference is passed andsubplots=Truea pie plot is drawn for each numerical column
independently.Parameters:yint or label, optionalLabel or position of the column to plot.
If not provided,subplots=Trueargument must be passed.**kwargsKeyword arguments to pass on toDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themA NumPy array is returned whensubplotsis True.See alsoSeries.plot.pieGenerate a pie plot for a Series.DataFrame.plotMake plots of a DataFrame.ExamplesIn the example below we have a DataFrame with the information about
planetfs mass and radius. We pass the emassf column to the
pie function to get a pie plot.>>>df=pd.DataFrame({'mass':[0.330,4.87,5.97],...'radius':[2439.7,6051.8,6378.1]},...index=['Mercury','Venus','Earth'])>>>plot=df.plot.pie(y='mass',figsize=(5,5))>>>plot=df.plot.pie(subplots=True,figsize=(11,6))"
Pandas,Series,pandas.Series.hist,"pandas.Series.hist#Series.hist(by=None,ax=None,grid=True,xlabelsize=None,xrot=None,ylabelsize=None,yrot=None,figsize=None,bins=10,backend=None,legend=False,**kwargs)[source]#Draw histogram of the input series using matplotlib.Parameters:byobject, optionalIf passed, then used to form histograms for separate groups.axmatplotlib axis objectIf not passed, uses gca().gridbool, default TrueWhether to show axis grid lines.xlabelsizeint, default NoneIf specified changes the x-axis label size.xrotfloat, default NoneRotation of x axis labels.ylabelsizeint, default NoneIf specified changes the y-axis label size.yrotfloat, default NoneRotation of y axis labels.figsizetuple, default NoneFigure size in inches by default.binsint or sequence, default 10Number of histogram bins to be used. If an integer is given, bins + 1
bin edges are calculated and returned. If bins is a sequence, gives
bin edges, including left edge of first bin and right edge of last
bin. In this case, bins is returned unmodified.backendstr, default NoneBackend to use instead of the backend specified in the optionplotting.backend. For instance, ematplotlibf. Alternatively, to
specify theplotting.backendfor the whole session, setpd.options.plotting.backend.legendbool, default FalseWhether to show the legend.**kwargsTo be passed to the actual plotting function.Returns:matplotlib.AxesSubplotA histogram plot.See alsomatplotlib.axes.Axes.histPlot a histogram using matplotlib.ExamplesFor Series:>>>lst=['a','a','a','b','b','b']>>>ser=pd.Series([1,2,2,4,6,6],index=lst)>>>hist=ser.hist()For Groupby:>>>lst=['a','a','a','b','b','b']>>>ser=pd.Series([1,2,2,4,6,6],index=lst)>>>hist=ser.groupby(level=0).hist()"
Pandas,Series,pandas.Series.to_pickle,"pandas.Series.to_pickle#Series.to_pickle(path,compression='infer',protocol=5,storage_options=None)[source]#Pickle (serialize) object to file.Parameters:pathstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. File path where
the pickled object will be stored.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epathf is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.protocolintInt which indicates which protocol should be used by the pickler,
default HIGHEST_PROTOCOL (see[1]paragraph 12.1.2). The possible
values are 0, 1, 2, 3, 4, 5. A negative value for the protocol
parameter is equivalent to setting its value to HIGHEST_PROTOCOL.[1]https://docs.python.org/3/library/pickle.html.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.See alsoread_pickleLoad pickled pandas object (or any object) from file.DataFrame.to_hdfWrite DataFrame to an HDF5 file.DataFrame.to_sqlWrite DataFrame to a SQL database.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.Examples>>>original_df=pd.DataFrame({""foo"":range(5),""bar"":range(5,10)})>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>original_df.to_pickle(""./dummy.pkl"")>>>unpickled_df=pd.read_pickle(""./dummy.pkl"")>>>unpickled_dffoo  bar0    0    51    1    62    2    73    3    84    4    9"
Pandas,Series,pandas.Series.to_csv,"pandas.Series.to_csv#Series.to_csv(path_or_buf=None,sep=',',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,mode='w',encoding=None,compression='infer',quoting=None,quotechar='""',lineterminator=None,chunksize=None,date_format=None,doublequote=True,escapechar=None,decimal='.',errors='strict',storage_options=None)[source]#Write object to a comma-separated values (csv) file.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string. If a non-binary file object is passed, it should
be opened withnewline=ff, disabling universal newlines. If a binary
file object is passed,modemight need to contain aebf.Changed in version 1.2.0:Support for binary file objects was introduced.sepstr, default e,fString of length 1. Field delimiter for the output file.na_repstr, default efMissing data representation.float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes
precedence over other numeric formatting parameters, like decimal.columnssequence, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, andheaderandindexare True, then the index names are used. A
sequence should be given if the object uses MultiIndex. If
False do not print fields for index names. Use index_label=False
for easier importing in R.mode{ewf, exf, eaf}, default ewfForwarded to eitheropen(mode=)orfsspec.open(mode=)to control
the file opening. Typical values include:ewf, truncate the file first.exf, exclusive creation, failing if the file already exists.eaf, append to the end of file if it exists.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to eutf-8f.encodingis not supported ifpath_or_bufis a non-binary file object.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epath_or_buff is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key emethodf as compression mode
and other entries as additional compression options if
compression mode is ezipf.Passing compression options as keys in dict is
supported for compression modes egzipf, ebz2f, ezstdf, and ezipf.Changed in version 1.2.0:Compression is supported for binary file objects.Changed in version 1.2.0:Previous versions forwarded dict entries for egzipf togzip.openinstead ofgzip.GzipFilewhich prevented
settingmtime.quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set afloat_formatthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC
will treat them as non-numeric.quotecharstr, default e""fString of length 1. Character used to quote fields.lineterminatorstr, optionalThe newline character or character sequence to use in the output
file. Defaults toos.linesep, which depends on the OS in which
this method is called (f\nf for linux, e\r\nf for Windows, i.e.).Changed in version 1.5.0:Previously was line_terminator, changed for consistency with
read_csv and the standard library ecsvf module.chunksizeint or NoneRows to write at a time.date_formatstr, default NoneFormat string for datetime objects.doublequotebool, default TrueControl quoting ofquotecharinside a field.escapecharstr, default NoneString of length 1. Character used to escapesepandquotecharwhen appropriate.decimalstr, default e.fCharacter recognized as decimal separator. E.g. use e,f for
European data.errorsstr, default estrictfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.Returns:None or strIf path_or_buf is None, returns the resulting csv format as a
string. Otherwise returns None.See alsoread_csvLoad a CSV file into a DataFrame.to_excelWrite DataFrame to an Excel file.Examples>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv(index=False)'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'Create eout.zipf containing eout.csvf>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first
need to create it using either Pathlib or os:>>>frompathlibimportPath>>>filepath=Path('folder/subfolder/out.csv')>>>filepath.parent.mkdir(parents=True,exist_ok=True)>>>df.to_csv(filepath)>>>importos>>>os.makedirs('folder/subfolder',exist_ok=True)>>>df.to_csv('folder/subfolder/out.csv')"
Pandas,Series,pandas.Series.to_dict,"pandas.Series.to_dict#Series.to_dict(into=<class'dict'>)[source]#Convert Series to {label -> value} dict or dict-like object.Parameters:intoclass, default dictThe collections.abc.Mapping subclass to use as the return
object. Can be the actual class or an empty
instance of the mapping type you want. If you want a
collections.defaultdict, you must pass it initialized.Returns:collections.abc.MappingKey-value representation of Series.Examples>>>s=pd.Series([1,2,3,4])>>>s.to_dict(){0: 1, 1: 2, 2: 3, 3: 4}>>>fromcollectionsimportOrderedDict,defaultdict>>>s.to_dict(OrderedDict)OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])>>>dd=defaultdict(list)>>>s.to_dict(dd)defaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})"
Pandas,Series,pandas.Series.to_excel,"pandas.Series.to_excel#Series.to_excel(excel_writer,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to
specify a target file name. To write to multiple sheets it is necessary to
create anExcelWriterobject with a target file name, and specify a sheet
in the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.
With all data written to the file it is necessary to save the changes.
Note that creating anExcelWriterobject with a file name that already
exists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default eSheet1fName of sheet which will contain DataFrame.na_repstr, default efMissing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=""%.2f""will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A
sequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, eopenpyxlf or exlsxwriterf. You can also set this
via the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default einffRepresentation for infinity (there is no native representation for
infinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that
is to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),
to_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further
data without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(""output.xlsx"")To specify the sheet name:>>>df1.to_excel(""output.xlsx"",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is
necessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,
you can pass theenginekeyword (the default engine is
automatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')"
Pandas,Series,pandas.Series.to_frame,"pandas.Series.to_frame#Series.to_frame(name=_NoDefault.no_default)[source]#Convert Series to DataFrame.Parameters:nameobject, optionalThe passed name should substitute for the series name (if it has
one).Returns:DataFrameDataFrame representation of Series.Examples>>>s=pd.Series([""a"",""b"",""c""],...name=""vals"")>>>s.to_frame()vals0    a1    b2    c"
Pandas,Series,pandas.Series.to_xarray,"pandas.Series.to_xarray#Series.to_xarray()[source]#Return an xarray object from the pandas object.Returns:xarray.DataArray or xarray.DatasetData in the pandas structure converted to Dataset if the object is
a DataFrame, or a DataArray if the object is a Series.See alsoDataFrame.to_hdfWrite DataFrame to an HDF5 file.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.NotesSee thexarray docsExamples>>>df=pd.DataFrame([('falcon','bird',389.0,2),...('parrot','bird',24.0,2),...('lion','mammal',80.5,4),...('monkey','mammal',np.nan,4)],...columns=['name','class','max_speed',...'num_legs'])>>>dfname   class  max_speed  num_legs0  falcon    bird      389.0         21  parrot    bird       24.0         22    lion  mammal       80.5         43  monkey  mammal        NaN         4>>>df.to_xarray()<xarray.Dataset>Dimensions:    (index: 4)Coordinates:* index      (index) int64 0 1 2 3Data variables:name       (index) object 'falcon' 'parrot' 'lion' 'monkey'class      (index) object 'bird' 'bird' 'mammal' 'mammal'max_speed  (index) float64 389.0 24.0 80.5 nannum_legs   (index) int64 2 2 4 4>>>df['max_speed'].to_xarray()<xarray.DataArray 'max_speed' (index: 4)>array([389. ,  24. ,  80.5,   nan])Coordinates:* index    (index) int64 0 1 2 3>>>dates=pd.to_datetime(['2018-01-01','2018-01-01',...'2018-01-02','2018-01-02'])>>>df_multiindex=pd.DataFrame({'date':dates,...'animal':['falcon','parrot',...'falcon','parrot'],...'speed':[350,18,361,15]})>>>df_multiindex=df_multiindex.set_index(['date','animal'])>>>df_multiindexspeeddate       animal2018-01-01 falcon    350parrot     182018-01-02 falcon    361parrot     15>>>df_multiindex.to_xarray()<xarray.Dataset>Dimensions:  (date: 2, animal: 2)Coordinates:* date     (date) datetime64[ns] 2018-01-01 2018-01-02* animal   (animal) object 'falcon' 'parrot'Data variables:speed    (date, animal) int64 350 18 361 15"
Pandas,Series,pandas.Series.to_hdf,"pandas.Series.to_hdf#Series.to_hdf(path_or_buf,key,mode='a',complevel=None,complib=None,append=False,format=None,index=True,min_itemsize=None,nan_rep=None,dropna=None,data_columns=None,errors='strict',encoding='UTF-8')[source]#Write the contained data to an HDF5 file using HDFStore.Hierarchical Data Format (HDF) is self-describing, allowing an
application to interpret the structure and contents of a file with
no outside information. One HDF file can hold a mix of related objects
which can be accessed as a group or as individual objects.In order to add another DataFrame or Series to an existing HDF file
please use append mode and a different a key.WarningOne can store a subclass ofDataFrameorSeriesto HDF5,
but the type of the subclass is lost upon storing.For more information see theuser guide.Parameters:path_or_bufstr or pandas.HDFStoreFile path or HDFStore object.keystrIdentifier for the group in the store.mode{eaf, ewf, er+f}, default eafMode to open file:ewf: write, a new file is created (an existing file with
the same name would be deleted).eaf: append, an existing file is opened for reading and
writing, and if the file does not exist it is created.er+f: similar to eaf, but the file must already exist.complevel{0-9}, default NoneSpecifies a compression level for data.
A value of 0 or None disables compression.complib{ezlibf, elzof, ebzip2f, ebloscf}, default ezlibfSpecifies the compression library to be used.
These additional compressors for Blosc are supported
(default if no compressor specified: eblosc:blosclzf):
{eblosc:blosclzf, eblosc:lz4f, eblosc:lz4hcf, eblosc:snappyf,
eblosc:zlibf, eblosc:zstdf}.
Specifying a compression library which is not available issues
a ValueError.appendbool, default FalseFor Table formats, append the input data to the existing.format{efixedf, etablef, None}, default efixedfPossible values:efixedf: Fixed format. Fast writing/reading. Not-appendable,
nor searchable.etablef: Table format. Write as a PyTables Table structure
which may perform worse but allow more flexible operations
like searching / selecting subsets of the data.If None, pd.get_option(eio.hdf.default_formatf) is checked,
followed by fallback to gfixedh.indexbool, default TrueWrite DataFrame index as a column.min_itemsizedict or int, optionalMap column names to minimum string sizes for columns.nan_repAny, optionalHow to represent null values as str.
Not allowed with append=True.dropnabool, default False, optionalRemove missing values.data_columnslist of columns or True, optionalList of columns to create as indexed data columns for on-disk
queries, or True to use all columns. By default only the axes
of the object are indexed. SeeQuery via data columns. for
more information.
Applicable only to format=ftablef.errorsstr, default estrictfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.encodingstr, default gUTF-8hSee alsoread_hdfRead from HDF file.DataFrame.to_orcWrite a DataFrame to the binary orc format.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.DataFrame.to_sqlWrite to a SQL table.DataFrame.to_featherWrite out feather-format for DataFrames.DataFrame.to_csvWrite out to a csv file.Examples>>>df=pd.DataFrame({'A':[1,2,3],'B':[4,5,6]},...index=['a','b','c'])>>>df.to_hdf('data.h5',key='df',mode='w')We can add another object to the same file:>>>s=pd.Series([1,2,3,4])>>>s.to_hdf('data.h5',key='s')Reading from HDF file:>>>pd.read_hdf('data.h5','df')A  Ba  1  4b  2  5c  3  6>>>pd.read_hdf('data.h5','s')0    11    22    33    4dtype: int64"
Pandas,Series,pandas.Series.to_sql,"pandas.Series.to_sql#Series.to_sql(name,con,*,schema=None,if_exists='fail',index=True,index_label=None,chunksize=None,dtype=None,method=None)[source]#Write records stored in a DataFrame to a SQL database.Databases supported by SQLAlchemy[1]are supported. Tables can be
newly created, appended to, or overwritten.Parameters:namestrName of SQL table.consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. Legacy support is provided for sqlite3.Connection objects. The user
is responsible for engine disposal and connection closure for the SQLAlchemy
connectable. Seehere.
If passing a sqlalchemy.engine.Connection which is already in a transaction,
the transaction will not be committed. If passing a sqlite3.Connection,
it will not be possible to roll back the record insertion.schemastr, optionalSpecify the schema (if database flavor supports this). If None, use
default schema.if_exists{efailf, ereplacef, eappendf}, default efailfHow to behave if the table already exists.fail: Raise a ValueError.replace: Drop the table before inserting new values.append: Insert new values to the existing table.indexbool, default TrueWrite DataFrame index as a column. Usesindex_labelas the column
name in the table.index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) andindexis True, then the index names are used.
A sequence should be given if the DataFrame uses MultiIndex.chunksizeint, optionalSpecify the number of rows in each batch to be written at a time.
By default, all rows will be written at once.dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the
keys should be the column names and the values should be the
SQLAlchemy types or strings for the sqlite3 legacy mode. If a
scalar is provided, it will be applied to all columns.method{None, emultif, callable}, optionalControls the SQL insertion clause used:None : Uses standard SQLINSERTclause (one per row).emultif: Pass multiple values in a singleINSERTclause.callable with signature(pd_table,conn,keys,data_iter).Details and a sample callable implementation can be found in the
sectioninsert method.Returns:None or intNumber of rows affected by to_sql. None is returned if the callable
passed intomethoddoes not return an integer number of rows.The number of returned rows affected is the sum of therowcountattribute ofsqlite3.Cursoror SQLAlchemy connectable which may not
reflect the exact number of written rows as stipulated in thesqlite3orSQLAlchemy.New in version 1.4.0.Raises:ValueErrorWhen the table already exists andif_existsis efailf (the
default).See alsoread_sqlRead a DataFrame from a table.NotesTimezone aware datetime columns will be written asTimestampwithtimezonetype with SQLAlchemy if supported by the
database. Otherwise, the datetimes will be stored as timezone unaware
timestamps local to the original timezone.References[1]https://docs.sqlalchemy.org[2]https://www.python.org/dev/peps/pep-0249/ExamplesCreate an in-memory SQLite database.>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine('sqlite://',echo=False)Create a table from scratch with 3 rows.>>>df=pd.DataFrame({'name':['User 1','User 2','User 3']})>>>dfname0  User 11  User 22  User 3>>>df.to_sql(name='users',con=engine)3>>>fromsqlalchemyimporttext>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]Ansqlalchemy.engine.Connectioncan also be passed tocon:>>>withengine.begin()asconnection:...df1=pd.DataFrame({'name':['User 4','User 5']})...df1.to_sql(name='users',con=connection,if_exists='append')2This is allowed to support operations that require that the same
DBAPI connection is used for the entire operation.>>>df2=pd.DataFrame({'name':['User 6','User 7']})>>>df2.to_sql(name='users',con=engine,if_exists='append')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),(0, 'User 4'), (1, 'User 5'), (0, 'User 6'),(1, 'User 7')]Overwrite the table with justdf2.>>>df2.to_sql(name='users',con=engine,if_exists='replace',...index_label='id')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 6'), (1, 'User 7')]Usemethodto define a callable insertion method to do nothing
if therefs a primary key conflict on a table in a PostgreSQL database.>>>fromsqlalchemy.dialects.postgresqlimportinsert>>>definsert_on_conflict_nothing(table,conn,keys,data_iter):...# ""a"" is the primary key in ""conflict_table""...data=[dict(zip(keys,row))forrowindata_iter]...stmt=insert(table.table).values(data).on_conflict_do_nothing(index_elements=[""a""])...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_nothing)0For MySQL, a callable to update columnsbandcif therefs a conflict
on a primary key.>>>fromsqlalchemy.dialects.mysqlimportinsert>>>definsert_on_conflict_update(table,conn,keys,data_iter):...# update columns ""b"" and ""c"" on primary key conflict...data=[dict(zip(keys,row))forrowindata_iter]...stmt=(...insert(table.table)....values(data)...)...stmt=stmt.on_duplicate_key_update(b=stmt.inserted.b,c=stmt.inserted.c)...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_update)2Specify the dtype (especially useful for integers with missing values).
Notice that while pandas is forced to store the data as floating point,
the database supports nullable integers. When fetching the data with
Python, we get back integer scalars.>>>df=pd.DataFrame({""A"":[1,None,2]})>>>dfA0  1.01  NaN2  2.0>>>fromsqlalchemy.typesimportInteger>>>df.to_sql(name='integers',con=engine,index=False,...dtype={""A"":Integer()})3>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM integers"")).fetchall()[(1,), (None,), (2,)]"
Pandas,Series,pandas.Series.to_json,"pandas.Series.to_json#Series.to_json(path_or_buf=None,orient=None,date_format=None,double_precision=10,force_ascii=True,date_unit='ms',default_handler=None,lines=False,compression='infer',index=None,indent=None,storage_options=None,mode='w')[source]#Convert the object to a JSON string.Note NaNfs and None will be converted to null and datetime objects
will be converted to UNIX timestamps.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string.orientstrIndication of expected JSON string format.Series:default is eindexfallowed values are: {esplitf, erecordsf, eindexf, etablef}.DataFrame:default is ecolumnsfallowed values are: {esplitf, erecordsf, eindexf, ecolumnsf,
evaluesf, etablef}.The format of the JSON string:esplitf : dict like {eindexf -> [index], ecolumnsf -> [columns],
edataf -> [values]}erecordsf : list like [{column -> value}, c , {column -> value}]eindexf : dict like {index -> {column -> value}}ecolumnsf : dict like {column -> {index -> value}}evaluesf : just the values arrayetablef : dict like {eschemaf: {schema}, edataf: {data}}Describing the data, where data component is likeorient='records'.date_format{None, eepochf, eisof}Type of date conversion. eepochf = epoch milliseconds,
eisof = ISO8601. The default depends on theorient. Fororient='table', the default is eisof. For all other orients,
the default is eepochf.double_precisionint, default 10The number of decimal places to use when encoding
floating point values. The possible maximal value is 15.
Passing double_precision greater than 15 will raise a ValueError.force_asciibool, default TrueForce encoded string to be ASCII.date_unitstr, default emsf (milliseconds)The time unit to encode to, governs timestamp and ISO8601
precision. One of esf, emsf, eusf, ensf for second, millisecond,
microsecond, and nanosecond respectively.default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a
suitable format for JSON. Should receive a single argument which is
the object to convert and return a serialisable object.linesbool, default FalseIf eorientf is erecordsf write out line-delimited json format. Will
throw ValueError if incorrect eorientf since others are not
list-like.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epath_or_buff is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.indexbool or None, default NoneThe index is only used when eorientf is esplitf, eindexf, ecolumnf,
or etablef. Of these, eindexf and ecolumnf do not supportindex=False.indentint, optionalLength of whitespace used to indent each record.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.modestr, default ewf (writing)Specify the IO mode for output when supplying a path_or_buf.
Accepted args are ewf (writing) and eaf (append) only.
mode=faf is only supported when lines is True and orient is erecordsf.Returns:None or strIf path_or_buf is None, returns the resulting json format as a
string. Otherwise returns None.See alsoread_jsonConvert a JSON string to pandas object.NotesThe behavior ofindent=0varies from the stdlib, which does not
indent the output but does insert newlines. Currently,indent=0and the defaultindent=Noneare equivalent in pandas, though this
may change in a future release.orient='table'contains a epandas_versionf field under eschemaf.
This stores the version ofpandasused in the latest revision of the
schema.Examples>>>fromjsonimportloads,dumps>>>df=pd.DataFrame(...[[""a"",""b""],[""c"",""d""]],...index=[""row 1"",""row 2""],...columns=[""col 1"",""col 2""],...)>>>result=df.to_json(orient=""split"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""columns"": [""col 1"",""col 2""],""index"": [""row 1"",""row 2""],""data"": [[""a"",""b""],[""c"",""d""]]}Encoding/decoding a Dataframe using'records'formatted JSON.
Note that index labels are not preserved with this encoding.>>>result=df.to_json(orient=""records"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[{""col 1"": ""a"",""col 2"": ""b""},{""col 1"": ""c"",""col 2"": ""d""}]Encoding/decoding a Dataframe using'index'formatted JSON:>>>result=df.to_json(orient=""index"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""row 1"": {""col 1"": ""a"",""col 2"": ""b""},""row 2"": {""col 1"": ""c"",""col 2"": ""d""}}Encoding/decoding a Dataframe using'columns'formatted JSON:>>>result=df.to_json(orient=""columns"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""col 1"": {""row 1"": ""a"",""row 2"": ""c""},""col 2"": {""row 1"": ""b"",""row 2"": ""d""}}Encoding/decoding a Dataframe using'values'formatted JSON:>>>result=df.to_json(orient=""values"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[[""a"",""b""],[""c"",""d""]]Encoding with Table Schema:>>>result=df.to_json(orient=""table"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""schema"": {""fields"": [{""name"": ""index"",""type"": ""string""},{""name"": ""col 1"",""type"": ""string""},{""name"": ""col 2"",""type"": ""string""}],""primaryKey"": [""index""],""pandas_version"": ""1.4.0""},""data"": [{""index"": ""row 1"",""col 1"": ""a"",""col 2"": ""b""},{""index"": ""row 2"",""col 1"": ""c"",""col 2"": ""d""}]}"
Pandas,Series,pandas.Series.to_string,"pandas.Series.to_string#Series.to_string(buf=None,na_rep='NaN',float_format=None,header=True,index=True,length=False,dtype=False,name=False,max_rows=None,min_rows=None)[source]#Render a string representation of the Series.Parameters:bufStringIO-like, optionalBuffer to write to.na_repstr, optionalString representation of NaN to use, default eNaNf.float_formatone-parameter function, optionalFormatter function to apply to columnsf elements if they are
floats, default None.headerbool, default TrueAdd the Series header (index name).indexbool, optionalAdd index (row) labels, default True.lengthbool, default FalseAdd the Series length.dtypebool, default FalseAdd the Series dtype.namebool, default FalseAdd the Series name if not None.max_rowsint, optionalMaximum number of rows to show before truncating. If None, show
all.min_rowsint, optionalThe number of rows to display in a truncated repr (when number
of rows is abovemax_rows).Returns:str or NoneString representation of Series ifbuf=None, otherwise None.Examples>>>ser=pd.Series([1,2,3]).to_string()>>>ser'0    1\n1    2\n2    3'"
Pandas,Series,pandas.Series.to_clipboard,"pandas.Series.to_clipboard#Series.to_clipboard(excel=True,sep=None,**kwargs)[source]#Copy object to the system clipboard.Write a text representation of object to the system clipboard.
This can be pasted into Excel, for example.Parameters:excelbool, default TrueProduce output in a csv format for easy pasting into excel.True, use the provided separator for csv pasting.False, write a string representation of the object to the clipboard.sepstr, default'\t'Field delimiter.**kwargsThese parameters will be passed to DataFrame.to_csv.See alsoDataFrame.to_csvWrite a DataFrame to a comma-separated values (csv) file.read_clipboardRead text from clipboard and pass to read_csv.NotesRequirements for your platform.Linux :xclip, orxsel(withPyQt4modules)Windows : nonemacOS : noneThis method uses the processes developed for the packagepyperclip. A
solution to render any output string format is given in the examples.ExamplesCopy the contents of a DataFrame to the clipboard.>>>df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['A','B','C'])>>>df.to_clipboard(sep=',')...# Wrote the following to the system clipboard:...# ,A,B,C...# 0,1,2,3...# 1,4,5,6We can omit the index by passing the keywordindexand setting
it to false.>>>df.to_clipboard(sep=',',index=False)...# Wrote the following to the system clipboard:...# A,B,C...# 1,2,3...# 4,5,6Using the originalpyperclippackage for any string output format.importpypercliphtml=df.style.to_html()pyperclip.copy(html)"
Pandas,Series,pandas.Series.to_latex,"pandas.Series.to_latex#Series.to_latex(buf=None,columns=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,bold_rows=False,column_format=None,longtable=None,escape=None,encoding=None,decimal='.',multicolumn=None,multicolumn_format=None,multirow=None,caption=None,label=None,position=None)[source]#Render object to a LaTeX tabular, longtable, or nested table.Requires\usepackage{{booktabs}}. The output can be copy/pasted
into a main LaTeX document or read from an external file
with\input{{table.tex}}.Changed in version 1.2.0:Added position argument, changed meaning of caption argument.Changed in version 2.0.0:Refactored to use the Styler implementation via jinja2 templating.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnslist of label, optionalThe subset of columns to write. Writes all columns by default.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given,
it is assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).na_repstr, default eNaNfMissing data representation.formatterslist of functions or dict of {{str: function}}, optionalFormatter functions to apply to columnsf elements by position or
name. The result of each function must be a unicode string.
List must be of length equal to the number of columns.float_formatone-parameter function or str, optional, default NoneFormatter for floating point numbers. For examplefloat_format=""%.2f""andfloat_format=""{{:0.2f}}"".formatwill
both result in 0.1234 being formatted as 0.12.sparsifybool, optionalSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row. By default, the value will be
read from the config module.index_namesbool, default TruePrints the names of the indexes.bold_rowsbool, default FalseMake the row labels bold in the output.column_formatstr, optionalThe columns format as specified inLaTeX table formate.g. erclf for 3
columns. By default, elf will be used for all columns except
columns of numbers, which default to erf.longtablebool, optionalUse a longtable environment instead of tabular. Requires
adding a usepackage{{longtable}} to your LaTeX preamble.
By default, the value will be read from the pandas config
module, and set toTrueif the optionstyler.latex.environmentisglongtableh.Changed in version 2.0.0:The pandas option affecting this argument has changed.escapebool, optionalBy default, the value will be read from the pandas config
module and set toTrueif the optionstyler.format.escapeisglatexh. When set to False prevents from escaping latex special
characters in column names.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value toFalse.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to eutf-8f.decimalstr, default e.fCharacter recognized as decimal separator, e.g. e,f in Europe.multicolumnbool, default TrueUse multicolumn to enhance MultiIndex columns.
The default will be read from the config module, and is set
as the optionstyler.sparse.columns.Changed in version 2.0.0:The pandas option affecting this argument has changed.multicolumn_formatstr, default erfThe alignment for multicolumns, similar tocolumn_formatThe default will be read from the config module, and is set as the optionstyler.latex.multicol_align.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value to grh.multirowbool, default TrueUse multirow to enhance MultiIndex rows. Requires adding a
usepackage{{multirow}} to your LaTeX preamble. Will print
centered labels (instead of top-aligned) across the contained
rows, separating groups via clines. The default will be read
from the pandas config module, and is set as the optionstyler.sparse.index.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value toTrue.captionstr or tuple, optionalTuple (full_caption, short_caption),
which results in\caption[short_caption]{{full_caption}};
if a single string is passed, no short caption will be set.Changed in version 1.2.0:Optionally allow caption to be a tuple(full_caption,short_caption).labelstr, optionalThe LaTeX label to be placed inside\label{{}}in the output.
This is used with\ref{{}}in the main.texfile.positionstr, optionalThe LaTeX positional argument for tables, to be placed after\begin{{}}in the output.New in version 1.2.0.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns None.See alsoio.formats.style.Styler.to_latexRender a DataFrame to LaTeX with conditional formatting.DataFrame.to_stringRender a DataFrame to a console-friendly tabular output.DataFrame.to_htmlRender a DataFrame as an HTML table.NotesAs of v2.0.0 this method has changed to use the Styler implementation as
part ofStyler.to_latex()viajinja2templating. This means
thatjinja2is a requirement, and needs to be installed, for this method
to function. It is advised that users switch to using Styler, since that
implementation is more frequently updated and contains much more
flexibility with the output.ExamplesConvert a general DataFrame to LaTeX with formatting:>>>df=pd.DataFrame(dict(name=['Raphael','Donatello'],...age=[26,45],...height=[181.23,177.65]))>>>print(df.to_latex(index=False,...formatters={""name"":str.upper},...float_format=""{:.1f}"".format,...))\begin{tabular}{lrr}\toprulename & age & height \\\midruleRAPHAEL & 26 & 181.2 \\DONATELLO & 45 & 177.7 \\\bottomrule\end{tabular}"
Pandas,Series,pandas.Series.to_markdown,"pandas.Series.to_markdown#Series.to_markdown(buf=None,mode='wt',index=True,storage_options=None,**kwargs)[source]#Print Series in Markdown-friendly format.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.modestr, optionalMode in which file is opened, gwth by default.indexbool, optional, default TrueAdd index (row) labels.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.**kwargsThese parameters will be passed totabulate.Returns:strSeries in Markdown-friendly format.NotesRequires thetabulatepackage.Examples>>>s=pd.Series([""elk"",""pig"",""dog"",""quetzal""],name=""animal"")>>>print(s.to_markdown())|    | animal   ||---:|:---------||  0 | elk      ||  1 | pig      ||  2 | dog      ||  3 | quetzal  |Output markdown with a tabulate option.>>>print(s.to_markdown(tablefmt=""grid""))+----+----------+|    | animal   |+====+==========+|  0 | elk      |+----+----------+|  1 | pig      |+----+----------+|  2 | dog      |+----+----------+|  3 | quetzal  |+----+----------+"
