ƒ‰ƒCƒuƒ‰ƒŠ–¼,Í,ß,“à—e
Pandas,DataFrame,pandas.DataFrame,"pandas.DataFrame#classpandas.DataFrame(data=None,index=None,columns=None,dtype=None,copy=None)[source]#Two-dimensional, size-mutable, potentially heterogeneous tabular data.Data structure also contains labeled axes (rows and columns).
Arithmetic operations align on both row and column labels. Can be
thought of as a dict-like container for Series objects. The primary
pandas data structure.Parameters:datandarray (structured or homogeneous), Iterable, dict, or DataFrameDict can contain Series, arrays, constants, dataclass or list-like objects. If
data is a dict, column order follows insertion-order. If a dict contains Series
which have an index defined, it is aligned by its index. This alignment also
occurs if data is a Series or a DataFrame itself. Alignment is done on
Series/DataFrame inputs.If data is a list of dicts, column order follows insertion-order.indexIndex or array-likeIndex to use for resulting frame. Will default to RangeIndex if
no indexing information part of input data and no index provided.columnsIndex or array-likeColumn labels to use for resulting frame when data does not have them,
defaulting to RangeIndex(0, 1, 2, c, n). If data contains column labels,
will perform column selection instead.dtypedtype, default NoneData type to force. Only a single dtype is allowed. If None, infer.copybool or None, default NoneCopy data from inputs.
For dict data, the default of None behaves likecopy=True. For DataFrame
or 2d ndarray input, the default of None behaves likecopy=False.
If data is a dict containing one or more Series (possibly of different dtypes),copy=Falsewill ensure that these inputs are not copied.Changed in version 1.3.0.See alsoDataFrame.from_recordsConstructor from tuples, also record arrays.DataFrame.from_dictFrom dicts of Series, arrays, or dicts.read_csvRead a comma-separated values (csv) file into DataFrame.read_tableRead general delimited file into DataFrame.read_clipboardRead text from clipboard into DataFrame.NotesPlease reference theUser Guidefor more information.ExamplesConstructing DataFrame from a dictionary.>>>d={'col1':[1,2],'col2':[3,4]}>>>df=pd.DataFrame(data=d)>>>dfcol1  col20     1     31     2     4Notice that the inferred dtype is int64.>>>df.dtypescol1    int64col2    int64dtype: objectTo enforce a single dtype:>>>df=pd.DataFrame(data=d,dtype=np.int8)>>>df.dtypescol1    int8col2    int8dtype: objectConstructing DataFrame from a dictionary including Series:>>>d={'col1':[0,1,2,3],'col2':pd.Series([2,3],index=[2,3])}>>>pd.DataFrame(data=d,index=[0,1,2,3])col1  col20     0   NaN1     1   NaN2     2   2.03     3   3.0Constructing DataFrame from numpy ndarray:>>>df2=pd.DataFrame(np.array([[1,2,3],[4,5,6],[7,8,9]]),...columns=['a','b','c'])>>>df2a  b  c0  1  2  31  4  5  62  7  8  9Constructing DataFrame from a numpy ndarray that has labeled columns:>>>data=np.array([(1,2,3),(4,5,6),(7,8,9)],...dtype=[(""a"",""i4""),(""b"",""i4""),(""c"",""i4"")])>>>df3=pd.DataFrame(data,columns=['c','a'])...>>>df3c  a0  3  11  6  42  9  7Constructing DataFrame from dataclass:>>>fromdataclassesimportmake_dataclass>>>Point=make_dataclass(""Point"",[(""x"",int),(""y"",int)])>>>pd.DataFrame([Point(0,0),Point(0,3),Point(2,3)])x  y0  0  01  0  32  2  3Constructing DataFrame from Series/DataFrame:>>>ser=pd.Series([1,2,3],index=[""a"",""b"",""c""])>>>df=pd.DataFrame(data=ser,index=[""a"",""c""])>>>df0a  1c  3>>>df1=pd.DataFrame([1,2,3],index=[""a"",""b"",""c""],columns=[""x""])>>>df2=pd.DataFrame(data=df1,index=[""a"",""c""])>>>df2xa  1c  3AttributesTThe transpose of the DataFrame.atAccess a single value for a row/column label pair.attrsDictionary of global attributes of this dataset.axesReturn a list representing the axes of the DataFrame.columnsThe column labels of the DataFrame.dtypesReturn the dtypes in the DataFrame.emptyIndicator whether Series/DataFrame is empty.flagsGet the properties associated with this pandas object.iatAccess a single value for a row/column pair by integer position.ilocPurely integer-location based indexing for selection by position.indexThe index (row labels) of the DataFrame.locAccess a group of rows and columns by label(s) or a boolean array.ndimReturn an int representing the number of axes / array dimensions.shapeReturn a tuple representing the dimensionality of the DataFrame.sizeReturn an int representing the number of elements in this object.styleReturns a Styler object.valuesReturn a Numpy representation of the DataFrame.Methodsabs()Return a Series/DataFrame with absolute numeric value of each element.add(other[,?axis,?level,?fill_value])Get Addition of dataframe and other, element-wise (binary operatoradd).add_prefix(prefix[,?axis])Prefix labels with stringprefix.add_suffix(suffix[,?axis])Suffix labels with stringsuffix.agg([func,?axis])Aggregate using one or more operations over the specified axis.aggregate([func,?axis])Aggregate using one or more operations over the specified axis.align(other[,?join,?axis,?level,?copy,?...])Align two objects on their axes with the specified join method.all([axis,?bool_only,?skipna])Return whether all elements are True, potentially over an axis.any(*[,?axis,?bool_only,?skipna])Return whether any element is True, potentially over an axis.apply(func[,?axis,?raw,?result_type,?args,?...])Apply a function along an axis of the DataFrame.applymap(func[,?na_action])(DEPRECATED) Apply a function to a Dataframe elementwise.asfreq(freq[,?method,?how,?normalize,?...])Convert time series to specified frequency.asof(where[,?subset])Return the last row(s) without any NaNs beforewhere.assign(**kwargs)Assign new columns to a DataFrame.astype(dtype[,?copy,?errors])Cast a pandas object to a specified dtypedtype.at_time(time[,?asof,?axis])Select values at particular time of day (e.g., 9:30AM).backfill(*[,?axis,?inplace,?limit,?downcast])(DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap.between_time(start_time,?end_time[,?...])Select values between particular times of the day (e.g., 9:00-9:30 AM).bfill(*[,?axis,?inplace,?limit,?downcast])Fill NA/NaN values by using the next valid observation to fill the gap.bool()(DEPRECATED) Return the bool of a single element Series or DataFrame.boxplot([column,?by,?ax,?fontsize,?rot,?...])Make a box plot from DataFrame columns.clip([lower,?upper,?axis,?inplace])Trim values at input threshold(s).combine(other,?func[,?fill_value,?overwrite])Perform column-wise combine with another DataFrame.combine_first(other)Update null elements with value in the same location inother.compare(other[,?align_axis,?keep_shape,?...])Compare to another DataFrame and show the differences.convert_dtypes([infer_objects,?...])Convert columns to the best possible dtypes using dtypes supportingpd.NA.copy([deep])Make a copy of this object's indices and data.corr([method,?min_periods,?numeric_only])Compute pairwise correlation of columns, excluding NA/null values.corrwith(other[,?axis,?drop,?method,?...])Compute pairwise correlation.count([axis,?numeric_only])Count non-NA cells for each column or row.cov([min_periods,?ddof,?numeric_only])Compute pairwise covariance of columns, excluding NA/null values.cummax([axis,?skipna])Return cumulative maximum over a DataFrame or Series axis.cummin([axis,?skipna])Return cumulative minimum over a DataFrame or Series axis.cumprod([axis,?skipna])Return cumulative product over a DataFrame or Series axis.cumsum([axis,?skipna])Return cumulative sum over a DataFrame or Series axis.describe([percentiles,?include,?exclude])Generate descriptive statistics.diff([periods,?axis])First discrete difference of element.div(other[,?axis,?level,?fill_value])Get Floating division of dataframe and other, element-wise (binary operatortruediv).divide(other[,?axis,?level,?fill_value])Get Floating division of dataframe and other, element-wise (binary operatortruediv).dot(other)Compute the matrix multiplication between the DataFrame and other.drop([labels,?axis,?index,?columns,?level,?...])Drop specified labels from rows or columns.drop_duplicates([subset,?keep,?inplace,?...])Return DataFrame with duplicate rows removed.droplevel(level[,?axis])Return Series/DataFrame with requested index / column level(s) removed.dropna(*[,?axis,?how,?thresh,?subset,?...])Remove missing values.duplicated([subset,?keep])Return boolean Series denoting duplicate rows.eq(other[,?axis,?level])Get Equal to of dataframe and other, element-wise (binary operatoreq).equals(other)Test whether two objects contain the same elements.eval(expr,?*[,?inplace])Evaluate a string describing operations on DataFrame columns.ewm([com,?span,?halflife,?alpha,?...])Provide exponentially weighted (EW) calculations.expanding([min_periods,?axis,?method])Provide expanding window calculations.explode(column[,?ignore_index])Transform each element of a list-like to a row, replicating index values.ffill(*[,?axis,?inplace,?limit,?downcast])Fill NA/NaN values by propagating the last valid observation to next valid.fillna([value,?method,?axis,?inplace,?...])Fill NA/NaN values using the specified method.filter([items,?like,?regex,?axis])Subset the dataframe rows or columns according to the specified index labels.first(offset)(DEPRECATED) Select initial periods of time series data based on a date offset.first_valid_index()Return index for first non-NA value or None, if no non-NA value is found.floordiv(other[,?axis,?level,?fill_value])Get Integer division of dataframe and other, element-wise (binary operatorfloordiv).from_dict(data[,?orient,?dtype,?columns])Construct DataFrame from dict of array-like or dicts.from_records(data[,?index,?exclude,?...])Convert structured or record ndarray to DataFrame.ge(other[,?axis,?level])Get Greater than or equal to of dataframe and other, element-wise (binary operatorge).get(key[,?default])Get item from object for given key (ex: DataFrame column).groupby([by,?axis,?level,?as_index,?sort,?...])Group DataFrame using a mapper or by a Series of columns.gt(other[,?axis,?level])Get Greater than of dataframe and other, element-wise (binary operatorgt).head([n])Return the firstnrows.hist([column,?by,?grid,?xlabelsize,?xrot,?...])Make a histogram of the DataFrame's columns.idxmax([axis,?skipna,?numeric_only])Return index of first occurrence of maximum over requested axis.idxmin([axis,?skipna,?numeric_only])Return index of first occurrence of minimum over requested axis.infer_objects([copy])Attempt to infer better dtypes for object columns.info([verbose,?buf,?max_cols,?memory_usage,?...])Print a concise summary of a DataFrame.insert(loc,?column,?value[,?allow_duplicates])Insert column into DataFrame at specified location.interpolate([method,?axis,?limit,?inplace,?...])Fill NaN values using an interpolation method.isetitem(loc,?value)Set the given value in the column with positionloc.isin(values)Whether each element in the DataFrame is contained in values.isna()Detect missing values.isnull()DataFrame.isnull is an alias for DataFrame.isna.items()Iterate over (column name, Series) pairs.iterrows()Iterate over DataFrame rows as (index, Series) pairs.itertuples([index,?name])Iterate over DataFrame rows as namedtuples.join(other[,?on,?how,?lsuffix,?rsuffix,?...])Join columns of another DataFrame.keys()Get the 'info axis' (see Indexing for more).kurt([axis,?skipna,?numeric_only])Return unbiased kurtosis over requested axis.kurtosis([axis,?skipna,?numeric_only])Return unbiased kurtosis over requested axis.last(offset)(DEPRECATED) Select final periods of time series data based on a date offset.last_valid_index()Return index for last non-NA value or None, if no non-NA value is found.le(other[,?axis,?level])Get Less than or equal to of dataframe and other, element-wise (binary operatorle).lt(other[,?axis,?level])Get Less than of dataframe and other, element-wise (binary operatorlt).map(func[,?na_action])Apply a function to a Dataframe elementwise.mask(cond[,?other,?inplace,?axis,?level])Replace values where the condition is True.max([axis,?skipna,?numeric_only])Return the maximum of the values over the requested axis.mean([axis,?skipna,?numeric_only])Return the mean of the values over the requested axis.median([axis,?skipna,?numeric_only])Return the median of the values over the requested axis.melt([id_vars,?value_vars,?var_name,?...])Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.memory_usage([index,?deep])Return the memory usage of each column in bytes.merge(right[,?how,?on,?left_on,?right_on,?...])Merge DataFrame or named Series objects with a database-style join.min([axis,?skipna,?numeric_only])Return the minimum of the values over the requested axis.mod(other[,?axis,?level,?fill_value])Get Modulo of dataframe and other, element-wise (binary operatormod).mode([axis,?numeric_only,?dropna])Get the mode(s) of each element along the selected axis.mul(other[,?axis,?level,?fill_value])Get Multiplication of dataframe and other, element-wise (binary operatormul).multiply(other[,?axis,?level,?fill_value])Get Multiplication of dataframe and other, element-wise (binary operatormul).ne(other[,?axis,?level])Get Not equal to of dataframe and other, element-wise (binary operatorne).nlargest(n,?columns[,?keep])Return the firstnrows ordered bycolumnsin descending order.notna()Detect existing (non-missing) values.notnull()DataFrame.notnull is an alias for DataFrame.notna.nsmallest(n,?columns[,?keep])Return the firstnrows ordered bycolumnsin ascending order.nunique([axis,?dropna])Count number of distinct elements in specified axis.pad(*[,?axis,?inplace,?limit,?downcast])(DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid.pct_change([periods,?fill_method,?limit,?freq])Fractional change between the current and a prior element.pipe(func,?*args,?**kwargs)Apply chainable functions that expect Series or DataFrames.pivot(*,?columns[,?index,?values])Return reshaped DataFrame organized by given index / column values.pivot_table([values,?index,?columns,?...])Create a spreadsheet-style pivot table as a DataFrame.plotalias ofPlotAccessorpop(item)Return item and drop from frame.pow(other[,?axis,?level,?fill_value])Get Exponential power of dataframe and other, element-wise (binary operatorpow).prod([axis,?skipna,?numeric_only,?min_count])Return the product of the values over the requested axis.product([axis,?skipna,?numeric_only,?min_count])Return the product of the values over the requested axis.quantile([q,?axis,?numeric_only,?...])Return values at the given quantile over requested axis.query(expr,?*[,?inplace])Query the columns of a DataFrame with a boolean expression.radd(other[,?axis,?level,?fill_value])Get Addition of dataframe and other, element-wise (binary operatorradd).rank([axis,?method,?numeric_only,?...])Compute numerical data ranks (1 through n) along axis.rdiv(other[,?axis,?level,?fill_value])Get Floating division of dataframe and other, element-wise (binary operatorrtruediv).reindex([labels,?index,?columns,?axis,?...])Conform DataFrame to new index with optional filling logic.reindex_like(other[,?method,?copy,?limit,?...])Return an object with matching indices as other object.rename([mapper,?index,?columns,?axis,?copy,?...])Rename columns or index labels.rename_axis([mapper,?index,?columns,?axis,?...])Set the name of the axis for the index or columns.reorder_levels(order[,?axis])Rearrange index levels using input order.replace([to_replace,?value,?inplace,?limit,?...])Replace values given into_replacewithvalue.resample(rule[,?axis,?closed,?label,?...])Resample time-series data.reset_index([level,?drop,?inplace,?...])Reset the index, or a level of it.rfloordiv(other[,?axis,?level,?fill_value])Get Integer division of dataframe and other, element-wise (binary operatorrfloordiv).rmod(other[,?axis,?level,?fill_value])Get Modulo of dataframe and other, element-wise (binary operatorrmod).rmul(other[,?axis,?level,?fill_value])Get Multiplication of dataframe and other, element-wise (binary operatorrmul).rolling(window[,?min_periods,?center,?...])Provide rolling window calculations.round([decimals])Round a DataFrame to a variable number of decimal places.rpow(other[,?axis,?level,?fill_value])Get Exponential power of dataframe and other, element-wise (binary operatorrpow).rsub(other[,?axis,?level,?fill_value])Get Subtraction of dataframe and other, element-wise (binary operatorrsub).rtruediv(other[,?axis,?level,?fill_value])Get Floating division of dataframe and other, element-wise (binary operatorrtruediv).sample([n,?frac,?replace,?weights,?...])Return a random sample of items from an axis of object.select_dtypes([include,?exclude])Return a subset of the DataFrame's columns based on the column dtypes.sem([axis,?skipna,?ddof,?numeric_only])Return unbiased standard error of the mean over requested axis.set_axis(labels,?*[,?axis,?copy])Assign desired index to given axis.set_flags(*[,?copy,?allows_duplicate_labels])Return a new object with updated flags.set_index(keys,?*[,?drop,?append,?inplace,?...])Set the DataFrame index using existing columns.shift([periods,?freq,?axis,?fill_value,?suffix])Shift index by desired number of periods with an optional timefreq.skew([axis,?skipna,?numeric_only])Return unbiased skew over requested axis.sort_index(*[,?axis,?level,?ascending,?...])Sort object by labels (along an axis).sort_values(by,?*[,?axis,?ascending,?...])Sort by the values along either axis.sparsealias ofSparseFrameAccessorsqueeze([axis])Squeeze 1 dimensional axis objects into scalars.stack([level,?dropna,?sort,?future_stack])Stack the prescribed level(s) from columns to index.std([axis,?skipna,?ddof,?numeric_only])Return sample standard deviation over requested axis.sub(other[,?axis,?level,?fill_value])Get Subtraction of dataframe and other, element-wise (binary operatorsub).subtract(other[,?axis,?level,?fill_value])Get Subtraction of dataframe and other, element-wise (binary operatorsub).sum([axis,?skipna,?numeric_only,?min_count])Return the sum of the values over the requested axis.swapaxes(axis1,?axis2[,?copy])(DEPRECATED) Interchange axes and swap values axes appropriately.swaplevel([i,?j,?axis])Swap levels i and j in aMultiIndex.tail([n])Return the lastnrows.take(indices[,?axis])Return the elements in the givenpositionalindices along an axis.to_clipboard([excel,?sep])Copy object to the system clipboard.to_csv([path_or_buf,?sep,?na_rep,?...])Write object to a comma-separated values (csv) file.to_dict([orient,?into,?index])Convert the DataFrame to a dictionary.to_excel(excel_writer[,?sheet_name,?na_rep,?...])Write object to an Excel sheet.to_feather(path,?**kwargs)Write a DataFrame to the binary Feather format.to_gbq(destination_table[,?project_id,?...])Write a DataFrame to a Google BigQuery table.to_hdf(path_or_buf,?key[,?mode,?complevel,?...])Write the contained data to an HDF5 file using HDFStore.to_html([buf,?columns,?col_space,?header,?...])Render a DataFrame as an HTML table.to_json([path_or_buf,?orient,?date_format,?...])Convert the object to a JSON string.to_latex([buf,?columns,?header,?index,?...])Render object to a LaTeX tabular, longtable, or nested table.to_markdown([buf,?mode,?index,?storage_options])Print DataFrame in Markdown-friendly format.to_numpy([dtype,?copy,?na_value])Convert the DataFrame to a NumPy array.to_orc([path,?engine,?index,?engine_kwargs])Write a DataFrame to the ORC format.to_parquet([path,?engine,?compression,?...])Write a DataFrame to the binary parquet format.to_period([freq,?axis,?copy])Convert DataFrame from DatetimeIndex to PeriodIndex.to_pickle(path[,?compression,?protocol,?...])Pickle (serialize) object to file.to_records([index,?column_dtypes,?index_dtypes])Convert DataFrame to a NumPy record array.to_sql(name,?con,?*[,?schema,?if_exists,?...])Write records stored in a DataFrame to a SQL database.to_stata(path,?*[,?convert_dates,?...])Export DataFrame object to Stata dta format.to_string([buf,?columns,?col_space,?header,?...])Render a DataFrame to a console-friendly tabular output.to_timestamp([freq,?how,?axis,?copy])Cast to DatetimeIndex of timestamps, atbeginningof period.to_xarray()Return an xarray object from the pandas object.to_xml([path_or_buffer,?index,?root_name,?...])Render a DataFrame to an XML document.transform(func[,?axis])Callfuncon self producing a DataFrame with the same axis shape as self.transpose(*args[,?copy])Transpose index and columns.truediv(other[,?axis,?level,?fill_value])Get Floating division of dataframe and other, element-wise (binary operatortruediv).truncate([before,?after,?axis,?copy])Truncate a Series or DataFrame before and after some index value.tz_convert(tz[,?axis,?level,?copy])Convert tz-aware axis to target time zone.tz_localize(tz[,?axis,?level,?copy,?...])Localize tz-naive index of a Series or DataFrame to target time zone.unstack([level,?fill_value,?sort])Pivot a level of the (necessarily hierarchical) index labels.update(other[,?join,?overwrite,?...])Modify in place using non-NA values from another DataFrame.value_counts([subset,?normalize,?sort,?...])Return a Series containing the frequency of each distinct row in the Dataframe.var([axis,?skipna,?ddof,?numeric_only])Return unbiased variance over requested axis.where(cond[,?other,?inplace,?axis,?level])Replace values where the condition is False.xs(key[,?axis,?level,?drop_level])Return cross-section from the Series/DataFrame."
Pandas,DataFrame,pandas.DataFrame.index,"pandas.DataFrame.index#DataFrame.index#The index (row labels) of the DataFrame.The index of a DataFrame is a series of labels that identify each row.
The labels can be integers, strings, or any other hashable type. The index
is used for label-based access and alignment, and can be accessed or
modified using this attribute.Returns:pandas.IndexThe index labels of the DataFrame.See alsoDataFrame.columnsThe column labels of the DataFrame.DataFrame.to_numpyConvert the DataFrame to a NumPy array.Examples>>>df=pd.DataFrame({'Name':['Alice','Bob','Aritra'],...'Age':[25,30,35],...'Location':['Seattle','New York','Kona']},...index=([10,20,30]))>>>df.indexIndex([10, 20, 30], dtype='int64')In this example, we create a DataFrame with 3 rows and 3 columns,
including Name, Age, and Location information. We set the index labels to
be the integers 10, 20, and 30. We then access theindexattribute of the
DataFrame, which returns anIndexobject containing the index labels.>>>df.index=[100,200,300]>>>dfName  Age Location100  Alice   25  Seattle200    Bob   30 New York300  Aritra  35    KonaIn this example, we modify the index labels of the DataFrame by assigning
a new list of labels to theindexattribute. The DataFrame is then
updated with the new labels, and the output shows the modified DataFrame."
Pandas,DataFrame,pandas.DataFrame.columns,"pandas.DataFrame.columns#DataFrame.columns#The column labels of the DataFrame.Examples>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>dfA  B0    1  31    2  4>>>df.columnsIndex(['A', 'B'], dtype='object')"
Pandas,DataFrame,pandas.DataFrame.dtypes,"pandas.DataFrame.dtypes#propertyDataFrame.dtypes[source]#Return the dtypes in the DataFrame.This returns a Series with the data type of each column.
The resultfs index is the original DataFramefs columns. Columns
with mixed types are stored with theobjectdtype. Seethe User Guidefor more.Returns:pandas.SeriesThe data type of each column.Examples>>>df=pd.DataFrame({'float':[1.0],...'int':[1],...'datetime':[pd.Timestamp('20180310')],...'string':['foo']})>>>df.dtypesfloat              float64int                  int64datetime    datetime64[ns]string              objectdtype: object"
Pandas,DataFrame,pandas.DataFrame.info,"pandas.DataFrame.info#DataFrame.info(verbose=None,buf=None,max_cols=None,memory_usage=None,show_counts=None)[source]#Print a concise summary of a DataFrame.This method prints information about a DataFrame including
the index dtype and columns, non-null values and memory usage.Parameters:verbosebool, optionalWhether to print the full summary. By default, the setting inpandas.options.display.max_info_columnsis followed.bufwritable buffer, defaults to sys.stdoutWhere to send the output. By default, the output is printed to
sys.stdout. Pass a writable buffer if you need to further process
the output.max_colsint, optionalWhen to switch from the verbose to the truncated output. If the
DataFrame has more thanmax_colscolumns, the truncated output
is used. By default, the setting inpandas.options.display.max_info_columnsis used.memory_usagebool, str, optionalSpecifies whether total memory usage of the DataFrame
elements (including the index) should be displayed. By default,
this follows thepandas.options.display.memory_usagesetting.True always show memory usage. False never shows memory usage.
A value of edeepf is equivalent to gTrue with deep introspectionh.
Memory usage is shown in human-readable units (base-2
representation). Without deep introspection a memory estimation is
made based in column dtype and number of rows assuming values
consume the same memory amount for corresponding dtypes. With deep
memory introspection, a real memory usage calculation is performed
at the cost of computational resources. See theFrequently Asked Questionsfor more
details.show_countsbool, optionalWhether to show the non-null counts. By default, this is shown
only if the DataFrame is smaller thanpandas.options.display.max_info_rowsandpandas.options.display.max_info_columns. A value of True always
shows the counts, and False never shows the counts.Returns:NoneThis method prints a summary of a DataFrame and returns None.See alsoDataFrame.describeGenerate descriptive statistics of DataFrame columns.DataFrame.memory_usageMemory usage of DataFrame columns.Examples>>>int_values=[1,2,3,4,5]>>>text_values=['alpha','beta','gamma','delta','epsilon']>>>float_values=[0.0,0.25,0.5,0.75,1.0]>>>df=pd.DataFrame({""int_col"":int_values,""text_col"":text_values,...""float_col"":float_values})>>>dfint_col text_col  float_col0        1    alpha       0.001        2     beta       0.252        3    gamma       0.503        4    delta       0.754        5  epsilon       1.00Prints information of all columns:>>>df.info(verbose=True)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Data columns (total 3 columns):#   Column     Non-Null Count  Dtype---  ------     --------------  -----0   int_col    5 non-null      int641   text_col   5 non-null      object2   float_col  5 non-null      float64dtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPrints a summary of columns count and its dtypes but not per column
information:>>>df.info(verbose=False)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Columns: 3 entries, int_col to float_coldtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPipe output of DataFrame.info to buffer instead of sys.stdout, get
buffer content and writes to a text file:>>>importio>>>buffer=io.StringIO()>>>df.info(buf=buffer)>>>s=buffer.getvalue()>>>withopen(""df_info.txt"",""w"",...encoding=""utf-8"")asf:...f.write(s)260Thememory_usageparameter allows deep introspection mode, specially
useful for big DataFrames and fine-tune memory optimization:>>>random_strings_array=np.random.choice(['a','b','c'],10**6)>>>df=pd.DataFrame({...'column_1':np.random.choice(['a','b','c'],10**6),...'column_2':np.random.choice(['a','b','c'],10**6),...'column_3':np.random.choice(['a','b','c'],10**6)...})>>>df.info()<class 'pandas.core.frame.DataFrame'>RangeIndex: 1000000 entries, 0 to 999999Data columns (total 3 columns):#   Column    Non-Null Count    Dtype---  ------    --------------    -----0   column_1  1000000 non-null  object1   column_2  1000000 non-null  object2   column_3  1000000 non-null  objectdtypes: object(3)memory usage: 22.9+ MB>>>df.info(memory_usage='deep')<class 'pandas.core.frame.DataFrame'>RangeIndex: 1000000 entries, 0 to 999999Data columns (total 3 columns):#   Column    Non-Null Count    Dtype---  ------    --------------    -----0   column_1  1000000 non-null  object1   column_2  1000000 non-null  object2   column_3  1000000 non-null  objectdtypes: object(3)memory usage: 165.9 MB"
Pandas,DataFrame,pandas.DataFrame.select_dtypes,"pandas.DataFrame.select_dtypes#DataFrame.select_dtypes(include=None,exclude=None)[source]#Return a subset of the DataFramefs columns based on the column dtypes.Parameters:include, excludescalar or list-likeA selection of dtypes or strings to be included/excluded. At least
one of these parameters must be supplied.Returns:DataFrameThe subset of the frame including the dtypes inincludeand
excluding the dtypes inexclude.Raises:ValueErrorIf both ofincludeandexcludeare emptyIfincludeandexcludehave overlapping elementsIf any kind of string dtype is passed in.See alsoDataFrame.dtypesReturn Series with the data type of each column.NotesTo select allnumerictypes, usenp.numberor'number'To select strings you must use theobjectdtype, but note that
this will returnallobject dtype columnsSee thenumpy dtype hierarchyTo select datetimes, usenp.datetime64,'datetime'or'datetime64'To select timedeltas, usenp.timedelta64,'timedelta'or'timedelta64'To select Pandas categorical dtypes, use'category'To select Pandas datetimetz dtypes, use'datetimetz'or'datetime64[ns,tz]'Examples>>>df=pd.DataFrame({'a':[1,2]*3,...'b':[True,False]*3,...'c':[1.0,2.0]*3})>>>dfa      b  c0       1   True  1.01       2  False  2.02       1   True  1.03       2  False  2.04       1   True  1.05       2  False  2.0>>>df.select_dtypes(include='bool')b0  True1  False2  True3  False4  True5  False>>>df.select_dtypes(include=['float64'])c0  1.01  2.02  1.03  2.04  1.05  2.0>>>df.select_dtypes(exclude=['int64'])b    c0   True  1.01  False  2.02   True  1.03  False  2.04   True  1.05  False  2.0"
Pandas,DataFrame,pandas.DataFrame.values,"pandas.DataFrame.values#propertyDataFrame.values[source]#Return a Numpy representation of the DataFrame.WarningWe recommend usingDataFrame.to_numpy()instead.Only the values in the DataFrame will be returned, the axes labels
will be removed.Returns:numpy.ndarrayThe values of the DataFrame.See alsoDataFrame.to_numpyRecommended alternative to this method.DataFrame.indexRetrieve the index labels.DataFrame.columnsRetrieving the column names.NotesThe dtype will be a lower-common-denominator dtype (implicit
upcasting); that is to say if the dtypes (even of numeric types)
are mixed, the one that accommodates all will be chosen. Use this
with care if you are not dealing with the blocks.e.g. If the dtypes are float16 and float32, dtype will be upcast to
float32. If dtypes are int32 and uint8, dtype will be upcast to
int32. Bynumpy.find_common_type()convention, mixing int64
and uint64 will result in a float64 dtype.ExamplesA DataFrame where all columns are the same type (e.g., int64) results
in an array of the same type.>>>df=pd.DataFrame({'age':[3,29],...'height':[94,170],...'weight':[31,115]})>>>dfage  height  weight0    3      94      311   29     170     115>>>df.dtypesage       int64height    int64weight    int64dtype: object>>>df.valuesarray([[  3,  94,  31],[ 29, 170, 115]])A DataFrame with mixed type columns(e.g., str/object, int64, float32)
results in an ndarray of the broadest type that accommodates these
mixed types (e.g., object).>>>df2=pd.DataFrame([('parrot',24.0,'second'),...('lion',80.5,1),...('monkey',np.nan,None)],...columns=('name','max_speed','rank'))>>>df2.dtypesname          objectmax_speed    float64rank          objectdtype: object>>>df2.valuesarray([['parrot', 24.0, 'second'],['lion', 80.5, 1],['monkey', nan, None]], dtype=object)"
Pandas,DataFrame,pandas.DataFrame.axes,"pandas.DataFrame.axes#propertyDataFrame.axes[source]#Return a list representing the axes of the DataFrame.It has the row axis labels and column axis labels as the only members.
They are returned in that order.Examples>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>df.axes[RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],dtype='object')]"
Pandas,DataFrame,pandas.DataFrame.ndim,"pandas.DataFrame.ndim#propertyDataFrame.ndim[source]#Return an int representing the number of axes / array dimensions.Return 1 if Series. Otherwise return 2 if DataFrame.See alsondarray.ndimNumber of array dimensions.Examples>>>s=pd.Series({'a':1,'b':2,'c':3})>>>s.ndim1>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>df.ndim2"
Pandas,DataFrame,pandas.DataFrame.size,"pandas.DataFrame.size#propertyDataFrame.size[source]#Return an int representing the number of elements in this object.Return the number of rows if Series. Otherwise return the number of
rows times number of columns if DataFrame.See alsondarray.sizeNumber of elements in the array.Examples>>>s=pd.Series({'a':1,'b':2,'c':3})>>>s.size3>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>df.size4"
Pandas,DataFrame,pandas.DataFrame.shape,"pandas.DataFrame.shape#propertyDataFrame.shape[source]#Return a tuple representing the dimensionality of the DataFrame.See alsondarray.shapeTuple of array dimensions.Examples>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>df.shape(2, 2)>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4],...'col3':[5,6]})>>>df.shape(2, 3)"
Pandas,DataFrame,pandas.DataFrame.memory_usage,"pandas.DataFrame.memory_usage#DataFrame.memory_usage(index=True,deep=False)[source]#Return the memory usage of each column in bytes.The memory usage can optionally include the contribution of
the index and elements ofobjectdtype.This value is displayed inDataFrame.infoby default. This can be
suppressed by settingpandas.options.display.memory_usageto False.Parameters:indexbool, default TrueSpecifies whether to include the memory usage of the DataFramefs
index in returned Series. Ifindex=True, the memory usage of
the index is the first item in the output.deepbool, default FalseIf True, introspect the data deeply by interrogatingobjectdtypes for system-level memory consumption, and include
it in the returned values.Returns:SeriesA Series whose index is the original column names and whose values
is the memory usage of each column in bytes.See alsonumpy.ndarray.nbytesTotal bytes consumed by the elements of an ndarray.Series.memory_usageBytes consumed by a Series.CategoricalMemory-efficient array for string values with many repeated values.DataFrame.infoConcise summary of a DataFrame.NotesSee theFrequently Asked Questionsfor more
details.Examples>>>dtypes=['int64','float64','complex128','object','bool']>>>data=dict([(t,np.ones(shape=5000,dtype=int).astype(t))...fortindtypes])>>>df=pd.DataFrame(data)>>>df.head()int64  float64            complex128  object  bool0      1      1.0              1.0+0.0j       1  True1      1      1.0              1.0+0.0j       1  True2      1      1.0              1.0+0.0j       1  True3      1      1.0              1.0+0.0j       1  True4      1      1.0              1.0+0.0j       1  True>>>df.memory_usage()Index           128int64         40000float64       40000complex128    80000object        40000bool           5000dtype: int64>>>df.memory_usage(index=False)int64         40000float64       40000complex128    80000object        40000bool           5000dtype: int64The memory footprint ofobjectdtype columns is ignored by default:>>>df.memory_usage(deep=True)Index            128int64          40000float64        40000complex128     80000object        180000bool            5000dtype: int64Use a Categorical for efficient storage of an object-dtype column with
many repeated values.>>>df['object'].astype('category').memory_usage(deep=True)5244"
Pandas,DataFrame,pandas.DataFrame.empty,"pandas.DataFrame.empty#propertyDataFrame.empty[source]#Indicator whether Series/DataFrame is empty.True if Series/DataFrame is entirely empty (no items), meaning any of the
axes are of length 0.Returns:boolIf Series/DataFrame is empty, return True, if not return False.See alsoSeries.dropnaReturn series without null values.DataFrame.dropnaReturn DataFrame with labels on given axis omitted where (all or any) data are missing.NotesIf Series/DataFrame contains only NaNs, it is still not considered empty. See
the example below.ExamplesAn example of an actual empty DataFrame. Notice the index is empty:>>>df_empty=pd.DataFrame({'A':[]})>>>df_emptyEmpty DataFrameColumns: [A]Index: []>>>df_empty.emptyTrueIf we only have NaNs in our DataFrame, it is not considered empty! We
will need to drop the NaNs to make the DataFrame empty:>>>df=pd.DataFrame({'A':[np.nan]})>>>dfA0 NaN>>>df.emptyFalse>>>df.dropna().emptyTrue>>>ser_empty=pd.Series({'A':[]})>>>ser_emptyA    []dtype: object>>>ser_empty.emptyFalse>>>ser_empty=pd.Series()>>>ser_empty.emptyTrue"
Pandas,DataFrame,pandas.DataFrame.set_flags,"pandas.DataFrame.set_flags#DataFrame.set_flags(*,copy=False,allows_duplicate_labels=None)[source]#Return a new object with updated flags.Parameters:copybool, default FalseSpecify if a copy of the object should be made.allows_duplicate_labelsbool, optionalWhether the returned object allows duplicate labels.Returns:Series or DataFrameThe same type as the caller.See alsoDataFrame.attrsGlobal metadata applying to this dataset.DataFrame.flagsGlobal flags applying to this object.NotesThis method returns a new object thatfs a view on the same data
as the input. Mutating the input or the output values will be reflected
in the other.This method is intended to be used in method chains.gFlagsh differ from gmetadatah. Flags reflect properties of the
pandas object (the Series or DataFrame). Metadata refer to properties
of the dataset, and should be stored inDataFrame.attrs.Examples>>>df=pd.DataFrame({""A"":[1,2]})>>>df.flags.allows_duplicate_labelsTrue>>>df2=df.set_flags(allows_duplicate_labels=False)>>>df2.flags.allows_duplicate_labelsFalse"
Pandas,DataFrame,pandas.DataFrame.astype,"pandas.DataFrame.astype#DataFrame.astype(dtype,copy=None,errors='raise')[source]#Cast a pandas object to a specified dtypedtype.Parameters:dtypestr, data type, Series or Mapping of column name -> data typeUse a str, numpy.dtype, pandas.ExtensionDtype or Python type to
cast entire pandas object to the same type. Alternatively, use a
mapping, e.g. {col: dtype, c}, where col is a column label and dtype is
a numpy.dtype or Python type to cast one or more of the DataFramefs
columns to column-specific types.copybool, default TrueReturn a copy whencopy=True(be very careful settingcopy=Falseas changes to values then may propagate to other
pandas objects).errors{eraisef, eignoref}, default eraisefControl raising of exceptions on invalid data for provided dtype.raise: allow exceptions to be raisedignore: suppress exceptions. On error return original object.Returns:same type as callerSee alsoto_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to a numeric type.numpy.ndarray.astypeCast a numpy array to a specified type.NotesChanged in version 2.0.0:Usingastypeto convert from timezone-naive dtype to
timezone-aware dtype will raise an exception.
UseSeries.dt.tz_localize()instead.ExamplesCreate a DataFrame:>>>d={'col1':[1,2],'col2':[3,4]}>>>df=pd.DataFrame(data=d)>>>df.dtypescol1    int64col2    int64dtype: objectCast all columns to int32:>>>df.astype('int32').dtypescol1    int32col2    int32dtype: objectCast col1 to int32 using a dictionary:>>>df.astype({'col1':'int32'}).dtypescol1    int32col2    int64dtype: objectCreate a series:>>>ser=pd.Series([1,2],dtype='int32')>>>ser0    11    2dtype: int32>>>ser.astype('int64')0    11    2dtype: int64Convert to categorical type:>>>ser.astype('category')0    11    2dtype: categoryCategories (2, int32): [1, 2]Convert to ordered categorical type with custom ordering:>>>frompandas.api.typesimportCategoricalDtype>>>cat_dtype=CategoricalDtype(...categories=[2,1],ordered=True)>>>ser.astype(cat_dtype)0    11    2dtype: categoryCategories (2, int64): [2 < 1]Create a series of dates:>>>ser_date=pd.Series(pd.date_range('20200101',periods=3))>>>ser_date0   2020-01-011   2020-01-022   2020-01-03dtype: datetime64[ns]"
Pandas,DataFrame,pandas.DataFrame.convert_dtypes,"pandas.DataFrame.convert_dtypes#DataFrame.convert_dtypes(infer_objects=True,convert_string=True,convert_integer=True,convert_boolean=True,convert_floating=True,dtype_backend='numpy_nullable')[source]#Convert columns to the best possible dtypes using dtypes supportingpd.NA.Parameters:infer_objectsbool, default TrueWhether object dtypes should be converted to the best possible types.convert_stringbool, default TrueWhether object dtypes should be converted toStringDtype().convert_integerbool, default TrueWhether, if possible, conversion can be done to integer extension types.convert_booleanbool, defaults TrueWhether object dtypes should be converted toBooleanDtypes().convert_floatingbool, defaults TrueWhether, if possible, conversion can be done to floating extension types.
Ifconvert_integeris also True, preference will be give to integer
dtypes if the floats can be faithfully casted to integers.New in version 1.2.0.dtype_backend{enumpy_nullablef, epyarrowf}, default enumpy_nullablefBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:Series or DataFrameCopy of input object with new dtype.See alsoinfer_objectsInfer dtypes of objects.to_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to a numeric type.NotesBy default,convert_dtypeswill attempt to convert a Series (or each
Series in a DataFrame) to dtypes that supportpd.NA. By using the optionsconvert_string,convert_integer,convert_booleanandconvert_floating, it is possible to turn off individual conversions
toStringDtype, the integer extension types,BooleanDtypeor floating extension types, respectively.For object-dtyped columns, ifinfer_objectsisTrue, use the inference
rules as during normal Series/DataFrame construction. Then, if possible,
convert toStringDtype,BooleanDtypeor an appropriate integer
or floating extension type, otherwise leave asobject.If the dtype is integer, convert to an appropriate integer extension type.If the dtype is numeric, and consists of all integers, convert to an
appropriate integer extension type. Otherwise, convert to an
appropriate floating extension type.Changed in version 1.2:Starting with pandas 1.2, this method also converts float columns
to the nullable floating extension type.In the future, as new dtypes are added that supportpd.NA, the results
of this method will change to support those new dtypes.Examples>>>df=pd.DataFrame(...{...""a"":pd.Series([1,2,3],dtype=np.dtype(""int32"")),...""b"":pd.Series([""x"",""y"",""z""],dtype=np.dtype(""O"")),...""c"":pd.Series([True,False,np.nan],dtype=np.dtype(""O"")),...""d"":pd.Series([""h"",""i"",np.nan],dtype=np.dtype(""O"")),...""e"":pd.Series([10,np.nan,20],dtype=np.dtype(""float"")),...""f"":pd.Series([np.nan,100.5,200],dtype=np.dtype(""float"")),...}...)Start with a DataFrame with default dtypes.>>>dfa  b      c    d     e      f0  1  x   True    h  10.0    NaN1  2  y  False    i   NaN  100.52  3  z    NaN  NaN  20.0  200.0>>>df.dtypesa      int32b     objectc     objectd     objecte    float64f    float64dtype: objectConvert the DataFrame to use best possible dtypes.>>>dfn=df.convert_dtypes()>>>dfna  b      c     d     e      f0  1  x   True     h    10   <NA>1  2  y  False     i  <NA>  100.52  3  z   <NA>  <NA>    20  200.0>>>dfn.dtypesa             Int32b    string[python]c           booleand    string[python]e             Int64f           Float64dtype: objectStart with a Series of strings and missing data represented bynp.nan.>>>s=pd.Series([""a"",""b"",np.nan])>>>s0      a1      b2    NaNdtype: objectObtain a Series with dtypeStringDtype.>>>s.convert_dtypes()0       a1       b2    <NA>dtype: string"
Pandas,DataFrame,pandas.DataFrame.infer_objects,"pandas.DataFrame.infer_objects#DataFrame.infer_objects(copy=None)[source]#Attempt to infer better dtypes for object columns.Attempts soft conversion of object-dtyped
columns, leaving non-object and unconvertible
columns unchanged. The inference rules are the
same as during normal Series/DataFrame construction.Parameters:copybool, default TrueWhether to make a copy for non-object or non-inferable columns
or Series.Returns:same type as input objectSee alsoto_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.to_numericConvert argument to numeric type.convert_dtypesConvert argument to best possible dtype.Examples>>>df=pd.DataFrame({""A"":[""a"",1,2,3]})>>>df=df.iloc[1:]>>>dfA1  12  23  3>>>df.dtypesA    objectdtype: object>>>df.infer_objects().dtypesA    int64dtype: object"
Pandas,DataFrame,pandas.DataFrame.copy,"pandas.DataFrame.copy#DataFrame.copy(deep=True)[source]#Make a copy of this objectfs indices and data.Whendeep=True(default), a new object will be created with a
copy of the calling objectfs data and indices. Modifications to
the data or indices of the copy will not be reflected in the
original object (see notes below).Whendeep=False, a new object will be created without copying
the calling objectfs data or index (only references to the data
and index are copied). Any changes to the data of the original
will be reflected in the shallow copy (and vice versa).Parameters:deepbool, default TrueMake a deep copy, including a copy of the data and the indices.
Withdeep=Falseneither the indices nor the data are copied.Returns:Series or DataFrameObject type matches caller.NotesWhendeep=True, data is copied but actual Python objects
will not be copied recursively, only the reference to the object.
This is in contrast tocopy.deepcopyin the Standard Library,
which recursively copies object data (see examples below).WhileIndexobjects are copied whendeep=True, the underlying
numpy array is not copied for performance reasons. SinceIndexis
immutable, the underlying data can be safely shared and a copy
is not needed.Since pandas is not thread safe, see thegotchaswhen copying in a threading
environment.Whencopy_on_writein pandas config is set toTrue, thecopy_on_writeconfig takes effect even whendeep=False.
This means that any changes to the copied data would make a new copy
of the data upon write (and vice versa). Changes made to either the
original or copied variable would not be reflected in the counterpart.
SeeCopy_on_Writefor more information.Examples>>>s=pd.Series([1,2],index=[""a"",""b""])>>>sa    1b    2dtype: int64>>>s_copy=s.copy()>>>s_copya    1b    2dtype: int64Shallow copy versus default (deep) copy:>>>s=pd.Series([1,2],index=[""a"",""b""])>>>deep=s.copy()>>>shallow=s.copy(deep=False)Shallow copy shares data and index with original.>>>sisshallowFalse>>>s.valuesisshallow.valuesands.indexisshallow.indexTrueDeep copy has own copy of data and index.>>>sisdeepFalse>>>s.valuesisdeep.valuesors.indexisdeep.indexFalseUpdates to the data shared by shallow copy and original is reflected
in both; deep copy remains unchanged.>>>s.iloc[0]=3>>>shallow.iloc[1]=4>>>sa    3b    4dtype: int64>>>shallowa    3b    4dtype: int64>>>deepa    1b    2dtype: int64Note that when copying an object containing Python objects, a deep copy
will copy the data, but will not do so recursively. Updating a nested
data object will be reflected in the deep copy.>>>s=pd.Series([[1,2],[3,4]])>>>deep=s.copy()>>>s[0][0]=10>>>s0    [10, 2]1     [3, 4]dtype: object>>>deep0    [10, 2]1     [3, 4]dtype: object** Copy-on-Write is set to true: **>>>withpd.option_context(""mode.copy_on_write"",True):...s=pd.Series([1,2],index=[""a"",""b""])...copy=s.copy(deep=False)...s.iloc[0]=100...sa    100b      2dtype: int64>>>copya    1b    2dtype: int64"
Pandas,DataFrame,pandas.DataFrame.bool,"pandas.DataFrame.bool#DataFrame.bool()[source]#Return the bool of a single element Series or DataFrame.Deprecated since version 2.1.0:bool is deprecated and will be removed in future version of pandasThis must be a boolean scalar value, either True or False. It will raise a
ValueError if the Series or DataFrame does not have exactly 1 element, or that
element is not boolean (integer values 0 and 1 will also raise an exception).Returns:boolThe value in the Series or DataFrame.See alsoSeries.astypeChange the data type of a Series, including to boolean.DataFrame.astypeChange the data type of a DataFrame, including to boolean.numpy.bool_NumPy boolean data type, used by pandas for boolean values.ExamplesThe method will only work for single element objects with a boolean value:>>>pd.Series([True]).bool()True>>>pd.Series([False]).bool()False>>>pd.DataFrame({'col':[True]}).bool()True>>>pd.DataFrame({'col':[False]}).bool()False"
Pandas,DataFrame,pandas.DataFrame.head,"pandas.DataFrame.head#DataFrame.head(n=5)[source]#Return the firstnrows.This function returns the firstnrows for the object based
on position. It is useful for quickly testing if your object
has the right type of data in it.For negative values ofn, this function returns all rows except
the last|n|rows, equivalent todf[:n].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:same type as callerThe firstnrows of the caller object.See alsoDataFrame.tailReturns the lastnrows.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the first 5 lines>>>df.head()animal0  alligator1        bee2     falcon3       lion4     monkeyViewing the firstnlines (three in this case)>>>df.head(3)animal0  alligator1        bee2     falconFor negative values ofn>>>df.head(-3)animal0  alligator1        bee2     falcon3       lion4     monkey5     parrot"
Pandas,DataFrame,pandas.DataFrame.at,"pandas.DataFrame.at#propertyDataFrame.at[source]#Access a single value for a row/column label pair.Similar toloc, in that both provide label-based lookups. Useatif you only need to get or set a single value in a DataFrame
or Series.Raises:KeyErrorIf getting a value and elabelf does not exist in a DataFrame orSeries.ValueErrorIf row/column label pair is not a tuple or if any label fromthe pair is not a scalar for DataFrame.If label is list-like (excludingNamedTuple) for Series.See alsoDataFrame.atAccess a single value for a row/column pair by label.DataFrame.iatAccess a single value for a row/column pair by integer position.DataFrame.locAccess a group of rows and columns by label(s).DataFrame.ilocAccess a group of rows and columns by integer position(s).Series.atAccess a single value by label.Series.iatAccess a single value by integer position.Series.locAccess a group of rows by label(s).Series.ilocAccess a group of rows by integer position(s).NotesSeeFast scalar value getting and settingfor more details.Examples>>>df=pd.DataFrame([[0,2,3],[0,4,1],[10,20,30]],...index=[4,5,6],columns=['A','B','C'])>>>dfA   B   C4   0   2   35   0   4   16  10  20  30Get value at specified row/column pair>>>df.at[4,'B']2Set value at specified row/column pair>>>df.at[4,'B']=10>>>df.at[4,'B']10Get value within a Series>>>df.loc[5].at['B']4"
Pandas,DataFrame,pandas.DataFrame.iat,"pandas.DataFrame.iat#propertyDataFrame.iat[source]#Access a single value for a row/column pair by integer position.Similar toiloc, in that both provide integer-based lookups. Useiatif you only need to get or set a single value in a DataFrame
or Series.Raises:IndexErrorWhen integer position is out of bounds.See alsoDataFrame.atAccess a single value for a row/column label pair.DataFrame.locAccess a group of rows and columns by label(s).DataFrame.ilocAccess a group of rows and columns by integer position(s).Examples>>>df=pd.DataFrame([[0,2,3],[0,4,1],[10,20,30]],...columns=['A','B','C'])>>>dfA   B   C0   0   2   31   0   4   12  10  20  30Get value at specified row/column pair>>>df.iat[1,2]1Set value at specified row/column pair>>>df.iat[1,2]=10>>>df.iat[1,2]10Get value within a series>>>df.loc[0].iat[1]2"
Pandas,DataFrame,pandas.DataFrame.loc,"pandas.DataFrame.loc#propertyDataFrame.loc[source]#Access a group of rows and columns by label(s) or a boolean array..loc[]is primarily label based, but may also be used with a
boolean array.Allowed inputs are:A single label, e.g.5or'a', (note that5is
interpreted as alabelof the index, andneveras an
integer position along the index).A list or array of labels, e.g.['a','b','c'].A slice object with labels, e.g.'a':'f'.WarningNote that contrary to usual python slices,boththe
start and the stop are includedA boolean array of the same length as the axis being sliced,
e.g.[True,False,True].An alignable boolean Series. The index of the key will be aligned before
masking.An alignable Index. The Index of the returned selection will be the input.Acallablefunction with one argument (the calling Series or
DataFrame) and that returns valid output for indexing (one of the above)See more atSelection by Label.Raises:KeyErrorIf any items are not found.IndexingErrorIf an indexed key is passed and its index is unalignable to the frame index.See alsoDataFrame.atAccess a single value for a row/column label pair.DataFrame.ilocAccess group of rows and columns by integer position(s).DataFrame.xsReturns a cross-section (row(s) or column(s)) from the Series/DataFrame.Series.locAccess group of values using labels.ExamplesGetting values>>>df=pd.DataFrame([[1,2],[4,5],[7,8]],...index=['cobra','viper','sidewinder'],...columns=['max_speed','shield'])>>>dfmax_speed  shieldcobra               1       2viper               4       5sidewinder          7       8Single label. Note this returns the row as a Series.>>>df.loc['viper']max_speed    4shield       5Name: viper, dtype: int64List of labels. Note using[[]]returns a DataFrame.>>>df.loc[['viper','sidewinder']]max_speed  shieldviper               4       5sidewinder          7       8Single label for row and column>>>df.loc['cobra','shield']2Slice with labels for row and single label for column. As mentioned
above, note that both the start and stop of the slice are included.>>>df.loc['cobra':'viper','max_speed']cobra    1viper    4Name: max_speed, dtype: int64Boolean list with the same length as the row axis>>>df.loc[[False,False,True]]max_speed  shieldsidewinder          7       8Alignable boolean Series:>>>df.loc[pd.Series([False,True,False],...index=['viper','sidewinder','cobra'])]max_speed  shieldsidewinder          7       8Index (same behavior asdf.reindex)>>>df.loc[pd.Index([""cobra"",""viper""],name=""foo"")]max_speed  shieldfoocobra          1       2viper          4       5Conditional that returns a boolean Series>>>df.loc[df['shield']>6]max_speed  shieldsidewinder          7       8Conditional that returns a boolean Series with column labels specified>>>df.loc[df['shield']>6,['max_speed']]max_speedsidewinder          7Multiple conditional using&that returns a boolean Series>>>df.loc[(df['max_speed']>1)&(df['shield']<8)]max_speed  shieldviper          4       5Multiple conditional using|that returns a boolean Series>>>df.loc[(df['max_speed']>4)|(df['shield']<5)]max_speed  shieldcobra               1       2sidewinder          7       8Please ensure that each condition is wrapped in parentheses().
See theuser guidefor more details and explanations of Boolean indexing.NoteIf you find yourself using 3 or more conditionals in.loc[],
consider usingadvanced indexing.See below for using.loc[]on MultiIndex DataFrames.Callable that returns a boolean Series>>>df.loc[lambdadf:df['shield']==8]max_speed  shieldsidewinder          7       8Setting valuesSet value for all items matching the list of labels>>>df.loc[['viper','sidewinder'],['shield']]=50>>>dfmax_speed  shieldcobra               1       2viper               4      50sidewinder          7      50Set value for an entire row>>>df.loc['cobra']=10>>>dfmax_speed  shieldcobra              10      10viper               4      50sidewinder          7      50Set value for an entire column>>>df.loc[:,'max_speed']=30>>>dfmax_speed  shieldcobra              30      10viper              30      50sidewinder         30      50Set value for rows matching callable condition>>>df.loc[df['shield']>35]=0>>>dfmax_speed  shieldcobra              30      10viper               0       0sidewinder          0       0Add value matching location>>>df.loc[""viper"",""shield""]+=5>>>dfmax_speed  shieldcobra              30      10viper               0       5sidewinder          0       0Setting using aSeriesor aDataFramesets the values matching the
index labels, not the index positions.>>>shuffled_df=df.loc[[""viper"",""cobra"",""sidewinder""]]>>>df.loc[:]+=shuffled_df>>>dfmax_speed  shieldcobra              60      20viper               0      10sidewinder          0       0Getting values on a DataFrame with an index that has integer labelsAnother example using integers for the index>>>df=pd.DataFrame([[1,2],[4,5],[7,8]],...index=[7,8,9],columns=['max_speed','shield'])>>>dfmax_speed  shield7          1       28          4       59          7       8Slice with integer labels for rows. As mentioned above, note that both
the start and stop of the slice are included.>>>df.loc[7:9]max_speed  shield7          1       28          4       59          7       8Getting values with a MultiIndexA number of examples using a DataFrame with a MultiIndex>>>tuples=[...('cobra','mark i'),('cobra','mark ii'),...('sidewinder','mark i'),('sidewinder','mark ii'),...('viper','mark ii'),('viper','mark iii')...]>>>index=pd.MultiIndex.from_tuples(tuples)>>>values=[[12,2],[0,4],[10,20],...[1,4],[7,1],[16,36]]>>>df=pd.DataFrame(values,columns=['max_speed','shield'],index=index)>>>dfmax_speed  shieldcobra      mark i           12       2mark ii           0       4sidewinder mark i           10      20mark ii           1       4viper      mark ii           7       1mark iii         16      36Single label. Note this returns a DataFrame with a single index.>>>df.loc['cobra']max_speed  shieldmark i          12       2mark ii          0       4Single index tuple. Note this returns a Series.>>>df.loc[('cobra','mark ii')]max_speed    0shield       4Name: (cobra, mark ii), dtype: int64Single label for row and column. Similar to passing in a tuple, this
returns a Series.>>>df.loc['cobra','mark i']max_speed    12shield        2Name: (cobra, mark i), dtype: int64Single tuple. Note using[[]]returns a DataFrame.>>>df.loc[[('cobra','mark ii')]]max_speed  shieldcobra mark ii          0       4Single tuple for the index with a single label for the column>>>df.loc[('cobra','mark i'),'shield']2Slice from index tuple to single label>>>df.loc[('cobra','mark i'):'viper']max_speed  shieldcobra      mark i           12       2mark ii           0       4sidewinder mark i           10      20mark ii           1       4viper      mark ii           7       1mark iii         16      36Slice from index tuple to index tuple>>>df.loc[('cobra','mark i'):('viper','mark ii')]max_speed  shieldcobra      mark i          12       2mark ii          0       4sidewinder mark i          10      20mark ii          1       4viper      mark ii          7       1Please see theuser guidefor more details and explanations of advanced indexing."
Pandas,DataFrame,pandas.DataFrame.iloc,"pandas.DataFrame.iloc#propertyDataFrame.iloc[source]#Purely integer-location based indexing for selection by position..iloc[]is primarily integer position based (from0tolength-1of the axis), but may also be used with a boolean
array.Allowed inputs are:An integer, e.g.5.A list or array of integers, e.g.[4,3,0].A slice object with ints, e.g.1:7.A boolean array.Acallablefunction with one argument (the calling Series or
DataFrame) and that returns valid output for indexing (one of the above).
This is useful in method chains, when you donft have a reference to the
calling object, but would like to base your selection on some value.A tuple of row and column indexes. The tuple elements consist of one of the
above inputs, e.g.(0,1)..ilocwill raiseIndexErrorif a requested indexer is
out-of-bounds, exceptsliceindexers which allow out-of-bounds
indexing (this conforms with python/numpyslicesemantics).See more atSelection by Position.See alsoDataFrame.iatFast integer location scalar accessor.DataFrame.locPurely label-location based indexer for selection by label.Series.ilocPurely integer-location based indexing for selection by position.Examples>>>mydict=[{'a':1,'b':2,'c':3,'d':4},...{'a':100,'b':200,'c':300,'d':400},...{'a':1000,'b':2000,'c':3000,'d':4000}]>>>df=pd.DataFrame(mydict)>>>dfa     b     c     d0     1     2     3     41   100   200   300   4002  1000  2000  3000  4000Indexing just the rowsWith a scalar integer.>>>type(df.iloc[0])<class 'pandas.core.series.Series'>>>>df.iloc[0]a    1b    2c    3d    4Name: 0, dtype: int64With a list of integers.>>>df.iloc[[0]]a  b  c  d0  1  2  3  4>>>type(df.iloc[[0]])<class 'pandas.core.frame.DataFrame'>>>>df.iloc[[0,1]]a    b    c    d0    1    2    3    41  100  200  300  400With asliceobject.>>>df.iloc[:3]a     b     c     d0     1     2     3     41   100   200   300   4002  1000  2000  3000  4000With a boolean mask the same length as the index.>>>df.iloc[[True,False,True]]a     b     c     d0     1     2     3     42  1000  2000  3000  4000With a callable, useful in method chains. Thexpassed
to thelambdais the DataFrame being sliced. This selects
the rows whose index label even.>>>df.iloc[lambdax:x.index%2==0]a     b     c     d0     1     2     3     42  1000  2000  3000  4000Indexing both axesYou can mix the indexer types for the index and columns. Use:to
select the entire axis.With scalar integers.>>>df.iloc[0,1]2With lists of integers.>>>df.iloc[[0,2],[1,3]]b     d0     2     42  2000  4000Withsliceobjects.>>>df.iloc[1:3,0:3]a     b     c1   100   200   3002  1000  2000  3000With a boolean array whose length matches the columns.>>>df.iloc[:,[True,False,True,False]]a     c0     1     31   100   3002  1000  3000With a callable function that expects the Series or DataFrame.>>>df.iloc[:,lambdadf:[0,2]]a     c0     1     31   100   3002  1000  3000"
Pandas,DataFrame,pandas.DataFrame.insert,"pandas.DataFrame.insert#DataFrame.insert(loc,column,value,allow_duplicates=_NoDefault.no_default)[source]#Insert column into DataFrame at specified location.Raises a ValueError ifcolumnis already contained in the DataFrame,
unlessallow_duplicatesis set to True.Parameters:locintInsertion index. Must verify 0 <= loc <= len(columns).columnstr, number, or hashable objectLabel of the inserted column.valueScalar, Series, or array-likeallow_duplicatesbool, optional, default lib.no_defaultSee alsoIndex.insertInsert new item by index.Examples>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>dfcol1  col20     1     31     2     4>>>df.insert(1,""newcol"",[99,99])>>>dfcol1  newcol  col20     1      99     31     2      99     4>>>df.insert(0,""col1"",[100,100],allow_duplicates=True)>>>dfcol1  col1  newcol  col20   100     1      99     31   100     2      99     4Notice that pandas uses index alignment in case ofvaluefrom typeSeries:>>>df.insert(0,""col0"",pd.Series([5,6],index=[1,2]))>>>dfcol0  col1  col1  newcol  col20   NaN   100     1      99     31   5.0   100     2      99     4"
Pandas,DataFrame,pandas.DataFrame.__iter__,"pandas.DataFrame.__iter__#DataFrame.__iter__()[source]#Iterate over info axis.Returns:iteratorInfo axis as iterator.Examples>>>df=pd.DataFrame({'A':[1,2,3],'B':[4,5,6]})>>>forxindf:...print(x)AB"
Pandas,DataFrame,pandas.DataFrame.items,"pandas.DataFrame.items#DataFrame.items()[source]#Iterate over (column name, Series) pairs.Iterates over the DataFrame columns, returning a tuple with
the column name and the content as a Series.Yields:labelobjectThe column names for the DataFrame being iterated over.contentSeriesThe column entries belonging to each label, as a Series.See alsoDataFrame.iterrowsIterate over DataFrame rows as (index, Series) pairs.DataFrame.itertuplesIterate over DataFrame rows as namedtuples of the values.Examples>>>df=pd.DataFrame({'species':['bear','bear','marsupial'],...'population':[1864,22000,80000]},...index=['panda','polar','koala'])>>>dfspecies   populationpanda   bear      1864polar   bear      22000koala   marsupial 80000>>>forlabel,contentindf.items():...print(f'label:{label}')...print(f'content:{content}',sep='\n')...label: speciescontent:panda         bearpolar         bearkoala    marsupialName: species, dtype: objectlabel: populationcontent:panda     1864polar    22000koala    80000Name: population, dtype: int64"
Pandas,DataFrame,pandas.DataFrame.keys,"pandas.DataFrame.keys#DataFrame.keys()[source]#Get the einfo axisf (see Indexing for more).This is index for Series, columns for DataFrame.Returns:IndexInfo axis.Examples>>>d=pd.DataFrame(data={'A':[1,2,3],'B':[0,4,8]},...index=['a','b','c'])>>>dA  Ba  1  0b  2  4c  3  8>>>d.keys()Index(['A', 'B'], dtype='object')"
Pandas,DataFrame,pandas.DataFrame.iterrows,"pandas.DataFrame.iterrows#DataFrame.iterrows()[source]#Iterate over DataFrame rows as (index, Series) pairs.Yields:indexlabel or tuple of labelThe index of the row. A tuple for aMultiIndex.dataSeriesThe data of the row as a Series.See alsoDataFrame.itertuplesIterate over DataFrame rows as namedtuples of the values.DataFrame.itemsIterate over (column name, Series) pairs.NotesBecauseiterrowsreturns a Series for each row,
it doesnotpreserve dtypes across the rows (dtypes are
preserved across columns for DataFrames).To preserve dtypes while iterating over the rows, it is better
to useitertuples()which returns namedtuples of the values
and which is generally faster thaniterrows.You shouldnever modifysomething you are iterating over.
This is not guaranteed to work in all cases. Depending on the
data types, the iterator returns a copy and not a view, and writing
to it will have no effect.Examples>>>df=pd.DataFrame([[1,1.5]],columns=['int','float'])>>>row=next(df.iterrows())[1]>>>rowint      1.0float    1.5Name: 0, dtype: float64>>>print(row['int'].dtype)float64>>>print(df['int'].dtype)int64"
Pandas,DataFrame,pandas.DataFrame.itertuples,"pandas.DataFrame.itertuples#DataFrame.itertuples(index=True,name='Pandas')[source]#Iterate over DataFrame rows as namedtuples.Parameters:indexbool, default TrueIf True, return the index as the first element of the tuple.namestr or None, default gPandashThe name of the returned namedtuples or None to return regular
tuples.Returns:iteratorAn object to iterate over namedtuples for each row in the
DataFrame with the first field possibly being the index and
following fields being the column values.See alsoDataFrame.iterrowsIterate over DataFrame rows as (index, Series) pairs.DataFrame.itemsIterate over (column name, Series) pairs.NotesThe column names will be renamed to positional names if they are
invalid Python identifiers, repeated, or start with an underscore.Examples>>>df=pd.DataFrame({'num_legs':[4,2],'num_wings':[0,2]},...index=['dog','hawk'])>>>dfnum_legs  num_wingsdog          4          0hawk         2          2>>>forrowindf.itertuples():...print(row)...Pandas(Index='dog', num_legs=4, num_wings=0)Pandas(Index='hawk', num_legs=2, num_wings=2)By setting theindexparameter to False we can remove the index
as the first element of the tuple:>>>forrowindf.itertuples(index=False):...print(row)...Pandas(num_legs=4, num_wings=0)Pandas(num_legs=2, num_wings=2)With thenameparameter set we set a custom name for the yielded
namedtuples:>>>forrowindf.itertuples(name='Animal'):...print(row)...Animal(Index='dog', num_legs=4, num_wings=0)Animal(Index='hawk', num_legs=2, num_wings=2)"
Pandas,DataFrame,pandas.DataFrame.pop,"pandas.DataFrame.pop#DataFrame.pop(item)[source]#Return item and drop from frame. Raise KeyError if not found.Parameters:itemlabelLabel of column to be popped.Returns:SeriesExamples>>>df=pd.DataFrame([('falcon','bird',389.0),...('parrot','bird',24.0),...('lion','mammal',80.5),...('monkey','mammal',np.nan)],...columns=('name','class','max_speed'))>>>dfname   class  max_speed0  falcon    bird      389.01  parrot    bird       24.02    lion  mammal       80.53  monkey  mammal        NaN>>>df.pop('class')0      bird1      bird2    mammal3    mammalName: class, dtype: object>>>dfname  max_speed0  falcon      389.01  parrot       24.02    lion       80.53  monkey        NaN"
Pandas,DataFrame,pandas.DataFrame.tail,"pandas.DataFrame.tail#DataFrame.tail(n=5)[source]#Return the lastnrows.This function returns lastnrows from the object based on
position. It is useful for quickly verifying data, for example,
after sorting or appending rows.For negative values ofn, this function returns all rows except
the first|n|rows, equivalent todf[|n|:].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:type of callerThe lastnrows of the caller object.See alsoDataFrame.headThe firstnrows of the caller object.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the last 5 lines>>>df.tail()animal4  monkey5  parrot6   shark7   whale8   zebraViewing the lastnlines (three in this case)>>>df.tail(3)animal6  shark7  whale8  zebraFor negative values ofn>>>df.tail(-3)animal3    lion4  monkey5  parrot6   shark7   whale8   zebra"
Pandas,DataFrame,pandas.DataFrame.xs,"pandas.DataFrame.xs#DataFrame.xs(key,axis=0,level=None,drop_level=True)[source]#Return cross-section from the Series/DataFrame.This method takes akeyargument to select data at a particular
level of a MultiIndex.Parameters:keylabel or tuple of labelLabel contained in the index, or partially in a MultiIndex.axis{0 or eindexf, 1 or ecolumnsf}, default 0Axis to retrieve cross-section on.levelobject, defaults to first n levels (n=1 or len(key))In case of a key partially contained in a MultiIndex, indicate
which levels are used. Levels can be referred by label or position.drop_levelbool, default TrueIf False, returns object with same levels as self.Returns:Series or DataFrameCross-section from the original Series or DataFrame
corresponding to the selected index levels.See alsoDataFrame.locAccess a group of rows and columns by label(s) or a boolean array.DataFrame.ilocPurely integer-location based indexing for selection by position.Notesxscan not be used to set values.MultiIndex Slicers is a generic way to get/set values on
any level or levels.
It is a superset ofxsfunctionality, seeMultiIndex Slicers.Examples>>>d={'num_legs':[4,4,2,2],...'num_wings':[0,0,2,2],...'class':['mammal','mammal','mammal','bird'],...'animal':['cat','dog','bat','penguin'],...'locomotion':['walks','walks','flies','walks']}>>>df=pd.DataFrame(data=d)>>>df=df.set_index(['class','animal','locomotion'])>>>dfnum_legs  num_wingsclass  animal  locomotionmammal cat     walks              4          0dog     walks              4          0bat     flies              2          2bird   penguin walks              2          2Get values at specified index>>>df.xs('mammal')num_legs  num_wingsanimal locomotioncat    walks              4          0dog    walks              4          0bat    flies              2          2Get values at several indexes>>>df.xs(('mammal','dog','walks'))num_legs     4num_wings    0Name: (mammal, dog, walks), dtype: int64Get values at specified index and level>>>df.xs('cat',level=1)num_legs  num_wingsclass  locomotionmammal walks              4          0Get values at several indexes and levels>>>df.xs(('bird','walks'),...level=[0,'locomotion'])num_legs  num_wingsanimalpenguin         2          2Get values at specified column and axis>>>df.xs('num_wings',axis=1)class   animal   locomotionmammal  cat      walks         0dog      walks         0bat      flies         2bird    penguin  walks         2Name: num_wings, dtype: int64"
Pandas,DataFrame,pandas.DataFrame.get,"pandas.DataFrame.get#DataFrame.get(key,default=None)[source]#Get item from object for given key (ex: DataFrame column).Returns default value if not found.Parameters:keyobjectReturns:same type as items contained in objectExamples>>>df=pd.DataFrame(...[...[24.3,75.7,""high""],...[31,87.8,""high""],...[22,71.6,""medium""],...[35,95,""medium""],...],...columns=[""temp_celsius"",""temp_fahrenheit"",""windspeed""],...index=pd.date_range(start=""2014-02-12"",end=""2014-02-15"",freq=""D""),...)>>>dftemp_celsius  temp_fahrenheit windspeed2014-02-12          24.3             75.7      high2014-02-13          31.0             87.8      high2014-02-14          22.0             71.6    medium2014-02-15          35.0             95.0    medium>>>df.get([""temp_celsius"",""windspeed""])temp_celsius windspeed2014-02-12          24.3      high2014-02-13          31.0      high2014-02-14          22.0    medium2014-02-15          35.0    medium>>>ser=df['windspeed']>>>ser.get('2014-02-13')'high'If the key isnft found, the default value will be used.>>>df.get([""temp_celsius"",""temp_kelvin""],default=""default_value"")'default_value'>>>ser.get('2014-02-10','[unknown]')'[unknown]'"
Pandas,DataFrame,pandas.DataFrame.isin,"pandas.DataFrame.isin#DataFrame.isin(values)[source]#Whether each element in the DataFrame is contained in values.Parameters:valuesiterable, Series, DataFrame or dictThe result will only be true at a location if all the
labels match. Ifvaluesis a Series, thatfs the index. Ifvaluesis a dict, the keys must be the column names,
which must match. Ifvaluesis a DataFrame,
then both the index and column labels must match.Returns:DataFrameDataFrame of booleans showing whether each element in the DataFrame
is contained in values.See alsoDataFrame.eqEquality test for DataFrame.Series.isinEquivalent method on Series.Series.str.containsTest if pattern or regex is contained within a string of a Series or Index.Examples>>>df=pd.DataFrame({'num_legs':[2,4],'num_wings':[2,0]},...index=['falcon','dog'])>>>dfnum_legs  num_wingsfalcon         2          2dog            4          0Whenvaluesis a list check whether every value in the DataFrame
is present in the list (which animals have 0 or 2 legs or wings)>>>df.isin([0,2])num_legs  num_wingsfalcon      True       Truedog        False       TrueTo check ifvaluesisnotin the DataFrame, use the~operator:>>>~df.isin([0,2])num_legs  num_wingsfalcon     False      Falsedog         True      FalseWhenvaluesis a dict, we can pass values to check for each
column separately:>>>df.isin({'num_wings':[0,3]})num_legs  num_wingsfalcon     False      Falsedog        False       TrueWhenvaluesis a Series or DataFrame the index and column must
match. Note that efalconf does not match based on the number of legs
in other.>>>other=pd.DataFrame({'num_legs':[8,3],'num_wings':[0,2]},...index=['spider','falcon'])>>>df.isin(other)num_legs  num_wingsfalcon     False       Truedog        False      False"
Pandas,DataFrame,pandas.DataFrame.where,"pandas.DataFrame.where#DataFrame.where(cond,other=nan,*,inplace=False,axis=None,level=None)[source]#Replace values where the condition is False.Parameters:condbool Series/DataFrame, array-like, or callableWherecondis True, keep the original value. Where
False, replace with corresponding value fromother.
Ifcondis callable, it is computed on the Series/DataFrame and
should return boolean Series/DataFrame or array. The callable must
not change input Series/DataFrame (though pandas doesnft check it).otherscalar, Series/DataFrame, or callableEntries wherecondis False are replaced with
corresponding value fromother.
If other is callable, it is computed on the Series/DataFrame and
should return scalar or Series/DataFrame. The callable must not
change input Series/DataFrame (though pandas doesnft check it).
If not specified, entries will be filled with the corresponding
NULL value (np.nanfor numpy dtypes,pd.NAfor extension
dtypes).inplacebool, default FalseWhether to perform the operation in place on the data.axisint, default NoneAlignment axis if needed. ForSeriesthis parameter is
unused and defaults to 0.levelint, default NoneAlignment level if needed.Returns:Same type as caller or None ifinplace=True.See alsoDataFrame.mask()Return an object of same shape as self.NotesThe where method is an application of the if-then idiom. For each
element in the calling DataFrame, ifcondisTruethe
element is used; otherwise the corresponding element from the DataFrameotheris used. If the axis ofotherdoes not align with axis ofcondSeries/DataFrame, the misaligned index positions will be filled with
False.The signature forDataFrame.where()differs fromnumpy.where(). Roughlydf1.where(m,df2)is equivalent tonp.where(m,df1,df2).For further details and examples see thewheredocumentation inindexing.The dtype of the object takes precedence. The fill value is casted to
the objectfs dtype, if this can be done losslessly.Examples>>>s=pd.Series(range(5))>>>s.where(s>0)0    NaN1    1.02    2.03    3.04    4.0dtype: float64>>>s.mask(s>0)0    0.01    NaN2    NaN3    NaN4    NaNdtype: float64>>>s=pd.Series(range(5))>>>t=pd.Series([True,False])>>>s.where(t,99)0     01    992    993    994    99dtype: int64>>>s.mask(t,99)0    991     12    993    994    99dtype: int64>>>s.where(s>1,10)0    101    102    23    34    4dtype: int64>>>s.mask(s>1,10)0     01     12    103    104    10dtype: int64>>>df=pd.DataFrame(np.arange(10).reshape(-1,2),columns=['A','B'])>>>dfA  B0  0  11  2  32  4  53  6  74  8  9>>>m=df%3==0>>>df.where(m,-df)A  B0  0 -11 -2  32 -4 -53  6 -74 -8  9>>>df.where(m,-df)==np.where(m,df,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True>>>df.where(m,-df)==df.mask(~m,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True"
Pandas,DataFrame,pandas.DataFrame.mask,"pandas.DataFrame.mask#DataFrame.mask(cond,other=_NoDefault.no_default,*,inplace=False,axis=None,level=None)[source]#Replace values where the condition is True.Parameters:condbool Series/DataFrame, array-like, or callableWherecondis False, keep the original value. Where
True, replace with corresponding value fromother.
Ifcondis callable, it is computed on the Series/DataFrame and
should return boolean Series/DataFrame or array. The callable must
not change input Series/DataFrame (though pandas doesnft check it).otherscalar, Series/DataFrame, or callableEntries wherecondis True are replaced with
corresponding value fromother.
If other is callable, it is computed on the Series/DataFrame and
should return scalar or Series/DataFrame. The callable must not
change input Series/DataFrame (though pandas doesnft check it).
If not specified, entries will be filled with the corresponding
NULL value (np.nanfor numpy dtypes,pd.NAfor extension
dtypes).inplacebool, default FalseWhether to perform the operation in place on the data.axisint, default NoneAlignment axis if needed. ForSeriesthis parameter is
unused and defaults to 0.levelint, default NoneAlignment level if needed.Returns:Same type as caller or None ifinplace=True.See alsoDataFrame.where()Return an object of same shape as self.NotesThe mask method is an application of the if-then idiom. For each
element in the calling DataFrame, ifcondisFalsethe
element is used; otherwise the corresponding element from the DataFrameotheris used. If the axis ofotherdoes not align with axis ofcondSeries/DataFrame, the misaligned index positions will be filled with
True.The signature forDataFrame.where()differs fromnumpy.where(). Roughlydf1.where(m,df2)is equivalent tonp.where(m,df1,df2).For further details and examples see themaskdocumentation inindexing.The dtype of the object takes precedence. The fill value is casted to
the objectfs dtype, if this can be done losslessly.Examples>>>s=pd.Series(range(5))>>>s.where(s>0)0    NaN1    1.02    2.03    3.04    4.0dtype: float64>>>s.mask(s>0)0    0.01    NaN2    NaN3    NaN4    NaNdtype: float64>>>s=pd.Series(range(5))>>>t=pd.Series([True,False])>>>s.where(t,99)0     01    992    993    994    99dtype: int64>>>s.mask(t,99)0    991     12    993    994    99dtype: int64>>>s.where(s>1,10)0    101    102    23    34    4dtype: int64>>>s.mask(s>1,10)0     01     12    103    104    10dtype: int64>>>df=pd.DataFrame(np.arange(10).reshape(-1,2),columns=['A','B'])>>>dfA  B0  0  11  2  32  4  53  6  74  8  9>>>m=df%3==0>>>df.where(m,-df)A  B0  0 -11 -2  32 -4 -53  6 -74 -8  9>>>df.where(m,-df)==np.where(m,df,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True>>>df.where(m,-df)==df.mask(~m,-df)A     B0  True  True1  True  True2  True  True3  True  True4  True  True"
Pandas,DataFrame,pandas.DataFrame.query,"pandas.DataFrame.query#DataFrame.query(expr,*,inplace=False,**kwargs)[source]#Query the columns of a DataFrame with a boolean expression.Parameters:exprstrThe query string to evaluate.You can refer to variables
in the environment by prefixing them with an e@f character like@a+b.You can refer to column names that are not valid Python variable names
by surrounding them in backticks. Thus, column names containing spaces
or punctuations (besides underscores) or starting with digits must be
surrounded by backticks. (For example, a column named gArea (cm^2)h would
be referenced as`Area(cm^2)`). Column names which are Python keywords
(like glisth, gforh, gimporth, etc) cannot be used.For example, if one of your columns is calledaaand you want
to sum it withb, your query should be`aa`+b.inplaceboolWhether to modify the DataFrame rather than creating a new one.**kwargsSee the documentation foreval()for complete details
on the keyword arguments accepted byDataFrame.query().Returns:DataFrame or NoneDataFrame resulting from the provided query expression or
None ifinplace=True.See alsoevalEvaluate a string describing operations on DataFrame columns.DataFrame.evalEvaluate a string describing operations on DataFrame columns.NotesThe result of the evaluation of this expression is first passed toDataFrame.locand if that fails because of a
multidimensional key (e.g., a DataFrame) then the result will be passed
toDataFrame.__getitem__().This method uses the top-leveleval()function to
evaluate the passed query.Thequery()method uses a slightly
modified Python syntax by default. For example, the&and|(bitwise) operators have the precedence of their boolean cousins,andandor. Thisissyntactically valid Python,
however the semantics are different.You can change the semantics of the expression by passing the keyword
argumentparser='python'. This enforces the same semantics as
evaluation in Python space. Likewise, you can passengine='python'to evaluate an expression using Python itself as a backend. This is not
recommended as it is inefficient compared to usingnumexpras the
engine.TheDataFrame.indexandDataFrame.columnsattributes of theDataFrameinstance are placed in the query namespace
by default, which allows you to treat both the index and columns of the
frame as a column in the frame.
The identifierindexis used for the frame index; you can also
use the name of the index to identify it in a query. Please note that
Python keywords may not be used as identifiers.For further details and examples see thequerydocumentation inindexing.Backtick quoted variablesBacktick quoted variables are parsed as literal Python code and
are converted internally to a Python valid identifier.
This can lead to the following problems.During parsing a number of disallowed characters inside the backtick
quoted string are replaced by strings that are allowed as a Python identifier.
These characters include all operators in Python, the space character, the
question mark, the exclamation mark, the dollar sign, and the euro sign.
For other characters that fall outside the ASCII range (U+0001..U+007F)
and those that are not further specified in PEP 3131,
the query parser will raise an error.
This excludes whitespace different than the space character,
but also the hashtag (as it is used for comments) and the backtick
itself (backtick can also not be escaped).In a special case, quotes that make a pair around a backtick can
confuse the parser.
For example,`it's`>`that's`will raise an error,
as it forms a quoted string ('s>`that') with a backtick inside.See also the Python documentation about lexical analysis
(https://docs.python.org/3/reference/lexical_analysis.html)
in combination with the source code inpandas.core.computation.parsing.Examples>>>df=pd.DataFrame({'A':range(1,6),...'B':range(10,0,-2),...'C C':range(10,5,-1)})>>>dfA   B  C C0  1  10   101  2   8    92  3   6    83  4   4    74  5   2    6>>>df.query('A > B')A  B  C C4  5  2    6The previous expression is equivalent to>>>df[df.A>df.B]A  B  C C4  5  2    6For columns with spaces in their name, you can use backtick quoting.>>>df.query('B == `C C`')A   B  C C0  1  10   10The previous expression is equivalent to>>>df[df.B==df['C C']]A   B  C C0  1  10   10"
Pandas,DataFrame,pandas.DataFrame.__add__,"pandas.DataFrame.__add__#DataFrame.__add__(other)[source]#Get Addition of DataFrame and other, column-wise.Equivalent toDataFrame.add(other).Parameters:otherscalar, sequence, Series, dict or DataFrameObject to be added to the DataFrame.Returns:DataFrameThe result of addingotherto DataFrame.See alsoDataFrame.addAdd a DataFrame and another object, with option for index- or column-oriented addition.Examples>>>df=pd.DataFrame({'height':[1.5,2.6],'weight':[500,800]},...index=['elk','moose'])>>>dfheight  weightelk       1.5     500moose     2.6     800Adding a scalar affects all rows and columns.>>>df[['height','weight']]+1.5height  weightelk       3.0   501.5moose     4.1   801.5Each element of a list is added to a column of the DataFrame, in order.>>>df[['height','weight']]+[0.5,1.5]height  weightelk       2.0   501.5moose     3.1   801.5Keys of a dictionary are aligned to the DataFrame, based on column names;
each value in the dictionary is added to the corresponding column.>>>df[['height','weight']]+{'height':0.5,'weight':1.5}height  weightelk       2.0   501.5moose     3.1   801.5Whenotheris aSeries, the index ofotheris aligned with the
columns of the DataFrame.>>>s1=pd.Series([0.5,1.5],index=['weight','height'])>>>df[['height','weight']]+s1height  weightelk       3.0   500.5moose     4.1   800.5Even when the index ofotheris the same as the index of the DataFrame,
theSerieswill not be reoriented. If index-wise alignment is desired,DataFrame.add()should be used withaxis=findexf.>>>s2=pd.Series([0.5,1.5],index=['elk','moose'])>>>df[['height','weight']]+s2elk  height  moose  weightelk    NaN     NaN    NaN     NaNmoose  NaN     NaN    NaN     NaN>>>df[['height','weight']].add(s2,axis='index')height  weightelk       2.0   500.5moose     4.1   801.5Whenotheris aDataFrame, both columns names and the
index are aligned.>>>other=pd.DataFrame({'height':[0.2,0.4,0.6]},...index=['elk','moose','deer'])>>>df[['height','weight']]+otherheight  weightdeer      NaN     NaNelk       1.7     NaNmoose     3.0     NaN"
Pandas,DataFrame,pandas.DataFrame.add,"pandas.DataFrame.add#DataFrame.add(other,axis='columns',level=None,fill_value=None)[source]#Get Addition of dataframe and other, element-wise (binary operatoradd).Equivalent todataframe+other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,radd.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.sub,"pandas.DataFrame.sub#DataFrame.sub(other,axis='columns',level=None,fill_value=None)[source]#Get Subtraction of dataframe and other, element-wise (binary operatorsub).Equivalent todataframe-other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rsub.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.mul,"pandas.DataFrame.mul#DataFrame.mul(other,axis='columns',level=None,fill_value=None)[source]#Get Multiplication of dataframe and other, element-wise (binary operatormul).Equivalent todataframe*other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rmul.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.div,"pandas.DataFrame.div#DataFrame.div(other,axis='columns',level=None,fill_value=None)[source]#Get Floating division of dataframe and other, element-wise (binary operatortruediv).Equivalent todataframe/other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rtruediv.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.truediv,"pandas.DataFrame.truediv#DataFrame.truediv(other,axis='columns',level=None,fill_value=None)[source]#Get Floating division of dataframe and other, element-wise (binary operatortruediv).Equivalent todataframe/other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rtruediv.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.floordiv,"pandas.DataFrame.floordiv#DataFrame.floordiv(other,axis='columns',level=None,fill_value=None)[source]#Get Integer division of dataframe and other, element-wise (binary operatorfloordiv).Equivalent todataframe//other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rfloordiv.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.mod,"pandas.DataFrame.mod#DataFrame.mod(other,axis='columns',level=None,fill_value=None)[source]#Get Modulo of dataframe and other, element-wise (binary operatormod).Equivalent todataframe%other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rmod.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.pow,"pandas.DataFrame.pow#DataFrame.pow(other,axis='columns',level=None,fill_value=None)[source]#Get Exponential power of dataframe and other, element-wise (binary operatorpow).Equivalent todataframe**other, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,rpow.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.dot,"pandas.DataFrame.dot#DataFrame.dot(other)[source]#Compute the matrix multiplication between the DataFrame and other.This method computes the matrix product between the DataFrame and the
values of an other Series, DataFrame or a numpy array.It can also be called usingself@other.Parameters:otherSeries, DataFrame or array-likeThe other object to compute the matrix product with.Returns:Series or DataFrameIf other is a Series, return the matrix product between self and
other as a Series. If other is a DataFrame or a numpy.array, return
the matrix product of self and other in a DataFrame of a np.array.See alsoSeries.dotSimilar method for Series.NotesThe dimensions of DataFrame and other must be compatible in order to
compute the matrix multiplication. In addition, the column names of
DataFrame and the index of other must contain the same values, as they
will be aligned prior to the multiplication.The dot method for Series computes the inner product, instead of the
matrix product here.ExamplesHere we multiply a DataFrame with a Series.>>>df=pd.DataFrame([[0,1,-2,-1],[1,1,1,1]])>>>s=pd.Series([1,1,2,1])>>>df.dot(s)0    -41     5dtype: int64Here we multiply a DataFrame with another DataFrame.>>>other=pd.DataFrame([[0,1],[1,2],[-1,-1],[2,0]])>>>df.dot(other)0   10   1   41   2   2Note that the dot method give the same result as @>>>df@other0   10   1   41   2   2The dot method works also if other is an np.array.>>>arr=np.array([[0,1],[1,2],[-1,-1],[2,0]])>>>df.dot(arr)0   10   1   41   2   2Note how shuffling of the objects does not change the result.>>>s2=s.reindex([1,0,2,3])>>>df.dot(s2)0    -41     5dtype: int64"
Pandas,DataFrame,pandas.DataFrame.radd,"pandas.DataFrame.radd#DataFrame.radd(other,axis='columns',level=None,fill_value=None)[source]#Get Addition of dataframe and other, element-wise (binary operatorradd).Equivalent toother+dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,add.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rsub,"pandas.DataFrame.rsub#DataFrame.rsub(other,axis='columns',level=None,fill_value=None)[source]#Get Subtraction of dataframe and other, element-wise (binary operatorrsub).Equivalent toother-dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,sub.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rmul,"pandas.DataFrame.rmul#DataFrame.rmul(other,axis='columns',level=None,fill_value=None)[source]#Get Multiplication of dataframe and other, element-wise (binary operatorrmul).Equivalent toother*dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,mul.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rdiv,"pandas.DataFrame.rdiv#DataFrame.rdiv(other,axis='columns',level=None,fill_value=None)[source]#Get Floating division of dataframe and other, element-wise (binary operatorrtruediv).Equivalent toother/dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,truediv.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rtruediv,"pandas.DataFrame.rtruediv#DataFrame.rtruediv(other,axis='columns',level=None,fill_value=None)[source]#Get Floating division of dataframe and other, element-wise (binary operatorrtruediv).Equivalent toother/dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,truediv.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rfloordiv,"pandas.DataFrame.rfloordiv#DataFrame.rfloordiv(other,axis='columns',level=None,fill_value=None)[source]#Get Integer division of dataframe and other, element-wise (binary operatorrfloordiv).Equivalent toother//dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,floordiv.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rmod,"pandas.DataFrame.rmod#DataFrame.rmod(other,axis='columns',level=None,fill_value=None)[source]#Get Modulo of dataframe and other, element-wise (binary operatorrmod).Equivalent toother%dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,mod.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.rpow,"pandas.DataFrame.rpow#DataFrame.rpow(other,axis='columns',level=None,fill_value=None)[source]#Get Exponential power of dataframe and other, element-wise (binary operatorrpow).Equivalent toother**dataframe, but with support to substitute a fill_value
for missing data in one of the inputs. With reverse version,pow.Among flexible wrappers (add,sub,mul,div,floordiv,mod,pow) to
arithmetic operators:+,-,*,/,//,%,**.Parameters:otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}Whether to compare by the index (0 or eindexf) or columns.
(1 or ecolumnsf). For Series input, axis to match Series index on.levelint or labelBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for
successful DataFrame alignment, with this value before computation.
If data in both corresponding DataFrame locations is missing
the result will be missing.Returns:DataFrameResult of the arithmetic operation.See alsoDataFrame.addAdd DataFrames.DataFrame.subSubtract DataFrames.DataFrame.mulMultiply DataFrames.DataFrame.divDivide DataFrames (float division).DataFrame.truedivDivide DataFrames (float division).DataFrame.floordivDivide DataFrames (integer division).DataFrame.modCalculate modulo (remainder after division).DataFrame.powCalculate exponential power.NotesMismatched indices will be unioned together.Examples>>>df=pd.DataFrame({'angles':[0,3,4],...'degrees':[360,180,360]},...index=['circle','triangle','rectangle'])>>>dfangles  degreescircle          0      360triangle        3      180rectangle       4      360Add a scalar with operator version which return the same
results.>>>df+1angles  degreescircle          1      361triangle        4      181rectangle       5      361>>>df.add(1)angles  degreescircle          1      361triangle        4      181rectangle       5      361Divide by constant with reverse version.>>>df.div(10)angles  degreescircle        0.0     36.0triangle      0.3     18.0rectangle     0.4     36.0>>>df.rdiv(10)angles   degreescircle          inf  0.027778triangle   3.333333  0.055556rectangle  2.500000  0.027778Subtract a list and Series by axis with operator version.>>>df-[1,2]angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub([1,2],axis='columns')angles  degreescircle         -1      358triangle        2      178rectangle       3      358>>>df.sub(pd.Series([1,1,1],index=['circle','triangle','rectangle']),...axis='index')angles  degreescircle         -1      359triangle        2      179rectangle       3      359Multiply a dictionary by axis.>>>df.mul({'angles':0,'degrees':2})angles  degreescircle           0      720triangle         0      360rectangle        0      720>>>df.mul({'circle':0,'triangle':2,'rectangle':3},axis='index')angles  degreescircle           0        0triangle         6      360rectangle       12     1080Multiply a DataFrame of different shape with operator version.>>>other=pd.DataFrame({'angles':[0,3,4]},...index=['circle','triangle','rectangle'])>>>otheranglescircle          0triangle        3rectangle       4>>>df*otherangles  degreescircle          0      NaNtriangle        9      NaNrectangle      16      NaN>>>df.mul(other,fill_value=0)angles  degreescircle          0      0.0triangle        9      0.0rectangle      16      0.0Divide by a MultiIndex by level.>>>df_multindex=pd.DataFrame({'angles':[0,3,4,4,5,6],...'degrees':[360,180,360,360,540,720]},...index=[['A','A','A','B','B','B'],...['circle','triangle','rectangle',...'square','pentagon','hexagon']])>>>df_multindexangles  degreesA circle          0      360triangle        3      180rectangle       4      360B square          4      360pentagon        5      540hexagon         6      720>>>df.div(df_multindex,level=1,fill_value=0)angles  degreesA circle        NaN      1.0triangle      1.0      1.0rectangle     1.0      1.0B square        0.0      0.0pentagon      0.0      0.0hexagon       0.0      0.0"
Pandas,DataFrame,pandas.DataFrame.lt,"pandas.DataFrame.lt#DataFrame.lt(other,axis='columns',level=None)[source]#Get Less than of dataframe and other, element-wise (binary operatorlt).Among flexible wrappers (eq,ne,le,lt,ge,gt) to comparison
operators.Equivalent to==,!=,<=,<,>=,>with support to choose axis
(rows or columns) and level for comparison.Parameters:otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}, default ecolumnsfWhether to compare by the index (0 or eindexf) or columns
(1 or ecolumnsf).levelint or labelBroadcast across a level, matching Index values on the passed
MultiIndex level.Returns:DataFrame of boolResult of the comparison.See alsoDataFrame.eqCompare DataFrames for equality elementwise.DataFrame.neCompare DataFrames for inequality elementwise.DataFrame.leCompare DataFrames for less than inequality or equality elementwise.DataFrame.ltCompare DataFrames for strictly less than inequality elementwise.DataFrame.geCompare DataFrames for greater than inequality or equality elementwise.DataFrame.gtCompare DataFrames for strictly greater than inequality elementwise.NotesMismatched indices will be unioned together.NaNvalues are considered different (i.e.NaN!=NaN).Examples>>>df=pd.DataFrame({'cost':[250,150,100],...'revenue':[100,250,300]},...index=['A','B','C'])>>>dfcost  revenueA   250      100B   150      250C   100      300Comparison with a scalar, using either the operator or method:>>>df==100cost  revenueA  False     TrueB  False    FalseC   True    False>>>df.eq(100)cost  revenueA  False     TrueB  False    FalseC   True    FalseWhenotheris aSeries, the columns of a DataFrame are aligned
with the index ofotherand broadcast:>>>df!=pd.Series([100,250],index=[""cost"",""revenue""])cost  revenueA   True     TrueB   True    FalseC  False     TrueUse the method to control the broadcast axis:>>>df.ne(pd.Series([100,300],index=[""A"",""D""]),axis='index')cost  revenueA  True    FalseB  True     TrueC  True     TrueD  True     TrueWhen comparing to an arbitrary sequence, the number of columns must
match the number elements inother:>>>df==[250,100]cost  revenueA   True     TrueB  False    FalseC  False    FalseUse the method to control the axis:>>>df.eq([250,250,100],axis='index')cost  revenueA   True    FalseB  False     TrueC   True    FalseCompare to a DataFrame of different shape.>>>other=pd.DataFrame({'revenue':[300,250,100,150]},...index=['A','B','C','D'])>>>otherrevenueA      300B      250C      100D      150>>>df.gt(other)cost  revenueA  False    FalseB  False    FalseC  False     TrueD  False    FalseCompare to a MultiIndex by level.>>>df_multindex=pd.DataFrame({'cost':[250,150,100,150,300,220],...'revenue':[100,250,300,200,175,225]},...index=[['Q1','Q1','Q1','Q2','Q2','Q2'],...['A','B','C','A','B','C']])>>>df_multindexcost  revenueQ1 A   250      100B   150      250C   100      300Q2 A   150      200B   300      175C   220      225>>>df.le(df_multindex,level=1)cost  revenueQ1 A   True     TrueB   True     TrueC   True     TrueQ2 A  False     TrueB   True    FalseC   True    False"
Pandas,DataFrame,pandas.DataFrame.gt,"pandas.DataFrame.gt#DataFrame.gt(other,axis='columns',level=None)[source]#Get Greater than of dataframe and other, element-wise (binary operatorgt).Among flexible wrappers (eq,ne,le,lt,ge,gt) to comparison
operators.Equivalent to==,!=,<=,<,>=,>with support to choose axis
(rows or columns) and level for comparison.Parameters:otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}, default ecolumnsfWhether to compare by the index (0 or eindexf) or columns
(1 or ecolumnsf).levelint or labelBroadcast across a level, matching Index values on the passed
MultiIndex level.Returns:DataFrame of boolResult of the comparison.See alsoDataFrame.eqCompare DataFrames for equality elementwise.DataFrame.neCompare DataFrames for inequality elementwise.DataFrame.leCompare DataFrames for less than inequality or equality elementwise.DataFrame.ltCompare DataFrames for strictly less than inequality elementwise.DataFrame.geCompare DataFrames for greater than inequality or equality elementwise.DataFrame.gtCompare DataFrames for strictly greater than inequality elementwise.NotesMismatched indices will be unioned together.NaNvalues are considered different (i.e.NaN!=NaN).Examples>>>df=pd.DataFrame({'cost':[250,150,100],...'revenue':[100,250,300]},...index=['A','B','C'])>>>dfcost  revenueA   250      100B   150      250C   100      300Comparison with a scalar, using either the operator or method:>>>df==100cost  revenueA  False     TrueB  False    FalseC   True    False>>>df.eq(100)cost  revenueA  False     TrueB  False    FalseC   True    FalseWhenotheris aSeries, the columns of a DataFrame are aligned
with the index ofotherand broadcast:>>>df!=pd.Series([100,250],index=[""cost"",""revenue""])cost  revenueA   True     TrueB   True    FalseC  False     TrueUse the method to control the broadcast axis:>>>df.ne(pd.Series([100,300],index=[""A"",""D""]),axis='index')cost  revenueA  True    FalseB  True     TrueC  True     TrueD  True     TrueWhen comparing to an arbitrary sequence, the number of columns must
match the number elements inother:>>>df==[250,100]cost  revenueA   True     TrueB  False    FalseC  False    FalseUse the method to control the axis:>>>df.eq([250,250,100],axis='index')cost  revenueA   True    FalseB  False     TrueC   True    FalseCompare to a DataFrame of different shape.>>>other=pd.DataFrame({'revenue':[300,250,100,150]},...index=['A','B','C','D'])>>>otherrevenueA      300B      250C      100D      150>>>df.gt(other)cost  revenueA  False    FalseB  False    FalseC  False     TrueD  False    FalseCompare to a MultiIndex by level.>>>df_multindex=pd.DataFrame({'cost':[250,150,100,150,300,220],...'revenue':[100,250,300,200,175,225]},...index=[['Q1','Q1','Q1','Q2','Q2','Q2'],...['A','B','C','A','B','C']])>>>df_multindexcost  revenueQ1 A   250      100B   150      250C   100      300Q2 A   150      200B   300      175C   220      225>>>df.le(df_multindex,level=1)cost  revenueQ1 A   True     TrueB   True     TrueC   True     TrueQ2 A  False     TrueB   True    FalseC   True    False"
Pandas,DataFrame,pandas.DataFrame.le,"pandas.DataFrame.le#DataFrame.le(other,axis='columns',level=None)[source]#Get Less than or equal to of dataframe and other, element-wise (binary operatorle).Among flexible wrappers (eq,ne,le,lt,ge,gt) to comparison
operators.Equivalent to==,!=,<=,<,>=,>with support to choose axis
(rows or columns) and level for comparison.Parameters:otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}, default ecolumnsfWhether to compare by the index (0 or eindexf) or columns
(1 or ecolumnsf).levelint or labelBroadcast across a level, matching Index values on the passed
MultiIndex level.Returns:DataFrame of boolResult of the comparison.See alsoDataFrame.eqCompare DataFrames for equality elementwise.DataFrame.neCompare DataFrames for inequality elementwise.DataFrame.leCompare DataFrames for less than inequality or equality elementwise.DataFrame.ltCompare DataFrames for strictly less than inequality elementwise.DataFrame.geCompare DataFrames for greater than inequality or equality elementwise.DataFrame.gtCompare DataFrames for strictly greater than inequality elementwise.NotesMismatched indices will be unioned together.NaNvalues are considered different (i.e.NaN!=NaN).Examples>>>df=pd.DataFrame({'cost':[250,150,100],...'revenue':[100,250,300]},...index=['A','B','C'])>>>dfcost  revenueA   250      100B   150      250C   100      300Comparison with a scalar, using either the operator or method:>>>df==100cost  revenueA  False     TrueB  False    FalseC   True    False>>>df.eq(100)cost  revenueA  False     TrueB  False    FalseC   True    FalseWhenotheris aSeries, the columns of a DataFrame are aligned
with the index ofotherand broadcast:>>>df!=pd.Series([100,250],index=[""cost"",""revenue""])cost  revenueA   True     TrueB   True    FalseC  False     TrueUse the method to control the broadcast axis:>>>df.ne(pd.Series([100,300],index=[""A"",""D""]),axis='index')cost  revenueA  True    FalseB  True     TrueC  True     TrueD  True     TrueWhen comparing to an arbitrary sequence, the number of columns must
match the number elements inother:>>>df==[250,100]cost  revenueA   True     TrueB  False    FalseC  False    FalseUse the method to control the axis:>>>df.eq([250,250,100],axis='index')cost  revenueA   True    FalseB  False     TrueC   True    FalseCompare to a DataFrame of different shape.>>>other=pd.DataFrame({'revenue':[300,250,100,150]},...index=['A','B','C','D'])>>>otherrevenueA      300B      250C      100D      150>>>df.gt(other)cost  revenueA  False    FalseB  False    FalseC  False     TrueD  False    FalseCompare to a MultiIndex by level.>>>df_multindex=pd.DataFrame({'cost':[250,150,100,150,300,220],...'revenue':[100,250,300,200,175,225]},...index=[['Q1','Q1','Q1','Q2','Q2','Q2'],...['A','B','C','A','B','C']])>>>df_multindexcost  revenueQ1 A   250      100B   150      250C   100      300Q2 A   150      200B   300      175C   220      225>>>df.le(df_multindex,level=1)cost  revenueQ1 A   True     TrueB   True     TrueC   True     TrueQ2 A  False     TrueB   True    FalseC   True    False"
Pandas,DataFrame,pandas.DataFrame.ge,"pandas.DataFrame.ge#DataFrame.ge(other,axis='columns',level=None)[source]#Get Greater than or equal to of dataframe and other, element-wise (binary operatorge).Among flexible wrappers (eq,ne,le,lt,ge,gt) to comparison
operators.Equivalent to==,!=,<=,<,>=,>with support to choose axis
(rows or columns) and level for comparison.Parameters:otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}, default ecolumnsfWhether to compare by the index (0 or eindexf) or columns
(1 or ecolumnsf).levelint or labelBroadcast across a level, matching Index values on the passed
MultiIndex level.Returns:DataFrame of boolResult of the comparison.See alsoDataFrame.eqCompare DataFrames for equality elementwise.DataFrame.neCompare DataFrames for inequality elementwise.DataFrame.leCompare DataFrames for less than inequality or equality elementwise.DataFrame.ltCompare DataFrames for strictly less than inequality elementwise.DataFrame.geCompare DataFrames for greater than inequality or equality elementwise.DataFrame.gtCompare DataFrames for strictly greater than inequality elementwise.NotesMismatched indices will be unioned together.NaNvalues are considered different (i.e.NaN!=NaN).Examples>>>df=pd.DataFrame({'cost':[250,150,100],...'revenue':[100,250,300]},...index=['A','B','C'])>>>dfcost  revenueA   250      100B   150      250C   100      300Comparison with a scalar, using either the operator or method:>>>df==100cost  revenueA  False     TrueB  False    FalseC   True    False>>>df.eq(100)cost  revenueA  False     TrueB  False    FalseC   True    FalseWhenotheris aSeries, the columns of a DataFrame are aligned
with the index ofotherand broadcast:>>>df!=pd.Series([100,250],index=[""cost"",""revenue""])cost  revenueA   True     TrueB   True    FalseC  False     TrueUse the method to control the broadcast axis:>>>df.ne(pd.Series([100,300],index=[""A"",""D""]),axis='index')cost  revenueA  True    FalseB  True     TrueC  True     TrueD  True     TrueWhen comparing to an arbitrary sequence, the number of columns must
match the number elements inother:>>>df==[250,100]cost  revenueA   True     TrueB  False    FalseC  False    FalseUse the method to control the axis:>>>df.eq([250,250,100],axis='index')cost  revenueA   True    FalseB  False     TrueC   True    FalseCompare to a DataFrame of different shape.>>>other=pd.DataFrame({'revenue':[300,250,100,150]},...index=['A','B','C','D'])>>>otherrevenueA      300B      250C      100D      150>>>df.gt(other)cost  revenueA  False    FalseB  False    FalseC  False     TrueD  False    FalseCompare to a MultiIndex by level.>>>df_multindex=pd.DataFrame({'cost':[250,150,100,150,300,220],...'revenue':[100,250,300,200,175,225]},...index=[['Q1','Q1','Q1','Q2','Q2','Q2'],...['A','B','C','A','B','C']])>>>df_multindexcost  revenueQ1 A   250      100B   150      250C   100      300Q2 A   150      200B   300      175C   220      225>>>df.le(df_multindex,level=1)cost  revenueQ1 A   True     TrueB   True     TrueC   True     TrueQ2 A  False     TrueB   True    FalseC   True    False"
Pandas,DataFrame,pandas.DataFrame.ne,"pandas.DataFrame.ne#DataFrame.ne(other,axis='columns',level=None)[source]#Get Not equal to of dataframe and other, element-wise (binary operatorne).Among flexible wrappers (eq,ne,le,lt,ge,gt) to comparison
operators.Equivalent to==,!=,<=,<,>=,>with support to choose axis
(rows or columns) and level for comparison.Parameters:otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}, default ecolumnsfWhether to compare by the index (0 or eindexf) or columns
(1 or ecolumnsf).levelint or labelBroadcast across a level, matching Index values on the passed
MultiIndex level.Returns:DataFrame of boolResult of the comparison.See alsoDataFrame.eqCompare DataFrames for equality elementwise.DataFrame.neCompare DataFrames for inequality elementwise.DataFrame.leCompare DataFrames for less than inequality or equality elementwise.DataFrame.ltCompare DataFrames for strictly less than inequality elementwise.DataFrame.geCompare DataFrames for greater than inequality or equality elementwise.DataFrame.gtCompare DataFrames for strictly greater than inequality elementwise.NotesMismatched indices will be unioned together.NaNvalues are considered different (i.e.NaN!=NaN).Examples>>>df=pd.DataFrame({'cost':[250,150,100],...'revenue':[100,250,300]},...index=['A','B','C'])>>>dfcost  revenueA   250      100B   150      250C   100      300Comparison with a scalar, using either the operator or method:>>>df==100cost  revenueA  False     TrueB  False    FalseC   True    False>>>df.eq(100)cost  revenueA  False     TrueB  False    FalseC   True    FalseWhenotheris aSeries, the columns of a DataFrame are aligned
with the index ofotherand broadcast:>>>df!=pd.Series([100,250],index=[""cost"",""revenue""])cost  revenueA   True     TrueB   True    FalseC  False     TrueUse the method to control the broadcast axis:>>>df.ne(pd.Series([100,300],index=[""A"",""D""]),axis='index')cost  revenueA  True    FalseB  True     TrueC  True     TrueD  True     TrueWhen comparing to an arbitrary sequence, the number of columns must
match the number elements inother:>>>df==[250,100]cost  revenueA   True     TrueB  False    FalseC  False    FalseUse the method to control the axis:>>>df.eq([250,250,100],axis='index')cost  revenueA   True    FalseB  False     TrueC   True    FalseCompare to a DataFrame of different shape.>>>other=pd.DataFrame({'revenue':[300,250,100,150]},...index=['A','B','C','D'])>>>otherrevenueA      300B      250C      100D      150>>>df.gt(other)cost  revenueA  False    FalseB  False    FalseC  False     TrueD  False    FalseCompare to a MultiIndex by level.>>>df_multindex=pd.DataFrame({'cost':[250,150,100,150,300,220],...'revenue':[100,250,300,200,175,225]},...index=[['Q1','Q1','Q1','Q2','Q2','Q2'],...['A','B','C','A','B','C']])>>>df_multindexcost  revenueQ1 A   250      100B   150      250C   100      300Q2 A   150      200B   300      175C   220      225>>>df.le(df_multindex,level=1)cost  revenueQ1 A   True     TrueB   True     TrueC   True     TrueQ2 A  False     TrueB   True    FalseC   True    False"
Pandas,DataFrame,pandas.DataFrame.eq,"pandas.DataFrame.eq#DataFrame.eq(other,axis='columns',level=None)[source]#Get Equal to of dataframe and other, element-wise (binary operatoreq).Among flexible wrappers (eq,ne,le,lt,ge,gt) to comparison
operators.Equivalent to==,!=,<=,<,>=,>with support to choose axis
(rows or columns) and level for comparison.Parameters:otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object.axis{0 or eindexf, 1 or ecolumnsf}, default ecolumnsfWhether to compare by the index (0 or eindexf) or columns
(1 or ecolumnsf).levelint or labelBroadcast across a level, matching Index values on the passed
MultiIndex level.Returns:DataFrame of boolResult of the comparison.See alsoDataFrame.eqCompare DataFrames for equality elementwise.DataFrame.neCompare DataFrames for inequality elementwise.DataFrame.leCompare DataFrames for less than inequality or equality elementwise.DataFrame.ltCompare DataFrames for strictly less than inequality elementwise.DataFrame.geCompare DataFrames for greater than inequality or equality elementwise.DataFrame.gtCompare DataFrames for strictly greater than inequality elementwise.NotesMismatched indices will be unioned together.NaNvalues are considered different (i.e.NaN!=NaN).Examples>>>df=pd.DataFrame({'cost':[250,150,100],...'revenue':[100,250,300]},...index=['A','B','C'])>>>dfcost  revenueA   250      100B   150      250C   100      300Comparison with a scalar, using either the operator or method:>>>df==100cost  revenueA  False     TrueB  False    FalseC   True    False>>>df.eq(100)cost  revenueA  False     TrueB  False    FalseC   True    FalseWhenotheris aSeries, the columns of a DataFrame are aligned
with the index ofotherand broadcast:>>>df!=pd.Series([100,250],index=[""cost"",""revenue""])cost  revenueA   True     TrueB   True    FalseC  False     TrueUse the method to control the broadcast axis:>>>df.ne(pd.Series([100,300],index=[""A"",""D""]),axis='index')cost  revenueA  True    FalseB  True     TrueC  True     TrueD  True     TrueWhen comparing to an arbitrary sequence, the number of columns must
match the number elements inother:>>>df==[250,100]cost  revenueA   True     TrueB  False    FalseC  False    FalseUse the method to control the axis:>>>df.eq([250,250,100],axis='index')cost  revenueA   True    FalseB  False     TrueC   True    FalseCompare to a DataFrame of different shape.>>>other=pd.DataFrame({'revenue':[300,250,100,150]},...index=['A','B','C','D'])>>>otherrevenueA      300B      250C      100D      150>>>df.gt(other)cost  revenueA  False    FalseB  False    FalseC  False     TrueD  False    FalseCompare to a MultiIndex by level.>>>df_multindex=pd.DataFrame({'cost':[250,150,100,150,300,220],...'revenue':[100,250,300,200,175,225]},...index=[['Q1','Q1','Q1','Q2','Q2','Q2'],...['A','B','C','A','B','C']])>>>df_multindexcost  revenueQ1 A   250      100B   150      250C   100      300Q2 A   150      200B   300      175C   220      225>>>df.le(df_multindex,level=1)cost  revenueQ1 A   True     TrueB   True     TrueC   True     TrueQ2 A  False     TrueB   True    FalseC   True    False"
Pandas,DataFrame,pandas.DataFrame.combine,"pandas.DataFrame.combine#DataFrame.combine(other,func,fill_value=None,overwrite=True)[source]#Perform column-wise combine with another DataFrame.Combines a DataFrame withotherDataFrame usingfuncto element-wise combine columns. The row and column indexes of the
resulting DataFrame will be the union of the two.Parameters:otherDataFrameThe DataFrame to merge column-wise.funcfunctionFunction that takes two series as inputs and return a Series or a
scalar. Used to merge the two dataframes column by columns.fill_valuescalar value, default NoneThe value to fill NaNs with prior to passing any column to the
merge func.overwritebool, default TrueIf True, columns inselfthat do not exist inotherwill be
overwritten with NaNs.Returns:DataFrameCombination of the provided DataFrames.See alsoDataFrame.combine_firstCombine two DataFrame objects and default to non-null values in frame calling the method.ExamplesCombine using a simple function that chooses the smaller column.>>>df1=pd.DataFrame({'A':[0,0],'B':[4,4]})>>>df2=pd.DataFrame({'A':[1,1],'B':[3,3]})>>>take_smaller=lambdas1,s2:s1ifs1.sum()<s2.sum()elses2>>>df1.combine(df2,take_smaller)A  B0  0  31  0  3Example using a true element-wise combine function.>>>df1=pd.DataFrame({'A':[5,0],'B':[2,4]})>>>df2=pd.DataFrame({'A':[1,1],'B':[3,3]})>>>df1.combine(df2,np.minimum)A  B0  1  21  0  3Usingfill_valuefills Nones prior to passing the column to the
merge function.>>>df1=pd.DataFrame({'A':[0,0],'B':[None,4]})>>>df2=pd.DataFrame({'A':[1,1],'B':[3,3]})>>>df1.combine(df2,take_smaller,fill_value=-5)A    B0  0 -5.01  0  4.0However, if the same element in both dataframes is None, that None
is preserved>>>df1=pd.DataFrame({'A':[0,0],'B':[None,4]})>>>df2=pd.DataFrame({'A':[1,1],'B':[None,3]})>>>df1.combine(df2,take_smaller,fill_value=-5)A    B0  0 -5.01  0  3.0Example that demonstrates the use ofoverwriteand behavior when
the axis differ between the dataframes.>>>df1=pd.DataFrame({'A':[0,0],'B':[4,4]})>>>df2=pd.DataFrame({'B':[3,3],'C':[-10,1],},index=[1,2])>>>df1.combine(df2,take_smaller)A    B     C0  NaN  NaN   NaN1  NaN  3.0 -10.02  NaN  3.0   1.0>>>df1.combine(df2,take_smaller,overwrite=False)A    B     C0  0.0  NaN   NaN1  0.0  3.0 -10.02  NaN  3.0   1.0Demonstrating the preference of the passed in dataframe.>>>df2=pd.DataFrame({'B':[3,3],'C':[1,1],},index=[1,2])>>>df2.combine(df1,take_smaller)A    B   C0  0.0  NaN NaN1  0.0  3.0 NaN2  NaN  3.0 NaN>>>df2.combine(df1,take_smaller,overwrite=False)A    B   C0  0.0  NaN NaN1  0.0  3.0 1.02  NaN  3.0 1.0"
Pandas,DataFrame,pandas.DataFrame.combine_first,"pandas.DataFrame.combine_first#DataFrame.combine_first(other)[source]#Update null elements with value in the same location inother.Combine two DataFrame objects by filling null values in one DataFrame
with non-null values from other DataFrame. The row and column indexes
of the resulting DataFrame will be the union of the two. The resulting
dataframe contains the efirstf dataframe values and overrides the
second one values where both first.loc[index, col] and
second.loc[index, col] are not missing values, upon calling
first.combine_first(second).Parameters:otherDataFrameProvided DataFrame to use to fill null values.Returns:DataFrameThe result of combining the provided DataFrame with the other object.See alsoDataFrame.combinePerform series-wise operation on two DataFrames using a given function.Examples>>>df1=pd.DataFrame({'A':[None,0],'B':[None,4]})>>>df2=pd.DataFrame({'A':[1,1],'B':[3,3]})>>>df1.combine_first(df2)A    B0  1.0  3.01  0.0  4.0Null values still persist if the location of that null value
does not exist inother>>>df1=pd.DataFrame({'A':[None,0],'B':[4,None]})>>>df2=pd.DataFrame({'B':[3,3],'C':[1,1]},index=[1,2])>>>df1.combine_first(df2)A    B    C0  NaN  4.0  NaN1  0.0  3.0  1.02  NaN  3.0  1.0"
Pandas,DataFrame,pandas.DataFrame.apply,"pandas.DataFrame.apply#DataFrame.apply(func,axis=0,raw=False,result_type=None,args=(),by_row='compat',**kwargs)[source]#Apply a function along an axis of the DataFrame.Objects passed to the function are Series objects whose index is
either the DataFramefs index (axis=0) or the DataFramefs columns
(axis=1). By default (result_type=None), the final return type
is inferred from the return type of the applied function. Otherwise,
it depends on theresult_typeargument.Parameters:funcfunctionFunction to apply to each column or row.axis{0 or eindexf, 1 or ecolumnsf}, default 0Axis along which the function is applied:0 or eindexf: apply function to each column.1 or ecolumnsf: apply function to each row.rawbool, default FalseDetermines if row or column is passed as a Series or ndarray object:False: passes each row or column as a Series to the
function.True: the passed function will receive ndarray objects
instead.
If you are just applying a NumPy reduction function this will
achieve much better performance.result_type{eexpandf, ereducef, ebroadcastf, None}, default NoneThese only act whenaxis=1(columns):eexpandf : list-like results will be turned into columns.ereducef : returns a Series if possible rather than expanding
list-like results. This is the opposite of eexpandf.ebroadcastf : results will be broadcast to the original shape
of the DataFrame, the original index and columns will be
retained.The default behaviour (None) depends on the return value of the
applied function: list-like results will be returned as a Series
of those. However if the apply function returns a Series these
are expanded to columns.argstuplePositional arguments to pass tofuncin addition to the
array/series.by_rowFalse or gcompath, default gcompathOnly has an effect whenfuncis a listlike or dictlike of funcs
and the func isnft a string.
If gcompath, will if possible first translate the func into pandas
methods (e.g.Series().apply(np.sum)will be translated toSeries().sum()). If that doesnft work, will try call to apply again withby_row=Trueand if that fails, will call apply again withby_row=False(backward compatible).
If False, the funcs will be passed the whole Series at once.New in version 2.1.0.**kwargsAdditional keyword arguments to pass as keywords arguments tofunc.Returns:Series or DataFrameResult of applyingfuncalong the given axis of the
DataFrame.See alsoDataFrame.mapFor elementwise operations.DataFrame.aggregateOnly perform aggregating type operations.DataFrame.transformOnly perform transforming type operations.NotesFunctions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame([[4,9]]*3,columns=['A','B'])>>>dfA  B0  4  91  4  92  4  9Using a numpy universal function (in this case the same asnp.sqrt(df)):>>>df.apply(np.sqrt)A    B0  2.0  3.01  2.0  3.02  2.0  3.0Using a reducing function on either axis>>>df.apply(np.sum,axis=0)A    12B    27dtype: int64>>>df.apply(np.sum,axis=1)0    131    132    13dtype: int64Returning a list-like will result in a Series>>>df.apply(lambdax:[1,2],axis=1)0    [1, 2]1    [1, 2]2    [1, 2]dtype: objectPassingresult_type='expand'will expand list-like results
to columns of a Dataframe>>>df.apply(lambdax:[1,2],axis=1,result_type='expand')0  10  1  21  1  22  1  2Returning a Series inside the function is similar to passingresult_type='expand'. The resulting column names
will be the Series index.>>>df.apply(lambdax:pd.Series([1,2],index=['foo','bar']),axis=1)foo  bar0    1    21    1    22    1    2Passingresult_type='broadcast'will ensure the same shape
result, whether list-like or scalar is returned by the function,
and broadcast it along the axis. The resulting column names will
be the originals.>>>df.apply(lambdax:[1,2],axis=1,result_type='broadcast')A  B0  1  21  1  22  1  2"
Pandas,DataFrame,pandas.DataFrame.map,"pandas.DataFrame.map#DataFrame.map(func,na_action=None,**kwargs)[source]#Apply a function to a Dataframe elementwise.New in version 2.1.0:DataFrame.applymap was deprecated and renamed to DataFrame.map.This method applies a function that accepts and returns a scalar
to every element of a DataFrame.Parameters:funccallablePython function, returns a single value from a single value.na_action{None, eignoref}, default NoneIf eignoref, propagate NaN values, without passing them to func.**kwargsAdditional keyword arguments to pass as keywords arguments tofunc.Returns:DataFrameTransformed DataFrame.See alsoDataFrame.applyApply a function along input axis of DataFrame.DataFrame.replaceReplace values given into_replacewithvalue.Series.mapApply a function elementwise on a Series.Examples>>>df=pd.DataFrame([[1,2.12],[3.356,4.567]])>>>df0      10  1.000  2.1201  3.356  4.567>>>df.map(lambdax:len(str(x)))0  10  3  41  5  5Like Series.map, NA values can be ignored:>>>df_copy=df.copy()>>>df_copy.iloc[0,0]=pd.NA>>>df_copy.map(lambdax:len(str(x)),na_action='ignore')0  10  NaN  41  5.0  5Note that a vectorized version offuncoften exists, which will
be much faster. You could square each number elementwise.>>>df.map(lambdax:x**2)0          10   1.000000   4.4944001  11.262736  20.857489But itfs better to avoid map in that case.>>>df**20          10   1.000000   4.4944001  11.262736  20.857489"
Pandas,DataFrame,pandas.DataFrame.applymap,"pandas.DataFrame.applymap#DataFrame.applymap(func,na_action=None,**kwargs)[source]#Apply a function to a Dataframe elementwise.Deprecated since version 2.1.0:DataFrame.applymap has been deprecated. Use DataFrame.map instead.This method applies a function that accepts and returns a scalar
to every element of a DataFrame.Parameters:funccallablePython function, returns a single value from a single value.na_action{None, eignoref}, default NoneIf eignoref, propagate NaN values, without passing them to func.**kwargsAdditional keyword arguments to pass as keywords arguments tofunc.Returns:DataFrameTransformed DataFrame.See alsoDataFrame.applyApply a function along input axis of DataFrame.DataFrame.mapApply a function along input axis of DataFrame.DataFrame.replaceReplace values given into_replacewithvalue.Examples>>>df=pd.DataFrame([[1,2.12],[3.356,4.567]])>>>df0      10  1.000  2.1201  3.356  4.567>>>df.map(lambdax:len(str(x)))0  10  3  41  5  5"
Pandas,DataFrame,pandas.DataFrame.pipe,"pandas.DataFrame.pipe#DataFrame.pipe(func,*args,**kwargs)[source]#Apply chainable functions that expect Series or DataFrames.Parameters:funcfunctionFunction to apply to the Series/DataFrame.args, andkwargsare passed intofunc.
Alternatively a(callable,data_keyword)tuple wheredata_keywordis a string indicating the keyword ofcallablethat expects the Series/DataFrame.*argsiterable, optionalPositional arguments passed intofunc.**kwargsmapping, optionalA dictionary of keyword arguments passed intofunc.Returns:the return type offunc.See alsoDataFrame.applyApply a function along input axis of DataFrame.DataFrame.mapApply a function elementwise on a whole DataFrame.Series.mapApply a mapping correspondence on aSeries.NotesUse.pipewhen chaining together functions that expect
Series, DataFrames or GroupBy objects.ExamplesConstructing a income DataFrame from a dictionary.>>>data=[[8000,1000],[9500,np.nan],[5000,2000]]>>>df=pd.DataFrame(data,columns=['Salary','Others'])>>>dfSalary  Others0    8000  1000.01    9500     NaN2    5000  2000.0Functions that perform tax reductions on an income DataFrame.>>>defsubtract_federal_tax(df):...returndf*0.9>>>defsubtract_state_tax(df,rate):...returndf*(1-rate)>>>defsubtract_national_insurance(df,rate,rate_increase):...new_rate=rate+rate_increase...returndf*(1-new_rate)Instead of writing>>>subtract_national_insurance(...subtract_state_tax(subtract_federal_tax(df),rate=0.12),...rate=0.05,...rate_increase=0.02)You can write>>>(...df.pipe(subtract_federal_tax)....pipe(subtract_state_tax,rate=0.12)....pipe(subtract_national_insurance,rate=0.05,rate_increase=0.02)...)Salary   Others0  5892.48   736.561  6997.32      NaN2  3682.80  1473.12If you have a function that takes the data as (say) the second
argument, pass a tuple indicating which keyword expects the
data. For example, supposenational_insurancetakes its data asdfin the second argument:>>>defsubtract_national_insurance(rate,df,rate_increase):...new_rate=rate+rate_increase...returndf*(1-new_rate)>>>(...df.pipe(subtract_federal_tax)....pipe(subtract_state_tax,rate=0.12)....pipe(...(subtract_national_insurance,'df'),...rate=0.05,...rate_increase=0.02...)...)Salary   Others0  5892.48   736.561  6997.32      NaN2  3682.80  1473.12"
Pandas,DataFrame,pandas.DataFrame.agg,"pandas.DataFrame.agg#DataFrame.agg(func=None,axis=0,*args,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either
work when passed a DataFrame or when passed to DataFrame.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.axis{0 or eindexf, 1 or ecolumnsf}, default 0If 0 or eindexf: apply function to each column.
If 1 or ecolumnsf: apply function to each row.*argsPositional arguments to pass tofunc.**kwargsKeyword arguments to pass tofunc.Returns:scalar, Series or DataFrameThe return can be:scalar : when Series.agg is called with single functionSeries : when DataFrame.agg is called with a single functionDataFrame : when DataFrame.agg is called with several functionsReturn scalar, Series or DataFrame.See alsoDataFrame.applyPerform any type of operations.DataFrame.transformPerform transformation type operations.core.groupby.GroupByPerform operations over groups.core.resample.ResamplerPerform operations over resampled bins.core.window.RollingPerform operations over rolling window.core.window.ExpandingPerform operations over expanding window.core.window.ExponentialMovingWindowPerform operation over exponential weighted window.NotesThe aggregation operations are always performed over an axis, either the
index (default) or the column axis. This behavior is different fromnumpyaggregation functions (mean,median,prod,sum,std,var), where the default is to compute the aggregation of the flattened
array, e.g.,numpy.mean(arr_2d)as opposed tonumpy.mean(arr_2d,axis=0).aggis an alias foraggregate. Use the alias.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.A passed user-defined-function will be passed a Series for evaluation.Examples>>>df=pd.DataFrame([[1,2,3],...[4,5,6],...[7,8,9],...[np.nan,np.nan,np.nan]],...columns=['A','B','C'])Aggregate these functions over the rows.>>>df.agg(['sum','min'])A     B     Csum  12.0  15.0  18.0min   1.0   2.0   3.0Different aggregations per column.>>>df.agg({'A':['sum','min'],'B':['min','max']})A    Bsum  12.0  NaNmin   1.0  2.0max   NaN  8.0Aggregate different functions over the columns and rename the index of the resulting
DataFrame.>>>df.agg(x=('A','max'),y=('B','min'),z=('C','mean'))A    B    Cx  7.0  NaN  NaNy  NaN  2.0  NaNz  NaN  NaN  6.0Aggregate over the columns.>>>df.agg(""mean"",axis=""columns"")0    2.01    5.02    8.03    NaNdtype: float64"
Pandas,DataFrame,pandas.DataFrame.aggregate,"pandas.DataFrame.aggregate#DataFrame.aggregate(func=None,axis=0,*args,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either
work when passed a DataFrame or when passed to DataFrame.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.axis{0 or eindexf, 1 or ecolumnsf}, default 0If 0 or eindexf: apply function to each column.
If 1 or ecolumnsf: apply function to each row.*argsPositional arguments to pass tofunc.**kwargsKeyword arguments to pass tofunc.Returns:scalar, Series or DataFrameThe return can be:scalar : when Series.agg is called with single functionSeries : when DataFrame.agg is called with a single functionDataFrame : when DataFrame.agg is called with several functionsReturn scalar, Series or DataFrame.See alsoDataFrame.applyPerform any type of operations.DataFrame.transformPerform transformation type operations.core.groupby.GroupByPerform operations over groups.core.resample.ResamplerPerform operations over resampled bins.core.window.RollingPerform operations over rolling window.core.window.ExpandingPerform operations over expanding window.core.window.ExponentialMovingWindowPerform operation over exponential weighted window.NotesThe aggregation operations are always performed over an axis, either the
index (default) or the column axis. This behavior is different fromnumpyaggregation functions (mean,median,prod,sum,std,var), where the default is to compute the aggregation of the flattened
array, e.g.,numpy.mean(arr_2d)as opposed tonumpy.mean(arr_2d,axis=0).aggis an alias foraggregate. Use the alias.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.A passed user-defined-function will be passed a Series for evaluation.Examples>>>df=pd.DataFrame([[1,2,3],...[4,5,6],...[7,8,9],...[np.nan,np.nan,np.nan]],...columns=['A','B','C'])Aggregate these functions over the rows.>>>df.agg(['sum','min'])A     B     Csum  12.0  15.0  18.0min   1.0   2.0   3.0Different aggregations per column.>>>df.agg({'A':['sum','min'],'B':['min','max']})A    Bsum  12.0  NaNmin   1.0  2.0max   NaN  8.0Aggregate different functions over the columns and rename the index of the resulting
DataFrame.>>>df.agg(x=('A','max'),y=('B','min'),z=('C','mean'))A    B    Cx  7.0  NaN  NaNy  NaN  2.0  NaNz  NaN  NaN  6.0Aggregate over the columns.>>>df.agg(""mean"",axis=""columns"")0    2.01    5.02    8.03    NaNdtype: float64"
Pandas,DataFrame,pandas.DataFrame.transform,"pandas.DataFrame.transform#DataFrame.transform(func,axis=0,*args,**kwargs)[source]#Callfuncon self producing a DataFrame with the same axis shape as self.Parameters:funcfunction, str, list-like or dict-likeFunction to use for transforming the data. If a function, must either
work when passed a DataFrame or when passed to DataFrame.apply. If func
is both list-like and dict-like, dict-like behavior takes precedence.Accepted combinations are:functionstring function namelist-like of functions and/or function names, e.g.[np.exp,'sqrt']dict-like of axis labels -> functions, function names or list-like of such.axis{0 or eindexf, 1 or ecolumnsf}, default 0If 0 or eindexf: apply function to each column.
If 1 or ecolumnsf: apply function to each row.*argsPositional arguments to pass tofunc.**kwargsKeyword arguments to pass tofunc.Returns:DataFrameA DataFrame that must have the same length as self.Raises:ValueErrorIf the returned DataFrame has a different length than self.See alsoDataFrame.aggOnly perform aggregating type operations.DataFrame.applyInvoke function on a DataFrame.NotesFunctions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':range(3),'B':range(1,4)})>>>dfA  B0  0  11  1  22  2  3>>>df.transform(lambdax:x+1)A  B0  1  21  2  32  3  4Even though the resulting DataFrame must have the same length as the
input DataFrame, it is possible to provide several input functions:>>>s=pd.Series(range(3))>>>s0    01    12    2dtype: int64>>>s.transform([np.sqrt,np.exp])sqrt        exp0  0.000000   1.0000001  1.000000   2.7182822  1.414214   7.389056You can call transform on a GroupBy object:>>>df=pd.DataFrame({...""Date"":[...""2015-05-08"",""2015-05-07"",""2015-05-06"",""2015-05-05"",...""2015-05-08"",""2015-05-07"",""2015-05-06"",""2015-05-05""],...""Data"":[5,8,6,1,50,100,60,120],...})>>>dfDate  Data0  2015-05-08     51  2015-05-07     82  2015-05-06     63  2015-05-05     14  2015-05-08    505  2015-05-07   1006  2015-05-06    607  2015-05-05   120>>>df.groupby('Date')['Data'].transform('sum')0     551    1082     663    1214     555    1086     667    121Name: Data, dtype: int64>>>df=pd.DataFrame({...""c"":[1,1,1,2,2,2,2],...""type"":[""m"",""n"",""o"",""m"",""m"",""n"",""n""]...})>>>dfc type0  1    m1  1    n2  1    o3  2    m4  2    m5  2    n6  2    n>>>df['size']=df.groupby('c')['type'].transform(len)>>>dfc type size0  1    m    31  1    n    32  1    o    33  2    m    44  2    m    45  2    n    46  2    n    4"
Pandas,DataFrame,pandas.DataFrame.groupby,"pandas.DataFrame.groupby#DataFrame.groupby(by=None,axis=_NoDefault.no_default,level=None,as_index=True,sort=True,group_keys=True,observed=_NoDefault.no_default,dropna=True)[source]#Group DataFrame using a mapper or by a Series of columns.A groupby operation involves some combination of splitting the
object, applying a function, and combining the results. This can be
used to group large amounts of data and compute operations on these
groups.Parameters:bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby.
Ifbyis a function, itfs called on each value of the objectfs
index. If a dict or Series is passed, the Series or dict VALUES
will be used to determine the groups (the Seriesf values are first
aligned; see.align()method). If a list or ndarray of length
equal to the selected axis is passed (see thegroupby user guide),
the values are used as-is to determine the groups. A label or list
of labels may be passed to group by the columns inself.
Notice that a tuple is interpreted as a (single) key.axis{0 or eindexf, 1 or ecolumnsf}, default 0Split along rows (0) or columns (1). ForSeriesthis parameter
is unused and defaults to 0.Deprecated since version 2.1.0:Will be removed and behave like axis=0 in a future version.
Foraxis=1, doframe.T.groupby(...)instead.levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular
level or levels. Do not specify bothbyandlevel.as_indexbool, default TrueReturn object with group labels as the
index. Only relevant for DataFrame input. as_index=False is
effectively gSQL-styleh grouped output. This argument has no effect
on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).sortbool, default TrueSort group keys. Get better performance by turning this off.
Note this does not influence the order of observations within each
group. Groupby preserves the order of rows within each group. If False,
the groups will appear in the same order as they did in the original DataFrame.
This argument has no effect on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).Changed in version 2.0.0:Specifyingsort=Falsewith an ordered categorical grouper will no
longer sort the values.group_keysbool, default TrueWhen calling apply and thebyargument produces a like-indexed
(i.e.a transform) result, add group keys to
index to identify pieces. By default group keys are not included
when the resultfs index (and column) labels match the inputs, and
are included otherwise.Changed in version 1.5.0:Warns thatgroup_keyswill no longer be ignored when the
result fromapplyis a like-indexed Series or DataFrame.
Specifygroup_keysexplicitly to include the group keys or
not.Changed in version 2.0.0:group_keysnow defaults toTrue.observedbool, default FalseThis only applies if any of the groupers are Categoricals.
If True: only show observed values for categorical groupers.
If False: show all values for categorical groupers.Deprecated since version 2.1.0:The default value will change to True in a future version of pandas.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together
with row/column will be dropped.
If False, NA values will also be treated as the key in groups.Returns:pandas.api.typing.DataFrameGroupByReturns a groupby object that contains information about the groups.See alsoresampleConvenience method for frequency conversion and resampling of time series.NotesSee theuser guidefor more
detailed usage and examples, including splitting an object into groups,
iterating through groups, selecting a group, aggregation, and more.Examples>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>dfAnimal  Max Speed0  Falcon      380.01  Falcon      370.02  Parrot       24.03  Parrot       26.0>>>df.groupby(['Animal']).mean()Max SpeedAnimalFalcon      375.0Parrot       25.0Hierarchical IndexesWe can groupby different levels of a hierarchical index
using thelevelparameter:>>>arrays=[['Falcon','Falcon','Parrot','Parrot'],...['Captive','Wild','Captive','Wild']]>>>index=pd.MultiIndex.from_arrays(arrays,names=('Animal','Type'))>>>df=pd.DataFrame({'Max Speed':[390.,350.,30.,20.]},...index=index)>>>dfMax SpeedAnimal TypeFalcon Captive      390.0Wild         350.0Parrot Captive       30.0Wild          20.0>>>df.groupby(level=0).mean()Max SpeedAnimalFalcon      370.0Parrot       25.0>>>df.groupby(level=""Type"").mean()Max SpeedTypeCaptive      210.0Wild         185.0We can also choose to include NA in group keys or not by settingdropnaparameter, the default setting isTrue.>>>l=[[1,2,3],[1,None,4],[2,1,3],[1,2,2]]>>>df=pd.DataFrame(l,columns=[""a"",""b"",""c""])>>>df.groupby(by=[""b""]).sum()a   cb1.0 2   32.0 2   5>>>df.groupby(by=[""b""],dropna=False).sum()a   cb1.0 2   32.0 2   5NaN 1   4>>>l=[[""a"",12,12],[None,12.3,33.],[""b"",12.3,123],[""a"",1,1]]>>>df=pd.DataFrame(l,columns=[""a"",""b"",""c""])>>>df.groupby(by=""a"").sum()b     caa   13.0   13.0b   12.3  123.0>>>df.groupby(by=""a"",dropna=False).sum()b     caa   13.0   13.0b   12.3  123.0NaN 12.3   33.0When using.apply(), usegroup_keysto include or exclude the
group keys. Thegroup_keysargument defaults toTrue(include).>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>df.groupby(""Animal"",group_keys=True).apply(lambdax:x)Animal  Max SpeedAnimalFalcon 0  Falcon      380.01  Falcon      370.0Parrot 2  Parrot       24.03  Parrot       26.0>>>df.groupby(""Animal"",group_keys=False).apply(lambdax:x)Animal  Max Speed0  Falcon      380.01  Falcon      370.02  Parrot       24.03  Parrot       26.0"
Pandas,DataFrame,pandas.DataFrame.rolling,"pandas.DataFrame.rolling#DataFrame.rolling(window,min_periods=None,center=False,win_type=None,on=None,axis=_NoDefault.no_default,closed=None,step=None,method='single')[source]#Provide rolling window calculations.Parameters:windowint, timedelta, str, offset, or BaseIndexer subclassSize of the moving window.If an integer, the fixed number of observations used for
each window.If a timedelta, str, or offset, the time period of each window. Each
window will be a variable sized based on the observations included in
the time-period. This is only valid for datetimelike indexes.
To learn more about the offsets & frequency strings, please seethis link.If a BaseIndexer subclass, the window boundaries
based on the definedget_window_boundsmethod. Additional rolling
keyword arguments, namelymin_periods,center,closedandstepwill be passed toget_window_bounds.min_periodsint, default NoneMinimum number of observations in window required to have a value;
otherwise, result isnp.nan.For a window that is specified by an offset,min_periodswill default to 1.For a window that is specified by an integer,min_periodswill default
to the size of the window.centerbool, default FalseIf False, set the window labels as the right edge of the window index.If True, set the window labels as the center of the window index.win_typestr, default NoneIfNone, all points are evenly weighted.If a string, it must be a validscipy.signal window function.Certain Scipy window types require additional parameters to be passed
in the aggregation function. The additional parameters must match
the keywords specified in the Scipy window type method signature.onstr, optionalFor a DataFrame, a column label or Index level on which
to calculate the rolling window, rather than the DataFramefs index.Provided integer column is ignored and excluded from result since
an integer index is not used to calculate the rolling window.axisint or str, default 0If0or'index', roll across the rows.If1or'columns', roll across the columns.ForSeriesthis parameter is unused and defaults to 0.closedstr, default NoneIf'right', the first point in the window is excluded from calculations.If'left', the last point in the window is excluded from calculations.If'both', the no points in the window are excluded from calculations.If'neither', the first and last points in the window are excluded
from calculations.DefaultNone('right').Changed in version 1.2.0:The closed parameter with fixed windows is now supported.stepint, default NoneNew in version 1.5.0.Evaluate the window at everystepresult, equivalent to slicing as[::step].windowmust be an integer. Using a step argument other
than None or 1 will produce a result with a different shape than the input.methodstr {esinglef, etablef}, default esinglefNew in version 1.3.0.Execute the rolling operation per single column or row ('single')
or over the entire object ('table').This argument is only implemented when specifyingengine='numba'in the method call.Returns:pandas.api.typing.Window or pandas.api.typing.RollingAn instance of Window is returned ifwin_typeis passed. Otherwise,
an instance of Rolling is returned.See alsoexpandingProvides expanding transformations.ewmProvides exponential weighted functions.NotesSeeWindowing Operationsfor further usage details
and examples.Examples>>>df=pd.DataFrame({'B':[0,1,2,np.nan,4]})>>>dfB0  0.01  1.02  2.03  NaN4  4.0windowRolling sum with a window length of 2 observations.>>>df.rolling(2).sum()B0  NaN1  1.02  3.03  NaN4  NaNRolling sum with a window span of 2 seconds.>>>df_time=pd.DataFrame({'B':[0,1,2,np.nan,4]},...index=[pd.Timestamp('20130101 09:00:00'),...pd.Timestamp('20130101 09:00:02'),...pd.Timestamp('20130101 09:00:03'),...pd.Timestamp('20130101 09:00:05'),...pd.Timestamp('20130101 09:00:06')])>>>df_timeB2013-01-01 09:00:00  0.02013-01-01 09:00:02  1.02013-01-01 09:00:03  2.02013-01-01 09:00:05  NaN2013-01-01 09:00:06  4.0>>>df_time.rolling('2s').sum()B2013-01-01 09:00:00  0.02013-01-01 09:00:02  1.02013-01-01 09:00:03  3.02013-01-01 09:00:05  NaN2013-01-01 09:00:06  4.0Rolling sum with forward looking windows with 2 observations.>>>indexer=pd.api.indexers.FixedForwardWindowIndexer(window_size=2)>>>df.rolling(window=indexer,min_periods=1).sum()B0  1.01  3.02  2.03  4.04  4.0min_periodsRolling sum with a window length of 2 observations, but only needs a minimum of 1
observation to calculate a value.>>>df.rolling(2,min_periods=1).sum()B0  0.01  1.02  3.03  2.04  4.0centerRolling sum with the result assigned to the center of the window index.>>>df.rolling(3,min_periods=1,center=True).sum()B0  1.01  3.02  3.03  6.04  4.0>>>df.rolling(3,min_periods=1,center=False).sum()B0  0.01  1.02  3.03  3.04  6.0stepRolling sum with a window length of 2 observations, minimum of 1 observation to
calculate a value, and a step of 2.>>>df.rolling(2,min_periods=1,step=2).sum()B0  0.02  3.04  4.0win_typeRolling sum with a window length of 2, using the Scipy'gaussian'window type.stdis required in the aggregation function.>>>df.rolling(2,win_type='gaussian').sum(std=3)B0       NaN1  0.9862072  2.9586213       NaN4       NaNonRolling sum with a window length of 2 days.>>>df=pd.DataFrame({...'A':[pd.to_datetime('2020-01-01'),...pd.to_datetime('2020-01-01'),...pd.to_datetime('2020-01-02'),],...'B':[1,2,3],},...index=pd.date_range('2020',periods=3))>>>dfA  B2020-01-01 2020-01-01  12020-01-02 2020-01-01  22020-01-03 2020-01-02  3>>>df.rolling('2D',on='A').sum()A    B2020-01-01 2020-01-01  1.02020-01-02 2020-01-01  3.02020-01-03 2020-01-02  6.0"
Pandas,DataFrame,pandas.DataFrame.expanding,"pandas.DataFrame.expanding#DataFrame.expanding(min_periods=1,axis=_NoDefault.no_default,method='single')[source]#Provide expanding window calculations.Parameters:min_periodsint, default 1Minimum number of observations in window required to have a value;
otherwise, result isnp.nan.axisint or str, default 0If0or'index', roll across the rows.If1or'columns', roll across the columns.ForSeriesthis parameter is unused and defaults to 0.methodstr {esinglef, etablef}, default esinglefExecute the rolling operation per single column or row ('single')
or over the entire object ('table').This argument is only implemented when specifyingengine='numba'in the method call.New in version 1.3.0.Returns:pandas.api.typing.ExpandingSee alsorollingProvides rolling window calculations.ewmProvides exponential weighted functions.NotesSeeWindowing Operationsfor further usage details
and examples.Examples>>>df=pd.DataFrame({""B"":[0,1,2,np.nan,4]})>>>dfB0  0.01  1.02  2.03  NaN4  4.0min_periodsExpanding sum with 1 vs 3 observations needed to calculate a value.>>>df.expanding(1).sum()B0  0.01  1.02  3.03  3.04  7.0>>>df.expanding(3).sum()B0  NaN1  NaN2  3.03  3.04  7.0"
Pandas,DataFrame,pandas.DataFrame.ewm,"pandas.DataFrame.ewm#DataFrame.ewm(com=None,span=None,halflife=None,alpha=None,min_periods=0,adjust=True,ignore_na=False,axis=_NoDefault.no_default,times=None,method='single')[source]#Provide exponentially weighted (EW) calculations.Exactly one ofcom,span,halflife, oralphamust be
provided iftimesis not provided. Iftimesis provided,halflifeand one ofcom,spanoralphamay be provided.Parameters:comfloat, optionalSpecify decay in terms of center of mass\(\alpha = 1 / (1 + com)\), for\(com \geq 0\).spanfloat, optionalSpecify decay in terms of span\(\alpha = 2 / (span + 1)\), for\(span \geq 1\).halflifefloat, str, timedelta, optionalSpecify decay in terms of half-life\(\alpha = 1 - \exp\left(-\ln(2) / halflife\right)\), for\(halflife > 0\).Iftimesis specified, a timedelta convertible unit over which an
observation decays to half its value. Only applicable tomean(),
and halflife value will not apply to the other functions.alphafloat, optionalSpecify smoothing factor\(\alpha\)directly\(0 < \alpha \leq 1\).min_periodsint, default 0Minimum number of observations in window required to have a value;
otherwise, result isnp.nan.adjustbool, default TrueDivide by decaying adjustment factor in beginning periods to account
for imbalance in relative weightings (viewing EWMA as a moving average).Whenadjust=True(default), the EW function is calculated using weights\(w_i = (1 - \alpha)^i\). For example, the EW moving average of the series
[\(x_0, x_1, ..., x_t\)] would be:\[y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ... + (1 -
\alpha)^t x_0}{1 + (1 - \alpha) + (1 - \alpha)^2 + ... + (1 - \alpha)^t}\]Whenadjust=False, the exponentially weighted function is calculated
recursively:\[\begin{split}\begin{split}
y_0 &= x_0\\
y_t &= (1 - \alpha) y_{t-1} + \alpha x_t,
\end{split}\end{split}\]ignore_nabool, default FalseIgnore missing values when calculating weights.Whenignore_na=False(default), weights are based on absolute positions.
For example, the weights of\(x_0\)and\(x_2\)used in calculating
the final weighted average of [\(x_0\), None,\(x_2\)] are\((1-\alpha)^2\)and\(1\)ifadjust=True, and\((1-\alpha)^2\)and\(\alpha\)ifadjust=False.Whenignore_na=True, weights are based
on relative positions. For example, the weights of\(x_0\)and\(x_2\)used in calculating the final weighted average of
[\(x_0\), None,\(x_2\)] are\(1-\alpha\)and\(1\)ifadjust=True, and\(1-\alpha\)and\(\alpha\)ifadjust=False.axis{0, 1}, default 0If0or'index', calculate across the rows.If1or'columns', calculate across the columns.ForSeriesthis parameter is unused and defaults to 0.timesnp.ndarray, Series, default NoneOnly applicable tomean().Times corresponding to the observations. Must be monotonically increasing anddatetime64[ns]dtype.If 1-D array like, a sequence with the same shape as the observations.methodstr {esinglef, etablef}, default esinglefNew in version 1.4.0.Execute the rolling operation per single column or row ('single')
or over the entire object ('table').This argument is only implemented when specifyingengine='numba'in the method call.Only applicable tomean()Returns:pandas.api.typing.ExponentialMovingWindowSee alsorollingProvides rolling window calculations.expandingProvides expanding transformations.NotesSeeWindowing Operationsfor further usage details and examples.Examples>>>df=pd.DataFrame({'B':[0,1,2,np.nan,4]})>>>dfB0  0.01  1.02  2.03  NaN4  4.0>>>df.ewm(com=0.5).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213>>>df.ewm(alpha=2/3).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213adjust>>>df.ewm(com=0.5,adjust=True).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213>>>df.ewm(com=0.5,adjust=False).mean()B0  0.0000001  0.6666672  1.5555563  1.5555564  3.650794ignore_na>>>df.ewm(com=0.5,ignore_na=True).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.225000>>>df.ewm(com=0.5,ignore_na=False).mean()B0  0.0000001  0.7500002  1.6153853  1.6153854  3.670213timesExponentially weighted mean with weights calculated with a timedeltahalfliferelative totimes.>>>times=['2020-01-01','2020-01-03','2020-01-10','2020-01-15','2020-01-17']>>>df.ewm(halflife='4 days',times=pd.DatetimeIndex(times)).mean()B0  0.0000001  0.5857862  1.5238893  1.5238894  3.233686"
Pandas,DataFrame,pandas.DataFrame.abs,"pandas.DataFrame.abs#DataFrame.abs()[source]#Return a Series/DataFrame with absolute numeric value of each element.This function only applies to elements that are all numeric.Returns:absSeries/DataFrame containing the absolute value of each element.See alsonumpy.absoluteCalculate the absolute value element-wise.NotesForcomplexinputs,1.2+1j, the absolute value is\(\sqrt{ a^2 + b^2 }\).ExamplesAbsolute numeric values in a Series.>>>s=pd.Series([-1.10,2,-3.33,4])>>>s.abs()0    1.101    2.002    3.333    4.00dtype: float64Absolute numeric values in a Series with complex numbers.>>>s=pd.Series([1.2+1j])>>>s.abs()0    1.56205dtype: float64Absolute numeric values in a Series with a Timedelta element.>>>s=pd.Series([pd.Timedelta('1 days')])>>>s.abs()0   1 daysdtype: timedelta64[ns]Select rows with data closest to certain value using argsort (fromStackOverflow).>>>df=pd.DataFrame({...'a':[4,5,6,7],...'b':[10,20,30,40],...'c':[100,50,-30,-50]...})>>>dfa    b    c0    4   10  1001    5   20   502    6   30  -303    7   40  -50>>>df.loc[(df.c-43).abs().argsort()]a    b    c1    5   20   500    4   10  1002    6   30  -303    7   40  -50"
Pandas,DataFrame,pandas.DataFrame.all,"pandas.DataFrame.all#DataFrame.all(axis=0,bool_only=False,skipna=True,**kwargs)[source]#Return whether all elements are True, potentially over an axis.Returns True unless there at least one element within a series or
along a Dataframe axis that is False or equivalent (e.g. zero or
empty).Parameters:axis{0 or eindexf, 1 or ecolumnsf, None}, default 0Indicate which axis or axes should be reduced. ForSeriesthis parameter
is unused and defaults to 0.0 / eindexf : reduce the index, return a Series whose index is the
original column labels.1 / ecolumnsf : reduce the columns, return a Series whose index is the
original index.None : reduce all axes, return a scalar.bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series.skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is
True, then the result will be True, as for an empty row/column.
If skipna is False, then NA are treated as True, because these are not
equal to zero.**kwargsany, default NoneAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:Series or DataFrameIf level is specified, then, DataFrame is returned; otherwise, Series
is returned.See alsoSeries.allReturn True if all elements are True.DataFrame.anyReturn True if one (or more) elements are True.ExamplesSeries>>>pd.Series([True,True]).all()True>>>pd.Series([True,False]).all()False>>>pd.Series([],dtype=""float64"").all()True>>>pd.Series([np.nan]).all()True>>>pd.Series([np.nan]).all(skipna=False)TrueDataFramesCreate a dataframe from a dictionary.>>>df=pd.DataFrame({'col1':[True,True],'col2':[True,False]})>>>dfcol1   col20  True   True1  True  FalseDefault behaviour checks if values in each column all return True.>>>df.all()col1     Truecol2    Falsedtype: boolSpecifyaxis='columns'to check if values in each row all return True.>>>df.all(axis='columns')0     True1    Falsedtype: boolOraxis=Nonefor whether every value is True.>>>df.all(axis=None)False"
Pandas,DataFrame,pandas.DataFrame.any,"pandas.DataFrame.any#DataFrame.any(*,axis=0,bool_only=False,skipna=True,**kwargs)[source]#Return whether any element is True, potentially over an axis.Returns False unless there is at least one element within a series or
along a Dataframe axis that is True or equivalent (e.g. non-zero or
non-empty).Parameters:axis{0 or eindexf, 1 or ecolumnsf, None}, default 0Indicate which axis or axes should be reduced. ForSeriesthis parameter
is unused and defaults to 0.0 / eindexf : reduce the index, return a Series whose index is the
original column labels.1 / ecolumnsf : reduce the columns, return a Series whose index is the
original index.None : reduce all axes, return a scalar.bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series.skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is
True, then the result will be False, as for an empty row/column.
If skipna is False, then NA are treated as True, because these are not
equal to zero.**kwargsany, default NoneAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:Series or DataFrameIf level is specified, then, DataFrame is returned; otherwise, Series
is returned.See alsonumpy.anyNumpy version of this method.Series.anyReturn whether any element is True.Series.allReturn whether all elements are True.DataFrame.anyReturn whether any element is True over requested axis.DataFrame.allReturn whether all elements are True over requested axis.ExamplesSeriesFor Series input, the output is a scalar indicating whether any element
is True.>>>pd.Series([False,False]).any()False>>>pd.Series([True,False]).any()True>>>pd.Series([],dtype=""float64"").any()False>>>pd.Series([np.nan]).any()False>>>pd.Series([np.nan]).any(skipna=False)TrueDataFrameWhether each column contains at least one True element (the default).>>>df=pd.DataFrame({""A"":[1,2],""B"":[0,2],""C"":[0,0]})>>>dfA  B  C0  1  0  01  2  2  0>>>df.any()A     TrueB     TrueC    Falsedtype: boolAggregating over the columns.>>>df=pd.DataFrame({""A"":[True,False],""B"":[1,2]})>>>dfA  B0   True  11  False  2>>>df.any(axis='columns')0    True1    Truedtype: bool>>>df=pd.DataFrame({""A"":[True,False],""B"":[1,0]})>>>dfA  B0   True  11  False  0>>>df.any(axis='columns')0    True1    Falsedtype: boolAggregating over the entire DataFrame withaxis=None.>>>df.any(axis=None)Trueanyfor an empty DataFrame is an empty Series.>>>pd.DataFrame([]).any()Series([], dtype: bool)"
Pandas,DataFrame,pandas.DataFrame.clip,"pandas.DataFrame.clip#DataFrame.clip(lower=None,upper=None,*,axis=None,inplace=False,**kwargs)[source]#Trim values at input threshold(s).Assigns values outside boundary to boundary values. Thresholds
can be singular values or array like, and in the latter case
the clipping is performed element-wise in the specified axis.Parameters:lowerfloat or array-like, default NoneMinimum threshold value. All values below this
threshold will be set to it. A missing
threshold (e.gNA) will not clip the value.upperfloat or array-like, default NoneMaximum threshold value. All values above this
threshold will be set to it. A missing
threshold (e.gNA) will not clip the value.axis{{0 or eindexf, 1 or ecolumnsf, None}}, default NoneAlign object with lower and upper along the given axis.
ForSeriesthis parameter is unused and defaults toNone.inplacebool, default FalseWhether to perform the operation in place on the data.*args, **kwargsAdditional keywords have no effect but might be accepted
for compatibility with numpy.Returns:Series or DataFrame or NoneSame type as calling object with the values outside the
clip boundaries replaced or None ifinplace=True.See alsoSeries.clipTrim values at input threshold in series.DataFrame.clipTrim values at input threshold in dataframe.numpy.clipClip (limit) the values in an array.Examples>>>data={'col_0':[9,-3,0,-1,5],'col_1':[-2,-7,6,8,-5]}>>>df=pd.DataFrame(data)>>>dfcol_0  col_10      9     -21     -3     -72      0      63     -1      84      5     -5Clips per column using lower and upper thresholds:>>>df.clip(-4,6)col_0  col_10      6     -21     -3     -42      0      63     -1      64      5     -4Clips using specific lower and upper thresholds per column element:>>>t=pd.Series([2,-4,-1,6,3])>>>t0    21   -42   -13    64    3dtype: int64>>>df.clip(t,t+4,axis=0)col_0  col_10      6      21     -3     -42      0      33      6      84      5      3Clips using specific lower threshold per column element, with missing values:>>>t=pd.Series([2,-4,np.nan,6,3])>>>t0    2.01   -4.02    NaN3    6.04    3.0dtype: float64>>>df.clip(t,axis=0)col_0  col_10      9      21     -3     -42      0      63      6      84      5      3"
Pandas,DataFrame,pandas.DataFrame.corr,"pandas.DataFrame.corr#DataFrame.corr(method='pearson',min_periods=1,numeric_only=False)[source]#Compute pairwise correlation of columns, excluding NA/null values.Parameters:method{epearsonf, ekendallf, espearmanf} or callableMethod of correlation:pearson : standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: callable with input two 1d ndarraysand returning a float. Note that the returned matrix from corr
will have 1 along the diagonals and will be symmetric
regardless of the callablefs behavior.min_periodsint, optionalMinimum number of observations required per pair of columns
to have a valid result. Currently only available for Pearson
and Spearman correlation.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:DataFrameCorrelation matrix.See alsoDataFrame.corrwithCompute pairwise correlation with another DataFrame or Series.Series.corrCompute the correlation between two Series.NotesPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.Pearson correlation coefficientKendall rank correlation coefficientSpearmanfs rank correlation coefficientExamples>>>defhistogram_intersection(a,b):...v=np.minimum(a,b).sum().round(decimals=1)...returnv>>>df=pd.DataFrame([(.2,.3),(.0,.6),(.6,.0),(.2,.1)],...columns=['dogs','cats'])>>>df.corr(method=histogram_intersection)dogs  catsdogs   1.0   0.3cats   0.3   1.0>>>df=pd.DataFrame([(1,1),(2,np.nan),(np.nan,3),(4,4)],...columns=['dogs','cats'])>>>df.corr(min_periods=3)dogs  catsdogs   1.0   NaNcats   NaN   1.0"
Pandas,DataFrame,pandas.DataFrame.corrwith,"pandas.DataFrame.corrwith#DataFrame.corrwith(other,axis=0,drop=False,method='pearson',numeric_only=False)[source]#Compute pairwise correlation.Pairwise correlation is computed between rows or columns of
DataFrame with rows or columns of Series or DataFrame. DataFrames
are first aligned along both axes before computing the
correlations.Parameters:otherDataFrame, SeriesObject with which to compute correlations.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to use. 0 or eindexf to compute row-wise, 1 or ecolumnsf for
column-wise.dropbool, default FalseDrop missing indices from result.method{epearsonf, ekendallf, espearmanf} or callableMethod of correlation:pearson : standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: callable with input two 1d ndarraysand returning a float.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:SeriesPairwise correlations.See alsoDataFrame.corrCompute pairwise correlation of columns.Examples>>>index=[""a"",""b"",""c"",""d"",""e""]>>>columns=[""one"",""two"",""three"",""four""]>>>df1=pd.DataFrame(np.arange(20).reshape(5,4),index=index,columns=columns)>>>df2=pd.DataFrame(np.arange(16).reshape(4,4),index=index[:4],columns=columns)>>>df1.corrwith(df2)one      1.0two      1.0three    1.0four     1.0dtype: float64>>>df2.corrwith(df1,axis=1)a    1.0b    1.0c    1.0d    1.0e    NaNdtype: float64"
Pandas,DataFrame,pandas.DataFrame.count,"pandas.DataFrame.count#DataFrame.count(axis=0,numeric_only=False)[source]#Count non-NA cells for each column or row.The valuesNone,NaN,NaT,pandas.NAare considered NA.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0If 0 or eindexf counts are generated for each column.
If 1 or ecolumnsf counts are generated for each row.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.Returns:SeriesFor each column/row the number of non-NA/null entries.See alsoSeries.countNumber of non-NA elements in a Series.DataFrame.value_countsCount unique combinations of columns.DataFrame.shapeNumber of DataFrame rows and columns (including NA elements).DataFrame.isnaBoolean same-sized DataFrame showing places of NA elements.ExamplesConstructing DataFrame from a dictionary:>>>df=pd.DataFrame({""Person"":...[""John"",""Myla"",""Lewis"",""John"",""Myla""],...""Age"":[24.,np.nan,21.,33,26],...""Single"":[False,True,True,True,False]})>>>dfPerson   Age  Single0    John  24.0   False1    Myla   NaN    True2   Lewis  21.0    True3    John  33.0    True4    Myla  26.0   FalseNotice the uncounted NA values:>>>df.count()Person    5Age       4Single    5dtype: int64Counts for eachrow:>>>df.count(axis='columns')0    31    22    33    34    3dtype: int64"
Pandas,DataFrame,pandas.DataFrame.cov,"pandas.DataFrame.cov#DataFrame.cov(min_periods=None,ddof=1,numeric_only=False)[source]#Compute pairwise covariance of columns, excluding NA/null values.Compute the pairwise covariance among the series of a DataFrame.
The returned data frame is thecovariance matrixof the columns
of the DataFrame.Both NA and null values are automatically excluded from the
calculation. (See the note below about bias from missing values.)
A threshold can be set for the minimum number of
observations for each value created. Comparisons with observations
below this threshold will be returned asNaN.This method is generally used for the analysis of time series data to
understand the relationship between different measures
across time.Parameters:min_periodsint, optionalMinimum number of observations required per pair of columns
to have a valid result.ddofint, default 1Delta degrees of freedom. The divisor used in calculations
isN-ddof, whereNrepresents the number of elements.
This argument is applicable only when nonanis in the dataframe.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:DataFrameThe covariance matrix of the series of the DataFrame.See alsoSeries.covCompute covariance with another Series.core.window.ewm.ExponentialMovingWindow.covExponential weighted sample covariance.core.window.expanding.Expanding.covExpanding sample covariance.core.window.rolling.Rolling.covRolling sample covariance.NotesReturns the covariance matrix of the DataFramefs time series.
The covariance is normalized by N-ddof.For DataFrames that have Series that are missing data (assuming that
data ismissing at random)
the returned covariance matrix will be an unbiased estimate
of the variance and covariance between the member Series.However, for many applications this estimate may not be acceptable
because the estimate covariance matrix is not guaranteed to be positive
semi-definite. This could lead to estimate correlations having
absolute values which are greater than one, and/or a non-invertible
covariance matrix. SeeEstimation of covariance matricesfor more details.Examples>>>df=pd.DataFrame([(1,2),(0,3),(2,0),(1,1)],...columns=['dogs','cats'])>>>df.cov()dogs      catsdogs  0.666667 -1.000000cats -1.000000  1.666667>>>np.random.seed(42)>>>df=pd.DataFrame(np.random.randn(1000,5),...columns=['a','b','c','d','e'])>>>df.cov()a         b         c         d         ea  0.998438 -0.020161  0.059277 -0.008943  0.014144b -0.020161  1.059352 -0.008543 -0.024738  0.009826c  0.059277 -0.008543  1.010670 -0.001486 -0.000271d -0.008943 -0.024738 -0.001486  0.921297 -0.013692e  0.014144  0.009826 -0.000271 -0.013692  0.977795Minimum number of periodsThis method also supports an optionalmin_periodskeyword
that specifies the required minimum number of non-NA observations for
each column pair in order to have a valid result:>>>np.random.seed(42)>>>df=pd.DataFrame(np.random.randn(20,3),...columns=['a','b','c'])>>>df.loc[df.index[:5],'a']=np.nan>>>df.loc[df.index[5:10],'b']=np.nan>>>df.cov(min_periods=12)a         b         ca  0.316741       NaN -0.150812b       NaN  1.248003  0.191417c -0.150812  0.191417  0.895202"
Pandas,DataFrame,pandas.DataFrame.cummax,"pandas.DataFrame.cummax#DataFrame.cummax(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative maximum over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
maximum.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:Series or DataFrameReturn cumulative maximum of Series or DataFrame.See alsocore.window.expanding.Expanding.maxSimilar functionality but ignoresNaNvalues.DataFrame.maxReturn the maximum over DataFrame axis.DataFrame.cummaxReturn cumulative maximum over DataFrame axis.DataFrame.cumminReturn cumulative minimum over DataFrame axis.DataFrame.cumsumReturn cumulative sum over DataFrame axis.DataFrame.cumprodReturn cumulative product over DataFrame axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cummax()0    2.01    NaN2    5.03    5.04    5.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cummax(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the maximum
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cummax()A    B0  2.0  1.01  3.0  NaN2  3.0  1.0To iterate over columns and find the maximum in each row,
useaxis=1>>>df.cummax(axis=1)A    B0  2.0  2.01  3.0  NaN2  1.0  1.0"
Pandas,DataFrame,pandas.DataFrame.cummin,"pandas.DataFrame.cummin#DataFrame.cummin(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative minimum over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
minimum.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:Series or DataFrameReturn cumulative minimum of Series or DataFrame.See alsocore.window.expanding.Expanding.minSimilar functionality but ignoresNaNvalues.DataFrame.minReturn the minimum over DataFrame axis.DataFrame.cummaxReturn cumulative maximum over DataFrame axis.DataFrame.cumminReturn cumulative minimum over DataFrame axis.DataFrame.cumsumReturn cumulative sum over DataFrame axis.DataFrame.cumprodReturn cumulative product over DataFrame axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cummin()0    2.01    NaN2    2.03   -1.04   -1.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cummin(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the minimum
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cummin()A    B0  2.0  1.01  2.0  NaN2  1.0  0.0To iterate over columns and find the minimum in each row,
useaxis=1>>>df.cummin(axis=1)A    B0  2.0  1.01  3.0  NaN2  1.0  0.0"
Pandas,DataFrame,pandas.DataFrame.cumprod,"pandas.DataFrame.cumprod#DataFrame.cumprod(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative product over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
product.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:Series or DataFrameReturn cumulative product of Series or DataFrame.See alsocore.window.expanding.Expanding.prodSimilar functionality but ignoresNaNvalues.DataFrame.prodReturn the product over DataFrame axis.DataFrame.cummaxReturn cumulative maximum over DataFrame axis.DataFrame.cumminReturn cumulative minimum over DataFrame axis.DataFrame.cumsumReturn cumulative sum over DataFrame axis.DataFrame.cumprodReturn cumulative product over DataFrame axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cumprod()0     2.01     NaN2    10.03   -10.04    -0.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cumprod(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the product
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cumprod()A    B0  2.0  1.01  6.0  NaN2  6.0  0.0To iterate over columns and find the product in each row,
useaxis=1>>>df.cumprod(axis=1)A    B0  2.0  2.01  3.0  NaN2  1.0  0.0"
Pandas,DataFrame,pandas.DataFrame.cumsum,"pandas.DataFrame.cumsum#DataFrame.cumsum(axis=None,skipna=True,*args,**kwargs)[source]#Return cumulative sum over a DataFrame or Series axis.Returns a DataFrame or Series of the same size containing the cumulative
sum.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The index or the name of the axis. 0 is equivalent to None or eindexf.
ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.*args, **kwargsAdditional keywords have no effect but might be accepted for
compatibility with NumPy.Returns:Series or DataFrameReturn cumulative sum of Series or DataFrame.See alsocore.window.expanding.Expanding.sumSimilar functionality but ignoresNaNvalues.DataFrame.sumReturn the sum over DataFrame axis.DataFrame.cummaxReturn cumulative maximum over DataFrame axis.DataFrame.cumminReturn cumulative minimum over DataFrame axis.DataFrame.cumsumReturn cumulative sum over DataFrame axis.DataFrame.cumprodReturn cumulative product over DataFrame axis.ExamplesSeries>>>s=pd.Series([2,np.nan,5,-1,0])>>>s0    2.01    NaN2    5.03   -1.04    0.0dtype: float64By default, NA values are ignored.>>>s.cumsum()0    2.01    NaN2    7.03    6.04    6.0dtype: float64To include NA values in the operation, useskipna=False>>>s.cumsum(skipna=False)0    2.01    NaN2    NaN3    NaN4    NaNdtype: float64DataFrame>>>df=pd.DataFrame([[2.0,1.0],...[3.0,np.nan],...[1.0,0.0]],...columns=list('AB'))>>>dfA    B0  2.0  1.01  3.0  NaN2  1.0  0.0By default, iterates over rows and finds the sum
in each column. This is equivalent toaxis=Noneoraxis='index'.>>>df.cumsum()A    B0  2.0  1.01  5.0  NaN2  6.0  1.0To iterate over columns and find the sum in each row,
useaxis=1>>>df.cumsum(axis=1)A    B0  2.0  3.01  3.0  NaN2  1.0  1.0"
Pandas,DataFrame,pandas.DataFrame.describe,"pandas.DataFrame.describe#DataFrame.describe(percentiles=None,include=None,exclude=None)[source]#Generate descriptive statistics.Descriptive statistics include those that summarize the central
tendency, dispersion and shape of a
datasetfs distribution, excludingNaNvalues.Analyzes both numeric and object series, as well
asDataFramecolumn sets of mixed data types. The output
will vary depending on what is provided. Refer to the notes
below for more detail.Parameters:percentileslist-like of numbers, optionalThe percentiles to include in the output. All should
fall between 0 and 1. The default is[.25,.5,.75], which returns the 25th, 50th, and
75th percentiles.includeeallf, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored
forSeries. Here are the options:eallf : All columns of the input will be included in the output.A list-like of dtypes : Limits the results to the
provided data types.
To limit the result to numeric types submitnumpy.number. To limit it instead to object columns submit
thenumpy.objectdata type. Strings
can also be used in the style ofselect_dtypes(e.g.df.describe(include=['O'])). To
select pandas categorical columns, use'category'None (default) : The result will include all numeric columns.excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored
forSeries. Here are the options:A list-like of dtypes : Excludes the provided data types
from the result. To exclude numeric types submitnumpy.number. To exclude object columns submit the data
typenumpy.object. Strings can also be used in the style ofselect_dtypes(e.g.df.describe(exclude=['O'])). To
exclude pandas categorical columns, use'category'None (default) : The result will exclude nothing.Returns:Series or DataFrameSummary statistics of the Series or Dataframe provided.See alsoDataFrame.countCount number of non-NA/null observations.DataFrame.maxMaximum of the values in the object.DataFrame.minMinimum of the values in the object.DataFrame.meanMean of the values.DataFrame.stdStandard deviation of the observations.DataFrame.select_dtypesSubset of a DataFrame including/excluding columns based on their dtype.NotesFor numeric data, the resultfs index will includecount,mean,std,min,maxas well as lower,50and
upper percentiles. By default the lower percentile is25and the
upper percentile is75. The50percentile is the
same as the median.For object data (e.g. strings or timestamps), the resultfs index
will includecount,unique,top, andfreq. Thetopis the most common value. Thefreqis the most common valuefs
frequency. Timestamps also include thefirstandlastitems.If multiple object values have the highest count, then thecountandtopresults will be arbitrarily chosen from
among those with the highest count.For mixed data types provided via aDataFrame, the default is to
return only an analysis of numeric columns. If the dataframe consists
only of object and categorical data without any numeric columns, the
default is to return an analysis of both the object and categorical
columns. Ifinclude='all'is provided as an option, the result
will include a union of attributes of each type.Theincludeandexcludeparameters can be used to limit
which columns in aDataFrameare analyzed for the output.
The parameters are ignored when analyzing aSeries.ExamplesDescribing a numericSeries.>>>s=pd.Series([1,2,3])>>>s.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0dtype: float64Describing a categoricalSeries.>>>s=pd.Series(['a','a','b','c'])>>>s.describe()count     4unique    3top       afreq      2dtype: objectDescribing a timestampSeries.>>>s=pd.Series([...np.datetime64(""2000-01-01""),...np.datetime64(""2010-01-01""),...np.datetime64(""2010-01-01"")...])>>>s.describe()count                      3mean     2006-09-01 08:00:00min      2000-01-01 00:00:0025%      2004-12-31 12:00:0050%      2010-01-01 00:00:0075%      2010-01-01 00:00:00max      2010-01-01 00:00:00dtype: objectDescribing aDataFrame. By default only numeric fields
are returned.>>>df=pd.DataFrame({'categorical':pd.Categorical(['d','e','f']),...'numeric':[1,2,3],...'object':['a','b','c']...})>>>df.describe()numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Describing all columns of aDataFrameregardless of data type.>>>df.describe(include='all')categorical  numeric objectcount            3      3.0      3unique           3      NaN      3top              f      NaN      afreq             1      NaN      1mean           NaN      2.0    NaNstd            NaN      1.0    NaNmin            NaN      1.0    NaN25%            NaN      1.5    NaN50%            NaN      2.0    NaN75%            NaN      2.5    NaNmax            NaN      3.0    NaNDescribing a column from aDataFrameby accessing it as
an attribute.>>>df.numeric.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0Name: numeric, dtype: float64Including only numeric columns in aDataFramedescription.>>>df.describe(include=[np.number])numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Including only string columns in aDataFramedescription.>>>df.describe(include=[object])objectcount       3unique      3top         afreq        1Including only categorical columns from aDataFramedescription.>>>df.describe(include=['category'])categoricalcount            3unique           3top              dfreq             1Excluding numeric columns from aDataFramedescription.>>>df.describe(exclude=[np.number])categorical objectcount            3      3unique           3      3top              f      afreq             1      1Excluding object columns from aDataFramedescription.>>>df.describe(exclude=[object])categorical  numericcount            3      3.0unique           3      NaNtop              f      NaNfreq             1      NaNmean           NaN      2.0std            NaN      1.0min            NaN      1.025%            NaN      1.550%            NaN      2.075%            NaN      2.5max            NaN      3.0"
Pandas,DataFrame,pandas.DataFrame.diff,"pandas.DataFrame.diff#DataFrame.diff(periods=1,axis=0)[source]#First discrete difference of element.Calculates the difference of a DataFrame element compared with another
element in the DataFrame (default is element in previous row).Parameters:periodsint, default 1Periods to shift for calculating difference, accepts negative
values.axis{0 or eindexf, 1 or ecolumnsf}, default 0Take difference over rows (0) or columns (1).Returns:DataFrameFirst differences of the Series.See alsoDataFrame.pct_changePercent change over given number of periods.DataFrame.shiftShift index by desired number of periods with an optional time freq.Series.diffFirst discrete difference of object.NotesFor boolean dtypes, this usesoperator.xor()rather thanoperator.sub().
The result is calculated according to current dtype in DataFrame,
however dtype of the result is always float64.ExamplesDifference with previous row>>>df=pd.DataFrame({'a':[1,2,3,4,5,6],...'b':[1,1,2,3,5,8],...'c':[1,4,9,16,25,36]})>>>dfa  b   c0  1  1   11  2  1   42  3  2   93  4  3  164  5  5  255  6  8  36>>>df.diff()a    b     c0  NaN  NaN   NaN1  1.0  0.0   3.02  1.0  1.0   5.03  1.0  1.0   7.04  1.0  2.0   9.05  1.0  3.0  11.0Difference with previous column>>>df.diff(axis=1)a  b   c0 NaN  0   01 NaN -1   32 NaN -1   73 NaN -1  134 NaN  0  205 NaN  2  28Difference with 3rd previous row>>>df.diff(periods=3)a    b     c0  NaN  NaN   NaN1  NaN  NaN   NaN2  NaN  NaN   NaN3  3.0  2.0  15.04  3.0  4.0  21.05  3.0  6.0  27.0Difference with following row>>>df.diff(periods=-1)a    b     c0 -1.0  0.0  -3.01 -1.0 -1.0  -5.02 -1.0 -1.0  -7.03 -1.0 -2.0  -9.04 -1.0 -3.0 -11.05  NaN  NaN   NaNOverflow in input dtype>>>df=pd.DataFrame({'a':[1,0]},dtype=np.uint8)>>>df.diff()a0    NaN1  255.0"
Pandas,DataFrame,pandas.DataFrame.eval,"pandas.DataFrame.eval#DataFrame.eval(expr,*,inplace=False,**kwargs)[source]#Evaluate a string describing operations on DataFrame columns.Operates on columns only, not specific rows or elements. This allowsevalto run arbitrary code, which can make you vulnerable to code
injection if you pass user input to this function.Parameters:exprstrThe expression string to evaluate.inplacebool, default FalseIf the expression contains an assignment, whether to perform the
operation inplace and mutate the existing DataFrame. Otherwise,
a new DataFrame is returned.**kwargsSee the documentation foreval()for complete details
on the keyword arguments accepted byquery().Returns:ndarray, scalar, pandas object, or NoneThe result of the evaluation or None ifinplace=True.See alsoDataFrame.queryEvaluates a boolean expression to query the columns of a frame.DataFrame.assignCan evaluate an expression or function to create new values for a column.evalEvaluate a Python expression as a string using various backends.NotesFor more details see the API documentation foreval().
For detailed examples seeenhancing performance with eval.Examples>>>df=pd.DataFrame({'A':range(1,6),'B':range(10,0,-2)})>>>dfA   B0  1  101  2   82  3   63  4   44  5   2>>>df.eval('A + B')0    111    102     93     84     7dtype: int64Assignment is allowed though by default the original DataFrame is not
modified.>>>df.eval('C = A + B')A   B   C0  1  10  111  2   8  102  3   6   93  4   4   84  5   2   7>>>dfA   B0  1  101  2   82  3   63  4   44  5   2Multiple columns can be assigned to using multi-line expressions:>>>df.eval(...'''...C = A + B...D = A - B...'''...)A   B   C  D0  1  10  11 -91  2   8  10 -62  3   6   9 -33  4   4   8  04  5   2   7  3"
Pandas,DataFrame,pandas.DataFrame.kurt,"pandas.DataFrame.kurt#DataFrame.kurt(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return unbiased kurtosis over requested axis.Kurtosis obtained using Fisherfs definition of
kurtosis (kurtosis of normal == 0.0). Normalized by N-1.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarExamples>>>s=pd.Series([1,2,2,3],index=['cat','dog','dog','mouse'])>>>scat    1dog    2dog    2mouse  3dtype: int64>>>s.kurt()1.5With a DataFrame>>>df=pd.DataFrame({'a':[1,2,2,3],'b':[3,4,4,4]},...index=['cat','dog','dog','mouse'])>>>dfa   bcat  1   3dog  2   4dog  2   4mouse  3   4>>>df.kurt()a   1.5b   4.0dtype: float64With axis=None>>>df.kurt(axis=None).round(6)-0.988693Using axis=1>>>df=pd.DataFrame({'a':[1,2],'b':[3,4],'c':[3,4],'d':[1,2]},...index=['cat','dog'])>>>df.kurt(axis=1)cat   -6.0dog   -6.0dtype: float64"
Pandas,DataFrame,pandas.DataFrame.kurtosis,"pandas.DataFrame.kurtosis#DataFrame.kurtosis(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return unbiased kurtosis over requested axis.Kurtosis obtained using Fisherfs definition of
kurtosis (kurtosis of normal == 0.0). Normalized by N-1.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarExamples>>>s=pd.Series([1,2,2,3],index=['cat','dog','dog','mouse'])>>>scat    1dog    2dog    2mouse  3dtype: int64>>>s.kurt()1.5With a DataFrame>>>df=pd.DataFrame({'a':[1,2,2,3],'b':[3,4,4,4]},...index=['cat','dog','dog','mouse'])>>>dfa   bcat  1   3dog  2   4dog  2   4mouse  3   4>>>df.kurt()a   1.5b   4.0dtype: float64With axis=None>>>df.kurt(axis=None).round(6)-0.988693Using axis=1>>>df=pd.DataFrame({'a':[1,2],'b':[3,4],'c':[3,4],'d':[1,2]},...index=['cat','dog'])>>>df.kurt(axis=1)cat   -6.0dog   -6.0dtype: float64"
Pandas,DataFrame,pandas.DataFrame.max,"pandas.DataFrame.max#DataFrame.max(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the maximum of the values over the requested axis.If you want theindexof the maximum, useidxmax. This is the equivalent of thenumpy.ndarraymethodargmax.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.Examples>>>idx=pd.MultiIndex.from_arrays([...['warm','warm','cold','cold'],...['dog','falcon','fish','spider']],...names=['blooded','animal'])>>>s=pd.Series([4,2,0,8],name='legs',index=idx)>>>sblooded  animalwarm     dog       4falcon    2cold     fish      0spider    8Name: legs, dtype: int64>>>s.max()8"
Pandas,DataFrame,pandas.DataFrame.mean,"pandas.DataFrame.mean#DataFrame.mean(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the mean of the values over the requested axis.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarExamples>>>s=pd.Series([1,2,3])>>>s.mean()2.0With a DataFrame>>>df=pd.DataFrame({'a':[1,2],'b':[2,3]},index=['tiger','zebra'])>>>dfa   btiger  1   2zebra  2   3>>>df.mean()a   1.5b   2.5dtype: float64Using axis=1>>>df.mean(axis=1)tiger   1.5zebra   2.5dtype: float64In this case,numeric_onlyshould be set toTrueto avoid
getting an error.>>>df=pd.DataFrame({'a':[1,2],'b':['T','Z']},...index=['tiger','zebra'])>>>df.mean(numeric_only=True)a   1.5dtype: float64"
Pandas,DataFrame,pandas.DataFrame.median,"pandas.DataFrame.median#DataFrame.median(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the median of the values over the requested axis.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarExamples>>>s=pd.Series([1,2,3])>>>s.median()2.0With a DataFrame>>>df=pd.DataFrame({'a':[1,2],'b':[2,3]},index=['tiger','zebra'])>>>dfa   btiger  1   2zebra  2   3>>>df.median()a   1.5b   2.5dtype: float64Using axis=1>>>df.median(axis=1)tiger   1.5zebra   2.5dtype: float64In this case,numeric_onlyshould be set toTrueto avoid getting an error.>>>df=pd.DataFrame({'a':[1,2],'b':['T','Z']},...index=['tiger','zebra'])>>>df.median(numeric_only=True)a   1.5dtype: float64"
Pandas,DataFrame,pandas.DataFrame.min,"pandas.DataFrame.min#DataFrame.min(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return the minimum of the values over the requested axis.If you want theindexof the minimum, useidxmin. This is the equivalent of thenumpy.ndarraymethodargmin.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.Examples>>>idx=pd.MultiIndex.from_arrays([...['warm','warm','cold','cold'],...['dog','falcon','fish','spider']],...names=['blooded','animal'])>>>s=pd.Series([4,2,0,8],name='legs',index=idx)>>>sblooded  animalwarm     dog       4falcon    2cold     fish      0spider    8Name: legs, dtype: int64>>>s.min()0"
Pandas,DataFrame,pandas.DataFrame.mode,"pandas.DataFrame.mode#DataFrame.mode(axis=0,numeric_only=False,dropna=True)[source]#Get the mode(s) of each element along the selected axis.The mode of a set of values is the value that appears most often.
It can be multiple values.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to iterate over while searching for the mode:0 or eindexf : get mode of each column1 or ecolumnsf : get mode of each row.numeric_onlybool, default FalseIf True, only apply to numeric columns.dropnabool, default TrueDonft consider counts of NaN/NaT.Returns:DataFrameThe modes of each column or row.See alsoSeries.modeReturn the highest frequency value in a Series.Series.value_countsReturn the counts of values in a Series.Examples>>>df=pd.DataFrame([('bird',2,2),...('mammal',4,np.nan),...('arthropod',8,0),...('bird',2,np.nan)],...index=('falcon','horse','spider','ostrich'),...columns=('species','legs','wings'))>>>dfspecies  legs  wingsfalcon        bird     2    2.0horse       mammal     4    NaNspider   arthropod     8    0.0ostrich       bird     2    NaNBy default, missing values are not considered, and the mode of wings
are both 0 and 2. Because the resulting DataFrame has two rows,
the second row ofspeciesandlegscontainsNaN.>>>df.mode()species  legs  wings0    bird   2.0    0.01     NaN   NaN    2.0Settingdropna=FalseNaNvalues are considered and they can be
the mode (like for wings).>>>df.mode(dropna=False)species  legs  wings0    bird     2    NaNSettingnumeric_only=True, only the mode of numeric columns is
computed, and columns of other types are ignored.>>>df.mode(numeric_only=True)legs  wings0   2.0    0.01   NaN    2.0To compute the mode over columns and not rows, use the axis parameter:>>>df.mode(axis='columns',numeric_only=True)0    1falcon   2.0  NaNhorse    4.0  NaNspider   0.0  8.0ostrich  2.0  NaN"
Pandas,DataFrame,pandas.DataFrame.pct_change,"pandas.DataFrame.pct_change#DataFrame.pct_change(periods=1,fill_method=_NoDefault.no_default,limit=_NoDefault.no_default,freq=None,**kwargs)[source]#Fractional change between the current and a prior element.Computes the fractional change from the immediately previous row by
default. This is useful in comparing the fraction of change in a time
series of elements.NoteDespite the name of this method, it calculates fractional change
(also known as per unit change or relative change) and not
percentage change. If you need the percentage change, multiply
these values by 100.Parameters:periodsint, default 1Periods to shift for forming percent change.fill_method{ebackfillf, ebfillf, epadf, effillf, None}, default epadfHow to handle NAsbeforecomputing percent changes.Deprecated since version 2.1:All options offill_methodare deprecated exceptfill_method=None.limitint, default NoneThe number of consecutive NAs to fill before stopping.Deprecated since version 2.1.freqDateOffset, timedelta, or str, optionalIncrement to use from time series API (e.g. eMf or BDay()).**kwargsAdditional keyword arguments are passed intoDataFrame.shiftorSeries.shift.Returns:Series or DataFrameThe same type as the calling object.See alsoSeries.diffCompute the difference of two elements in a Series.DataFrame.diffCompute the difference of two elements in a DataFrame.Series.shiftShift the index by some number of periods.DataFrame.shiftShift the index by some number of periods.ExamplesSeries>>>s=pd.Series([90,91,85])>>>s0    901    912    85dtype: int64>>>s.pct_change()0         NaN1    0.0111112   -0.065934dtype: float64>>>s.pct_change(periods=2)0         NaN1         NaN2   -0.055556dtype: float64See the percentage change in a Series where filling NAs with last
valid observation forward to next valid.>>>s=pd.Series([90,91,None,85])>>>s0    90.01    91.02     NaN3    85.0dtype: float64>>>s.ffill().pct_change()0         NaN1    0.0111112    0.0000003   -0.065934dtype: float64DataFramePercentage change in French franc, Deutsche Mark, and Italian lira from
1980-01-01 to 1980-03-01.>>>df=pd.DataFrame({...'FR':[4.0405,4.0963,4.3149],...'GR':[1.7246,1.7482,1.8519],...'IT':[804.74,810.01,860.13]},...index=['1980-01-01','1980-02-01','1980-03-01'])>>>dfFR      GR      IT1980-01-01  4.0405  1.7246  804.741980-02-01  4.0963  1.7482  810.011980-03-01  4.3149  1.8519  860.13>>>df.pct_change()FR        GR        IT1980-01-01       NaN       NaN       NaN1980-02-01  0.013810  0.013684  0.0065491980-03-01  0.053365  0.059318  0.061876Percentage of change in GOOG and APPL stock volume. Shows computing
the percentage change between columns.>>>df=pd.DataFrame({...'2016':[1769950,30586265],...'2015':[1500923,40912316],...'2014':[1371819,41403351]},...index=['GOOG','APPL'])>>>df2016      2015      2014GOOG   1769950   1500923   1371819APPL  30586265  40912316  41403351>>>df.pct_change(axis='columns',periods=-1)2016      2015  2014GOOG  0.179241  0.094112   NaNAPPL -0.252395 -0.011860   NaN"
Pandas,DataFrame,pandas.DataFrame.prod,"pandas.DataFrame.prod#DataFrame.prod(axis=0,skipna=True,numeric_only=False,min_count=0,**kwargs)[source]#Return the product of the values over the requested axis.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.min_countint, default 0The required number of valid values to perform the operation. If fewer thanmin_countnon-NA values are present the result will be NA.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.ExamplesBy default, the product of an empty or all-NA Series is1>>>pd.Series([],dtype=""float64"").prod()1.0This can be controlled with themin_countparameter>>>pd.Series([],dtype=""float64"").prod(min_count=1)nanThanks to theskipnaparameter,min_counthandles all-NA and
empty series identically.>>>pd.Series([np.nan]).prod()1.0>>>pd.Series([np.nan]).prod(min_count=1)nan"
Pandas,DataFrame,pandas.DataFrame.product,"pandas.DataFrame.product#DataFrame.product(axis=0,skipna=True,numeric_only=False,min_count=0,**kwargs)[source]#Return the product of the values over the requested axis.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.min_countint, default 0The required number of valid values to perform the operation. If fewer thanmin_countnon-NA values are present the result will be NA.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.ExamplesBy default, the product of an empty or all-NA Series is1>>>pd.Series([],dtype=""float64"").prod()1.0This can be controlled with themin_countparameter>>>pd.Series([],dtype=""float64"").prod(min_count=1)nanThanks to theskipnaparameter,min_counthandles all-NA and
empty series identically.>>>pd.Series([np.nan]).prod()1.0>>>pd.Series([np.nan]).prod(min_count=1)nan"
Pandas,DataFrame,pandas.DataFrame.quantile,"pandas.DataFrame.quantile#DataFrame.quantile(q=0.5,axis=0,numeric_only=False,interpolation='linear',method='single')[source]#Return values at the given quantile over requested axis.Parameters:qfloat or array-like, default 0.5 (50% quantile)Value between 0 <= q <= 1, the quantile(s) to compute.axis{0 or eindexf, 1 or ecolumnsf}, default 0Equals 0 or eindexf for row-wise, 1 or ecolumnsf for column-wise.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.interpolation{elinearf, elowerf, ehigherf, emidpointf, enearestf}This optional parameter specifies the interpolation method to use,
when the desired quantile lies between two data pointsiandj:linear:i + (j - i) * fraction, wherefractionis the
fractional part of the index surrounded byiandj.lower:i.higher:j.nearest:iorjwhichever is nearest.midpoint: (i+j) / 2.method{esinglef, etablef}, default esinglefWhether to compute quantiles per-column (esinglef) or over all columns
(etablef). When etablef, the only allowed interpolation methods are
enearestf, elowerf, and ehigherf.Returns:Series or DataFrameIfqis an array, a DataFrame will be returned where theindex isq, the columns are the columns of self, and the
values are the quantiles.Ifqis a float, a Series will be returned where theindex is the columns of self and the values are the quantiles.See alsocore.window.rolling.Rolling.quantileRolling quantile.numpy.percentileNumpy function to compute the percentile.Examples>>>df=pd.DataFrame(np.array([[1,1],[2,10],[3,100],[4,100]]),...columns=['a','b'])>>>df.quantile(.1)a    1.3b    3.7Name: 0.1, dtype: float64>>>df.quantile([.1,.5])a     b0.1  1.3   3.70.5  2.5  55.0Specifyingmethod=ftablefwill compute the quantile over all columns.>>>df.quantile(.1,method=""table"",interpolation=""nearest"")a    1b    1Name: 0.1, dtype: int64>>>df.quantile([.1,.5],method=""table"",interpolation=""nearest"")a    b0.1  1    10.5  3  100Specifyingnumeric_only=Falsewill also compute the quantile of
datetime and timedelta data.>>>df=pd.DataFrame({'A':[1,2],...'B':[pd.Timestamp('2010'),...pd.Timestamp('2011')],...'C':[pd.Timedelta('1 days'),...pd.Timedelta('2 days')]})>>>df.quantile(0.5,numeric_only=False)A                    1.5B    2010-07-02 12:00:00C        1 days 12:00:00Name: 0.5, dtype: object"
Pandas,DataFrame,pandas.DataFrame.rank,"pandas.DataFrame.rank#DataFrame.rank(axis=0,method='average',numeric_only=False,na_option='keep',ascending=True,pct=False)[source]#Compute numerical data ranks (1 through n) along axis.By default, equal values are assigned a rank that is the average of the
ranks of those values.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0Index to direct ranking.
ForSeriesthis parameter is unused and defaults to 0.method{eaveragef, eminf, emaxf, efirstf, edensef}, default eaveragefHow to rank the group of records that have the same value (i.e. ties):average: average rank of the groupmin: lowest rank in the groupmax: highest rank in the groupfirst: ranks assigned in order they appear in the arraydense: like eminf, but rank always increases by 1 between groups.numeric_onlybool, default FalseFor DataFrame objects, rank only numeric columns if set to True.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.na_option{ekeepf, etopf, ebottomf}, default ekeepfHow to rank NaN values:keep: assign NaN rank to NaN valuestop: assign lowest rank to NaN valuesbottom: assign highest rank to NaN valuesascendingbool, default TrueWhether or not the elements should be ranked in ascending order.pctbool, default FalseWhether or not to display the returned rankings in percentile
form.Returns:same type as callerReturn a Series or DataFrame with data ranks as values.See alsocore.groupby.DataFrameGroupBy.rankRank of values within each group.core.groupby.SeriesGroupBy.rankRank of values within each group.Examples>>>df=pd.DataFrame(data={'Animal':['cat','penguin','dog',...'spider','snake'],...'Number_legs':[4,2,4,8,np.nan]})>>>dfAnimal  Number_legs0      cat          4.01  penguin          2.02      dog          4.03   spider          8.04    snake          NaNTies are assigned the mean of the ranks (by default) for the group.>>>s=pd.Series(range(5),index=list(""abcde""))>>>s[""d""]=s[""b""]>>>s.rank()a    1.0b    2.5c    4.0d    2.5e    5.0dtype: float64The following example shows how the method behaves with the above
parameters:default_rank: this is the default behaviour obtained without using
any parameter.max_rank: settingmethod='max'the records that have the
same values are ranked using the highest rank (e.g.: since ecatf
and edogf are both in the 2nd and 3rd position, rank 3 is assigned.)NA_bottom: choosingna_option='bottom', if there are records
with NaN values they are placed at the bottom of the ranking.pct_rank: when settingpct=True, the ranking is expressed as
percentile rank.>>>df['default_rank']=df['Number_legs'].rank()>>>df['max_rank']=df['Number_legs'].rank(method='max')>>>df['NA_bottom']=df['Number_legs'].rank(na_option='bottom')>>>df['pct_rank']=df['Number_legs'].rank(pct=True)>>>dfAnimal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank0      cat          4.0           2.5       3.0        2.5     0.6251  penguin          2.0           1.0       1.0        1.0     0.2502      dog          4.0           2.5       3.0        2.5     0.6253   spider          8.0           4.0       4.0        4.0     1.0004    snake          NaN           NaN       NaN        5.0       NaN"
Pandas,DataFrame,pandas.DataFrame.round,"pandas.DataFrame.round#DataFrame.round(decimals=0,*args,**kwargs)[source]#Round a DataFrame to a variable number of decimal places.Parameters:decimalsint, dict, SeriesNumber of decimal places to round each column to. If an int is
given, round each column to the same number of places.
Otherwise dict and Series round to variable numbers of places.
Column names should be in the keys ifdecimalsis a
dict-like, or in the index ifdecimalsis a Series. Any
columns not included indecimalswill be left as is. Elements
ofdecimalswhich are not columns of the input will be
ignored.*argsAdditional keywords have no effect but might be accepted for
compatibility with numpy.**kwargsAdditional keywords have no effect but might be accepted for
compatibility with numpy.Returns:DataFrameA DataFrame with the affected columns rounded to the specified
number of decimal places.See alsonumpy.aroundRound a numpy array to the given number of decimals.Series.roundRound a Series to the given number of decimals.Examples>>>df=pd.DataFrame([(.21,.32),(.01,.67),(.66,.03),(.21,.18)],...columns=['dogs','cats'])>>>dfdogs  cats0  0.21  0.321  0.01  0.672  0.66  0.033  0.21  0.18By providing an integer each column is rounded to the same number
of decimal places>>>df.round(1)dogs  cats0   0.2   0.31   0.0   0.72   0.7   0.03   0.2   0.2With a dict, the number of places for specific columns can be
specified with the column names as key and the number of decimal
places as value>>>df.round({'dogs':1,'cats':0})dogs  cats0   0.2   0.01   0.0   1.02   0.7   0.03   0.2   0.0Using a Series, the number of places for specific columns can be
specified with the column names as index and the number of
decimal places as value>>>decimals=pd.Series([0,1],index=['cats','dogs'])>>>df.round(decimals)dogs  cats0   0.2   0.01   0.0   1.02   0.7   0.03   0.2   0.0"
Pandas,DataFrame,pandas.DataFrame.sem,"pandas.DataFrame.sem#DataFrame.sem(axis=0,skipna=True,ddof=1,numeric_only=False,**kwargs)[source]#Return unbiased standard error of the mean over requested axis.Normalized by N-1 by default. This can be changed using the ddof argumentParameters:axis{index (0), columns (1)}ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
where N represents the number of elements.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.Returns:Series or DataFrame (if level specified)Examples>>>s=pd.Series([1,2,3])>>>s.sem().round(6)0.57735With a DataFrame>>>df=pd.DataFrame({'a':[1,2],'b':[2,3]},index=['tiger','zebra'])>>>dfa   btiger  1   2zebra  2   3>>>df.sem()a   0.5b   0.5dtype: float64Using axis=1>>>df.sem(axis=1)tiger   0.5zebra   0.5dtype: float64In this case,numeric_onlyshould be set toTrueto avoid getting an error.>>>df=pd.DataFrame({'a':[1,2],'b':['T','Z']},...index=['tiger','zebra'])>>>df.sem(numeric_only=True)a   0.5dtype: float64"
Pandas,DataFrame,pandas.DataFrame.skew,"pandas.DataFrame.skew#DataFrame.skew(axis=0,skipna=True,numeric_only=False,**kwargs)[source]#Return unbiased skew over requested axis.Normalized by N-1.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarExamples>>>s=pd.Series([1,2,3])>>>s.skew()0.0With a DataFrame>>>df=pd.DataFrame({'a':[1,2,3],'b':[2,3,4],'c':[1,3,5]},...index=['tiger','zebra','cow'])>>>dfa   b   ctiger   1   2   1zebra   2   3   3cow     3   4   5>>>df.skew()a   0.0b   0.0c   0.0dtype: float64Using axis=1>>>df.skew(axis=1)tiger   1.732051zebra  -1.732051cow     0.000000dtype: float64In this case,numeric_onlyshould be set toTrueto avoid
getting an error.>>>df=pd.DataFrame({'a':[1,2,3],'b':['T','Z','X']},...index=['tiger','zebra','cow'])>>>df.skew(numeric_only=True)a   0.0dtype: float64"
Pandas,DataFrame,pandas.DataFrame.sum,"pandas.DataFrame.sum#DataFrame.sum(axis=0,skipna=True,numeric_only=False,min_count=0,**kwargs)[source]#Return the sum of the values over the requested axis.This is equivalent to the methodnumpy.sum.Parameters:axis{index (0), columns (1)}Axis for the function to be applied on.
ForSeriesthis parameter is unused and defaults to 0.For DataFrames, specifyingaxis=Nonewill apply the aggregation
across both axes.New in version 2.0.0.skipnabool, default TrueExclude NA/null values when computing the result.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.min_countint, default 0The required number of valid values to perform the operation. If fewer thanmin_countnon-NA values are present the result will be NA.**kwargsAdditional keyword arguments to be passed to the function.Returns:Series or scalarSee alsoSeries.sumReturn the sum.Series.minReturn the minimum.Series.maxReturn the maximum.Series.idxminReturn the index of the minimum.Series.idxmaxReturn the index of the maximum.DataFrame.sumReturn the sum over the requested axis.DataFrame.minReturn the minimum over the requested axis.DataFrame.maxReturn the maximum over the requested axis.DataFrame.idxminReturn the index of the minimum over the requested axis.DataFrame.idxmaxReturn the index of the maximum over the requested axis.Examples>>>idx=pd.MultiIndex.from_arrays([...['warm','warm','cold','cold'],...['dog','falcon','fish','spider']],...names=['blooded','animal'])>>>s=pd.Series([4,2,0,8],name='legs',index=idx)>>>sblooded  animalwarm     dog       4falcon    2cold     fish      0spider    8Name: legs, dtype: int64>>>s.sum()14By default, the sum of an empty or all-NA Series is0.>>>pd.Series([],dtype=""float64"").sum()# min_count=0 is the default0.0This can be controlled with themin_countparameter. For example, if
youfd like the sum of an empty series to be NaN, passmin_count=1.>>>pd.Series([],dtype=""float64"").sum(min_count=1)nanThanks to theskipnaparameter,min_counthandles all-NA and
empty series identically.>>>pd.Series([np.nan]).sum()0.0>>>pd.Series([np.nan]).sum(min_count=1)nan"
Pandas,DataFrame,pandas.DataFrame.std,"pandas.DataFrame.std#DataFrame.std(axis=0,skipna=True,ddof=1,numeric_only=False,**kwargs)[source]#Return sample standard deviation over requested axis.Normalized by N-1 by default. This can be changed using the ddof argument.Parameters:axis{index (0), columns (1)}ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
where N represents the number of elements.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.Returns:Series or DataFrame (if level specified)NotesTo have the same behaviour asnumpy.std, useddof=0(instead of the
defaultddof=1)Examples>>>df=pd.DataFrame({'person_id':[0,1,2,3],...'age':[21,25,62,43],...'height':[1.61,1.87,1.49,2.01]}...).set_index('person_id')>>>dfage  heightperson_id0           21    1.611           25    1.872           62    1.493           43    2.01The standard deviation of the columns can be found as follows:>>>df.std()age       18.786076height     0.237417dtype: float64Alternatively,ddof=0can be set to normalize by N instead of N-1:>>>df.std(ddof=0)age       16.269219height     0.205609dtype: float64"
Pandas,DataFrame,pandas.DataFrame.var,"pandas.DataFrame.var#DataFrame.var(axis=0,skipna=True,ddof=1,numeric_only=False,**kwargs)[source]#Return unbiased variance over requested axis.Normalized by N-1 by default. This can be changed using the ddof argument.Parameters:axis{index (0), columns (1)}ForSeriesthis parameter is unused and defaults to 0.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
where N represents the number of elements.numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series.Returns:Series or DataFrame (if level specified)Examples>>>df=pd.DataFrame({'person_id':[0,1,2,3],...'age':[21,25,62,43],...'height':[1.61,1.87,1.49,2.01]}...).set_index('person_id')>>>dfage  heightperson_id0           21    1.611           25    1.872           62    1.493           43    2.01>>>df.var()age       352.916667height      0.056367dtype: float64Alternatively,ddof=0can be set to normalize by N instead of N-1:>>>df.var(ddof=0)age       264.687500height      0.042275dtype: float64"
Pandas,DataFrame,pandas.DataFrame.nunique,"pandas.DataFrame.nunique#DataFrame.nunique(axis=0,dropna=True)[source]#Count number of distinct elements in specified axis.Return Series with number of distinct elements. Can ignore NaN
values.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to use. 0 or eindexf for row-wise, 1 or ecolumnsf for
column-wise.dropnabool, default TrueDonft include NaN in the counts.Returns:SeriesSee alsoSeries.nuniqueMethod nunique for Series.DataFrame.countCount non-NA cells for each column or row.Examples>>>df=pd.DataFrame({'A':[4,5,6],'B':[4,1,1]})>>>df.nunique()A    3B    2dtype: int64>>>df.nunique(axis=1)0    11    22    2dtype: int64"
Pandas,DataFrame,pandas.DataFrame.value_counts,"pandas.DataFrame.value_counts#DataFrame.value_counts(subset=None,normalize=False,sort=True,ascending=False,dropna=True)[source]#Return a Series containing the frequency of each distinct row in the Dataframe.Parameters:subsetlabel or list of labels, optionalColumns to use when counting unique combinations.normalizebool, default FalseReturn proportions rather than frequencies.sortbool, default TrueSort by frequencies when True. Sort by DataFrame column values when False.ascendingbool, default FalseSort in ascending order.dropnabool, default TrueDonft include counts of rows that contain NA values.New in version 1.3.0.Returns:SeriesSee alsoSeries.value_countsEquivalent method on Series.NotesThe returned Series will have a MultiIndex with one level per input
column but an Index (non-multi) for a single label. By default, rows
that contain any NA values are omitted from the result. By default,
the resulting Series will be in descending order so that the first
element is the most frequently-occurring row.Examples>>>df=pd.DataFrame({'num_legs':[2,4,4,6],...'num_wings':[2,0,0,0]},...index=['falcon','dog','cat','ant'])>>>dfnum_legs  num_wingsfalcon         2          2dog            4          0cat            4          0ant            6          0>>>df.value_counts()num_legs  num_wings4         0            22         2            16         0            1Name: count, dtype: int64>>>df.value_counts(sort=False)num_legs  num_wings2         2            14         0            26         0            1Name: count, dtype: int64>>>df.value_counts(ascending=True)num_legs  num_wings2         2            16         0            14         0            2Name: count, dtype: int64>>>df.value_counts(normalize=True)num_legs  num_wings4         0            0.502         2            0.256         0            0.25Name: proportion, dtype: float64Withdropnaset toFalsewe can also count rows with NA values.>>>df=pd.DataFrame({'first_name':['John','Anne','John','Beth'],...'middle_name':['Smith',pd.NA,pd.NA,'Louise']})>>>dffirst_name middle_name0       John       Smith1       Anne        <NA>2       John        <NA>3       Beth      Louise>>>df.value_counts()first_name  middle_nameBeth        Louise         1John        Smith          1Name: count, dtype: int64>>>df.value_counts(dropna=False)first_name  middle_nameAnne        NaN            1Beth        Louise         1John        Smith          1NaN            1Name: count, dtype: int64>>>df.value_counts(""first_name"")first_nameJohn    2Anne    1Beth    1Name: count, dtype: int64"
Pandas,DataFrame,pandas.DataFrame.add_prefix,"pandas.DataFrame.add_prefix#DataFrame.add_prefix(prefix,axis=None)[source]#Prefix labels with stringprefix.For Series, the row labels are prefixed.
For DataFrame, the column labels are prefixed.Parameters:prefixstrThe string to add before each label.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneAxis to add prefix onNew in version 2.0.0.Returns:Series or DataFrameNew Series or DataFrame with updated labels.See alsoSeries.add_suffixSuffix row labels with stringsuffix.DataFrame.add_suffixSuffix column labels with stringsuffix.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.add_prefix('item_')item_0    1item_1    2item_2    3item_3    4dtype: int64>>>df=pd.DataFrame({'A':[1,2,3,4],'B':[3,4,5,6]})>>>dfA  B0  1  31  2  42  3  53  4  6>>>df.add_prefix('col_')col_A  col_B0       1       31       2       42       3       53       4       6"
Pandas,DataFrame,pandas.DataFrame.add_suffix,"pandas.DataFrame.add_suffix#DataFrame.add_suffix(suffix,axis=None)[source]#Suffix labels with stringsuffix.For Series, the row labels are suffixed.
For DataFrame, the column labels are suffixed.Parameters:suffixstrThe string to add after each label.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneAxis to add suffix onNew in version 2.0.0.Returns:Series or DataFrameNew Series or DataFrame with updated labels.See alsoSeries.add_prefixPrefix row labels with stringprefix.DataFrame.add_prefixPrefix column labels with stringprefix.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.add_suffix('_item')0_item    11_item    22_item    33_item    4dtype: int64>>>df=pd.DataFrame({'A':[1,2,3,4],'B':[3,4,5,6]})>>>dfA  B0  1  31  2  42  3  53  4  6>>>df.add_suffix('_col')A_col  B_col0       1       31       2       42       3       53       4       6"
Pandas,DataFrame,pandas.DataFrame.align,"pandas.DataFrame.align#DataFrame.align(other,join='outer',axis=None,level=None,copy=None,fill_value=None,method=_NoDefault.no_default,limit=_NoDefault.no_default,fill_axis=_NoDefault.no_default,broadcast_axis=_NoDefault.no_default)[source]#Align two objects on their axes with the specified join method.Join method is specified for each axis Index.Parameters:otherDataFrame or Seriesjoin{eouterf, einnerf, eleftf, erightf}, default eouterfType of alignment to be performed.left: use only keys from left frame, preserve key order.right: use only keys from right frame, preserve key order.outer: use union of keys from both frames, sort keys lexicographically.inner: use intersection of keys from both frames,
preserve the order of the left keys.axisallowed axis of the other object, default NoneAlign on index (0), columns (1), or both (None).levelint or level name, default NoneBroadcast across a level, matching Index values on the
passed MultiIndex level.copybool, default TrueAlways returns new objects. If copy=False and no reindexing is
required then original objects are returned.fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any
gcompatibleh value.method{ebackfillf, ebfillf, epadf, effillf, None}, default NoneMethod to use for filling holes in reindexed Series:pad / ffill: propagate last valid observation forward to next valid.backfill / bfill: use NEXT valid observation to fill gap.Deprecated since version 2.1.limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.Deprecated since version 2.1.fill_axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrame, default 0Filling axis, method and limit.Deprecated since version 2.1.broadcast_axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrame, default NoneBroadcast values along this axis, if aligning two objects of
different dimensions.Deprecated since version 2.1.Returns:tuple of (Series/DataFrame, type of other)Aligned objects.Examples>>>df=pd.DataFrame(...[[1,2,3,4],[6,7,8,9]],columns=[""D"",""B"",""E"",""A""],index=[1,2]...)>>>other=pd.DataFrame(...[[10,20,30,40],[60,70,80,90],[600,700,800,900]],...columns=[""A"",""B"",""C"",""D""],...index=[2,3,4],...)>>>dfD  B  E  A1  1  2  3  42  6  7  8  9>>>otherA    B    C    D2   10   20   30   403   60   70   80   904  600  700  800  900Align on columns:>>>left,right=df.align(other,join=""outer"",axis=1)>>>leftA  B   C  D  E1  4  2 NaN  1  32  9  7 NaN  6  8>>>rightA    B    C    D   E2   10   20   30   40 NaN3   60   70   80   90 NaN4  600  700  800  900 NaNWe can also align on the index:>>>left,right=df.align(other,join=""outer"",axis=0)>>>leftD    B    E    A1  1.0  2.0  3.0  4.02  6.0  7.0  8.0  9.03  NaN  NaN  NaN  NaN4  NaN  NaN  NaN  NaN>>>rightA      B      C      D1    NaN    NaN    NaN    NaN2   10.0   20.0   30.0   40.03   60.0   70.0   80.0   90.04  600.0  700.0  800.0  900.0Finally, the defaultaxis=Nonewill align on both index and columns:>>>left,right=df.align(other,join=""outer"",axis=None)>>>leftA    B   C    D    E1  4.0  2.0 NaN  1.0  3.02  9.0  7.0 NaN  6.0  8.03  NaN  NaN NaN  NaN  NaN4  NaN  NaN NaN  NaN  NaN>>>rightA      B      C      D   E1    NaN    NaN    NaN    NaN NaN2   10.0   20.0   30.0   40.0 NaN3   60.0   70.0   80.0   90.0 NaN4  600.0  700.0  800.0  900.0 NaN"
Pandas,DataFrame,pandas.DataFrame.at_time,"pandas.DataFrame.at_time#DataFrame.at_time(time,asof=False,axis=None)[source]#Select values at particular time of day (e.g., 9:30AM).Parameters:timedatetime.time or strThe values to select.axis{0 or eindexf, 1 or ecolumnsf}, default 0ForSeriesthis parameter is unused and defaults to 0.Returns:Series or DataFrameRaises:TypeErrorIf the index is not aDatetimeIndexSee alsobetween_timeSelect values between particular times of the day.firstSelect initial periods of time series based on a date offset.lastSelect final periods of time series based on a date offset.DatetimeIndex.indexer_at_timeGet just the index locations for values at particular time of the day.Examples>>>i=pd.date_range('2018-04-09',periods=4,freq='12H')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09 00:00:00  12018-04-09 12:00:00  22018-04-10 00:00:00  32018-04-10 12:00:00  4>>>ts.at_time('12:00')A2018-04-09 12:00:00  22018-04-10 12:00:00  4"
Pandas,DataFrame,pandas.DataFrame.between_time,"pandas.DataFrame.between_time#DataFrame.between_time(start_time,end_time,inclusive='both',axis=None)[source]#Select values between particular times of the day (e.g., 9:00-9:30 AM).By settingstart_timeto be later thanend_time,
you can get the times that arenotbetween the two times.Parameters:start_timedatetime.time or strInitial time as a time filter limit.end_timedatetime.time or strEnd time as a time filter limit.inclusive{gbothh, gneitherh, glefth, grighth}, default gbothhInclude boundaries; whether to set each bound as closed or open.axis{0 or eindexf, 1 or ecolumnsf}, default 0Determine range time on index or columns value.
ForSeriesthis parameter is unused and defaults to 0.Returns:Series or DataFrameData from the original object filtered to the specified dates range.Raises:TypeErrorIf the index is not aDatetimeIndexSee alsoat_timeSelect values at a particular time of the day.firstSelect initial periods of time series based on a date offset.lastSelect final periods of time series based on a date offset.DatetimeIndex.indexer_between_timeGet just the index locations for values between particular times of the day.Examples>>>i=pd.date_range('2018-04-09',periods=4,freq='1D20min')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09 00:00:00  12018-04-10 00:20:00  22018-04-11 00:40:00  32018-04-12 01:00:00  4>>>ts.between_time('0:15','0:45')A2018-04-10 00:20:00  22018-04-11 00:40:00  3You get the times that arenotbetween two times by settingstart_timelater thanend_time:>>>ts.between_time('0:45','0:15')A2018-04-09 00:00:00  12018-04-12 01:00:00  4"
Pandas,DataFrame,pandas.DataFrame.drop,"pandas.DataFrame.drop#DataFrame.drop(labels=None,*,axis=0,index=None,columns=None,level=None,inplace=False,errors='raise')[source]#Drop specified labels from rows or columns.Remove rows or columns by specifying label names and corresponding
axis, or by directly specifying index or column names. When using a
multi-index, labels on different levels can be removed by specifying
the level. See theuser guidefor more information about the now unused levels.Parameters:labelssingle label or list-likeIndex or column labels to drop. A tuple will be used as a single
label and not treated as a list-like.axis{0 or eindexf, 1 or ecolumnsf}, default 0Whether to drop labels from the index (0 or eindexf) or
columns (1 or ecolumnsf).indexsingle label or list-likeAlternative to specifying axis (labels,axis=0is equivalent toindex=labels).columnssingle label or list-likeAlternative to specifying axis (labels,axis=1is equivalent tocolumns=labels).levelint or level name, optionalFor MultiIndex, level from which the labels will be removed.inplacebool, default FalseIf False, return a copy. Otherwise, do operation
in place and return None.errors{eignoref, eraisef}, default eraisefIf eignoref, suppress error and only existing labels are
dropped.Returns:DataFrame or NoneReturns DataFrame or None DataFrame with the specified
index or column labels removed or None if inplace=True.Raises:KeyErrorIf any of the labels is not found in the selected axis.See alsoDataFrame.locLabel-location based indexer for selection by label.DataFrame.dropnaReturn DataFrame with labels on given axis omitted where (all or any) data are missing.DataFrame.drop_duplicatesReturn DataFrame with duplicate rows removed, optionally only considering certain columns.Series.dropReturn Series with specified index labels removed.Examples>>>df=pd.DataFrame(np.arange(12).reshape(3,4),...columns=['A','B','C','D'])>>>dfA  B   C   D0  0  1   2   31  4  5   6   72  8  9  10  11Drop columns>>>df.drop(['B','C'],axis=1)A   D0  0   31  4   72  8  11>>>df.drop(columns=['B','C'])A   D0  0   31  4   72  8  11Drop a row by index>>>df.drop([0,1])A  B   C   D2  8  9  10  11Drop columns and/or rows of MultiIndex DataFrame>>>midx=pd.MultiIndex(levels=[['llama','cow','falcon'],...['speed','weight','length']],...codes=[[0,0,0,1,1,1,2,2,2],...[0,1,2,0,1,2,0,1,2]])>>>df=pd.DataFrame(index=midx,columns=['big','small'],...data=[[45,30],[200,100],[1.5,1],[30,20],...[250,150],[1.5,0.8],[320,250],...[1,0.8],[0.3,0.2]])>>>dfbig     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0weight  1.0     0.8length  0.3     0.2Drop a specific index combination from the MultiIndex
DataFrame, i.e., drop the combination'falcon'and'weight', which deletes only the corresponding row>>>df.drop(index=('falcon','weight'))big     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0length  0.3     0.2>>>df.drop(index='cow',columns='small')bigllama   speed   45.0weight  200.0length  1.5falcon  speed   320.0weight  1.0length  0.3>>>df.drop(index='length',level=1)big     smallllama   speed   45.0    30.0weight  200.0   100.0cow     speed   30.0    20.0weight  250.0   150.0falcon  speed   320.0   250.0weight  1.0     0.8"
Pandas,DataFrame,pandas.DataFrame.drop_duplicates,"pandas.DataFrame.drop_duplicates#DataFrame.drop_duplicates(subset=None,*,keep='first',inplace=False,ignore_index=False)[source]#Return DataFrame with duplicate rows removed.Considering certain columns is optional. Indexes, including time indexes
are ignored.Parameters:subsetcolumn label or sequence of labels, optionalOnly consider certain columns for identifying duplicates, by
default use all of the columns.keep{efirstf, elastf,False}, default efirstfDetermines which duplicates (if any) to keep.efirstf : Drop duplicates except for the first occurrence.elastf : Drop duplicates except for the last occurrence.False: Drop all duplicates.inplacebool, defaultFalseWhether to modify the DataFrame rather than creating a new one.ignore_indexbool, defaultFalseIfTrue, the resulting axis will be labeled 0, 1, c, n - 1.Returns:DataFrame or NoneDataFrame with duplicates removed or None ifinplace=True.See alsoDataFrame.value_countsCount unique combinations of columns.ExamplesConsider dataset containing ramen rating.>>>df=pd.DataFrame({...'brand':['Yum Yum','Yum Yum','Indomie','Indomie','Indomie'],...'style':['cup','cup','cup','pack','pack'],...'rating':[4,4,3.5,15,5]...})>>>dfbrand style  rating0  Yum Yum   cup     4.01  Yum Yum   cup     4.02  Indomie   cup     3.53  Indomie  pack    15.04  Indomie  pack     5.0By default, it removes duplicate rows based on all columns.>>>df.drop_duplicates()brand style  rating0  Yum Yum   cup     4.02  Indomie   cup     3.53  Indomie  pack    15.04  Indomie  pack     5.0To remove duplicates on specific column(s), usesubset.>>>df.drop_duplicates(subset=['brand'])brand style  rating0  Yum Yum   cup     4.02  Indomie   cup     3.5To remove duplicates and keep last occurrences, usekeep.>>>df.drop_duplicates(subset=['brand','style'],keep='last')brand style  rating1  Yum Yum   cup     4.02  Indomie   cup     3.54  Indomie  pack     5.0"
Pandas,DataFrame,pandas.DataFrame.duplicated,"pandas.DataFrame.duplicated#DataFrame.duplicated(subset=None,keep='first')[source]#Return boolean Series denoting duplicate rows.Considering certain columns is optional.Parameters:subsetcolumn label or sequence of labels, optionalOnly consider certain columns for identifying duplicates, by
default use all of the columns.keep{efirstf, elastf, False}, default efirstfDetermines which duplicates (if any) to mark.first: Mark duplicates asTrueexcept for the first occurrence.last: Mark duplicates asTrueexcept for the last occurrence.False : Mark all duplicates asTrue.Returns:SeriesBoolean series for each duplicated rows.See alsoIndex.duplicatedEquivalent method on index.Series.duplicatedEquivalent method on Series.Series.drop_duplicatesRemove duplicate values from Series.DataFrame.drop_duplicatesRemove duplicate values from DataFrame.ExamplesConsider dataset containing ramen rating.>>>df=pd.DataFrame({...'brand':['Yum Yum','Yum Yum','Indomie','Indomie','Indomie'],...'style':['cup','cup','cup','pack','pack'],...'rating':[4,4,3.5,15,5]...})>>>dfbrand style  rating0  Yum Yum   cup     4.01  Yum Yum   cup     4.02  Indomie   cup     3.53  Indomie  pack    15.04  Indomie  pack     5.0By default, for each set of duplicated values, the first occurrence
is set on False and all others on True.>>>df.duplicated()0    False1     True2    False3    False4    Falsedtype: boolBy using elastf, the last occurrence of each set of duplicated values
is set on False and all others on True.>>>df.duplicated(keep='last')0     True1    False2    False3    False4    Falsedtype: boolBy settingkeepon False, all duplicates are True.>>>df.duplicated(keep=False)0     True1     True2    False3    False4    Falsedtype: boolTo find duplicates on specific column(s), usesubset.>>>df.duplicated(subset=['brand'])0    False1     True2    False3     True4     Truedtype: bool"
Pandas,DataFrame,pandas.DataFrame.equals,"pandas.DataFrame.equals#DataFrame.equals(other)[source]#Test whether two objects contain the same elements.This function allows two Series or DataFrames to be compared against
each other to see if they have the same shape and elements. NaNs in
the same location are considered equal.The row/column index do not need to have the same type, as long
as the values are considered equal. Corresponding columns must be of
the same dtype.Parameters:otherSeries or DataFrameThe other Series or DataFrame to be compared with the first.Returns:boolTrue if all elements are the same in both objects, False
otherwise.See alsoSeries.eqCompare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise.DataFrame.eqCompare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise.testing.assert_series_equalRaises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others.testing.assert_frame_equalLike assert_series_equal, but targets DataFrames.numpy.array_equalReturn True if two arrays have the same shape and elements, False otherwise.Examples>>>df=pd.DataFrame({1:[10],2:[20]})>>>df1   20  10  20DataFrames df and exactly_equal have the same types and values for
their elements and column labels, which will return True.>>>exactly_equal=pd.DataFrame({1:[10],2:[20]})>>>exactly_equal1   20  10  20>>>df.equals(exactly_equal)TrueDataFrames df and different_column_type have the same element
types and values, but have different types for the column labels,
which will still return True.>>>different_column_type=pd.DataFrame({1.0:[10],2.0:[20]})>>>different_column_type1.0  2.00   10   20>>>df.equals(different_column_type)TrueDataFrames df and different_data_type have different types for the
same values for their elements, and will return False even though
their column labels are the same values and types.>>>different_data_type=pd.DataFrame({1:[10.0],2:[20.0]})>>>different_data_type1     20  10.0  20.0>>>df.equals(different_data_type)False"
Pandas,DataFrame,pandas.DataFrame.filter,"pandas.DataFrame.filter#DataFrame.filter(items=None,like=None,regex=None,axis=None)[source]#Subset the dataframe rows or columns according to the specified index labels.Note that this routine does not filter a dataframe on its
contents. The filter is applied to the labels of the index.Parameters:itemslist-likeKeep labels from axis which are in items.likestrKeep labels from axis for which glike in label == Trueh.regexstr (regular expression)Keep labels from axis for which re.search(regex, label) == True.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneThe axis to filter on, expressed either as an index (int)
or axis name (str). By default this is the info axis, ecolumnsf for
DataFrame. ForSeriesthis parameter is unused and defaults toNone.Returns:same type as input objectSee alsoDataFrame.locAccess a group of rows and columns by label(s) or a boolean array.NotesTheitems,like, andregexparameters are
enforced to be mutually exclusive.axisdefaults to the info axis that is used when indexing
with[].Examples>>>df=pd.DataFrame(np.array(([1,2,3],[4,5,6])),...index=['mouse','rabbit'],...columns=['one','two','three'])>>>dfone  two  threemouse     1    2      3rabbit    4    5      6>>># select columns by name>>>df.filter(items=['one','three'])one  threemouse     1      3rabbit    4      6>>># select columns by regular expression>>>df.filter(regex='e$',axis=1)one  threemouse     1      3rabbit    4      6>>># select rows containing 'bbi'>>>df.filter(like='bbi',axis=0)one  two  threerabbit    4    5      6"
Pandas,DataFrame,pandas.DataFrame.first,"pandas.DataFrame.first#DataFrame.first(offset)[source]#Select initial periods of time series data based on a date offset.Deprecated since version 2.1:first()is deprecated and will be removed in a future version.
Please create a mask and filter using.locinstead.For a DataFrame with a sorted DatetimeIndex, this function can
select the first few rows based on a date offset.Parameters:offsetstr, DateOffset or dateutil.relativedeltaThe offset length of the data that will be selected. For instance,
e1Mf will display all the rows having their index within the first month.Returns:Series or DataFrameA subset of the caller.Raises:TypeErrorIf the index is not aDatetimeIndexSee alsolastSelect final periods of time series based on a date offset.at_timeSelect values at a particular time of the day.between_timeSelect values between particular times of the day.Examples>>>i=pd.date_range('2018-04-09',periods=4,freq='2D')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09  12018-04-11  22018-04-13  32018-04-15  4Get the rows for the first 3 days:>>>ts.first('3D')A2018-04-09  12018-04-11  2Notice the data for 3 first calendar days were returned, not the first
3 days observed in the dataset, and therefore data for 2018-04-13 was
not returned."
Pandas,DataFrame,pandas.DataFrame.head,"pandas.DataFrame.head#DataFrame.head(n=5)[source]#Return the firstnrows.This function returns the firstnrows for the object based
on position. It is useful for quickly testing if your object
has the right type of data in it.For negative values ofn, this function returns all rows except
the last|n|rows, equivalent todf[:n].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:same type as callerThe firstnrows of the caller object.See alsoDataFrame.tailReturns the lastnrows.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the first 5 lines>>>df.head()animal0  alligator1        bee2     falcon3       lion4     monkeyViewing the firstnlines (three in this case)>>>df.head(3)animal0  alligator1        bee2     falconFor negative values ofn>>>df.head(-3)animal0  alligator1        bee2     falcon3       lion4     monkey5     parrot"
Pandas,DataFrame,pandas.DataFrame.idxmax,"pandas.DataFrame.idxmax#DataFrame.idxmax(axis=0,skipna=True,numeric_only=False)[source]#Return index of first occurrence of maximum over requested axis.NA/null values are excluded.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to use. 0 or eindexf for row-wise, 1 or ecolumnsf for column-wise.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Returns:SeriesIndexes of maxima along the specified axis.Raises:ValueErrorIf the row/column is emptySee alsoSeries.idxmaxReturn index of the maximum element.NotesThis method is the DataFrame version ofndarray.argmax.ExamplesConsider a dataset containing food consumption in Argentina.>>>df=pd.DataFrame({'consumption':[10.51,103.11,55.48],...'co2_emissions':[37.2,19.66,1712]},...index=['Pork','Wheat Products','Beef'])>>>dfconsumption  co2_emissionsPork                  10.51         37.20Wheat Products       103.11         19.66Beef                  55.48       1712.00By default, it returns the index for the maximum value in each column.>>>df.idxmax()consumption     Wheat Productsco2_emissions             Beefdtype: objectTo return the index for the maximum value in each row, useaxis=""columns"".>>>df.idxmax(axis=""columns"")Pork              co2_emissionsWheat Products     consumptionBeef              co2_emissionsdtype: object"
Pandas,DataFrame,pandas.DataFrame.idxmin,"pandas.DataFrame.idxmin#DataFrame.idxmin(axis=0,skipna=True,numeric_only=False)[source]#Return index of first occurrence of minimum over requested axis.NA/null values are excluded.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to use. 0 or eindexf for row-wise, 1 or ecolumnsf for column-wise.skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result
will be NA.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Returns:SeriesIndexes of minima along the specified axis.Raises:ValueErrorIf the row/column is emptySee alsoSeries.idxminReturn index of the minimum element.NotesThis method is the DataFrame version ofndarray.argmin.ExamplesConsider a dataset containing food consumption in Argentina.>>>df=pd.DataFrame({'consumption':[10.51,103.11,55.48],...'co2_emissions':[37.2,19.66,1712]},...index=['Pork','Wheat Products','Beef'])>>>dfconsumption  co2_emissionsPork                  10.51         37.20Wheat Products       103.11         19.66Beef                  55.48       1712.00By default, it returns the index for the minimum value in each column.>>>df.idxmin()consumption                Porkco2_emissions    Wheat Productsdtype: objectTo return the index for the minimum value in each row, useaxis=""columns"".>>>df.idxmin(axis=""columns"")Pork                consumptionWheat Products    co2_emissionsBeef                consumptiondtype: object"
Pandas,DataFrame,pandas.DataFrame.last,"pandas.DataFrame.last#DataFrame.last(offset)[source]#Select final periods of time series data based on a date offset.Deprecated since version 2.1:last()is deprecated and will be removed in a future version.
Please create a mask and filter using.locinstead.For a DataFrame with a sorted DatetimeIndex, this function
selects the last few rows based on a date offset.Parameters:offsetstr, DateOffset, dateutil.relativedeltaThe offset length of the data that will be selected. For instance,
e3Df will display all the rows having their index within the last 3 days.Returns:Series or DataFrameA subset of the caller.Raises:TypeErrorIf the index is not aDatetimeIndexSee alsofirstSelect initial periods of time series based on a date offset.at_timeSelect values at a particular time of the day.between_timeSelect values between particular times of the day.NotesDeprecated since version 2.1.0:Please create a mask and filter using.locinsteadExamples>>>i=pd.date_range('2018-04-09',periods=4,freq='2D')>>>ts=pd.DataFrame({'A':[1,2,3,4]},index=i)>>>tsA2018-04-09  12018-04-11  22018-04-13  32018-04-15  4Get the rows for the last 3 days:>>>ts.last('3D')A2018-04-13  32018-04-15  4Notice the data for 3 last calendar days were returned, not the last
3 observed days in the dataset, and therefore data for 2018-04-11 was
not returned."
Pandas,DataFrame,pandas.DataFrame.reindex,"pandas.DataFrame.reindex#DataFrame.reindex(labels=None,*,index=None,columns=None,axis=None,method=None,copy=None,level=None,fill_value=nan,limit=None,tolerance=None)[source]#Conform DataFrame to new index with optional filling logic.Places NA/NaN in locations having no value in the previous index. A new object
is produced unless the new index is equivalent to the current one andcopy=False.Parameters:labelsarray-like, optionalNew labels / index to conform the axis specified by eaxisf to.indexarray-like, optionalNew labels for the index. Preferably an Index object to avoid
duplicating data.columnsarray-like, optionalNew labels for the columns. Preferably an Index object to avoid
duplicating data.axisint or str, optionalAxis to target. Can be either the axis name (eindexf, ecolumnsf)
or number (0, 1).method{None, ebackfillf/fbfillf, epadf/fffillf, enearestf}Method to use for filling holes in reindexed DataFrame.
Please note: this is only applicable to DataFrames/Series with a
monotonically increasing/decreasing index.None (default): donft fill gapspad / ffill: Propagate last valid observation forward to next
valid.backfill / bfill: Use next valid observation to fill gap.nearest: Use nearest valid observations to fill gap.copybool, default TrueReturn a new object, even if the passed indexes are the same.levelint or nameBroadcast across a level, matching Index values on the
passed MultiIndex level.fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any
gcompatibleh value.limitint, default NoneMaximum number of consecutive elements to forward or backward fill.toleranceoptionalMaximum distance between original and new labels for inexact
matches. The values of the index at the matching locations most
satisfy the equationabs(index[indexer]-target)<=tolerance.Tolerance may be a scalar value, which applies the same tolerance
to all values, or list-like, which applies variable tolerance per
element. List-like includes list, tuple, array, Series, and must be
the same size as the index and its dtype must exactly match the
indexfs type.Returns:DataFrame with changed index.See alsoDataFrame.set_indexSet row labels.DataFrame.reset_indexRemove row labels or move them to new columns.DataFrame.reindex_likeChange to same indices as other DataFrame.ExamplesDataFrame.reindexsupports two calling conventions(index=index_labels,columns=column_labels,...)(labels,axis={'index','columns'},...)Wehighlyrecommend using keyword arguments to clarify your
intent.Create a dataframe with some fictional data.>>>index=['Firefox','Chrome','Safari','IE10','Konqueror']>>>df=pd.DataFrame({'http_status':[200,200,404,404,301],...'response_time':[0.04,0.02,0.07,0.08,1.0]},...index=index)>>>dfhttp_status  response_timeFirefox            200           0.04Chrome             200           0.02Safari             404           0.07IE10               404           0.08Konqueror          301           1.00Create a new index and reindex the dataframe. By default
values in the new index that do not have corresponding
records in the dataframe are assignedNaN.>>>new_index=['Safari','Iceweasel','Comodo Dragon','IE10',...'Chrome']>>>df.reindex(new_index)http_status  response_timeSafari               404.0           0.07Iceweasel              NaN            NaNComodo Dragon          NaN            NaNIE10                 404.0           0.08Chrome               200.0           0.02We can fill in the missing values by passing a value to
the keywordfill_value. Because the index is not monotonically
increasing or decreasing, we cannot use arguments to the keywordmethodto fill theNaNvalues.>>>df.reindex(new_index,fill_value=0)http_status  response_timeSafari                 404           0.07Iceweasel                0           0.00Comodo Dragon            0           0.00IE10                   404           0.08Chrome                 200           0.02>>>df.reindex(new_index,fill_value='missing')http_status response_timeSafari                404          0.07Iceweasel         missing       missingComodo Dragon     missing       missingIE10                  404          0.08Chrome                200          0.02We can also reindex the columns.>>>df.reindex(columns=['http_status','user_agent'])http_status  user_agentFirefox            200         NaNChrome             200         NaNSafari             404         NaNIE10               404         NaNKonqueror          301         NaNOr we can use gaxis-styleh keyword arguments>>>df.reindex(['http_status','user_agent'],axis=""columns"")http_status  user_agentFirefox            200         NaNChrome             200         NaNSafari             404         NaNIE10               404         NaNKonqueror          301         NaNTo further illustrate the filling functionality inreindex, we will create a dataframe with a
monotonically increasing index (for example, a sequence
of dates).>>>date_index=pd.date_range('1/1/2010',periods=6,freq='D')>>>df2=pd.DataFrame({""prices"":[100,101,np.nan,100,89,88]},...index=date_index)>>>df2prices2010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.0Suppose we decide to expand the dataframe to cover a wider
date range.>>>date_index2=pd.date_range('12/29/2009',periods=10,freq='D')>>>df2.reindex(date_index2)prices2009-12-29     NaN2009-12-30     NaN2009-12-31     NaN2010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.02010-01-07     NaNThe index entries that did not have a value in the original data frame
(for example, e2009-12-29f) are by default filled withNaN.
If desired, we can fill in the missing values using one of several
options.For example, to back-propagate the last valid value to fill theNaNvalues, passbfillas an argument to themethodkeyword.>>>df2.reindex(date_index2,method='bfill')prices2009-12-29   100.02009-12-30   100.02009-12-31   100.02010-01-01   100.02010-01-02   101.02010-01-03     NaN2010-01-04   100.02010-01-05    89.02010-01-06    88.02010-01-07     NaNPlease note that theNaNvalue present in the original dataframe
(at index value 2010-01-03) will not be filled by any of the
value propagation schemes. This is because filling while reindexing
does not look at dataframe values, but only compares the original and
desired indexes. If you do want to fill in theNaNvalues present
in the original dataframe, use thefillna()method.See theuser guidefor more."
Pandas,DataFrame,pandas.DataFrame.reindex_like,"pandas.DataFrame.reindex_like#DataFrame.reindex_like(other,method=None,copy=None,limit=None,tolerance=None)[source]#Return an object with matching indices as other object.Conform the object to the same index on all axes. Optional
filling logic, placing NaN in locations having no value
in the previous index. A new object is produced unless the
new index is equivalent to the current one and copy=False.Parameters:otherObject of the same data typeIts row and column indices are used to define the new indices
of this object.method{None, ebackfillf/fbfillf, epadf/fffillf, enearestf}Method to use for filling holes in reindexed DataFrame.
Please note: this is only applicable to DataFrames/Series with a
monotonically increasing/decreasing index.None (default): donft fill gapspad / ffill: propagate last valid observation forward to next
validbackfill / bfill: use next valid observation to fill gapnearest: use nearest valid observations to fill gap.copybool, default TrueReturn a new object, even if the passed indexes are the same.limitint, default NoneMaximum number of consecutive labels to fill for inexact matches.toleranceoptionalMaximum distance between original and new labels for inexact
matches. The values of the index at the matching locations must
satisfy the equationabs(index[indexer]-target)<=tolerance.Tolerance may be a scalar value, which applies the same tolerance
to all values, or list-like, which applies variable tolerance per
element. List-like includes list, tuple, array, Series, and must be
the same size as the index and its dtype must exactly match the
indexfs type.Returns:Series or DataFrameSame type as caller, but with changed indices on each axis.See alsoDataFrame.set_indexSet row labels.DataFrame.reset_indexRemove row labels or move them to new columns.DataFrame.reindexChange to new indices or expand indices.NotesSame as calling.reindex(index=other.index,columns=other.columns,...).Examples>>>df1=pd.DataFrame([[24.3,75.7,'high'],...[31,87.8,'high'],...[22,71.6,'medium'],...[35,95,'medium']],...columns=['temp_celsius','temp_fahrenheit',...'windspeed'],...index=pd.date_range(start='2014-02-12',...end='2014-02-15',freq='D'))>>>df1temp_celsius  temp_fahrenheit windspeed2014-02-12          24.3             75.7      high2014-02-13          31.0             87.8      high2014-02-14          22.0             71.6    medium2014-02-15          35.0             95.0    medium>>>df2=pd.DataFrame([[28,'low'],...[30,'low'],...[35.1,'medium']],...columns=['temp_celsius','windspeed'],...index=pd.DatetimeIndex(['2014-02-12','2014-02-13',...'2014-02-15']))>>>df2temp_celsius windspeed2014-02-12          28.0       low2014-02-13          30.0       low2014-02-15          35.1    medium>>>df2.reindex_like(df1)temp_celsius  temp_fahrenheit windspeed2014-02-12          28.0              NaN       low2014-02-13          30.0              NaN       low2014-02-14           NaN              NaN       NaN2014-02-15          35.1              NaN    medium"
Pandas,DataFrame,pandas.DataFrame.rename,"pandas.DataFrame.rename#DataFrame.rename(mapper=None,*,index=None,columns=None,axis=None,copy=None,inplace=False,level=None,errors='ignore')[source]#Rename columns or index labels.Function / dict values must be unique (1-to-1). Labels not contained in
a dict / Series will be left as-is. Extra labels listed donft throw an
error.See theuser guidefor more.Parameters:mapperdict-like or functionDict-like or function transformations to apply to
that axisf values. Use eithermapperandaxisto
specify the axis to target withmapper, orindexandcolumns.indexdict-like or functionAlternative to specifying axis (mapper,axis=0is equivalent toindex=mapper).columnsdict-like or functionAlternative to specifying axis (mapper,axis=1is equivalent tocolumns=mapper).axis{0 or eindexf, 1 or ecolumnsf}, default 0Axis to target withmapper. Can be either the axis name
(eindexf, ecolumnsf) or number (0, 1). The default is eindexf.copybool, default TrueAlso copy underlying data.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.
If True then value of copy is ignored.levelint or level name, default NoneIn case of a MultiIndex, only rename labels in the specified
level.errors{eignoref, eraisef}, default eignorefIf eraisef, raise aKeyErrorwhen a dict-likemapper,index,
orcolumnscontains labels that are not present in the Index
being transformed.
If eignoref, existing keys will be renamed and extra keys will be
ignored.Returns:DataFrame or NoneDataFrame with the renamed axis labels or None ifinplace=True.Raises:KeyErrorIf any of the labels is not found in the selected axis and
gerrors=fraisefh.See alsoDataFrame.rename_axisSet the name of the axis.ExamplesDataFrame.renamesupports two calling conventions(index=index_mapper,columns=columns_mapper,...)(mapper,axis={'index','columns'},...)Wehighlyrecommend using keyword arguments to clarify your
intent.Rename columns using a mapping:>>>df=pd.DataFrame({""A"":[1,2,3],""B"":[4,5,6]})>>>df.rename(columns={""A"":""a"",""B"":""c""})a  c0  1  41  2  52  3  6Rename index using a mapping:>>>df.rename(index={0:""x"",1:""y"",2:""z""})A  Bx  1  4y  2  5z  3  6Cast index labels to a different type:>>>df.indexRangeIndex(start=0, stop=3, step=1)>>>df.rename(index=str).indexIndex(['0', '1', '2'], dtype='object')>>>df.rename(columns={""A"":""a"",""B"":""b"",""C"":""c""},errors=""raise"")Traceback (most recent call last):KeyError:['C'] not found in axisUsing axis-style parameters:>>>df.rename(str.lower,axis='columns')a  b0  1  41  2  52  3  6>>>df.rename({1:2,2:4},axis='index')A  B0  1  42  2  54  3  6"
Pandas,DataFrame,pandas.DataFrame.rename_axis,"pandas.DataFrame.rename_axis#DataFrame.rename_axis(mapper=_NoDefault.no_default,*,index=_NoDefault.no_default,columns=_NoDefault.no_default,axis=0,copy=None,inplace=False)[source]#Set the name of the axis for the index or columns.Parameters:mapperscalar, list-like, optionalValue to set the axis name attribute.index, columnsscalar, list-like, dict-like or function, optionalA scalar, list-like, dict-like or functions transformations to
apply to that axisf values.
Note that thecolumnsparameter is not allowed if the
object is a Series. This parameter only apply for DataFrame
type objects.Use eithermapperandaxisto
specify the axis to target withmapper, orindexand/orcolumns.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to rename. ForSeriesthis parameter is unused and defaults to 0.copybool, default NoneAlso copy underlying data.inplacebool, default FalseModifies the object directly, instead of creating a new Series
or DataFrame.Returns:Series, DataFrame, or NoneThe same type as the caller or None ifinplace=True.See alsoSeries.renameAlter Series index labels or name.DataFrame.renameAlter DataFrame index labels or name.Index.renameSet new names on index.NotesDataFrame.rename_axissupports two calling conventions(index=index_mapper,columns=columns_mapper,...)(mapper,axis={'index','columns'},...)The first calling convention will only modify the names of
the index and/or the names of the Index object that is the columns.
In this case, the parametercopyis ignored.The second calling convention will modify the names of the
corresponding index if mapper is a list or a scalar.
However, if mapper is dict-like or a function, it will use the
deprecated behavior of modifying the axislabels.Wehighlyrecommend using keyword arguments to clarify your
intent.ExamplesSeries>>>s=pd.Series([""dog"",""cat"",""monkey""])>>>s0       dog1       cat2    monkeydtype: object>>>s.rename_axis(""animal"")animal0    dog1    cat2    monkeydtype: objectDataFrame>>>df=pd.DataFrame({""num_legs"":[4,4,2],...""num_arms"":[0,0,2]},...[""dog"",""cat"",""monkey""])>>>dfnum_legs  num_armsdog            4         0cat            4         0monkey         2         2>>>df=df.rename_axis(""animal"")>>>dfnum_legs  num_armsanimaldog            4         0cat            4         0monkey         2         2>>>df=df.rename_axis(""limbs"",axis=""columns"")>>>dflimbs   num_legs  num_armsanimaldog            4         0cat            4         0monkey         2         2MultiIndex>>>df.index=pd.MultiIndex.from_product([['mammal'],...['dog','cat','monkey']],...names=['type','name'])>>>dflimbs          num_legs  num_armstype   namemammal dog            4         0cat            4         0monkey         2         2>>>df.rename_axis(index={'type':'class'})limbs          num_legs  num_armsclass  namemammal dog            4         0cat            4         0monkey         2         2>>>df.rename_axis(columns=str.upper)LIMBS          num_legs  num_armstype   namemammal dog            4         0cat            4         0monkey         2         2"
Pandas,DataFrame,pandas.DataFrame.reset_index,"pandas.DataFrame.reset_index#DataFrame.reset_index(level=None,*,drop=False,inplace=False,col_level=0,col_fill='',allow_duplicates=_NoDefault.no_default,names=None)[source]#Reset the index, or a level of it.Reset the index of the DataFrame, and use the default one instead.
If the DataFrame has a MultiIndex, this method can remove one or more
levels.Parameters:levelint, str, tuple, or list, default NoneOnly remove the given levels from the index. Removes all levels by
default.dropbool, default FalseDo not try to insert index into dataframe columns. This resets
the index to the default integer index.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.col_levelint or str, default 0If the columns have multiple levels, determines which level the
labels are inserted into. By default it is inserted into the first
level.col_fillobject, default efIf the columns have multiple levels, determines how the other
levels are named. If None then the index name is repeated.allow_duplicatesbool, optional, default lib.no_defaultAllow duplicate column labels to be created.New in version 1.5.0.namesint, str or 1-dimensional list, default NoneUsing the given string, rename the DataFrame column which contains the
index data. If the DataFrame has a MultiIndex, this has to be a list or
tuple with length equal to the number of levels.New in version 1.5.0.Returns:DataFrame or NoneDataFrame with the new index or None ifinplace=True.See alsoDataFrame.set_indexOpposite of reset_index.DataFrame.reindexChange to new indices or expand indices.DataFrame.reindex_likeChange to same indices as other DataFrame.Examples>>>df=pd.DataFrame([('bird',389.0),...('bird',24.0),...('mammal',80.5),...('mammal',np.nan)],...index=['falcon','parrot','lion','monkey'],...columns=('class','max_speed'))>>>dfclass  max_speedfalcon    bird      389.0parrot    bird       24.0lion    mammal       80.5monkey  mammal        NaNWhen we reset the index, the old index is added as a column, and a
new sequential index is used:>>>df.reset_index()index   class  max_speed0  falcon    bird      389.01  parrot    bird       24.02    lion  mammal       80.53  monkey  mammal        NaNWe can use thedropparameter to avoid the old index being added as
a column:>>>df.reset_index(drop=True)class  max_speed0    bird      389.01    bird       24.02  mammal       80.53  mammal        NaNYou can also usereset_indexwithMultiIndex.>>>index=pd.MultiIndex.from_tuples([('bird','falcon'),...('bird','parrot'),...('mammal','lion'),...('mammal','monkey')],...names=['class','name'])>>>columns=pd.MultiIndex.from_tuples([('speed','max'),...('species','type')])>>>df=pd.DataFrame([(389.0,'fly'),...(24.0,'fly'),...(80.5,'run'),...(np.nan,'jump')],...index=index,...columns=columns)>>>dfspeed speciesmax    typeclass  namebird   falcon  389.0     flyparrot   24.0     flymammal lion     80.5     runmonkey    NaN    jumpUsing thenamesparameter, choose a name for the index column:>>>df.reset_index(names=['classes','names'])classes   names  speed speciesmax    type0    bird  falcon  389.0     fly1    bird  parrot   24.0     fly2  mammal    lion   80.5     run3  mammal  monkey    NaN    jumpIf the index has multiple levels, we can reset a subset of them:>>>df.reset_index(level='class')class  speed speciesmax    typenamefalcon    bird  389.0     flyparrot    bird   24.0     flylion    mammal   80.5     runmonkey  mammal    NaN    jumpIf we are not dropping the index, by default, it is placed in the top
level. We can place it in another level:>>>df.reset_index(level='class',col_level=1)speed speciesclass    max    typenamefalcon    bird  389.0     flyparrot    bird   24.0     flylion    mammal   80.5     runmonkey  mammal    NaN    jumpWhen the index is inserted under another level, we can specify under
which one with the parametercol_fill:>>>df.reset_index(level='class',col_level=1,col_fill='species')species  speed speciesclass    max    typenamefalcon           bird  389.0     flyparrot           bird   24.0     flylion           mammal   80.5     runmonkey         mammal    NaN    jumpIf we specify a nonexistent level forcol_fill, it is created:>>>df.reset_index(level='class',col_level=1,col_fill='genus')genus  speed speciesclass    max    typenamefalcon           bird  389.0     flyparrot           bird   24.0     flylion           mammal   80.5     runmonkey         mammal    NaN    jump"
Pandas,DataFrame,pandas.DataFrame.sample,"pandas.DataFrame.sample#DataFrame.sample(n=None,frac=None,replace=False,weights=None,random_state=None,axis=None,ignore_index=False)[source]#Return a random sample of items from an axis of object.You can userandom_statefor reproducibility.Parameters:nint, optionalNumber of items from axis to return. Cannot be used withfrac.
Default = 1 iffrac= None.fracfloat, optionalFraction of axis items to return. Cannot be used withn.replacebool, default FalseAllow or disallow sampling of the same row more than once.weightsstr or ndarray-like, optionalDefault eNonef results in equal probability weighting.
If passed a Series, will align with target object on index. Index
values in weights not found in sampled object will be ignored and
index values in sampled object not in weights will be assigned
weights of zero.
If called on a DataFrame, will accept the name of a column
when axis = 0.
Unless weights are a Series, weights must be same length as axis
being sampled.
If weights do not sum to 1, they will be normalized to sum to 1.
Missing values in the weights column will be treated as zero.
Infinite values not allowed.random_stateint, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optionalIf int, array-like, or BitGenerator, seed for random number generator.
If np.random.RandomState or np.random.Generator, use as given.Changed in version 1.4.0:np.random.Generator objects now acceptedaxis{0 or eindexf, 1 or ecolumnsf, None}, default NoneAxis to sample. Accepts axis number or name. Default is stat axis
for given data type. ForSeriesthis parameter is unused and defaults toNone.ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, c, n - 1.New in version 1.3.0.Returns:Series or DataFrameA new object of same type as caller containingnitems randomly
sampled from the caller object.See alsoDataFrameGroupBy.sampleGenerates random samples from each group of a DataFrame object.SeriesGroupBy.sampleGenerates random samples from each group of a Series object.numpy.random.choiceGenerates a random sample from a given 1-D numpy array.NotesIffrac> 1,replacementshould be set toTrue.Examples>>>df=pd.DataFrame({'num_legs':[2,4,8,0],...'num_wings':[2,0,0,0],...'num_specimen_seen':[10,2,1,8]},...index=['falcon','dog','spider','fish'])>>>dfnum_legs  num_wings  num_specimen_seenfalcon         2          2                 10dog            4          0                  2spider         8          0                  1fish           0          0                  8Extract 3 random elements from theSeriesdf['num_legs']:
Note that we userandom_stateto ensure the reproducibility of
the examples.>>>df['num_legs'].sample(n=3,random_state=1)fish      0spider    8falcon    2Name: num_legs, dtype: int64A random 50% sample of theDataFramewith replacement:>>>df.sample(frac=0.5,replace=True,random_state=1)num_legs  num_wings  num_specimen_seendog          4          0                  2fish         0          0                  8An upsample sample of theDataFramewith replacement:
Note thatreplaceparameter has to beTrueforfracparameter > 1.>>>df.sample(frac=2,replace=True,random_state=1)num_legs  num_wings  num_specimen_seendog            4          0                  2fish           0          0                  8falcon         2          2                 10falcon         2          2                 10fish           0          0                  8dog            4          0                  2fish           0          0                  8dog            4          0                  2Using a DataFrame column as weights. Rows with larger value in thenum_specimen_seencolumn are more likely to be sampled.>>>df.sample(n=2,weights='num_specimen_seen',random_state=1)num_legs  num_wings  num_specimen_seenfalcon         2          2                 10fish           0          0                  8"
Pandas,DataFrame,pandas.DataFrame.set_axis,"pandas.DataFrame.set_axis#DataFrame.set_axis(labels,*,axis=0,copy=None)[source]#Assign desired index to given axis.Indexes for column or row labels can be changed by assigning
a list-like or Index.Parameters:labelslist-like, IndexThe values for the new index.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to update. The value 0 identifies the rows. ForSeriesthis parameter is unused and defaults to 0.copybool, default TrueWhether to make a copy of the underlying data.New in version 1.5.0.Returns:DataFrameAn object of type DataFrame.See alsoDataFrame.rename_axisAlter the name of the index or columns.Examples>>>df=pd.DataFrame({""A"":[1,2,3],""B"":[4,5,6]})Change the row labels.>>>df.set_axis(['a','b','c'],axis='index')A  Ba  1  4b  2  5c  3  6Change the column labels.>>>df.set_axis(['I','II'],axis='columns')I  II0  1   41  2   52  3   6"
Pandas,DataFrame,pandas.DataFrame.set_index,"pandas.DataFrame.set_index#DataFrame.set_index(keys,*,drop=True,append=False,inplace=False,verify_integrity=False)[source]#Set the DataFrame index using existing columns.Set the DataFrame index (row labels) using one or more existing
columns or arrays (of the correct length). The index can replace the
existing index or expand on it.Parameters:keyslabel or array-like or list of labels/arraysThis parameter can be either a single column key, a single array of
the same length as the calling DataFrame, or a list containing an
arbitrary combination of column keys and arrays. Here, garrayh
encompassesSeries,Index,np.ndarray, and
instances ofIterator.dropbool, default TrueDelete columns to be used as the new index.appendbool, default FalseWhether to append columns to existing index.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.verify_integritybool, default FalseCheck the new index for duplicates. Otherwise defer the check until
necessary. Setting to False will improve the performance of this
method.Returns:DataFrame or NoneChanged row labels or None ifinplace=True.See alsoDataFrame.reset_indexOpposite of set_index.DataFrame.reindexChange to new indices or expand indices.DataFrame.reindex_likeChange to same indices as other DataFrame.Examples>>>df=pd.DataFrame({'month':[1,4,7,10],...'year':[2012,2014,2013,2014],...'sale':[55,40,84,31]})>>>dfmonth  year  sale0      1  2012    551      4  2014    402      7  2013    843     10  2014    31Set the index to become the emonthf column:>>>df.set_index('month')year  salemonth1      2012    554      2014    407      2013    8410     2014    31Create a MultiIndex using columns eyearf and emonthf:>>>df.set_index(['year','month'])saleyear  month2012  1     552014  4     402013  7     842014  10    31Create a MultiIndex using an Index and a column:>>>df.set_index([pd.Index([1,2,3,4]),'year'])month  saleyear1  2012  1      552  2014  4      403  2013  7      844  2014  10     31Create a MultiIndex using two Series:>>>s=pd.Series([1,2,3,4])>>>df.set_index([s,s**2])month  year  sale1 1       1  2012    552 4       4  2014    403 9       7  2013    844 16     10  2014    31"
Pandas,DataFrame,pandas.DataFrame.tail,"pandas.DataFrame.tail#DataFrame.tail(n=5)[source]#Return the lastnrows.This function returns lastnrows from the object based on
position. It is useful for quickly verifying data, for example,
after sorting or appending rows.For negative values ofn, this function returns all rows except
the first|n|rows, equivalent todf[|n|:].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:type of callerThe lastnrows of the caller object.See alsoDataFrame.headThe firstnrows of the caller object.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the last 5 lines>>>df.tail()animal4  monkey5  parrot6   shark7   whale8   zebraViewing the lastnlines (three in this case)>>>df.tail(3)animal6  shark7  whale8  zebraFor negative values ofn>>>df.tail(-3)animal3    lion4  monkey5  parrot6   shark7   whale8   zebra"
Pandas,DataFrame,pandas.DataFrame.take,"pandas.DataFrame.take#DataFrame.take(indices,axis=0,**kwargs)[source]#Return the elements in the givenpositionalindices along an axis.This means that we are not indexing according to actual values in
the index attribute of the object. We are indexing according to the
actual position of the element in the object.Parameters:indicesarray-likeAn array of ints indicating which positions to take.axis{0 or eindexf, 1 or ecolumnsf, None}, default 0The axis on which to select elements.0means that we are
selecting rows,1means that we are selecting columns.
ForSeriesthis parameter is unused and defaults to 0.**kwargsFor compatibility withnumpy.take(). Has no effect on the
output.Returns:same type as callerAn array-like containing the elements taken from the object.See alsoDataFrame.locSelect a subset of a DataFrame by labels.DataFrame.ilocSelect a subset of a DataFrame by positions.numpy.takeTake elements from an array along an axis.Examples>>>df=pd.DataFrame([('falcon','bird',389.0),...('parrot','bird',24.0),...('lion','mammal',80.5),...('monkey','mammal',np.nan)],...columns=['name','class','max_speed'],...index=[0,2,3,1])>>>dfname   class  max_speed0  falcon    bird      389.02  parrot    bird       24.03    lion  mammal       80.51  monkey  mammal        NaNTake elements at positions 0 and 3 along the axis 0 (default).Note how the actual indices selected (0 and 1) do not correspond to
our selected indices 0 and 3. Thatfs because we are selecting the 0th
and 3rd rows, not rows whose indices equal 0 and 3.>>>df.take([0,3])name   class  max_speed0  falcon    bird      389.01  monkey  mammal        NaNTake elements at indices 1 and 2 along the axis 1 (column selection).>>>df.take([1,2],axis=1)class  max_speed0    bird      389.02    bird       24.03  mammal       80.51  mammal        NaNWe may take elements using negative integers for positive indices,
starting from the end of the object, just like with Python lists.>>>df.take([-1,-2])name   class  max_speed1  monkey  mammal        NaN3    lion  mammal       80.5"
Pandas,DataFrame,pandas.DataFrame.truncate,"pandas.DataFrame.truncate#DataFrame.truncate(before=None,after=None,axis=None,copy=None)[source]#Truncate a Series or DataFrame before and after some index value.This is a useful shorthand for boolean indexing based on index
values above or below certain thresholds.Parameters:beforedate, str, intTruncate all rows before this index value.afterdate, str, intTruncate all rows after this index value.axis{0 or eindexf, 1 or ecolumnsf}, optionalAxis to truncate. Truncates the index (rows) by default.
ForSeriesthis parameter is unused and defaults to 0.copybool, default is True,Return a copy of the truncated section.Returns:type of callerThe truncated Series or DataFrame.See alsoDataFrame.locSelect a subset of a DataFrame by label.DataFrame.ilocSelect a subset of a DataFrame by position.NotesIf the index being truncated contains only datetime values,beforeandaftermay be specified as strings instead of
Timestamps.Examples>>>df=pd.DataFrame({'A':['a','b','c','d','e'],...'B':['f','g','h','i','j'],...'C':['k','l','m','n','o']},...index=[1,2,3,4,5])>>>dfA  B  C1  a  f  k2  b  g  l3  c  h  m4  d  i  n5  e  j  o>>>df.truncate(before=2,after=4)A  B  C2  b  g  l3  c  h  m4  d  i  nThe columns of a DataFrame can be truncated.>>>df.truncate(before=""A"",after=""B"",axis=""columns"")A  B1  a  f2  b  g3  c  h4  d  i5  e  jFor Series, only rows can be truncated.>>>df['A'].truncate(before=2,after=4)2    b3    c4    dName: A, dtype: objectThe index values intruncatecan be datetimes or string
dates.>>>dates=pd.date_range('2016-01-01','2016-02-01',freq='s')>>>df=pd.DataFrame(index=dates,data={'A':1})>>>df.tail()A2016-01-31 23:59:56  12016-01-31 23:59:57  12016-01-31 23:59:58  12016-01-31 23:59:59  12016-02-01 00:00:00  1>>>df.truncate(before=pd.Timestamp('2016-01-05'),...after=pd.Timestamp('2016-01-10')).tail()A2016-01-09 23:59:56  12016-01-09 23:59:57  12016-01-09 23:59:58  12016-01-09 23:59:59  12016-01-10 00:00:00  1Because the index is a DatetimeIndex containing only dates, we can
specifybeforeandafteras strings. They will be coerced to
Timestamps before truncation.>>>df.truncate('2016-01-05','2016-01-10').tail()A2016-01-09 23:59:56  12016-01-09 23:59:57  12016-01-09 23:59:58  12016-01-09 23:59:59  12016-01-10 00:00:00  1Note thattruncateassumes a 0 value for any unspecified time
component (midnight). This differs from partial string slicing, which
returns any partially matching dates.>>>df.loc['2016-01-05':'2016-01-10',:].tail()A2016-01-10 23:59:55  12016-01-10 23:59:56  12016-01-10 23:59:57  12016-01-10 23:59:58  12016-01-10 23:59:59  1"
Pandas,DataFrame,pandas.DataFrame.backfill,"pandas.DataFrame.backfill#DataFrame.backfill(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by using the next valid observation to fill the gap.Deprecated since version 2.0:Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead.Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.ExamplesPlease see examples forDataFrame.bfill()orSeries.bfill()."
Pandas,DataFrame,pandas.DataFrame.bfill,"pandas.DataFrame.bfill#DataFrame.bfill(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by using the next valid observation to fill the gap.Parameters:axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string einferf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.ExamplesFor Series:>>>s=pd.Series([1,None,None,2])>>>s.bfill()0    1.01    2.02    2.03    2.0dtype: float64>>>s.bfill(limit=1)0    1.01    NaN2    2.03    2.0dtype: float64With DataFrame:>>>df=pd.DataFrame({'A':[1,None,None,4],'B':[None,5,None,7]})>>>dfA     B0   1.0   NaN1   NaN   5.02   NaN   NaN3   4.0   7.0>>>df.bfill()A     B0   1.0   5.01   4.0   5.02   4.0   7.03   4.0   7.0>>>df.bfill(limit=1)A     B0   1.0   5.01   NaN   5.02   4.0   7.03   4.0   7.0"
Pandas,DataFrame,pandas.DataFrame.dropna,"pandas.DataFrame.dropna#DataFrame.dropna(*,axis=0,how=_NoDefault.no_default,thresh=_NoDefault.no_default,subset=None,inplace=False,ignore_index=False)[source]#Remove missing values.See theUser Guidefor more on which values are
considered missing, and how to work with missing data.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0Determine if rows or columns which contain missing values are
removed.0, or eindexf : Drop rows which contain missing values.1, or ecolumnsf : Drop columns which contain missing value.Only a single axis is allowed.how{eanyf, eallf}, default eanyfDetermine if row or column is removed from DataFrame, when we have
at least one NA or all NA.eanyf : If any NA values are present, drop that row or column.eallf : If all values are NA, drop that row or column.threshint, optionalRequire that many non-NA values. Cannot be combined with how.subsetcolumn label or sequence of labels, optionalLabels along other axis to consider, e.g. if you are dropping rows
these would be a list of columns to include.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.ignore_indexbool, defaultFalseIfTrue, the resulting axis will be labeled 0, 1, c, n - 1.New in version 2.0.0.Returns:DataFrame or NoneDataFrame with NA entries dropped from it or None ifinplace=True.See alsoDataFrame.isnaIndicate missing values.DataFrame.notnaIndicate existing (non-missing) values.DataFrame.fillnaReplace missing values.Series.dropnaDrop missing values.Index.dropnaDrop missing indices.Examples>>>df=pd.DataFrame({""name"":['Alfred','Batman','Catwoman'],...""toy"":[np.nan,'Batmobile','Bullwhip'],...""born"":[pd.NaT,pd.Timestamp(""1940-04-25""),...pd.NaT]})>>>dfname        toy       born0    Alfred        NaN        NaT1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaTDrop the rows where at least one element is missing.>>>df.dropna()name        toy       born1  Batman  Batmobile 1940-04-25Drop the columns where at least one element is missing.>>>df.dropna(axis='columns')name0    Alfred1    Batman2  CatwomanDrop the rows where all elements are missing.>>>df.dropna(how='all')name        toy       born0    Alfred        NaN        NaT1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaTKeep only the rows with at least 2 non-NA values.>>>df.dropna(thresh=2)name        toy       born1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaTDefine in which columns to look for missing values.>>>df.dropna(subset=['name','toy'])name        toy       born1    Batman  Batmobile 1940-04-252  Catwoman   Bullwhip        NaT"
Pandas,DataFrame,pandas.DataFrame.ffill,"pandas.DataFrame.ffill#DataFrame.ffill(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by propagating the last valid observation to next valid.Parameters:axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string einferf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.Examples>>>df=pd.DataFrame([[np.nan,2,np.nan,0],...[3,4,np.nan,1],...[np.nan,np.nan,np.nan,np.nan],...[np.nan,3,np.nan,4]],...columns=list(""ABCD""))>>>dfA    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  NaN  NaN NaN  NaN3  NaN  3.0 NaN  4.0>>>df.ffill()A    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  3.0  4.0 NaN  1.03  3.0  3.0 NaN  4.0>>>ser=pd.Series([1,np.nan,2,3])>>>ser.ffill()0   1.01   1.02   2.03   3.0dtype: float64"
Pandas,DataFrame,pandas.DataFrame.fillna,"pandas.DataFrame.fillna#DataFrame.fillna(value=None,*,method=None,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values using the specified method.Parameters:valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a
dict/Series/DataFrame of values specifying which value to use for
each index (for a Series) or column (for a DataFrame). Values not
in the dict/Series/DataFrame will not be filled. This value cannot
be a list.method{ebackfillf, ebfillf, effillf, None}, default NoneMethod to use for filling holes in reindexed Series:ffill: propagate last valid observation forward to next valid.backfill / bfill: use next valid observation to fill gap.Deprecated since version 2.1.0:Use ffill or bfill instead.axis{0 or eindexf} for Series, {0 or eindexf, 1 or ecolumnsf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string einferf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.See alsoffillFill values by propagating the last valid observation to next valid.bfillFill values by using the next valid observation to fill the gap.interpolateFill NaN values using interpolation.reindexConform object to new index.asfreqConvert TimeSeries to specified frequency.Examples>>>df=pd.DataFrame([[np.nan,2,np.nan,0],...[3,4,np.nan,1],...[np.nan,np.nan,np.nan,np.nan],...[np.nan,3,np.nan,4]],...columns=list(""ABCD""))>>>dfA    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  NaN  NaN NaN  NaN3  NaN  3.0 NaN  4.0Replace all NaN elements with 0s.>>>df.fillna(0)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  0.03  0.0  3.0  0.0  4.0Replace all NaN elements in column eAf, eBf, eCf, and eDf, with 0, 1,
2, and 3 respectively.>>>values={""A"":0,""B"":1,""C"":2,""D"":3}>>>df.fillna(value=values)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  2.0  1.02  0.0  1.0  2.0  3.03  0.0  3.0  2.0  4.0Only replace the first NaN element.>>>df.fillna(value=values,limit=1)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  NaN  1.02  NaN  1.0  NaN  3.03  NaN  3.0  NaN  4.0When filling using a DataFrame, replacement happens along
the same column names and same indices>>>df2=pd.DataFrame(np.zeros((4,4)),columns=list(""ABCE""))>>>df.fillna(df2)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  NaN3  0.0  3.0  0.0  4.0Note that column D is not affected since it is not present in df2."
Pandas,DataFrame,pandas.DataFrame.interpolate,"pandas.DataFrame.interpolate#DataFrame.interpolate(method='linear',*,axis=0,limit=None,inplace=False,limit_direction=None,limit_area=None,downcast=_NoDefault.no_default,**kwargs)[source]#Fill NaN values using an interpolation method.Please note that onlymethod='linear'is supported for
DataFrame/Series with a MultiIndex.Parameters:methodstr, default elinearfInterpolation technique to use. One of:elinearf: Ignore the index and treat the values as equally
spaced. This is the only method supported on MultiIndexes.etimef: Works on daily and higher resolution data to interpolate
given length of interval.eindexf, evaluesf: use the actual numerical values of the index.epadf: Fill in NaNs using existing values.enearestf, ezerof, eslinearf, equadraticf, ecubicf,
ebarycentricf, epolynomialf: Passed toscipy.interpolate.interp1d, whereas esplinef is passed toscipy.interpolate.UnivariateSpline. These methods use the numerical
values of the index. Both epolynomialf and esplinef require that
you also specify anorder(int), e.g.df.interpolate(method='polynomial',order=5). Note that,slinearmethod in Pandas refers to the Scipy first ordersplineinstead of Pandas first orderspline.ekroghf, epiecewise_polynomialf, esplinef, epchipf, eakimaf,
ecubicsplinef: Wrappers around the SciPy interpolation methods of
similar names. SeeNotes.efrom_derivativesf: Refers toscipy.interpolate.BPoly.from_derivatives.axis{{0 or eindexf, 1 or ecolumnsf, None}}, default NoneAxis to interpolate along. ForSeriesthis parameter is unused
and defaults to 0.limitint, optionalMaximum number of consecutive NaNs to fill. Must be greater than
0.inplacebool, default FalseUpdate the data in place if possible.limit_direction{{eforwardf, ebackwardf, ebothf}}, OptionalConsecutive NaNs will be filled in this direction.If limit is specified:If emethodf is epadf or effillf, elimit_directionf must be eforwardf.If emethodf is ebackfillf or ebfillf, elimit_directionf must be
ebackwardsf.If elimitf is not specified:If emethodf is ebackfillf or ebfillf, the default is ebackwardfelse the default is eforwardfraises ValueError iflimit_directionis eforwardf or ebothf andmethod is ebackfillf or ebfillf.raises ValueError iflimit_directionis ebackwardf or ebothf andmethod is epadf or effillf.limit_area{{None, einsidef, eoutsidef}}, default NoneIf limit is specified, consecutive NaNs will be filled with this
restriction.None: No fill restriction.einsidef: Only fill NaNs surrounded by valid values
(interpolate).eoutsidef: Only fill NaNs outside valid values (extrapolate).downcastoptional, einferf or None, defaults to NoneDowncast dtypes if possible.Deprecated since version 2.1.0.``**kwargs``optionalKeyword arguments to pass on to the interpolating function.Returns:Series or DataFrame or NoneReturns the same object type as the caller, interpolated at
some or allNaNvalues or None ifinplace=True.See alsofillnaFill missing values using different methods.scipy.interpolate.Akima1DInterpolatorPiecewise cubic polynomials (Akima interpolator).scipy.interpolate.BPoly.from_derivativesPiecewise polynomial in the Bernstein basis.scipy.interpolate.interp1dInterpolate a 1-D function.scipy.interpolate.KroghInterpolatorInterpolate polynomial (Krogh interpolator).scipy.interpolate.PchipInterpolatorPCHIP 1-d monotonic cubic interpolation.scipy.interpolate.CubicSplineCubic spline data interpolator.NotesThe ekroghf, epiecewise_polynomialf, esplinef, epchipf and eakimaf
methods are wrappers around the respective SciPy implementations of
similar names. These use the actual numerical values of the index.
For more information on their behavior, see theSciPy documentation.ExamplesFilling inNaNin aSeriesvia linear
interpolation.>>>s=pd.Series([0,1,np.nan,3])>>>s0    0.01    1.02    NaN3    3.0dtype: float64>>>s.interpolate()0    0.01    1.02    2.03    3.0dtype: float64Filling inNaNin a Series via polynomial interpolation or splines:
Both epolynomialf and esplinef methods require that you also specify
anorder(int).>>>s=pd.Series([0,2,np.nan,8])>>>s.interpolate(method='polynomial',order=2)0    0.0000001    2.0000002    4.6666673    8.000000dtype: float64Fill the DataFrame forward (that is, going down) along each column
using linear interpolation.Note how the last entry in column eaf is interpolated differently,
because there is no entry after it to use for interpolation.
Note how the first entry in column ebf remainsNaN, because there
is no entry before it to use for interpolation.>>>df=pd.DataFrame([(0.0,np.nan,-1.0,1.0),...(np.nan,2.0,np.nan,np.nan),...(2.0,3.0,np.nan,9.0),...(np.nan,4.0,-4.0,16.0)],...columns=list('abcd'))>>>dfa    b    c     d0  0.0  NaN -1.0   1.01  NaN  2.0  NaN   NaN2  2.0  3.0  NaN   9.03  NaN  4.0 -4.0  16.0>>>df.interpolate(method='linear',limit_direction='forward',axis=0)a    b    c     d0  0.0  NaN -1.0   1.01  1.0  2.0 -2.0   5.02  2.0  3.0 -3.0   9.03  2.0  4.0 -4.0  16.0Using polynomial interpolation.>>>df['d'].interpolate(method='polynomial',order=2)0     1.01     4.02     9.03    16.0Name: d, dtype: float64"
Pandas,DataFrame,pandas.DataFrame.isna,"pandas.DataFrame.isna#DataFrame.isna()[source]#Detect missing values.Return a boolean same-sized object indicating if the values are NA.
NA values, such as None ornumpy.NaN, gets mapped to True
values.
Everything else gets mapped to False values. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).Returns:DataFrameMask of bool values for each element in DataFrame that
indicates whether an element is an NA value.See alsoDataFrame.isnullAlias of isna.DataFrame.notnaBoolean inverse of isna.DataFrame.dropnaOmit axes labels with missing values.isnaTop-level isna.ExamplesShow which entries in a DataFrame are NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.isna()age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  FalseShow which entries in a Series are NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.isna()0    False1    False2     Truedtype: bool"
Pandas,DataFrame,pandas.DataFrame.isnull,"pandas.DataFrame.isnull#DataFrame.isnull()[source]#DataFrame.isnull is an alias for DataFrame.isna.Detect missing values.Return a boolean same-sized object indicating if the values are NA.
NA values, such as None ornumpy.NaN, gets mapped to True
values.
Everything else gets mapped to False values. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).Returns:DataFrameMask of bool values for each element in DataFrame that
indicates whether an element is an NA value.See alsoDataFrame.isnullAlias of isna.DataFrame.notnaBoolean inverse of isna.DataFrame.dropnaOmit axes labels with missing values.isnaTop-level isna.ExamplesShow which entries in a DataFrame are NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.isna()age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  FalseShow which entries in a Series are NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.isna()0    False1    False2     Truedtype: bool"
Pandas,DataFrame,pandas.DataFrame.notna,"pandas.DataFrame.notna#DataFrame.notna()[source]#Detect existing (non-missing) values.Return a boolean same-sized object indicating if the values are not NA.
Non-missing values get mapped to True. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).
NA values, such as None ornumpy.NaN, get mapped to False
values.Returns:DataFrameMask of bool values for each element in DataFrame that
indicates whether an element is not an NA value.See alsoDataFrame.notnullAlias of notna.DataFrame.isnaBoolean inverse of notna.DataFrame.dropnaOmit axes labels with missing values.notnaTop-level notna.ExamplesShow which entries in a DataFrame are not NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.notna()age   born  name    toy0   True  False  True  False1   True   True  True   True2  False   True  True   TrueShow which entries in a Series are not NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.notna()0     True1     True2    Falsedtype: bool"
Pandas,DataFrame,pandas.DataFrame.notnull,"pandas.DataFrame.notnull#DataFrame.notnull()[source]#DataFrame.notnull is an alias for DataFrame.notna.Detect existing (non-missing) values.Return a boolean same-sized object indicating if the values are not NA.
Non-missing values get mapped to True. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).
NA values, such as None ornumpy.NaN, get mapped to False
values.Returns:DataFrameMask of bool values for each element in DataFrame that
indicates whether an element is not an NA value.See alsoDataFrame.notnullAlias of notna.DataFrame.isnaBoolean inverse of notna.DataFrame.dropnaOmit axes labels with missing values.notnaTop-level notna.ExamplesShow which entries in a DataFrame are not NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.notna()age   born  name    toy0   True  False  True  False1   True   True  True   True2  False   True  True   TrueShow which entries in a Series are not NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.notna()0     True1     True2    Falsedtype: bool"
Pandas,DataFrame,pandas.DataFrame.pad,"pandas.DataFrame.pad#DataFrame.pad(*,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values by propagating the last valid observation to next valid.Deprecated since version 2.0:Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead.Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.ExamplesPlease see examples forDataFrame.ffill()orSeries.ffill()."
Pandas,DataFrame,pandas.DataFrame.replace,"pandas.DataFrame.replace#DataFrame.replace(to_replace=None,value=_NoDefault.no_default,*,inplace=False,limit=None,regex=False,method=_NoDefault.no_default)[source]#Replace values given into_replacewithvalue.Values of the Series/DataFrame are replaced with other values dynamically.
This differs from updating with.locor.iloc, which require
you to specify a location to update with some value.Parameters:to_replacestr, regex, list, dict, Series, int, float, or NoneHow to find the values that will be replaced.numeric, str or regex:numeric: numeric values equal toto_replacewill be
replaced withvaluestr: string exactly matchingto_replacewill be replaced
withvalueregex: regexs matchingto_replacewill be replaced withvaluelist of str, regex, or numeric:First, ifto_replaceandvalueare both lists, theymustbe the same length.Second, ifregex=Truethen all of the strings inbothlists will be interpreted as regexs otherwise they will match
directly. This doesnft matter much forvaluesince there
are only a few possible substitution regexes you can use.str, regex and numeric rules apply as above.dict:Dicts can be used to specify different replacement values
for different existing values. For example,{'a':'b','y':'z'}replaces the value eaf with ebf and
eyf with ezf. To use a dict in this way, the optionalvalueparameter should not be given.For a DataFrame a dict can specify that different values
should be replaced in different columns. For example,{'a':1,'b':'z'}looks for the value 1 in column eaf
and the value ezf in column ebf and replaces these values
with whatever is specified invalue. Thevalueparameter
should not beNonein this case. You can treat this as a
special case of passing two lists except that you are
specifying the column to search in.For a DataFrame nested dictionaries, e.g.,{'a':{'b':np.nan}}, are read as follows: look in column
eaf for the value ebf and replace it with NaN. The optionalvalueparameter should not be specified to use a nested dict in this
way. You can nest regular expressions as well. Note that
column names (the top-level dictionary keys in a nested
dictionary)cannotbe regular expressions.None:This means that theregexargument must be a string,
compiled regular expression, or list, dict, ndarray or
Series of such elements. Ifvalueis alsoNonethen
thismustbe a nested dictionary or Series.See the examples section for examples of each of these.valuescalar, dict, list, str, regex, default NoneValue to replace any values matchingto_replacewith.
For a DataFrame a dict of values can be used to specify which
value to use for each column (columns not in the dict will not be
filled). Regular expressions, strings and lists or dicts of such
objects are also allowed.inplacebool, default FalseIf True, performs operation inplace and returns None.limitint, default NoneMaximum size gap to forward or backward fill.Deprecated since version 2.1.0.regexbool or same types asto_replace, default FalseWhether to interpretto_replaceand/orvalueas regular
expressions. If this isTruethento_replacemustbe a
string. Alternatively, this could be a regular expression or a
list, dict, or array of regular expressions in which caseto_replacemust beNone.method{epadf, effillf, ebfillf}The method to use when for replacement, whento_replaceis a
scalar, list or tuple andvalueisNone.Deprecated since version 2.1.0.Returns:Series/DataFrameObject after replacement.Raises:AssertionErrorIfregexis not aboolandto_replaceis notNone.TypeErrorIfto_replaceis not a scalar, array-like,dict, orNoneIfto_replaceis adictandvalueis not alist,dict,ndarray, orSeriesIfto_replaceisNoneandregexis not compilable
into a regular expression or is a list, dict, ndarray, or
Series.When replacing multipleboolordatetime64objects and
the arguments toto_replacedoes not match the type of the
value being replacedValueErrorIf alistor anndarrayis passed toto_replaceandvaluebut they are not the same length.See alsoSeries.fillnaFill NA values.DataFrame.fillnaFill NA values.Series.whereReplace values based on boolean condition.DataFrame.whereReplace values based on boolean condition.DataFrame.mapApply a function to a Dataframe elementwise.Series.mapMap values of Series according to an input mapping or function.Series.str.replaceSimple string replacement.NotesRegex substitution is performed under the hood withre.sub. The
rules for substitution forre.subare the same.Regular expressions will only substitute on strings, meaning you
cannot provide, for example, a regular expression matching floating
point numbers and expect the columns in your frame that have a
numeric dtype to be matched. However, if those floating point
numbersarestrings, then you can do this.This method hasa lotof options. You are encouraged to experiment
and play with this method to gain intuition about how it works.When dict is used as theto_replacevalue, it is like
key(s) in the dict are the to_replace part and
value(s) in the dict are the value parameter.ExamplesScalar `to_replace` and `value`>>>s=pd.Series([1,2,3,4,5])>>>s.replace(1,5)0    51    22    33    44    5dtype: int64>>>df=pd.DataFrame({'A':[0,1,2,3,4],...'B':[5,6,7,8,9],...'C':['a','b','c','d','e']})>>>df.replace(0,5)A  B  C0  5  5  a1  1  6  b2  2  7  c3  3  8  d4  4  9  eList-like `to_replace`>>>df.replace([0,1,2,3],4)A  B  C0  4  5  a1  4  6  b2  4  7  c3  4  8  d4  4  9  e>>>df.replace([0,1,2,3],[4,3,2,1])A  B  C0  4  5  a1  3  6  b2  2  7  c3  1  8  d4  4  9  e>>>s.replace([1,2],method='bfill')0    31    32    33    44    5dtype: int64dict-like `to_replace`>>>df.replace({0:10,1:100})A  B  C0   10  5  a1  100  6  b2    2  7  c3    3  8  d4    4  9  e>>>df.replace({'A':0,'B':5},100)A    B  C0  100  100  a1    1    6  b2    2    7  c3    3    8  d4    4    9  e>>>df.replace({'A':{0:100,4:400}})A  B  C0  100  5  a1    1  6  b2    2  7  c3    3  8  d4  400  9  eRegular expression `to_replace`>>>df=pd.DataFrame({'A':['bat','foo','bait'],...'B':['abc','bar','xyz']})>>>df.replace(to_replace=r'^ba.$',value='new',regex=True)A    B0   new  abc1   foo  new2  bait  xyz>>>df.replace({'A':r'^ba.$'},{'A':'new'},regex=True)A    B0   new  abc1   foo  bar2  bait  xyz>>>df.replace(regex=r'^ba.$',value='new')A    B0   new  abc1   foo  new2  bait  xyz>>>df.replace(regex={r'^ba.$':'new','foo':'xyz'})A    B0   new  abc1   xyz  new2  bait  xyz>>>df.replace(regex=[r'^ba.$','foo'],value='new')A    B0   new  abc1   new  new2  bait  xyzCompare the behavior ofs.replace({'a':None})ands.replace('a',None)to understand the peculiarities
of theto_replaceparameter:>>>s=pd.Series([10,'a','a','b','a'])When one uses a dict as theto_replacevalue, it is like the
value(s) in the dict are equal to thevalueparameter.s.replace({'a':None})is equivalent tos.replace(to_replace={'a':None},value=None,method=None):>>>s.replace({'a':None})0      101    None2    None3       b4    Nonedtype: objectWhenvalueis not explicitly passed andto_replaceis a scalar, list
or tuple,replaceuses the method parameter (default epadf) to do the
replacement. So this is why the eaf values are being replaced by 10
in rows 1 and 2 and ebf in row 4 in this case.>>>s.replace('a')0    101    102    103     b4     bdtype: objectDeprecated since version 2.1.0:The emethodf parameter and padding behavior are deprecated.On the other hand, ifNoneis explicitly passed forvalue, it will
be respected:>>>s.replace('a',None)0      101    None2    None3       b4    Nonedtype: objectChanged in version 1.4.0:Previously the explicitNonewas silently ignored."
Pandas,DataFrame,pandas.DataFrame.droplevel,"pandas.DataFrame.droplevel#DataFrame.droplevel(level,axis=0)[source]#Return Series/DataFrame with requested index / column level(s) removed.Parameters:levelint, str, or list-likeIf a string is given, must be the name of a level
If list-like, elements must be names or positional indexes
of levels.axis{0 or eindexf, 1 or ecolumnsf}, default 0Axis along which the level(s) is removed:0 or eindexf: remove level(s) in column.1 or ecolumnsf: remove level(s) in row.ForSeriesthis parameter is unused and defaults to 0.Returns:Series/DataFrameSeries/DataFrame with requested index / column level(s) removed.Examples>>>df=pd.DataFrame([...[1,2,3,4],...[5,6,7,8],...[9,10,11,12]...]).set_index([0,1]).rename_axis(['a','b'])>>>df.columns=pd.MultiIndex.from_tuples([...('c','e'),('d','f')...],names=['level_1','level_2'])>>>dflevel_1   c   dlevel_2   e   fa b1 2      3   45 6      7   89 10    11  12>>>df.droplevel('a')level_1   c   dlevel_2   e   fb2        3   46        7   810      11  12>>>df.droplevel('level_2',axis=1)level_1   c   da b1 2      3   45 6      7   89 10    11  12"
Pandas,DataFrame,pandas.DataFrame.pivot,"pandas.DataFrame.pivot#DataFrame.pivot(*,columns,index=_NoDefault.no_default,values=_NoDefault.no_default)[source]#Return reshaped DataFrame organized by given index / column values.Reshape data (produce a gpivoth table) based on column values. Uses
unique values from specifiedindex/columnsto form axes of the
resulting DataFrame. This function does not support data
aggregation, multiple values will result in a MultiIndex in the
columns. See theUser Guidefor more on reshaping.Parameters:columnsstr or object or a list of strColumn to use to make new framefs columns.indexstr or object or a list of str, optionalColumn to use to make new framefs index. If not given, uses existing index.valuesstr, object or a list of the previous, optionalColumn(s) to use for populating new framefs values. If not
specified, all remaining columns will be used and the result will
have hierarchically indexed columns.Returns:DataFrameReturns reshaped DataFrame.Raises:ValueError:When there are anyindex,columnscombinations with multiple
values.DataFrame.pivot_tablewhen you need to aggregate.See alsoDataFrame.pivot_tableGeneralization of pivot that can handle duplicate values for one index/column pair.DataFrame.unstackPivot based on the index values instead of a column.wide_to_longWide panel to long format. Less flexible but more user-friendly than melt.NotesFor finer-tuned control, see hierarchical indexing documentation along
with the related stack/unstack methods.Referencethe user guidefor more examples.Examples>>>df=pd.DataFrame({'foo':['one','one','one','two','two',...'two'],...'bar':['A','B','C','A','B','C'],...'baz':[1,2,3,4,5,6],...'zoo':['x','y','z','q','w','t']})>>>dffoo   bar  baz  zoo0   one   A    1    x1   one   B    2    y2   one   C    3    z3   two   A    4    q4   two   B    5    w5   two   C    6    t>>>df.pivot(index='foo',columns='bar',values='baz')bar  A   B   Cfooone  1   2   3two  4   5   6>>>df.pivot(index='foo',columns='bar')['baz']bar  A   B   Cfooone  1   2   3two  4   5   6>>>df.pivot(index='foo',columns='bar',values=['baz','zoo'])baz       zoobar   A  B  C   A  B  Cfooone   1  2  3   x  y  ztwo   4  5  6   q  w  tYou could also assign a list of column names or a list of index names.>>>df=pd.DataFrame({...""lev1"":[1,1,1,2,2,2],...""lev2"":[1,1,2,1,1,2],...""lev3"":[1,2,1,2,1,2],...""lev4"":[1,2,3,4,5,6],...""values"":[0,1,2,3,4,5]})>>>dflev1 lev2 lev3 lev4 values0   1    1    1    1    01   1    1    2    2    12   1    2    1    3    23   2    1    2    4    34   2    1    1    5    45   2    2    2    6    5>>>df.pivot(index=""lev1"",columns=[""lev2"",""lev3""],values=""values"")lev2    1         2lev3    1    2    1    2lev11     0.0  1.0  2.0  NaN2     4.0  3.0  NaN  5.0>>>df.pivot(index=[""lev1"",""lev2""],columns=[""lev3""],values=""values"")lev3    1    2lev1  lev21     1  0.0  1.02  2.0  NaN2     1  4.0  3.02  NaN  5.0A ValueError is raised if there are any duplicates.>>>df=pd.DataFrame({""foo"":['one','one','two','two'],...""bar"":['A','A','B','C'],...""baz"":[1,2,3,4]})>>>dffoo bar  baz0  one   A    11  one   A    22  two   B    33  two   C    4Notice that the first two rows are the same for ourindexandcolumnsarguments.>>>df.pivot(index='foo',columns='bar',values='baz')Traceback (most recent call last):...ValueError:Index contains duplicate entries, cannot reshape"
Pandas,DataFrame,pandas.DataFrame.pivot_table,"pandas.DataFrame.pivot_table#DataFrame.pivot_table(values=None,index=None,columns=None,aggfunc='mean',fill_value=None,margins=False,dropna=True,margins_name='All',observed=False,sort=True)[source]#Create a spreadsheet-style pivot table as a DataFrame.The levels in the pivot table will be stored in MultiIndex objects
(hierarchical indexes) on the index and columns of the result DataFrame.Parameters:valueslist-like or scalar, optionalColumn or columns to aggregate.indexcolumn, Grouper, array, or list of the previousKeys to group by on the pivot table index. If a list is passed,
it can contain any of the other types (except list). If an array is
passed, it must be the same length as the data and will be used in
the same manner as column values.columnscolumn, Grouper, array, or list of the previousKeys to group by on the pivot table column. If a list is passed,
it can contain any of the other types (except list). If an array is
passed, it must be the same length as the data and will be used in
the same manner as column values.aggfuncfunction, list of functions, dict, default gmeanhIf a list of functions is passed, the resulting pivot table will have
hierarchical columns whose top level are the function names
(inferred from the function objects themselves).
If a dict is passed, the key is column to aggregate and the value is
function or list of functions. Ifmargin=True, aggfunc will be
used to calculate the partial aggregates.fill_valuescalar, default NoneValue to replace missing values with (in the resulting pivot table,
after aggregation).marginsbool, default FalseIfmargins=True, specialAllcolumns and rows
will be added with partial group aggregates across the categories
on the rows and columns.dropnabool, default TrueDo not include columns whose entries are all NaN. If True,
rows with a NaN value in any column will be omitted before
computing margins.margins_namestr, default eAllfName of the row / column that will contain the totals
when margins is True.observedbool, default FalseThis only applies if any of the groupers are Categoricals.
If True: only show observed values for categorical groupers.
If False: show all values for categorical groupers.sortbool, default TrueSpecifies if the result should be sorted.New in version 1.3.0.Returns:DataFrameAn Excel style pivot table.See alsoDataFrame.pivotPivot without aggregation that can handle non-numeric data.DataFrame.meltUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.wide_to_longWide panel to long format. Less flexible but more user-friendly than melt.NotesReferencethe user guidefor more examples.Examples>>>df=pd.DataFrame({""A"":[""foo"",""foo"",""foo"",""foo"",""foo"",...""bar"",""bar"",""bar"",""bar""],...""B"":[""one"",""one"",""one"",""two"",""two"",...""one"",""one"",""two"",""two""],...""C"":[""small"",""large"",""large"",""small"",...""small"",""large"",""small"",""small"",...""large""],...""D"":[1,2,2,3,3,4,5,6,7],...""E"":[2,4,5,5,6,6,8,9,9]})>>>dfA    B      C  D  E0  foo  one  small  1  21  foo  one  large  2  42  foo  one  large  2  53  foo  two  small  3  54  foo  two  small  3  65  bar  one  large  4  66  bar  one  small  5  87  bar  two  small  6  98  bar  two  large  7  9This first example aggregates values by taking the sum.>>>table=pd.pivot_table(df,values='D',index=['A','B'],...columns=['C'],aggfunc=""sum"")>>>tableC        large  smallA   Bbar one    4.0    5.0two    7.0    6.0foo one    4.0    1.0two    NaN    6.0We can also fill missing values using thefill_valueparameter.>>>table=pd.pivot_table(df,values='D',index=['A','B'],...columns=['C'],aggfunc=""sum"",fill_value=0)>>>tableC        large  smallA   Bbar one      4      5two      7      6foo one      4      1two      0      6The next example aggregates by taking the mean across multiple columns.>>>table=pd.pivot_table(df,values=['D','E'],index=['A','C'],...aggfunc={'D':""mean"",'E':""mean""})>>>tableD         EA   Cbar large  5.500000  7.500000small  5.500000  8.500000foo large  2.000000  4.500000small  2.333333  4.333333We can also calculate multiple types of aggregations for any given
value column.>>>table=pd.pivot_table(df,values=['D','E'],index=['A','C'],...aggfunc={'D':""mean"",...'E':[""min"",""max"",""mean""]})>>>tableD   Emean max      mean  minA   Cbar large  5.500000   9  7.500000    6small  5.500000   9  8.500000    8foo large  2.000000   5  4.500000    4small  2.333333   6  4.333333    2"
Pandas,DataFrame,pandas.DataFrame.reorder_levels,"pandas.DataFrame.reorder_levels#DataFrame.reorder_levels(order,axis=0)[source]#Rearrange index levels using input order. May not drop or duplicate levels.Parameters:orderlist of int or list of strList representing new level order. Reference level by number
(position) or by key (label).axis{0 or eindexf, 1 or ecolumnsf}, default 0Where to reorder levels.Returns:DataFrameExamples>>>data={...""class"":[""Mammals"",""Mammals"",""Reptiles""],...""diet"":[""Omnivore"",""Carnivore"",""Carnivore""],...""species"":[""Humans"",""Dogs"",""Snakes""],...}>>>df=pd.DataFrame(data,columns=[""class"",""diet"",""species""])>>>df=df.set_index([""class"",""diet""])>>>dfspeciesclass      dietMammals    Omnivore                HumansCarnivore                 DogsReptiles   Carnivore               SnakesLetfs reorder the levels of the index:>>>df.reorder_levels([""diet"",""class""])speciesdiet      classOmnivore  Mammals                  HumansCarnivore Mammals                    DogsReptiles                 Snakes"
Pandas,DataFrame,pandas.DataFrame.sort_values,"pandas.DataFrame.sort_values#DataFrame.sort_values(by,*,axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last',ignore_index=False,key=None)[source]#Sort by the values along either axis.Parameters:bystr or list of strName or list of names to sort by.ifaxisis 0 oreindexfthenbymay contain index
levels and/or column labels.ifaxisis 1 orecolumnsfthenbymay contain column
levels and/or index labels.axisg{0 or eindexf, 1 or ecolumnsf}h, default 0Axis to be sorted.ascendingbool or list of bool, default TrueSort ascending vs. descending. Specify list for multiple sort
orders. If this is a list of bools, must match the length of
the by.inplacebool, default FalseIf True, perform operation in-place.kind{equicksortf, emergesortf, eheapsortf, establef}, default equicksortfChoice of sorting algorithm. See alsonumpy.sort()for more
information.mergesortandstableare the only stable algorithms. For
DataFrames, this option is only applied when sorting on a single
column or label.na_position{efirstf, elastf}, default elastfPuts NaNs at the beginning iffirst;lastputs NaNs at the
end.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, c, n - 1.keycallable, optionalApply the key function to the values
before sorting. This is similar to thekeyargument in the
builtinsorted()function, with the notable difference that
thiskeyfunction should bevectorized. It should expect aSeriesand return a Series with the same shape as the input.
It will be applied to each column inbyindependently.Returns:DataFrame or NoneDataFrame with sorted values or None ifinplace=True.See alsoDataFrame.sort_indexSort a DataFrame by the index.Series.sort_valuesSimilar method for a Series.Examples>>>df=pd.DataFrame({...'col1':['A','A','B',np.nan,'D','C'],...'col2':[2,1,9,8,7,4],...'col3':[0,1,9,4,2,3],...'col4':['a','B','c','D','e','F']...})>>>dfcol1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FSort by col1>>>df.sort_values(by=['col1'])col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort by multiple columns>>>df.sort_values(by=['col1','col2'])col1  col2  col3 col41    A     1     1    B0    A     2     0    a2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort Descending>>>df.sort_values(by='col1',ascending=False)col1  col2  col3 col44    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    B3  NaN     8     4    DPutting NAs first>>>df.sort_values(by='col1',ascending=False,na_position='first')col1  col2  col3 col43  NaN     8     4    D4    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    BSorting with a key function>>>df.sort_values(by='col4',key=lambdacol:col.str.lower())col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FNatural sort with the key argument,
using thenatsort <https://github.com/SethMMorton/natsort>package.>>>df=pd.DataFrame({...""time"":['0hr','128hr','72hr','48hr','96hr'],...""value"":[10,20,30,40,50]...})>>>dftime  value0    0hr     101  128hr     202   72hr     303   48hr     404   96hr     50>>>fromnatsortimportindex_natsorted>>>df.sort_values(...by=""time"",...key=lambdax:np.argsort(index_natsorted(df[""time""]))...)time  value0    0hr     103   48hr     402   72hr     304   96hr     501  128hr     20"
Pandas,DataFrame,pandas.DataFrame.sort_index,"pandas.DataFrame.sort_index#DataFrame.sort_index(*,axis=0,level=None,ascending=True,inplace=False,kind='quicksort',na_position='last',sort_remaining=True,ignore_index=False,key=None)[source]#Sort object by labels (along an axis).Returns a new DataFrame sorted by label ifinplaceargument isFalse, otherwise updates the original DataFrame and returns None.Parameters:axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis along which to sort. The value 0 identifies the rows,
and 1 identifies the columns.levelint or level name or list of ints or list of level namesIf not None, sort on values in specified index level(s).ascendingbool or list-like of bools, default TrueSort ascending vs. descending. When the index is a MultiIndex the
sort direction can be controlled for each level individually.inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one.kind{equicksortf, emergesortf, eheapsortf, establef}, default equicksortfChoice of sorting algorithm. See alsonumpy.sort()for more
information.mergesortandstableare the only stable algorithms. For
DataFrames, this option is only applied when sorting on a single
column or label.na_position{efirstf, elastf}, default elastfPuts NaNs at the beginning iffirst;lastputs NaNs at the end.
Not implemented for MultiIndex.sort_remainingbool, default TrueIf True and sorting by level and index is multilevel, sort by other
levels too (in order) after sorting by specified level.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, c, n - 1.keycallable, optionalIf not None, apply the key function to the index values
before sorting. This is similar to thekeyargument in the
builtinsorted()function, with the notable difference that
thiskeyfunction should bevectorized. It should expect anIndexand return anIndexof the same shape. For MultiIndex
inputs, the key is appliedper level.Returns:DataFrame or NoneThe original DataFrame sorted by the labels or None ifinplace=True.See alsoSeries.sort_indexSort Series by the index.DataFrame.sort_valuesSort DataFrame by the value.Series.sort_valuesSort Series by the value.Examples>>>df=pd.DataFrame([1,2,3,4,5],index=[100,29,234,1,150],...columns=['A'])>>>df.sort_index()A1    429   2100  1150  5234  3By default, it sorts in ascending order, to sort in descending order,
useascending=False>>>df.sort_index(ascending=False)A234  3150  5100  129   21    4A key function can be specified which is applied to the index before
sorting. For aMultiIndexthis is applied to each level separately.>>>df=pd.DataFrame({""a"":[1,2,3,4]},index=['A','b','C','d'])>>>df.sort_index(key=lambdax:x.str.lower())aA  1b  2C  3d  4"
Pandas,DataFrame,pandas.DataFrame.nlargest,"pandas.DataFrame.nlargest#DataFrame.nlargest(n,columns,keep='first')[source]#Return the firstnrows ordered bycolumnsin descending order.Return the firstnrows with the largest values incolumns, in
descending order. The columns that are not specified are returned as
well, but not used for ordering.This method is equivalent todf.sort_values(columns,ascending=False).head(n), but more
performant.Parameters:nintNumber of rows to return.columnslabel or list of labelsColumn label(s) to order by.keep{efirstf, elastf, eallf}, default efirstfWhere there are duplicate values:first: prioritize the first occurrence(s)last: prioritize the last occurrence(s)all: do not drop any duplicates, even it means
selecting more thannitems.Returns:DataFrameThe firstnrows ordered by the given columns in descending
order.See alsoDataFrame.nsmallestReturn the firstnrows ordered bycolumnsin ascending order.DataFrame.sort_valuesSort DataFrame by the values.DataFrame.headReturn the firstnrows without re-ordering.NotesThis function cannot be used with all column types. For example, when
specifying columns withobjectorcategorydtypes,TypeErroris
raised.Examples>>>df=pd.DataFrame({'population':[59000000,65000000,434000,...434000,434000,337000,11300,...11300,11300],...'GDP':[1937894,2583560,12011,4520,12128,...17036,182,38,311],...'alpha-2':[""IT"",""FR"",""MT"",""MV"",""BN"",...""IS"",""NR"",""TV"",""AI""]},...index=[""Italy"",""France"",""Malta"",...""Maldives"",""Brunei"",""Iceland"",...""Nauru"",""Tuvalu"",""Anguilla""])>>>dfpopulation      GDP alpha-2Italy       59000000  1937894      ITFrance      65000000  2583560      FRMalta         434000    12011      MTMaldives      434000     4520      MVBrunei        434000    12128      BNIceland       337000    17036      ISNauru          11300      182      NRTuvalu         11300       38      TVAnguilla       11300      311      AIIn the following example, we will usenlargestto select the three
rows having the largest values in column gpopulationh.>>>df.nlargest(3,'population')population      GDP alpha-2France    65000000  2583560      FRItaly     59000000  1937894      ITMalta       434000    12011      MTWhen usingkeep='last', ties are resolved in reverse order:>>>df.nlargest(3,'population',keep='last')population      GDP alpha-2France    65000000  2583560      FRItaly     59000000  1937894      ITBrunei      434000    12128      BNWhen usingkeep='all', all duplicate items are maintained:>>>df.nlargest(3,'population',keep='all')population      GDP alpha-2France      65000000  2583560      FRItaly       59000000  1937894      ITMalta         434000    12011      MTMaldives      434000     4520      MVBrunei        434000    12128      BNTo order by the largest values in column gpopulationh and then gGDPh,
we can specify multiple columns like in the next example.>>>df.nlargest(3,['population','GDP'])population      GDP alpha-2France    65000000  2583560      FRItaly     59000000  1937894      ITBrunei      434000    12128      BN"
Pandas,DataFrame,pandas.DataFrame.nsmallest,"pandas.DataFrame.nsmallest#DataFrame.nsmallest(n,columns,keep='first')[source]#Return the firstnrows ordered bycolumnsin ascending order.Return the firstnrows with the smallest values incolumns, in
ascending order. The columns that are not specified are returned as
well, but not used for ordering.This method is equivalent todf.sort_values(columns,ascending=True).head(n), but more
performant.Parameters:nintNumber of items to retrieve.columnslist or strColumn name or names to order by.keep{efirstf, elastf, eallf}, default efirstfWhere there are duplicate values:first: take the first occurrence.last: take the last occurrence.all: do not drop any duplicates, even it means
selecting more thannitems.Returns:DataFrameSee alsoDataFrame.nlargestReturn the firstnrows ordered bycolumnsin descending order.DataFrame.sort_valuesSort DataFrame by the values.DataFrame.headReturn the firstnrows without re-ordering.Examples>>>df=pd.DataFrame({'population':[59000000,65000000,434000,...434000,434000,337000,337000,...11300,11300],...'GDP':[1937894,2583560,12011,4520,12128,...17036,182,38,311],...'alpha-2':[""IT"",""FR"",""MT"",""MV"",""BN"",...""IS"",""NR"",""TV"",""AI""]},...index=[""Italy"",""France"",""Malta"",...""Maldives"",""Brunei"",""Iceland"",...""Nauru"",""Tuvalu"",""Anguilla""])>>>dfpopulation      GDP alpha-2Italy       59000000  1937894      ITFrance      65000000  2583560      FRMalta         434000    12011      MTMaldives      434000     4520      MVBrunei        434000    12128      BNIceland       337000    17036      ISNauru         337000      182      NRTuvalu         11300       38      TVAnguilla       11300      311      AIIn the following example, we will usensmallestto select the
three rows having the smallest values in column gpopulationh.>>>df.nsmallest(3,'population')population    GDP alpha-2Tuvalu         11300     38      TVAnguilla       11300    311      AIIceland       337000  17036      ISWhen usingkeep='last', ties are resolved in reverse order:>>>df.nsmallest(3,'population',keep='last')population  GDP alpha-2Anguilla       11300  311      AITuvalu         11300   38      TVNauru         337000  182      NRWhen usingkeep='all', all duplicate items are maintained:>>>df.nsmallest(3,'population',keep='all')population    GDP alpha-2Tuvalu         11300     38      TVAnguilla       11300    311      AIIceland       337000  17036      ISNauru         337000    182      NRTo order by the smallest values in column gpopulationh and then gGDPh, we can
specify multiple columns like in the next example.>>>df.nsmallest(3,['population','GDP'])population  GDP alpha-2Tuvalu         11300   38      TVAnguilla       11300  311      AINauru         337000  182      NR"
Pandas,DataFrame,pandas.DataFrame.swaplevel,"pandas.DataFrame.swaplevel#DataFrame.swaplevel(i=-2,j=-1,axis=0)[source]#Swap levels i and j in aMultiIndex.Default is to swap the two innermost levels of the index.Parameters:i, jint or strLevels of the indices to be swapped. Can pass level name as string.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to swap levels on. 0 or eindexf for row-wise, 1 or
ecolumnsf for column-wise.Returns:DataFrameDataFrame with levels swapped in MultiIndex.Examples>>>df=pd.DataFrame(...{""Grade"":[""A"",""B"",""A"",""C""]},...index=[...[""Final exam"",""Final exam"",""Coursework"",""Coursework""],...[""History"",""Geography"",""History"",""Geography""],...[""January"",""February"",""March"",""April""],...],...)>>>dfGradeFinal exam  History     January      AGeography   February     BCoursework  History     March        AGeography   April        CIn the following example, we will swap the levels of the indices.
Here, we will swap the levels column-wise, but levels can be swapped row-wise
in a similar manner. Note that column-wise is the default behaviour.
By not supplying any arguments for i and j, we swap the last and second to
last indices.>>>df.swaplevel()GradeFinal exam  January     History         AFebruary    Geography       BCoursework  March       History         AApril       Geography       CBy supplying one argument, we can choose which index to swap the last
index with. We can for example swap the first index with the last one as
follows.>>>df.swaplevel(0)GradeJanuary     History     Final exam      AFebruary    Geography   Final exam      BMarch       History     Coursework      AApril       Geography   Coursework      CWe can also define explicitly which indices we want to swap by supplying values
for both i and j. Here, we for example swap the first and second indices.>>>df.swaplevel(0,1)GradeHistory     Final exam  January         AGeography   Final exam  February        BHistory     Coursework  March           AGeography   Coursework  April           C"
Pandas,DataFrame,pandas.DataFrame.stack,"pandas.DataFrame.stack#DataFrame.stack(level=-1,dropna=_NoDefault.no_default,sort=_NoDefault.no_default,future_stack=False)[source]#Stack the prescribed level(s) from columns to index.Return a reshaped DataFrame or Series having a multi-level
index with one or more new inner-most levels compared to the current
DataFrame. The new inner-most levels are created by pivoting the
columns of the current dataframe:if the columns have a single level, the output is a Series;if the columns have multiple levels, the new index
level(s) is (are) taken from the prescribed level(s) and
the output is a DataFrame.Parameters:levelint, str, list, default -1Level(s) to stack from the column axis onto the index
axis, defined as one index or label, or a list of indices
or labels.dropnabool, default TrueWhether to drop rows in the resulting Frame/Series with
missing values. Stacking a column level onto the index
axis can create combinations of index and column values
that are missing from the original dataframe. See Examples
section.sortbool, default TrueWhether to sort the levels of the resulting MultiIndex.future_stackbool, default FalseWhether to use the new implementation that will replace the current
implementation in pandas 3.0. When True, dropna and sort have no impact
on the result and must remain unspecified. Seepandas 2.1.0 Release
notesfor more details.Returns:DataFrame or SeriesStacked dataframe or series.See alsoDataFrame.unstackUnstack prescribed level(s) from index axis onto column axis.DataFrame.pivotReshape dataframe from long format to wide format.DataFrame.pivot_tableCreate a spreadsheet-style pivot table as a DataFrame.NotesThe function is named by analogy with a collection of books
being reorganized from being side by side on a horizontal
position (the columns of the dataframe) to being stacked
vertically on top of each other (in the index of the
dataframe).Referencethe user guidefor more examples.ExamplesSingle level columns>>>df_single_level_cols=pd.DataFrame([[0,1],[2,3]],...index=['cat','dog'],...columns=['weight','height'])Stacking a dataframe with a single level column axis returns a Series:>>>df_single_level_colsweight heightcat       0      1dog       2      3>>>df_single_level_cols.stack(future_stack=True)cat  weight    0height    1dog  weight    2height    3dtype: int64Multi level columns: simple case>>>multicol1=pd.MultiIndex.from_tuples([('weight','kg'),...('weight','pounds')])>>>df_multi_level_cols1=pd.DataFrame([[1,2],[2,4]],...index=['cat','dog'],...columns=multicol1)Stacking a dataframe with a multi-level column axis:>>>df_multi_level_cols1weightkg    poundscat       1        2dog       2        4>>>df_multi_level_cols1.stack(future_stack=True)weightcat kg           1pounds       2dog kg           2pounds       4Missing values>>>multicol2=pd.MultiIndex.from_tuples([('weight','kg'),...('height','m')])>>>df_multi_level_cols2=pd.DataFrame([[1.0,2.0],[3.0,4.0]],...index=['cat','dog'],...columns=multicol2)It is common to have missing values when stacking a dataframe
with multi-level columns, as the stacked dataframe typically
has more values than the original dataframe. Missing values
are filled with NaNs:>>>df_multi_level_cols2weight heightkg      mcat    1.0    2.0dog    3.0    4.0>>>df_multi_level_cols2.stack(future_stack=True)weight  heightcat kg     1.0     NaNm      NaN     2.0dog kg     3.0     NaNm      NaN     4.0Prescribing the level(s) to be stackedThe first parameter controls which level or levels are stacked:>>>df_multi_level_cols2.stack(0,future_stack=True)kg    mcat weight  1.0  NaNheight  NaN  2.0dog weight  3.0  NaNheight  NaN  4.0>>>df_multi_level_cols2.stack([0,1],future_stack=True)cat  weight  kg    1.0height  m     2.0dog  weight  kg    3.0height  m     4.0dtype: float64Dropping missing values>>>df_multi_level_cols3=pd.DataFrame([[None,1.0],[2.0,3.0]],...index=['cat','dog'],...columns=multicol2)Note that rows where all values are missing are dropped by
default but this behaviour can be controlled via the dropna
keyword parameter:>>>df_multi_level_cols3weight heightkg      mcat    NaN    1.0dog    2.0    3.0>>>df_multi_level_cols3.stack(dropna=False)weight  heightcat kg     NaN     NaNm      NaN     1.0dog kg     2.0     NaNm      NaN     3.0>>>df_multi_level_cols3.stack(dropna=True)weight  heightcat m      NaN     1.0dog kg     2.0     NaNm      NaN     3.0"
Pandas,DataFrame,pandas.DataFrame.unstack,"pandas.DataFrame.unstack#DataFrame.unstack(level=-1,fill_value=None,sort=True)[source]#Pivot a level of the (necessarily hierarchical) index labels.Returns a DataFrame having a new level of column labels whose inner-most level
consists of the pivoted index labels.If the index is not a MultiIndex, the output will be a Series
(the analogue of stack when the columns are not a MultiIndex).Parameters:levelint, str, or list of these, default -1 (last level)Level(s) of index to unstack, can pass level name.fill_valueint, str or dictReplace NaN with this value if the unstack produces missing values.sortbool, default TrueSort the level(s) in the resulting MultiIndex columns.Returns:Series or DataFrameSee alsoDataFrame.pivotPivot a table based on column values.DataFrame.stackPivot a level of the column labels (inverse operation fromunstack).NotesReferencethe user guidefor more examples.Examples>>>index=pd.MultiIndex.from_tuples([('one','a'),('one','b'),...('two','a'),('two','b')])>>>s=pd.Series(np.arange(1.0,5.0),index=index)>>>sone  a   1.0b   2.0two  a   3.0b   4.0dtype: float64>>>s.unstack(level=-1)a   bone  1.0  2.0two  3.0  4.0>>>s.unstack(level=0)one  twoa  1.0   3.0b  2.0   4.0>>>df=s.unstack(level=0)>>>df.unstack()one  a  1.0b  2.0two  a  3.0b  4.0dtype: float64"
Pandas,DataFrame,pandas.DataFrame.swapaxes,"pandas.DataFrame.swapaxes#DataFrame.swapaxes(axis1,axis2,copy=None)[source]#Interchange axes and swap values axes appropriately.Deprecated since version 2.1.0:swapaxesis deprecated and will be removed.
Please usetransposeinstead.Returns:same as inputExamplesPlease see examples forDataFrame.transpose()."
Pandas,DataFrame,pandas.DataFrame.melt,"pandas.DataFrame.melt#DataFrame.melt(id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None,ignore_index=True)[source]#Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.This function is useful to massage a DataFrame into a format where one
or more columns are identifier variables (id_vars), while all other
columns, considered measured variables (value_vars), are gunpivotedh to
the row axis, leaving just two non-identifier columns, evariablef and
evaluef.Parameters:id_varstuple, list, or ndarray, optionalColumn(s) to use as identifier variables.value_varstuple, list, or ndarray, optionalColumn(s) to unpivot. If not specified, uses all columns that
are not set asid_vars.var_namescalarName to use for the evariablef column. If None it usesframe.columns.nameor evariablef.value_namescalar, default evaluefName to use for the evaluef column.col_levelint or str, optionalIf columns are a MultiIndex then use this level to melt.ignore_indexbool, default TrueIf True, original index is ignored. If False, the original index is retained.
Index labels will be repeated as necessary.Returns:DataFrameUnpivoted DataFrame.See alsomeltIdentical method.pivot_tableCreate a spreadsheet-style pivot table as a DataFrame.DataFrame.pivotReturn reshaped DataFrame organized by given index / column values.DataFrame.explodeExplode a DataFrame from list-like columns to long format.NotesReferencethe user guidefor more examples.Examples>>>df=pd.DataFrame({'A':{0:'a',1:'b',2:'c'},...'B':{0:1,1:3,2:5},...'C':{0:2,1:4,2:6}})>>>dfA  B  C0  a  1  21  b  3  42  c  5  6>>>df.melt(id_vars=['A'],value_vars=['B'])A variable  value0  a        B      11  b        B      32  c        B      5>>>df.melt(id_vars=['A'],value_vars=['B','C'])A variable  value0  a        B      11  b        B      32  c        B      53  a        C      24  b        C      45  c        C      6The names of evariablef and evaluef columns can be customized:>>>df.melt(id_vars=['A'],value_vars=['B'],...var_name='myVarname',value_name='myValname')A myVarname  myValname0  a         B          11  b         B          32  c         B          5Original index values can be kept around:>>>df.melt(id_vars=['A'],value_vars=['B','C'],ignore_index=False)A variable  value0  a        B      11  b        B      32  c        B      50  a        C      21  b        C      42  c        C      6If you have multi-index columns:>>>df.columns=[list('ABC'),list('DEF')]>>>dfA  B  CD  E  F0  a  1  21  b  3  42  c  5  6>>>df.melt(col_level=0,id_vars=['A'],value_vars=['B'])A variable  value0  a        B      11  b        B      32  c        B      5>>>df.melt(id_vars=[('A','D')],value_vars=[('B','E')])(A, D) variable_0 variable_1  value0      a          B          E      11      b          B          E      32      c          B          E      5"
Pandas,DataFrame,pandas.DataFrame.explode,"pandas.DataFrame.explode#DataFrame.explode(column,ignore_index=False)[source]#Transform each element of a list-like to a row, replicating index values.Parameters:columnIndexLabelColumn(s) to explode.
For multiple columns, specify a non-empty list with each element
be str or tuple, and all specified columns their list-like data
on same row of the frame must have matching length.New in version 1.3.0:Multi-column explodeignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, c, n - 1.Returns:DataFrameExploded lists to rows of the subset columns;
index will be duplicated for these rows.Raises:ValueErrorIf columns of the frame are not unique.If specified columns to explode is empty list.If specified columns to explode have not matching count of
elements rowwise in the frame.See alsoDataFrame.unstackPivot a level of the (necessarily hierarchical) index labels.DataFrame.meltUnpivot a DataFrame from wide format to long format.Series.explodeExplode a DataFrame from list-like columns to long format.NotesThis routine will explode list-likes including lists, tuples, sets,
Series, and np.ndarray. The result dtype of the subset rows will
be object. Scalars will be returned unchanged, and empty list-likes will
result in a np.nan for that row. In addition, the ordering of rows in the
output will be non-deterministic when exploding sets.Referencethe user guidefor more examples.Examples>>>df=pd.DataFrame({'A':[[0,1,2],'foo',[],[3,4]],...'B':1,...'C':[['a','b','c'],np.nan,[],['d','e']]})>>>dfA  B          C0  [0, 1, 2]  1  [a, b, c]1        foo  1        NaN2         []  1         []3     [3, 4]  1     [d, e]Single-column explode.>>>df.explode('A')A  B          C0    0  1  [a, b, c]0    1  1  [a, b, c]0    2  1  [a, b, c]1  foo  1        NaN2  NaN  1         []3    3  1     [d, e]3    4  1     [d, e]Multi-column explode.>>>df.explode(list('AC'))A  B    C0    0  1    a0    1  1    b0    2  1    c1  foo  1  NaN2  NaN  1  NaN3    3  1    d3    4  1    e"
Pandas,DataFrame,pandas.DataFrame.squeeze,"pandas.DataFrame.squeeze#DataFrame.squeeze(axis=None)[source]#Squeeze 1 dimensional axis objects into scalars.Series or DataFrames with a single element are squeezed to a scalar.
DataFrames with a single column or a single row are squeezed to a
Series. Otherwise the object is unchanged.This method is most useful when you donft know if your
object is a Series or DataFrame, but you do know it has just a single
column. In that case you can safely callsqueezeto ensure you have a
Series.Parameters:axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneA specific axis to squeeze. By default, all length-1 axes are
squeezed. ForSeriesthis parameter is unused and defaults toNone.Returns:DataFrame, Series, or scalarThe projection after squeezingaxisor all the axes.See alsoSeries.ilocInteger-location based indexing for selecting scalars.DataFrame.ilocInteger-location based indexing for selecting Series.Series.to_frameInverse of DataFrame.squeeze for a single-column DataFrame.Examples>>>primes=pd.Series([2,3,5,7])Slicing might produce a Series with a single value:>>>even_primes=primes[primes%2==0]>>>even_primes0    2dtype: int64>>>even_primes.squeeze()2Squeezing objects with more than one value in every axis does nothing:>>>odd_primes=primes[primes%2==1]>>>odd_primes1    32    53    7dtype: int64>>>odd_primes.squeeze()1    32    53    7dtype: int64Squeezing is even more effective when used with DataFrames.>>>df=pd.DataFrame([[1,2],[3,4]],columns=['a','b'])>>>dfa  b0  1  21  3  4Slicing a single column will produce a DataFrame with the columns
having only one value:>>>df_a=df[['a']]>>>df_aa0  11  3So the columns can be squeezed down, resulting in a Series:>>>df_a.squeeze('columns')0    11    3Name: a, dtype: int64Slicing a single row from a single column will produce a single
scalar DataFrame:>>>df_0a=df.loc[df.index<1,['a']]>>>df_0aa0  1Squeezing the rows produces a single scalar Series:>>>df_0a.squeeze('rows')a    1Name: 0, dtype: int64Squeezing all axes will project directly into a scalar:>>>df_0a.squeeze()1"
Pandas,DataFrame,pandas.DataFrame.to_xarray,"pandas.DataFrame.to_xarray#DataFrame.to_xarray()[source]#Return an xarray object from the pandas object.Returns:xarray.DataArray or xarray.DatasetData in the pandas structure converted to Dataset if the object is
a DataFrame, or a DataArray if the object is a Series.See alsoDataFrame.to_hdfWrite DataFrame to an HDF5 file.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.NotesSee thexarray docsExamples>>>df=pd.DataFrame([('falcon','bird',389.0,2),...('parrot','bird',24.0,2),...('lion','mammal',80.5,4),...('monkey','mammal',np.nan,4)],...columns=['name','class','max_speed',...'num_legs'])>>>dfname   class  max_speed  num_legs0  falcon    bird      389.0         21  parrot    bird       24.0         22    lion  mammal       80.5         43  monkey  mammal        NaN         4>>>df.to_xarray()<xarray.Dataset>Dimensions:    (index: 4)Coordinates:* index      (index) int64 0 1 2 3Data variables:name       (index) object 'falcon' 'parrot' 'lion' 'monkey'class      (index) object 'bird' 'bird' 'mammal' 'mammal'max_speed  (index) float64 389.0 24.0 80.5 nannum_legs   (index) int64 2 2 4 4>>>df['max_speed'].to_xarray()<xarray.DataArray 'max_speed' (index: 4)>array([389. ,  24. ,  80.5,   nan])Coordinates:* index    (index) int64 0 1 2 3>>>dates=pd.to_datetime(['2018-01-01','2018-01-01',...'2018-01-02','2018-01-02'])>>>df_multiindex=pd.DataFrame({'date':dates,...'animal':['falcon','parrot',...'falcon','parrot'],...'speed':[350,18,361,15]})>>>df_multiindex=df_multiindex.set_index(['date','animal'])>>>df_multiindexspeeddate       animal2018-01-01 falcon    350parrot     182018-01-02 falcon    361parrot     15>>>df_multiindex.to_xarray()<xarray.Dataset>Dimensions:  (date: 2, animal: 2)Coordinates:* date     (date) datetime64[ns] 2018-01-01 2018-01-02* animal   (animal) object 'falcon' 'parrot'Data variables:speed    (date, animal) int64 350 18 361 15"
Pandas,DataFrame,pandas.DataFrame.T,"pandas.DataFrame.T#propertyDataFrame.T[source]#The transpose of the DataFrame.Returns:DataFrameThe transposed DataFrame.See alsoDataFrame.transposeTranspose index and columns.Examples>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>dfcol1  col20     1     31     2     4>>>df.T0  1col1  1  2col2  3  4"
Pandas,DataFrame,pandas.DataFrame.transpose,"pandas.DataFrame.transpose#DataFrame.transpose(*args,copy=False)[source]#Transpose index and columns.Reflect the DataFrame over its main diagonal by writing rows as columns
and vice-versa. The propertyTis an accessor to the methodtranspose().Parameters:*argstuple, optionalAccepted for compatibility with NumPy.copybool, default FalseWhether to copy the data after transposing, even for DataFrames
with a single dtype.Note that a copy is always required for mixed dtype DataFrames,
or for DataFrames with any extension types.Returns:DataFrameThe transposed DataFrame.See alsonumpy.transposePermute the dimensions of a given array.NotesTransposing a DataFrame with mixed dtypes will result in a homogeneous
DataFrame with theobjectdtype. In such a case, a copy of the data
is always made.ExamplesSquare DataFrame with homogeneous dtype>>>d1={'col1':[1,2],'col2':[3,4]}>>>df1=pd.DataFrame(data=d1)>>>df1col1  col20     1     31     2     4>>>df1_transposed=df1.T# or df1.transpose()>>>df1_transposed0  1col1  1  2col2  3  4When the dtype is homogeneous in the original DataFrame, we get a
transposed DataFrame with the same dtype:>>>df1.dtypescol1    int64col2    int64dtype: object>>>df1_transposed.dtypes0    int641    int64dtype: objectNon-square DataFrame with mixed dtypes>>>d2={'name':['Alice','Bob'],...'score':[9.5,8],...'employed':[False,True],...'kids':[0,0]}>>>df2=pd.DataFrame(data=d2)>>>df2name  score  employed  kids0  Alice    9.5     False     01    Bob    8.0      True     0>>>df2_transposed=df2.T# or df2.transpose()>>>df2_transposed0     1name      Alice   Bobscore       9.5   8.0employed  False  Truekids          0     0When the DataFrame has mixed dtypes, we get a transposed DataFrame with
theobjectdtype:>>>df2.dtypesname         objectscore       float64employed       boolkids          int64dtype: object>>>df2_transposed.dtypes0    object1    objectdtype: object"
Pandas,DataFrame,pandas.DataFrame.assign,"pandas.DataFrame.assign#DataFrame.assign(**kwargs)[source]#Assign new columns to a DataFrame.Returns a new object with all original columns in addition to new ones.
Existing columns that are re-assigned will be overwritten.Parameters:**kwargsdict of {str: callable or Series}The column names are keywords. If the values are
callable, they are computed on the DataFrame and
assigned to the new columns. The callable must not
change input DataFrame (though pandas doesnft check it).
If the values are not callable, (e.g. a Series, scalar, or array),
they are simply assigned.Returns:DataFrameA new DataFrame with the new columns in addition to
all the existing columns.NotesAssigning multiple columns within the sameassignis possible.
Later items in e**kwargsf may refer to newly created or modified
columns in edff; items are computed and assigned into edff in order.Examples>>>df=pd.DataFrame({'temp_c':[17.0,25.0]},...index=['Portland','Berkeley'])>>>dftemp_cPortland    17.0Berkeley    25.0Where the value is a callable, evaluated ondf:>>>df.assign(temp_f=lambdax:x.temp_c*9/5+32)temp_c  temp_fPortland    17.0    62.6Berkeley    25.0    77.0Alternatively, the same behavior can be achieved by directly
referencing an existing Series or sequence:>>>df.assign(temp_f=df['temp_c']*9/5+32)temp_c  temp_fPortland    17.0    62.6Berkeley    25.0    77.0You can create multiple columns within the same assign where one
of the columns depends on another one defined within the same assign:>>>df.assign(temp_f=lambdax:x['temp_c']*9/5+32,...temp_k=lambdax:(x['temp_f']+459.67)*5/9)temp_c  temp_f  temp_kPortland    17.0    62.6  290.15Berkeley    25.0    77.0  298.15"
Pandas,DataFrame,pandas.DataFrame.compare,"pandas.DataFrame.compare#DataFrame.compare(other,align_axis=1,keep_shape=False,keep_equal=False,result_names=('self','other'))[source]#Compare to another DataFrame and show the differences.Parameters:otherDataFrameObject to compare with.align_axis{0 or eindexf, 1 or ecolumnsf}, default 1Determine which axis to align the comparison on.0, or eindexfResulting differences are stacked verticallywith rows drawn alternately from self and other.1, or ecolumnsfResulting differences are aligned horizontallywith columns drawn alternately from self and other.keep_shapebool, default FalseIf true, all rows and columns are kept.
Otherwise, only the ones with different values are kept.keep_equalbool, default FalseIf true, the result keeps values that are equal.
Otherwise, equal values are shown as NaNs.result_namestuple, default (eselff, eotherf)Set the dataframes names in the comparison.New in version 1.5.0.Returns:DataFrameDataFrame that shows the differences stacked side by side.The resulting index will be a MultiIndex with eselff and eotherf
stacked alternately at the inner level.Raises:ValueErrorWhen the two DataFrames donft have identical labels or shape.See alsoSeries.compareCompare with another Series and show differences.DataFrame.equalsTest whether two objects contain the same elements.NotesMatching NaNs will not appear as a difference.Can only compare identically-labeled
(i.e. same shape, identical row and column labels) DataFramesExamples>>>df=pd.DataFrame(...{...""col1"":[""a"",""a"",""b"",""b"",""a""],...""col2"":[1.0,2.0,3.0,np.nan,5.0],...""col3"":[1.0,2.0,3.0,4.0,5.0]...},...columns=[""col1"",""col2"",""col3""],...)>>>dfcol1  col2  col30    a   1.0   1.01    a   2.0   2.02    b   3.0   3.03    b   NaN   4.04    a   5.0   5.0>>>df2=df.copy()>>>df2.loc[0,'col1']='c'>>>df2.loc[2,'col3']=4.0>>>df2col1  col2  col30    c   1.0   1.01    a   2.0   2.02    b   3.0   4.03    b   NaN   4.04    a   5.0   5.0Align the differences on columns>>>df.compare(df2)col1       col3self other self other0    a     c  NaN   NaN2  NaN   NaN  3.0   4.0Assign result_names>>>df.compare(df2,result_names=(""left"",""right""))col1       col3left right left right0    a     c  NaN   NaN2  NaN   NaN  3.0   4.0Stack the differences on rows>>>df.compare(df2,align_axis=0)col1  col30 self     a   NaNother    c   NaN2 self   NaN   3.0other  NaN   4.0Keep the equal values>>>df.compare(df2,keep_equal=True)col1       col3self other self other0    a     c  1.0   1.02    b     b  3.0   4.0Keep all original rows and columns>>>df.compare(df2,keep_shape=True)col1       col2       col3self other self other self other0    a     c  NaN   NaN  NaN   NaN1  NaN   NaN  NaN   NaN  NaN   NaN2  NaN   NaN  NaN   NaN  3.0   4.03  NaN   NaN  NaN   NaN  NaN   NaN4  NaN   NaN  NaN   NaN  NaN   NaNKeep all original rows and columns and also all original values>>>df.compare(df2,keep_shape=True,keep_equal=True)col1       col2       col3self other self other self other0    a     c  1.0   1.0  1.0   1.01    a     a  2.0   2.0  2.0   2.02    b     b  3.0   3.0  3.0   4.03    b     b  NaN   NaN  4.0   4.04    a     a  5.0   5.0  5.0   5.0"
Pandas,DataFrame,pandas.DataFrame.join,"pandas.DataFrame.join#DataFrame.join(other,on=None,how='left',lsuffix='',rsuffix='',sort=False,validate=None)[source]#Join columns of another DataFrame.Join columns withotherDataFrame either on index or on a key
column. Efficiently join multiple DataFrame objects by index at once by
passing a list.Parameters:otherDataFrame, Series, or a list containing any combination of themIndex should be similar to one of the columns in this one. If a
Series is passed, its name attribute must be set, and that will be
used as the column name in the resulting joined DataFrame.onstr, list of str, or array-like, optionalColumn or index level name(s) in the caller to join on the index
inother, otherwise joins index-on-index. If multiple
values given, theotherDataFrame must have a MultiIndex. Can
pass an array as the join key if it is not already contained in
the calling DataFrame. Like an Excel VLOOKUP operation.how{eleftf, erightf, eouterf, einnerf, ecrossf}, default eleftfHow to handle the operation of the two objects.left: use calling framefs index (or column if on is specified)right: useotherfs index.outer: form union of calling framefs index (or column if on is
specified) withotherfs index, and sort it lexicographically.inner: form intersection of calling framefs index (or column if
on is specified) withotherfs index, preserving the order
of the callingfs one.cross: creates the cartesian product from both frames, preserves the order
of the left keys.New in version 1.2.0.lsuffixstr, default efSuffix to use from left framefs overlapping columns.rsuffixstr, default efSuffix to use from right framefs overlapping columns.sortbool, default FalseOrder result DataFrame lexicographically by the join key. If False,
the order of the join key depends on the join type (how keyword).validatestr, optionalIf specified, checks if join is of specified type.gone_to_oneh or g1:1h: check if join keys are unique in both left
and right datasets.gone_to_manyh or g1:mh: check if join keys are unique in left dataset.gmany_to_oneh or gm:1h: check if join keys are unique in right dataset.gmany_to_manyh or gm:mh: allowed, but does not result in checks.New in version 1.5.0.Returns:DataFrameA dataframe containing columns from both the caller andother.See alsoDataFrame.mergeFor column(s)-on-column(s) operations.NotesParameterson,lsuffix, andrsuffixare not supported when
passing a list ofDataFrameobjects.Examples>>>df=pd.DataFrame({'key':['K0','K1','K2','K3','K4','K5'],...'A':['A0','A1','A2','A3','A4','A5']})>>>dfkey   A0  K0  A01  K1  A12  K2  A23  K3  A34  K4  A45  K5  A5>>>other=pd.DataFrame({'key':['K0','K1','K2'],...'B':['B0','B1','B2']})>>>otherkey   B0  K0  B01  K1  B12  K2  B2Join DataFrames using their indexes.>>>df.join(other,lsuffix='_caller',rsuffix='_other')key_caller   A key_other    B0         K0  A0        K0   B01         K1  A1        K1   B12         K2  A2        K2   B23         K3  A3       NaN  NaN4         K4  A4       NaN  NaN5         K5  A5       NaN  NaNIf we want to join using the key columns, we need to set key to be
the index in bothdfandother. The joined DataFrame will have
key as its index.>>>df.set_index('key').join(other.set_index('key'))A    BkeyK0   A0   B0K1   A1   B1K2   A2   B2K3   A3  NaNK4   A4  NaNK5   A5  NaNAnother option to join using the key columns is to use theonparameter. DataFrame.join always usesotherfs index but we can use
any column indf. This method preserves the original DataFramefs
index in the result.>>>df.join(other.set_index('key'),on='key')key   A    B0  K0  A0   B01  K1  A1   B12  K2  A2   B23  K3  A3  NaN4  K4  A4  NaN5  K5  A5  NaNUsing non-unique key values shows how they are matched.>>>df=pd.DataFrame({'key':['K0','K1','K1','K3','K0','K1'],...'A':['A0','A1','A2','A3','A4','A5']})>>>dfkey   A0  K0  A01  K1  A12  K1  A23  K3  A34  K0  A45  K1  A5>>>df.join(other.set_index('key'),on='key',validate='m:1')key   A    B0  K0  A0   B01  K1  A1   B12  K1  A2   B13  K3  A3  NaN4  K0  A4   B05  K1  A5   B1"
Pandas,DataFrame,pandas.DataFrame.merge,"pandas.DataFrame.merge#DataFrame.merge(right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=None,indicator=False,validate=None)[source]#Merge DataFrame or named Series objects with a database-style join.A named Series object is treated as a DataFrame with a single named column.The join is done on columns or indexes. If joining columns on
columns, the DataFrame indexeswill be ignored. Otherwise if joining indexes
on indexes or indexes on a column or columns, the index will be passed on.
When performing a cross merge, no column specifications to merge on are
allowed.WarningIf both key columns contain rows where the key is a null value, those
rows will be matched against each other. This is different from usual SQL
join behaviour and can lead to unexpected results.Parameters:rightDataFrame or named SeriesObject to merge with.how{eleftf, erightf, eouterf, einnerf, ecrossf}, default einnerfType of merge to be performed.left: use only keys from left frame, similar to a SQL left outer join;
preserve key order.right: use only keys from right frame, similar to a SQL right outer join;
preserve key order.outer: use union of keys from both frames, similar to a SQL full outer
join; sort keys lexicographically.inner: use intersection of keys from both frames, similar to a SQL inner
join; preserve the order of the left keys.cross: creates the cartesian product from both frames, preserves the order
of the left keys.New in version 1.2.0.onlabel or listColumn or index level names to join on. These must be found in both
DataFrames. Ifonis None and not merging on indexes then this defaults
to the intersection of the columns in both DataFrames.left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also
be an array or list of arrays of the length of the left DataFrame.
These arrays are treated as if they are columns.right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also
be an array or list of arrays of the length of the right DataFrame.
These arrays are treated as if they are columns.left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a
MultiIndex, the number of keys in the other DataFrame (either the index
or a number of columns) must match the number of levels.right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as
left_index.sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False,
the order of the join keys depends on the join type (how keyword).suffixeslist-like, default is (g_xh, g_yh)A length-2 sequence where each element is optionally a string
indicating the suffix to add to overlapping column names inleftandrightrespectively. Pass a value ofNoneinstead
of a string to indicate that the column name fromleftorrightshould be left as-is, with no suffix. At least one of the
values must not be None.copybool, default TrueIf False, avoid copy if possible.indicatorbool or str, default FalseIf True, adds a column to the output DataFrame called g_mergeh with
information on the source of each row. The column can be given a different
name by providing a string argument. The column will have a Categorical
type with the value of gleft_onlyh for observations whose merge key only
appears in the left DataFrame, gright_onlyh for observations
whose merge key only appears in the right DataFrame, and gbothh
if the observationfs merge key is found in both DataFrames.validatestr, optionalIf specified, checks if merge is of specified type.gone_to_oneh or g1:1h: check if merge keys are unique in both
left and right datasets.gone_to_manyh or g1:mh: check if merge keys are unique in left
dataset.gmany_to_oneh or gm:1h: check if merge keys are unique in right
dataset.gmany_to_manyh or gm:mh: allowed, but does not result in checks.Returns:DataFrameA DataFrame of the two merged objects.See alsomerge_orderedMerge with optional filling/interpolation.merge_asofMerge on nearest keys.DataFrame.joinSimilar method using indices.Examples>>>df1=pd.DataFrame({'lkey':['foo','bar','baz','foo'],...'value':[1,2,3,5]})>>>df2=pd.DataFrame({'rkey':['foo','bar','baz','foo'],...'value':[5,6,7,8]})>>>df1lkey value0   foo      11   bar      22   baz      33   foo      5>>>df2rkey value0   foo      51   bar      62   baz      73   foo      8Merge df1 and df2 on the lkey and rkey columns. The value columns have
the default suffixes, _x and _y, appended.>>>df1.merge(df2,left_on='lkey',right_on='rkey')lkey  value_x rkey  value_y0  foo        1  foo        51  foo        1  foo        82  foo        5  foo        53  foo        5  foo        84  bar        2  bar        65  baz        3  baz        7Merge DataFrames df1 and df2 with specified left and right suffixes
appended to any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',...suffixes=('_left','_right'))lkey  value_left rkey  value_right0  foo           1  foo            51  foo           1  foo            82  foo           5  foo            53  foo           5  foo            84  bar           2  bar            65  baz           3  baz            7Merge DataFrames df1 and df2, but raise an exception if the DataFrames have
any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',suffixes=(False,False))Traceback (most recent call last):...ValueError:columns overlap but no suffix specified:Index(['value'], dtype='object')>>>df1=pd.DataFrame({'a':['foo','bar'],'b':[1,2]})>>>df2=pd.DataFrame({'a':['foo','baz'],'c':[3,4]})>>>df1a  b0   foo  11   bar  2>>>df2a  c0   foo  31   baz  4>>>df1.merge(df2,how='inner',on='a')a  b  c0   foo  1  3>>>df1.merge(df2,how='left',on='a')a  b  c0   foo  1  3.01   bar  2  NaN>>>df1=pd.DataFrame({'left':['foo','bar']})>>>df2=pd.DataFrame({'right':[7,8]})>>>df1left0   foo1   bar>>>df2right0   71   8>>>df1.merge(df2,how='cross')left  right0   foo      71   foo      82   bar      73   bar      8"
Pandas,DataFrame,pandas.DataFrame.update,"pandas.DataFrame.update#DataFrame.update(other,join='left',overwrite=True,filter_func=None,errors='ignore')[source]#Modify in place using non-NA values from another DataFrame.Aligns on indices. There is no return value.Parameters:otherDataFrame, or object coercible into a DataFrameShould have at least one matching index/column label
with the original DataFrame. If a Series is passed,
its name attribute must be set, and that will be
used as the column name to align with the original DataFrame.join{eleftf}, default eleftfOnly left join is implemented, keeping the index and columns of the
original object.overwritebool, default TrueHow to handle non-NA values for overlapping keys:True: overwrite original DataFramefs values
with values fromother.False: only update values that are NA in
the original DataFrame.filter_funccallable(1d-array) -> bool 1d-array, optionalCan choose to replace values other than NA. Return True for values
that should be updated.errors{eraisef, eignoref}, default eignorefIf eraisef, will raise a ValueError if the DataFrame andotherboth contain non-NA data in the same place.Returns:NoneThis method directly changes calling object.Raises:ValueErrorWhenerrors=fraisefand therefs overlapping non-NA data.Whenerrorsis not eithereignoreforeraisefNotImplementedErrorIfjoin != eleftfSee alsodict.updateSimilar method for dictionaries.DataFrame.mergeFor column(s)-on-column(s) operations.Examples>>>df=pd.DataFrame({'A':[1,2,3],...'B':[400,500,600]})>>>new_df=pd.DataFrame({'B':[4,5,6],...'C':[7,8,9]})>>>df.update(new_df)>>>dfA  B0  1  41  2  52  3  6The DataFramefs length does not increase as a result of the update,
only values at matching index/column labels are updated.>>>df=pd.DataFrame({'A':['a','b','c'],...'B':['x','y','z']})>>>new_df=pd.DataFrame({'B':['d','e','f','g','h','i']})>>>df.update(new_df)>>>dfA  B0  a  d1  b  e2  c  fFor Series, its name attribute must be set.>>>df=pd.DataFrame({'A':['a','b','c'],...'B':['x','y','z']})>>>new_column=pd.Series(['d','e'],name='B',index=[0,2])>>>df.update(new_column)>>>dfA  B0  a  d1  b  y2  c  e>>>df=pd.DataFrame({'A':['a','b','c'],...'B':['x','y','z']})>>>new_df=pd.DataFrame({'B':['d','e']},index=[1,2])>>>df.update(new_df)>>>dfA  B0  a  x1  b  d2  c  eIfothercontains NaNs the corresponding values are not updated
in the original dataframe.>>>df=pd.DataFrame({'A':[1,2,3],...'B':[400,500,600]})>>>new_df=pd.DataFrame({'B':[4,np.nan,6]})>>>df.update(new_df)>>>dfA    B0  1    41  2  5002  3    6"
Pandas,DataFrame,pandas.DataFrame.asfreq,"pandas.DataFrame.asfreq#DataFrame.asfreq(freq,method=None,how=None,normalize=False,fill_value=None)[source]#Convert time series to specified frequency.Returns the original data conformed to a new index with the specified
frequency.If the index of this Series/DataFrame is aPeriodIndex, the new index
is the result of transforming the original index withPeriodIndex.asfreq(so the original index
will map one-to-one to the new index).Otherwise, the new index will be equivalent topd.date_range(start,end,freq=freq)wherestartandendare, respectively, the first and
last entries in the original index (seepandas.date_range()). The
values corresponding to any timesteps in the new index which were not present
in the original index will be null (NaN), unless a method for filling
such unknowns is provided (see themethodparameter below).Theresample()method is more appropriate if an operation on each group of
timesteps (such as an aggregate) is necessary to represent the data at the new
frequency.Parameters:freqDateOffset or strFrequency DateOffset or string.method{ebackfillf/fbfillf, epadf/fffillf}, default NoneMethod to use for filling holes in reindexed Series (note this
does not fill NaNs that already were present):epadf / effillf: propagate last valid observation forward to next
validebackfillf / ebfillf: use NEXT valid observation to fill.how{estartf, eendf}, default endFor PeriodIndex only (see PeriodIndex.asfreq).normalizebool, default FalseWhether to reset output index to midnight.fill_valuescalar, optionalValue to use for missing values, applied during upsampling (note
this does not fill NaNs that already were present).Returns:Series/DataFrameSeries/DataFrame object reindexed to the specified frequency.See alsoreindexConform DataFrame to new index with optional filling logic.NotesTo learn more about the frequency strings, please seethis link.ExamplesStart by creating a series with 4 one minute timestamps.>>>index=pd.date_range('1/1/2000',periods=4,freq='T')>>>series=pd.Series([0.0,None,2.0,3.0],index=index)>>>df=pd.DataFrame({'s':series})>>>dfs2000-01-01 00:00:00    0.02000-01-01 00:01:00    NaN2000-01-01 00:02:00    2.02000-01-01 00:03:00    3.0Upsample the series into 30 second bins.>>>df.asfreq(freq='30S')s2000-01-01 00:00:00    0.02000-01-01 00:00:30    NaN2000-01-01 00:01:00    NaN2000-01-01 00:01:30    NaN2000-01-01 00:02:00    2.02000-01-01 00:02:30    NaN2000-01-01 00:03:00    3.0Upsample again, providing afillvalue.>>>df.asfreq(freq='30S',fill_value=9.0)s2000-01-01 00:00:00    0.02000-01-01 00:00:30    9.02000-01-01 00:01:00    NaN2000-01-01 00:01:30    9.02000-01-01 00:02:00    2.02000-01-01 00:02:30    9.02000-01-01 00:03:00    3.0Upsample again, providing amethod.>>>df.asfreq(freq='30S',method='bfill')s2000-01-01 00:00:00    0.02000-01-01 00:00:30    NaN2000-01-01 00:01:00    NaN2000-01-01 00:01:30    2.02000-01-01 00:02:00    2.02000-01-01 00:02:30    3.02000-01-01 00:03:00    3.0"
Pandas,DataFrame,pandas.DataFrame.asof,"pandas.DataFrame.asof#DataFrame.asof(where,subset=None)[source]#Return the last row(s) without any NaNs beforewhere.The last row (for each element inwhere, if list) without any
NaN is taken.
In case of aDataFrame, the last row without NaN
considering only the subset of columns (if notNone)If there is no good value, NaN is returned for a Series or
a Series of NaN values for a DataFrameParameters:wheredate or array-like of datesDate(s) before which the last row(s) are returned.subsetstr or array-like of str, defaultNoneFor DataFrame, if notNone, only use these columns to
check for NaNs.Returns:scalar, Series, or DataFrameThe return can be:scalar : whenselfis a Series andwhereis a scalarSeries: whenselfis a Series andwhereis an array-like,
or whenselfis a DataFrame andwhereis a scalarDataFrame : whenselfis a DataFrame andwhereis an
array-likeReturn scalar, Series, or DataFrame.See alsomerge_asofPerform an asof merge. Similar to left join.NotesDates are assumed to be sorted. Raises if this is not the case.ExamplesA Series and a scalarwhere.>>>s=pd.Series([1,2,np.nan,4],index=[10,20,30,40])>>>s10    1.020    2.030    NaN40    4.0dtype: float64>>>s.asof(20)2.0For a sequencewhere, a Series is returned. The first value is
NaN, because the first element ofwhereis before the first
index value.>>>s.asof([5,20])5     NaN20    2.0dtype: float64Missing values are not considered. The following is2.0, not
NaN, even though NaN is at the index location for30.>>>s.asof(30)2.0Take all columns into consideration>>>df=pd.DataFrame({'a':[10.,20.,30.,40.,50.],...'b':[None,None,None,None,500]},...index=pd.DatetimeIndex(['2018-02-27 09:01:00',...'2018-02-27 09:02:00',...'2018-02-27 09:03:00',...'2018-02-27 09:04:00',...'2018-02-27 09:05:00']))>>>df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',...'2018-02-27 09:04:30']))a   b2018-02-27 09:03:30 NaN NaN2018-02-27 09:04:30 NaN NaNTake a single column into consideration>>>df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',...'2018-02-27 09:04:30']),...subset=['a'])a   b2018-02-27 09:03:30  30.0 NaN2018-02-27 09:04:30  40.0 NaN"
Pandas,DataFrame,pandas.DataFrame.shift,"pandas.DataFrame.shift#DataFrame.shift(periods=1,freq=None,axis=0,fill_value=_NoDefault.no_default,suffix=None)[source]#Shift index by desired number of periods with an optional timefreq.Whenfreqis not passed, shift the index without realigning the data.
Iffreqis passed (in this case, the index must be date or datetime,
or it will raise aNotImplementedError), the index will be
increased using the periods and thefreq.freqcan be inferred
when specified as ginferh as long as either freq or inferred_freq
attribute is set in the index.Parameters:periodsint or SequenceNumber of periods to shift. Can be positive or negative.
If an iterable of ints, the data will be shifted once by each int.
This is equivalent to shifting by one value at a time and
concatenating all resulting frames. The resulting columns will have
the shift suffixed to their column names. For multiple periods,
axis must not be 1.freqDateOffset, tseries.offsets, timedelta, or str, optionalOffset to use from the tseries module or time rule (e.g. eEOMf).
Iffreqis specified then the index values are shifted but the
data is not realigned. That is, usefreqif you would like to
extend the index when shifting and preserve the original data.
Iffreqis specified as ginferh then it will be inferred from
the freq or inferred_freq attributes of the index. If neither of
those attributes exist, a ValueError is thrown.axis{0 or eindexf, 1 or ecolumnsf, None}, default NoneShift direction. ForSeriesthis parameter is unused and defaults to 0.fill_valueobject, optionalThe scalar value to use for newly introduced missing values.
the default depends on the dtype ofself.
For numeric data,np.nanis used.
For datetime, timedelta, or period data, etc.NaTis used.
For extension dtypes,self.dtype.na_valueis used.suffixstr, optionalIf str and periods is an iterable, this is added after the column
name and before the shift value for each shifted column name.Returns:DataFrameCopy of input object, shifted.See alsoIndex.shiftShift values of Index.DatetimeIndex.shiftShift values of DatetimeIndex.PeriodIndex.shiftShift values of PeriodIndex.Examples>>>df=pd.DataFrame({""Col1"":[10,20,15,30,45],...""Col2"":[13,23,18,33,48],...""Col3"":[17,27,22,37,52]},...index=pd.date_range(""2020-01-01"",""2020-01-05""))>>>dfCol1  Col2  Col32020-01-01    10    13    172020-01-02    20    23    272020-01-03    15    18    222020-01-04    30    33    372020-01-05    45    48    52>>>df.shift(periods=3)Col1  Col2  Col32020-01-01   NaN   NaN   NaN2020-01-02   NaN   NaN   NaN2020-01-03   NaN   NaN   NaN2020-01-04  10.0  13.0  17.02020-01-05  20.0  23.0  27.0>>>df.shift(periods=1,axis=""columns"")Col1  Col2  Col32020-01-01   NaN    10    132020-01-02   NaN    20    232020-01-03   NaN    15    182020-01-04   NaN    30    332020-01-05   NaN    45    48>>>df.shift(periods=3,fill_value=0)Col1  Col2  Col32020-01-01     0     0     02020-01-02     0     0     02020-01-03     0     0     02020-01-04    10    13    172020-01-05    20    23    27>>>df.shift(periods=3,freq=""D"")Col1  Col2  Col32020-01-04    10    13    172020-01-05    20    23    272020-01-06    15    18    222020-01-07    30    33    372020-01-08    45    48    52>>>df.shift(periods=3,freq=""infer"")Col1  Col2  Col32020-01-04    10    13    172020-01-05    20    23    272020-01-06    15    18    222020-01-07    30    33    372020-01-08    45    48    52>>>df['Col1'].shift(periods=[0,1,2])Col1_0  Col1_1  Col1_22020-01-01      10     NaN     NaN2020-01-02      20    10.0     NaN2020-01-03      15    20.0    10.02020-01-04      30    15.0    20.02020-01-05      45    30.0    15.0"
Pandas,DataFrame,pandas.DataFrame.first_valid_index,"pandas.DataFrame.first_valid_index#DataFrame.first_valid_index()[source]#Return index for first non-NA value or None, if no non-NA value is found.Returns:type of indexNotesIf all elements are non-NA/null, returns None.
Also returns None for empty Series/DataFrame.ExamplesFor Series:>>>s=pd.Series([None,3,4])>>>s.first_valid_index()1>>>s.last_valid_index()2For DataFrame:>>>df=pd.DataFrame({'A':[None,None,2],'B':[None,3,4]})>>>dfA      B0  NaN    NaN1  NaN    3.02  2.0    4.0>>>df.first_valid_index()1>>>df.last_valid_index()2"
Pandas,DataFrame,pandas.DataFrame.last_valid_index,"pandas.DataFrame.last_valid_index#DataFrame.last_valid_index()[source]#Return index for last non-NA value or None, if no non-NA value is found.Returns:type of indexNotesIf all elements are non-NA/null, returns None.
Also returns None for empty Series/DataFrame.ExamplesFor Series:>>>s=pd.Series([None,3,4])>>>s.first_valid_index()1>>>s.last_valid_index()2For DataFrame:>>>df=pd.DataFrame({'A':[None,None,2],'B':[None,3,4]})>>>dfA      B0  NaN    NaN1  NaN    3.02  2.0    4.0>>>df.first_valid_index()1>>>df.last_valid_index()2"
Pandas,DataFrame,pandas.DataFrame.resample,"pandas.DataFrame.resample#DataFrame.resample(rule,axis=_NoDefault.no_default,closed=None,label=None,convention='start',kind=None,on=None,level=None,origin='start_day',offset=None,group_keys=False)[source]#Resample time-series data.Convenience method for frequency conversion and resampling of time series.
The object must have a datetime-like index (DatetimeIndex,PeriodIndex,
orTimedeltaIndex), or the caller must pass the label of a datetime-like
series/index to theon/levelkeyword parameter.Parameters:ruleDateOffset, Timedelta or strThe offset string or object representing target conversion.axis{0 or eindexf, 1 or ecolumnsf}, default 0Which axis to use for up- or down-sampling. ForSeriesthis parameter
is unused and defaults to 0. Must beDatetimeIndex,TimedeltaIndexorPeriodIndex.Deprecated since version 2.0.0:Use frame.T.resample(c) instead.closed{erightf, eleftf}, default NoneWhich side of bin interval is closed. The default is eleftf
for all frequency offsets except for eMf, eAf, eQf, eBMf,
eBAf, eBQf, and eWf which all have a default of erightf.label{erightf, eleftf}, default NoneWhich bin edge label to label bucket with. The default is eleftf
for all frequency offsets except for eMf, eAf, eQf, eBMf,
eBAf, eBQf, and eWf which all have a default of erightf.convention{estartf, eendf, esf, eef}, default estartfForPeriodIndexonly, controls whether to use the start or
end ofrule.kind{etimestampf, eperiodf}, optional, default NonePass etimestampf to convert the resulting index to aDateTimeIndexor eperiodf to convert it to aPeriodIndex.
By default the input representation is retained.onstr, optionalFor a DataFrame, column to use instead of index for resampling.
Column must be datetime-like.levelstr or int, optionalFor a MultiIndex, level (name or number) to use for
resampling.levelmust be datetime-like.originTimestamp or str, default estart_dayfThe timestamp on which to adjust the grouping. The timezone of origin
must match the timezone of the index.
If string, must be one of the following:eepochf:originis 1970-01-01estartf:originis the first value of the timeseriesestart_dayf:originis the first day at midnight of the timeserieseendf:originis the last value of the timeserieseend_dayf:originis the ceiling midnight of the last dayNew in version 1.3.0.NoteOnly takes effect for Tick-frequencies (i.e. fixed frequencies like
days, hours, and minutes, rather than months or quarters).offsetTimedelta or str, default is NoneAn offset timedelta added to the origin.group_keysbool, default FalseWhether to include the group keys in the result index when using.apply()on the resampled object.New in version 1.5.0:Not specifyinggroup_keyswill retain values-dependent behavior
from pandas 1.4 and earlier (seepandas 1.5.0 Release notesfor examples).Changed in version 2.0.0:group_keysnow defaults toFalse.Returns:pandas.api.typing.ResamplerResamplerobject.See alsoSeries.resampleResample a Series.DataFrame.resampleResample a DataFrame.groupbyGroup Series/DataFrame by mapping, function, label, or list of labels.asfreqReindex a Series/DataFrame with the given frequency without grouping.NotesSee theuser guidefor more.To learn more about the offset strings, please seethis link.ExamplesStart by creating a series with 9 one minute timestamps.>>>index=pd.date_range('1/1/2000',periods=9,freq='T')>>>series=pd.Series(range(9),index=index)>>>series2000-01-01 00:00:00    02000-01-01 00:01:00    12000-01-01 00:02:00    22000-01-01 00:03:00    32000-01-01 00:04:00    42000-01-01 00:05:00    52000-01-01 00:06:00    62000-01-01 00:07:00    72000-01-01 00:08:00    8Freq: T, dtype: int64Downsample the series into 3 minute bins and sum the values
of the timestamps falling into a bin.>>>series.resample('3T').sum()2000-01-01 00:00:00     32000-01-01 00:03:00    122000-01-01 00:06:00    21Freq: 3T, dtype: int64Downsample the series into 3 minute bins as above, but label each
bin using the right edge instead of the left. Please note that the
value in the bucket used as the label is not included in the bucket,
which it labels. For example, in the original series the
bucket2000-01-0100:03:00contains the value 3, but the summed
value in the resampled bucket with the label2000-01-0100:03:00does not include 3 (if it did, the summed value would be 6, not 3).
To include this value close the right side of the bin interval as
illustrated in the example below this one.>>>series.resample('3T',label='right').sum()2000-01-01 00:03:00     32000-01-01 00:06:00    122000-01-01 00:09:00    21Freq: 3T, dtype: int64Downsample the series into 3 minute bins as above, but close the right
side of the bin interval.>>>series.resample('3T',label='right',closed='right').sum()2000-01-01 00:00:00     02000-01-01 00:03:00     62000-01-01 00:06:00    152000-01-01 00:09:00    15Freq: 3T, dtype: int64Upsample the series into 30 second bins.>>>series.resample('30S').asfreq()[0:5]# Select first 5 rows2000-01-01 00:00:00   0.02000-01-01 00:00:30   NaN2000-01-01 00:01:00   1.02000-01-01 00:01:30   NaN2000-01-01 00:02:00   2.0Freq: 30S, dtype: float64Upsample the series into 30 second bins and fill theNaNvalues using theffillmethod.>>>series.resample('30S').ffill()[0:5]2000-01-01 00:00:00    02000-01-01 00:00:30    02000-01-01 00:01:00    12000-01-01 00:01:30    12000-01-01 00:02:00    2Freq: 30S, dtype: int64Upsample the series into 30 second bins and fill theNaNvalues using thebfillmethod.>>>series.resample('30S').bfill()[0:5]2000-01-01 00:00:00    02000-01-01 00:00:30    12000-01-01 00:01:00    12000-01-01 00:01:30    22000-01-01 00:02:00    2Freq: 30S, dtype: int64Pass a custom function viaapply>>>defcustom_resampler(arraylike):...returnnp.sum(arraylike)+5...>>>series.resample('3T').apply(custom_resampler)2000-01-01 00:00:00     82000-01-01 00:03:00    172000-01-01 00:06:00    26Freq: 3T, dtype: int64For a Series with a PeriodIndex, the keywordconventioncan be
used to control whether to use the start or end ofrule.Resample a year by quarter using estartfconvention. Values are
assigned to the first quarter of the period.>>>s=pd.Series([1,2],index=pd.period_range('2012-01-01',...freq='A',...periods=2))>>>s2012    12013    2Freq: A-DEC, dtype: int64>>>s.resample('Q',convention='start').asfreq()2012Q1    1.02012Q2    NaN2012Q3    NaN2012Q4    NaN2013Q1    2.02013Q2    NaN2013Q3    NaN2013Q4    NaNFreq: Q-DEC, dtype: float64Resample quarters by month using eendfconvention. Values are
assigned to the last month of the period.>>>q=pd.Series([1,2,3,4],index=pd.period_range('2018-01-01',...freq='Q',...periods=4))>>>q2018Q1    12018Q2    22018Q3    32018Q4    4Freq: Q-DEC, dtype: int64>>>q.resample('M',convention='end').asfreq()2018-03    1.02018-04    NaN2018-05    NaN2018-06    2.02018-07    NaN2018-08    NaN2018-09    3.02018-10    NaN2018-11    NaN2018-12    4.0Freq: M, dtype: float64For DataFrame objects, the keywordoncan be used to specify the
column instead of the index for resampling.>>>d={'price':[10,11,9,13,14,18,17,19],...'volume':[50,60,40,100,50,100,40,50]}>>>df=pd.DataFrame(d)>>>df['week_starting']=pd.date_range('01/01/2018',...periods=8,...freq='W')>>>dfprice  volume week_starting0     10      50    2018-01-071     11      60    2018-01-142      9      40    2018-01-213     13     100    2018-01-284     14      50    2018-02-045     18     100    2018-02-116     17      40    2018-02-187     19      50    2018-02-25>>>df.resample('M',on='week_starting').mean()price  volumeweek_starting2018-01-31     10.75    62.52018-02-28     17.00    60.0For a DataFrame with MultiIndex, the keywordlevelcan be used to
specify on which level the resampling needs to take place.>>>days=pd.date_range('1/1/2000',periods=4,freq='D')>>>d2={'price':[10,11,9,13,14,18,17,19],...'volume':[50,60,40,100,50,100,40,50]}>>>df2=pd.DataFrame(...d2,...index=pd.MultiIndex.from_product(...[days,['morning','afternoon']]...)...)>>>df2price  volume2000-01-01 morning       10      50afternoon     11      602000-01-02 morning        9      40afternoon     13     1002000-01-03 morning       14      50afternoon     18     1002000-01-04 morning       17      40afternoon     19      50>>>df2.resample('D',level=0).sum()price  volume2000-01-01     21     1102000-01-02     22     1402000-01-03     32     1502000-01-04     36      90If you want to adjust the start of the bins based on a fixed timestamp:>>>start,end='2000-10-01 23:30:00','2000-10-02 00:30:00'>>>rng=pd.date_range(start,end,freq='7min')>>>ts=pd.Series(np.arange(len(rng))*3,index=rng)>>>ts2000-10-01 23:30:00     02000-10-01 23:37:00     32000-10-01 23:44:00     62000-10-01 23:51:00     92000-10-01 23:58:00    122000-10-02 00:05:00    152000-10-02 00:12:00    182000-10-02 00:19:00    212000-10-02 00:26:00    24Freq: 7T, dtype: int64>>>ts.resample('17min').sum()2000-10-01 23:14:00     02000-10-01 23:31:00     92000-10-01 23:48:00    212000-10-02 00:05:00    542000-10-02 00:22:00    24Freq: 17T, dtype: int64>>>ts.resample('17min',origin='epoch').sum()2000-10-01 23:18:00     02000-10-01 23:35:00    182000-10-01 23:52:00    272000-10-02 00:09:00    392000-10-02 00:26:00    24Freq: 17T, dtype: int64>>>ts.resample('17min',origin='2000-01-01').sum()2000-10-01 23:24:00     32000-10-01 23:41:00    152000-10-01 23:58:00    452000-10-02 00:15:00    45Freq: 17T, dtype: int64If you want to adjust the start of the bins with anoffsetTimedelta, the two
following lines are equivalent:>>>ts.resample('17min',origin='start').sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17T, dtype: int64>>>ts.resample('17min',offset='23h30min').sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17T, dtype: int64If you want to take the largest Timestamp as the end of the bins:>>>ts.resample('17min',origin='end').sum()2000-10-01 23:35:00     02000-10-01 23:52:00    182000-10-02 00:09:00    272000-10-02 00:26:00    63Freq: 17T, dtype: int64In contrast with thestart_day, you can useend_dayto take the ceiling
midnight of the largest Timestamp as the end of the bins and drop the bins
not containing data:>>>ts.resample('17min',origin='end_day').sum()2000-10-01 23:38:00     32000-10-01 23:55:00    152000-10-02 00:12:00    452000-10-02 00:29:00    45Freq: 17T, dtype: int64"
Pandas,DataFrame,pandas.DataFrame.to_period,"pandas.DataFrame.to_period#DataFrame.to_period(freq=None,axis=0,copy=None)[source]#Convert DataFrame from DatetimeIndex to PeriodIndex.Convert DataFrame from DatetimeIndex to PeriodIndex with desired
frequency (inferred from index if not passed).Parameters:freqstr, defaultFrequency of the PeriodIndex.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to convert (the index by default).copybool, default TrueIf False then underlying input data is not copied.Returns:DataFrameThe DataFrame has a PeriodIndex.Examples>>>idx=pd.to_datetime(...[...""2001-03-31 00:00:00"",...""2002-05-31 00:00:00"",...""2003-08-31 00:00:00"",...]...)>>>idxDatetimeIndex(['2001-03-31', '2002-05-31', '2003-08-31'],dtype='datetime64[ns]', freq=None)>>>idx.to_period(""M"")PeriodIndex(['2001-03', '2002-05', '2003-08'], dtype='period[M]')For the yearly frequency>>>idx.to_period(""Y"")PeriodIndex(['2001', '2002', '2003'], dtype='period[A-DEC]')"
Pandas,DataFrame,pandas.DataFrame.to_timestamp,"pandas.DataFrame.to_timestamp#DataFrame.to_timestamp(freq=None,how='start',axis=0,copy=None)[source]#Cast to DatetimeIndex of timestamps, atbeginningof period.Parameters:freqstr, default frequency of PeriodIndexDesired frequency.how{esf, eef, estartf, eendf}Convention for converting period to timestamp; start of period
vs. end.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to convert (the index by default).copybool, default TrueIf False then underlying input data is not copied.Returns:DataFrameThe DataFrame has a DatetimeIndex.Examples>>>idx=pd.PeriodIndex(['2023','2024'],freq='Y')>>>d={'col1':[1,2],'col2':[3,4]}>>>df1=pd.DataFrame(data=d,index=idx)>>>df1col1   col22023     1      32024     2      4The resulting timestamps will be at the beginning of the year in this case>>>df1=df1.to_timestamp()>>>df1col1   col22023-01-01     1      32024-01-01     2      4>>>df1.indexDatetimeIndex(['2023-01-01', '2024-01-01'], dtype='datetime64[ns]', freq=None)Usingfreqwhich is the offset that the Timestamps will have>>>df2=pd.DataFrame(data=d,index=idx)>>>df2=df2.to_timestamp(freq='M')>>>df2col1   col22023-01-31     1      32024-01-31     2      4>>>df2.indexDatetimeIndex(['2023-01-31', '2024-01-31'], dtype='datetime64[ns]', freq=None)"
Pandas,DataFrame,pandas.DataFrame.tz_convert,"pandas.DataFrame.tz_convert#DataFrame.tz_convert(tz,axis=0,level=None,copy=None)[source]#Convert tz-aware axis to target time zone.Parameters:tzstr or tzinfo object or NoneTarget time zone. PassingNonewill convert to
UTC and remove the timezone information.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to convertlevelint, str, default NoneIf axis is a MultiIndex, convert a specific level. Otherwise
must be None.copybool, default TrueAlso make a copy of the underlying data.Returns:Series/DataFrameObject with time zone converted axis.Raises:TypeErrorIf the axis is tz-naive.ExamplesChange to another time zone:>>>s=pd.Series(...[1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']),...)>>>s.tz_convert('Asia/Shanghai')2018-09-15 07:30:00+08:00    1dtype: int64Pass None to convert to UTC and get a tz-naive index:>>>s=pd.Series([1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))>>>s.tz_convert(None)2018-09-14 23:30:00    1dtype: int64"
Pandas,DataFrame,pandas.DataFrame.tz_localize,"pandas.DataFrame.tz_localize#DataFrame.tz_localize(tz,axis=0,level=None,copy=None,ambiguous='raise',nonexistent='raise')[source]#Localize tz-naive index of a Series or DataFrame to target time zone.This operation localizes the Index. To localize the values in a
timezone-naive Series, useSeries.dt.tz_localize().Parameters:tzstr or tzinfo or NoneTime zone to localize. PassingNonewill remove the
time zone information and preserve local time.axis{0 or eindexf, 1 or ecolumnsf}, default 0The axis to localizelevelint, str, default NoneIf axis ia a MultiIndex, localize a specific level. Otherwise
must be None.copybool, default TrueAlso make a copy of the underlying data.ambiguouseinferf, bool-ndarray, eNaTf, default eraisefWhen clocks moved backward due to DST, ambiguous times may arise.
For example in Central European Time (UTC+01), when going from
03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at
00:30:00 UTC and at 01:30:00 UTC. In such a situation, theambiguousparameter dictates how ambiguous times should be
handled.einferf will attempt to infer fall dst-transition hours based on
orderbool-ndarray where True signifies a DST time, False designates
a non-DST time (note that this flag is only applicable for
ambiguous times)eNaTf will return NaT where there are ambiguous timeseraisef will raise an AmbiguousTimeError if there are ambiguous
times.nonexistentstr, default eraisefA nonexistent time does not exist in a particular timezone
where clocks moved forward due to DST. Valid values are:eshift_forwardf will shift the nonexistent time forward to the
closest existing timeeshift_backwardf will shift the nonexistent time backward to the
closest existing timeeNaTf will return NaT where there are nonexistent timestimedelta objects will shift nonexistent times by the timedeltaeraisef will raise an NonExistentTimeError if there are
nonexistent times.Returns:Series/DataFrameSame type as the input.Raises:TypeErrorIf the TimeSeries is tz-aware and tz is not None.ExamplesLocalize local times:>>>s=pd.Series(...[1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00']),...)>>>s.tz_localize('CET')2018-09-15 01:30:00+02:00    1dtype: int64Pass None to convert to tz-naive index and preserve local time:>>>s=pd.Series([1],...index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))>>>s.tz_localize(None)2018-09-15 01:30:00    1dtype: int64Be careful with DST changes. When there is sequential data, pandas
can infer the DST time:>>>s=pd.Series(range(7),...index=pd.DatetimeIndex(['2018-10-28 01:30:00',...'2018-10-28 02:00:00',...'2018-10-28 02:30:00',...'2018-10-28 02:00:00',...'2018-10-28 02:30:00',...'2018-10-28 03:00:00',...'2018-10-28 03:30:00']))>>>s.tz_localize('CET',ambiguous='infer')2018-10-28 01:30:00+02:00    02018-10-28 02:00:00+02:00    12018-10-28 02:30:00+02:00    22018-10-28 02:00:00+01:00    32018-10-28 02:30:00+01:00    42018-10-28 03:00:00+01:00    52018-10-28 03:30:00+01:00    6dtype: int64In some cases, inferring the DST is impossible. In such cases, you can
pass an ndarray to the ambiguous parameter to set the DST explicitly>>>s=pd.Series(range(3),...index=pd.DatetimeIndex(['2018-10-28 01:20:00',...'2018-10-28 02:36:00',...'2018-10-28 03:46:00']))>>>s.tz_localize('CET',ambiguous=np.array([True,True,False]))2018-10-28 01:20:00+02:00    02018-10-28 02:36:00+02:00    12018-10-28 03:46:00+01:00    2dtype: int64If the DST transition causes nonexistent times, you can shift these
dates forward or backward with a timedelta object oreshift_forwardforeshift_backwardf.>>>s=pd.Series(range(2),...index=pd.DatetimeIndex(['2015-03-29 02:30:00',...'2015-03-29 03:30:00']))>>>s.tz_localize('Europe/Warsaw',nonexistent='shift_forward')2015-03-29 03:00:00+02:00    02015-03-29 03:30:00+02:00    1dtype: int64>>>s.tz_localize('Europe/Warsaw',nonexistent='shift_backward')2015-03-29 01:59:59.999999999+01:00    02015-03-29 03:30:00+02:00              1dtype: int64>>>s.tz_localize('Europe/Warsaw',nonexistent=pd.Timedelta('1H'))2015-03-29 03:30:00+02:00    02015-03-29 03:30:00+02:00    1dtype: int64"
Pandas,DataFrame,pandas.Flags,"pandas.Flags#classpandas.Flags(obj,*,allows_duplicate_labels)[source]#Flags that apply to pandas objects.New in version 1.2.0.Parameters:objSeries or DataFrameThe object these flags are associated with.allows_duplicate_labelsbool, default TrueWhether to allow duplicate labels in this object. By default,
duplicate labels are permitted. Setting this toFalsewill
cause anerrors.DuplicateLabelErrorto be raised whenindex(or columns for DataFrame) is not unique, or any
subsequent operation on introduces duplicates.
SeeDisallowing Duplicate Labelsfor more.WarningThis is an experimental feature. Currently, many methods fail to
propagate theallows_duplicate_labelsvalue. In future versions
it is expected that every method taking or returning one or more
DataFrame or Series objects will propagateallows_duplicate_labels.ExamplesAttributes can be set in two ways:>>>df=pd.DataFrame()>>>df.flags<Flags(allows_duplicate_labels=True)>>>>df.flags.allows_duplicate_labels=False>>>df.flags<Flags(allows_duplicate_labels=False)>>>>df.flags['allows_duplicate_labels']=True>>>df.flags<Flags(allows_duplicate_labels=True)>Attributesallows_duplicate_labelsWhether this object allows duplicate labels."
Pandas,DataFrame,pandas.DataFrame.attrs,"pandas.DataFrame.attrs#propertyDataFrame.attrs[source]#Dictionary of global attributes of this dataset.Warningattrs is experimental and may change without warning.See alsoDataFrame.flagsGlobal flags applying to this object.ExamplesFor Series:>>>ser=pd.Series([1,2,3])>>>ser.attrs={""A"":[10,20,30]}>>>ser.attrs{'A': [10, 20, 30]}For DataFrame:>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>df.attrs={""A"":[10,20,30]}>>>df.attrs{'A': [10, 20, 30]}"
Pandas,DataFrame,pandas.DataFrame.plot,"pandas.DataFrame.plot#DataFrame.plot(*args,**kwargs)[source]#Make plots of Series or DataFrame.Uses the backend specified by the
optionplotting.backend. By default, matplotlib is used.Parameters:dataSeries or DataFrameThe object for which the method is called.xlabel or position, default NoneOnly used if data is a DataFrame.ylabel, position or list of label, positions, default NoneAllows plotting of one column versus another. Only used if data is a
DataFrame.kindstrThe kind of plot to produce:elinef : line plot (default)ebarf : vertical bar plotebarhf : horizontal bar plotehistf : histogrameboxf : boxplotekdef : Kernel Density Estimation plotedensityf : same as ekdefeareaf : area plotepief : pie plotescatterf : scatter plot (DataFrame only)ehexbinf : hexbin plot (DataFrame only)axmatplotlib axes object, default NoneAn axes of the current figure.subplotsbool or sequence of iterables, default FalseWhether to group columns into subplots:False: No subplots will be usedTrue: Make separate subplots for each column.sequence of iterables of column labels: Create a subplot for each
group of columns. For example[(eaf, ecf), (ebf, edf)]will
create 2 subplots: one with columns eaf and ecf, and one
with columns ebf and edf. Remaining columns that arenft specified
will be plotted in additional subplots (one per column).New in version 1.5.0.sharexbool, default True if ax is None else FalseIn casesubplots=True, share x axis and set some x axis labels
to invisible; defaults to True if ax is None otherwise False if
an ax is passed in; Be aware, that passing in both an ax andsharex=Truewill alter all x axis labels for all axis in a figure.shareybool, default FalseIn casesubplots=True, share y axis and set some y axis labels to invisible.layouttuple, optional(rows, columns) for the layout of subplots.figsizea tuple (width, height) in inchesSize of a figure object.use_indexbool, default TrueUse index as ticks for x axis.titlestr or listTitle to use for the plot. If a string is passed, print the string
at the top of the figure. If a list is passed andsubplotsis
True, print each item in the list above the corresponding subplot.gridbool, default None (matlab style default)Axis grid lines.legendbool or {ereversef}Place legend on axis subplots.stylelist or dictThe matplotlib line style per column.logxbool or esymf, default FalseUse log scaling or symlog scaling on x axis.logybool or esymf default FalseUse log scaling or symlog scaling on y axis.loglogbool or esymf, default FalseUse log scaling or symlog scaling on both x and y axes.xtickssequenceValues to use for the xticks.ytickssequenceValues to use for the yticks.xlim2-tuple/listSet the x limits of the current axes.ylim2-tuple/listSet the y limits of the current axes.xlabellabel, optionalName to use for the xlabel on x-axis. Default uses index name as xlabel, or the
x-column name for planar plots.Changed in version 1.2.0:Now applicable to planar plots (scatter,hexbin).Changed in version 2.0.0:Now applicable to histograms.ylabellabel, optionalName to use for the ylabel on y-axis. Default will show no ylabel, or the
y-column name for planar plots.Changed in version 1.2.0:Now applicable to planar plots (scatter,hexbin).Changed in version 2.0.0:Now applicable to histograms.rotfloat, default NoneRotation for ticks (xticks for vertical, yticks for horizontal
plots).fontsizefloat, default NoneFont size for xticks and yticks.colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If string, load colormap with that
name from matplotlib.colorbarbool, optionalIf True, plot colorbar (only relevant for escatterf and ehexbinf
plots).positionfloatSpecify relative alignments for bar plot layout.
From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5
(center).tablebool, Series or DataFrame, default FalseIf True, draw a table using the data in the DataFrame and the data
will be transposed to meet matplotlibfs default layout.
If a Series or DataFrame is passed, use passed data to draw a
table.yerrDataFrame, Series, array-like, dict and strSeePlotting with Error Barsfor
detail.xerrDataFrame, Series, array-like, dict and strEquivalent to yerr.stackedbool, default False in line and bar plots, and True in area plotIf True, create stacked plot.secondary_ybool or sequence, default FalseWhether to plot on the secondary y-axis if a list/tuple, which
columns to plot on secondary y-axis.mark_rightbool, default TrueWhen using a secondary_y axis, automatically mark the column
labels with g(right)h in the legend.include_boolbool, default is FalseIf True, boolean values can be plotted.backendstr, default NoneBackend to use instead of the backend specified in the optionplotting.backend. For instance, ematplotlibf. Alternatively, to
specify theplotting.backendfor the whole session, setpd.options.plotting.backend.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.Axesor numpy.ndarray of themIf the backend is not the default matplotlib one, the return value
will be the object returned by the backend.NotesSee matplotlib documentation online for more on this subjectIfkind= ebarf or ebarhf, you can specify relative alignments
for bar plot layout bypositionkeyword.
From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5
(center)ExamplesFor Series:>>>ser=pd.Series([1,2,3,3])>>>plot=ser.plot(kind='hist',title=""My plot"")For DataFrame:>>>df=pd.DataFrame({'length':[1.5,0.5,1.2,0.9,3],...'width':[0.7,0.2,0.15,0.2,1.1]},...index=['pig','rabbit','duck','chicken','horse'])>>>plot=df.plot(title=""DataFrame Plot"")For SeriesGroupBy:>>>lst=[-1,-2,-3,1,2,3]>>>ser=pd.Series([1,2,2,4,6,6],index=lst)>>>plot=ser.groupby(lambdax:x>0).plot(title=""SeriesGroupBy Plot"")For DataFrameGroupBy:>>>df=pd.DataFrame({""col1"":[1,2,3,4],...""col2"":[""A"",""B"",""A"",""B""]})>>>plot=df.groupby(""col2"").plot(kind=""bar"",title=""DataFrameGroupBy Plot"")"
Pandas,DataFrame,pandas.DataFrame.plot.area,"pandas.DataFrame.plot.area#DataFrame.plot.area(x=None,y=None,stacked=True,**kwargs)[source]#Draw a stacked area plot.An area plot displays quantitative data visually.
This function wraps the matplotlib area function.Parameters:xlabel or position, optionalCoordinates for the X axis. By default uses the index.ylabel or position, optionalColumn to plot. By default uses all columns.stackedbool, default TrueArea plots are stacked by default. Set to False to create a
unstacked plot.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or numpy.ndarrayArea plot, or array of area plots if subplots is True.See alsoDataFrame.plotMake plots of DataFrame using matplotlib / pylab.ExamplesDraw an area plot based on basic business metrics:>>>df=pd.DataFrame({...'sales':[3,2,3,9,10,6],...'signups':[5,5,6,12,14,13],...'visits':[20,42,28,62,81,50],...},index=pd.date_range(start='2018/01/01',end='2018/07/01',...freq='M'))>>>ax=df.plot.area()Area plots are stacked by default. To produce an unstacked plot,
passstacked=False:>>>ax=df.plot.area(stacked=False)Draw an area plot for a single column:>>>ax=df.plot.area(y='sales')Draw with a differentx:>>>df=pd.DataFrame({...'sales':[3,2,3],...'visits':[20,42,28],...'day':[1,2,3],...})>>>ax=df.plot.area(x='day')"
Pandas,DataFrame,pandas.DataFrame.plot.bar,"pandas.DataFrame.plot.bar#DataFrame.plot.bar(x=None,y=None,**kwargs)[source]#Vertical bar plot.A bar plot is a plot that presents categorical data with
rectangular bars with lengths proportional to the values that they
represent. A bar plot shows comparisons among discrete categories. One
axis of the plot shows the specific categories being compared, and the
other axis represents a measured value.Parameters:xlabel or position, optionalAllows plotting of one column versus another. If not specified,
the index of the DataFrame is used.ylabel or position, optionalAllows plotting of one column versus another. If not specified,
all numerical columns are used.colorstr, array-like, or dict, optionalThe color for each of the DataFramefs columns. Possible values are:A single color string referred to by name, RGB or RGBA code,for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For
instance [egreenf,fyellowf] each columnfs bar will be filled in
green or yellow, alternatively. If there is only a single column to
be plotted, then only the first color from the color list will be
used.A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are calledaandb, then passing {eaf: egreenf, ebf: eredf} will color bars for
columnain green and bars for columnbin red.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with onematplotlib.axes.Axesper column whensubplots=True.See alsoDataFrame.plot.barhHorizontal bar plot.DataFrame.plotMake plots of a DataFrame.matplotlib.pyplot.barMake a bar plot with matplotlib.ExamplesBasic plot.>>>df=pd.DataFrame({'lab':['A','B','C'],'val':[10,30,20]})>>>ax=df.plot.bar(x='lab',y='val',rot=0)Plot a whole dataframe to a bar plot. Each column is assigned a
distinct color, and each row is nested in a group along the
horizontal axis.>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.bar(rot=0)Plot stacked bar charts for the DataFrame>>>ax=df.plot.bar(stacked=True)Instead of nesting, the figure can be split by column withsubplots=True. In this case, anumpy.ndarrayofmatplotlib.axes.Axesare returned.>>>axes=df.plot.bar(rot=0,subplots=True)>>>axes[1].legend(loc=2)If you donft like the default colours, you can specify how youfd
like each column to be colored.>>>axes=df.plot.bar(...rot=0,subplots=True,color={""speed"":""red"",""lifespan"":""green""}...)>>>axes[1].legend(loc=2)Plot a single column.>>>ax=df.plot.bar(y='speed',rot=0)Plot only selected categories for the DataFrame.>>>ax=df.plot.bar(x='lifespan',rot=0)"
Pandas,DataFrame,pandas.DataFrame.plot.barh,"pandas.DataFrame.plot.barh#DataFrame.plot.barh(x=None,y=None,**kwargs)[source]#Make a horizontal bar plot.A horizontal bar plot is a plot that presents quantitative data with
rectangular bars with lengths proportional to the values that they
represent. A bar plot shows comparisons among discrete categories. One
axis of the plot shows the specific categories being compared, and the
other axis represents a measured value.Parameters:xlabel or position, optionalAllows plotting of one column versus another. If not specified,
the index of the DataFrame is used.ylabel or position, optionalAllows plotting of one column versus another. If not specified,
all numerical columns are used.colorstr, array-like, or dict, optionalThe color for each of the DataFramefs columns. Possible values are:A single color string referred to by name, RGB or RGBA code,for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For
instance [egreenf,fyellowf] each columnfs bar will be filled in
green or yellow, alternatively. If there is only a single column to
be plotted, then only the first color from the color list will be
used.A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are calledaandb, then passing {eaf: egreenf, ebf: eredf} will color bars for
columnain green and bars for columnbin red.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with onematplotlib.axes.Axesper column whensubplots=True.See alsoDataFrame.plot.barVertical bar plot.DataFrame.plotMake plots of DataFrame using matplotlib.matplotlib.axes.Axes.barPlot a vertical bar plot using matplotlib.ExamplesBasic example>>>df=pd.DataFrame({'lab':['A','B','C'],'val':[10,30,20]})>>>ax=df.plot.barh(x='lab',y='val')Plot a whole DataFrame to a horizontal bar plot>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.barh()Plot stacked barh charts for the DataFrame>>>ax=df.plot.barh(stacked=True)We can specify colors for each column>>>ax=df.plot.barh(color={""speed"":""red"",""lifespan"":""green""})Plot a column of the DataFrame to a horizontal bar plot>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.barh(y='speed')Plot DataFrame versus the desired column>>>speed=[0.1,17.5,40,48,52,69,88]>>>lifespan=[2,8,70,1.5,25,12,28]>>>index=['snail','pig','elephant',...'rabbit','giraffe','coyote','horse']>>>df=pd.DataFrame({'speed':speed,...'lifespan':lifespan},index=index)>>>ax=df.plot.barh(x='lifespan')"
Pandas,DataFrame,pandas.DataFrame.plot.box,"pandas.DataFrame.plot.box#DataFrame.plot.box(by=None,**kwargs)[source]#Make a box plot of the DataFrame columns.A box plot is a method for graphically depicting groups of numerical
data through their quartiles.
The box extends from the Q1 to Q3 quartile values of the data,
with a line at the median (Q2). The whiskers extend from the edges
of box to show the range of the data. The position of the whiskers
is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the
box. Outlier points are those past the end of the whiskers.For further details see Wikipediafs
entry forboxplot.A consideration when using this chart is that the box and the whiskers
can overlap, which is very common when plotting small sets of data.Parameters:bystr or sequenceColumn in the DataFrame to group by.Changed in version 1.4.0:Previously,byis silently ignore and makes no groupings**kwargsAdditional keywords are documented inDataFrame.plot().Returns:matplotlib.axes.Axesor numpy.ndarray of themSee alsoDataFrame.boxplotAnother method to draw a box plot.Series.plot.boxDraw a box plot from a Series object.matplotlib.pyplot.boxplotDraw a box plot in matplotlib.ExamplesDraw a box plot from a DataFrame with four columns of randomly
generated data.>>>data=np.random.randn(25,4)>>>df=pd.DataFrame(data,columns=list('ABCD'))>>>ax=df.plot.box()You can also generate groupings if you specify thebyparameter (which
can take a column name, or a list or tuple of column names):Changed in version 1.4.0.>>>age_list=[8,10,12,14,72,74,76,78,20,25,30,35,60,85]>>>df=pd.DataFrame({""gender"":list(""MMMMMMMMFFFFFF""),""age"":age_list})>>>ax=df.plot.box(column=""age"",by=""gender"",figsize=(10,8))"
Pandas,DataFrame,pandas.DataFrame.plot.density,"pandas.DataFrame.plot.density#DataFrame.plot.density(bw_method=None,ind=None,**kwargs)[source]#Generate Kernel Density Estimate plot using Gaussian kernels.In statistics,kernel density estimation(KDE) is a non-parametric
way to estimate the probability density function (PDF) of a random
variable. This function uses Gaussian kernels and includes automatic
bandwidth determination.Parameters:bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be
escottf, esilvermanf, a scalar constant or a callable.
If None (default), escottf is used.
Seescipy.stats.gaussian_kdefor more information.indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default),
1000 equally spaced points are used. Ifindis a NumPy array, the
KDE is evaluated at the points passed. Ifindis an integer,indnumber of equally spaced points are used.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or numpy.ndarray of themSee alsoscipy.stats.gaussian_kdeRepresentation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.ExamplesGiven a Series of points randomly sampled from an unknown
distribution, estimate its PDF using KDE with automatic
bandwidth determination and plot the results, evaluating them at
1000 equally spaced points (default):>>>s=pd.Series([1,2,2.5,3,3.5,4,5])>>>ax=s.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=s.plot.kde(bw_method=0.3)>>>ax=s.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=s.plot.kde(ind=[1,2,3,4,5])For DataFrame, it works in the same way:>>>df=pd.DataFrame({...'x':[1,2,2.5,3,3.5,4,5],...'y':[4,4,4.5,5,5.5,6,6],...})>>>ax=df.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=df.plot.kde(bw_method=0.3)>>>ax=df.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=df.plot.kde(ind=[1,2,3,4,5,6])"
Pandas,DataFrame,pandas.DataFrame.plot.hexbin,"pandas.DataFrame.plot.hexbin#DataFrame.plot.hexbin(x,y,C=None,reduce_C_function=None,gridsize=None,**kwargs)[source]#Generate a hexagonal binning plot.Generate a hexagonal binning plot ofxversusy. IfCisNone(the default), this is a histogram of the number of occurrences
of the observations at(x[i],y[i]).IfCis specified, specifies values at given coordinates(x[i],y[i]). These values are accumulated for each hexagonal
bin and then reduced according toreduce_C_function,
having as default the NumPyfs mean function (numpy.mean()).
(IfCis specified, it must also be a 1-D sequence
of the same length asxandy, or a column label.)Parameters:xint or strThe column label or position for x points.yint or strThe column label or position for y points.Cint or str, optionalThe column label or position for the value of(x, y)point.reduce_C_functioncallable, defaultnp.meanFunction of one argument that reduces all the values in a bin to
a single number (e.g.np.mean,np.max,np.sum,np.std).gridsizeint or tuple of (int, int), default 100The number of hexagons in the x-direction.
The corresponding number of hexagons in the y-direction is
chosen in a way that the hexagons are approximately regular.
Alternatively, gridsize can be a tuple with two elements
specifying the number of hexagons in the x-direction and the
y-direction.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.AxesSubplotThe matplotlibAxeson which the hexbin is plotted.See alsoDataFrame.plotMake plots of a DataFrame.matplotlib.pyplot.hexbinHexagonal binning plot using matplotlib, the matplotlib function that is used under the hood.ExamplesThe following examples are generated with random data from
a normal distribution.>>>n=10000>>>df=pd.DataFrame({'x':np.random.randn(n),...'y':np.random.randn(n)})>>>ax=df.plot.hexbin(x='x',y='y',gridsize=20)The next example usesCandnp.sumasreduce_C_function.
Note thateobservationsfvalues ranges from 1 to 5 but the result
plot shows values up to more than 25. This is because of thereduce_C_function.>>>n=500>>>df=pd.DataFrame({...'coord_x':np.random.uniform(-3,3,size=n),...'coord_y':np.random.uniform(30,50,size=n),...'observations':np.random.randint(1,5,size=n)...})>>>ax=df.plot.hexbin(x='coord_x',...y='coord_y',...C='observations',...reduce_C_function=np.sum,...gridsize=10,...cmap=""viridis"")"
Pandas,DataFrame,pandas.DataFrame.plot.hist,"pandas.DataFrame.plot.hist#DataFrame.plot.hist(by=None,bins=10,**kwargs)[source]#Draw one histogram of the DataFramefs columns.A histogram is a representation of the distribution of data.
This function groups the values of all given Series in the DataFrame
into bins and draws all bins in onematplotlib.axes.Axes.
This is useful when the DataFramefs Series are in a similar scale.Parameters:bystr or sequence, optionalColumn in the DataFrame to group by.Changed in version 1.4.0:Previously,byis silently ignore and makes no groupingsbinsint, default 10Number of histogram bins to be used.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:class:matplotlib.AxesSubplotReturn a histogram plot.See alsoDataFrame.histDraw histograms per DataFramefs Series.Series.histDraw a histogram with Seriesf data.ExamplesWhen we roll a die 6000 times, we expect to get each value around 1000
times. But when we roll two dice and sum the result, the distribution
is going to be quite different. A histogram illustrates those
distributions.>>>df=pd.DataFrame(...np.random.randint(1,7,6000),...columns=['one'])>>>df['two']=df['one']+np.random.randint(1,7,6000)>>>ax=df.plot.hist(bins=12,alpha=0.5)A grouped histogram can be generated by providing the parameterby(which
can be a column name, or a list of column names):>>>age_list=[8,10,12,14,72,74,76,78,20,25,30,35,60,85]>>>df=pd.DataFrame({""gender"":list(""MMMMMMMMFFFFFF""),""age"":age_list})>>>ax=df.plot.hist(column=[""age""],by=""gender"",figsize=(10,8))"
Pandas,DataFrame,pandas.DataFrame.plot.kde,"pandas.DataFrame.plot.kde#DataFrame.plot.kde(bw_method=None,ind=None,**kwargs)[source]#Generate Kernel Density Estimate plot using Gaussian kernels.In statistics,kernel density estimation(KDE) is a non-parametric
way to estimate the probability density function (PDF) of a random
variable. This function uses Gaussian kernels and includes automatic
bandwidth determination.Parameters:bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be
escottf, esilvermanf, a scalar constant or a callable.
If None (default), escottf is used.
Seescipy.stats.gaussian_kdefor more information.indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default),
1000 equally spaced points are used. Ifindis a NumPy array, the
KDE is evaluated at the points passed. Ifindis an integer,indnumber of equally spaced points are used.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or numpy.ndarray of themSee alsoscipy.stats.gaussian_kdeRepresentation of a kernel-density estimate using Gaussian kernels. This is the function used internally to estimate the PDF.ExamplesGiven a Series of points randomly sampled from an unknown
distribution, estimate its PDF using KDE with automatic
bandwidth determination and plot the results, evaluating them at
1000 equally spaced points (default):>>>s=pd.Series([1,2,2.5,3,3.5,4,5])>>>ax=s.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=s.plot.kde(bw_method=0.3)>>>ax=s.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=s.plot.kde(ind=[1,2,3,4,5])For DataFrame, it works in the same way:>>>df=pd.DataFrame({...'x':[1,2,2.5,3,3.5,4,5],...'y':[4,4,4.5,5,5.5,6,6],...})>>>ax=df.plot.kde()A scalar bandwidth can be specified. Using a small bandwidth value can
lead to over-fitting, while using a large bandwidth value may result
in under-fitting:>>>ax=df.plot.kde(bw_method=0.3)>>>ax=df.plot.kde(bw_method=3)Finally, theindparameter determines the evaluation points for the
plot of the estimated PDF:>>>ax=df.plot.kde(ind=[1,2,3,4,5,6])"
Pandas,DataFrame,pandas.DataFrame.plot.line,"pandas.DataFrame.plot.line#DataFrame.plot.line(x=None,y=None,**kwargs)[source]#Plot Series or DataFrame as lines.This function is useful to plot lines using DataFramefs values
as coordinates.Parameters:xlabel or position, optionalAllows plotting of one column versus another. If not specified,
the index of the DataFrame is used.ylabel or position, optionalAllows plotting of one column versus another. If not specified,
all numerical columns are used.colorstr, array-like, or dict, optionalThe color for each of the DataFramefs columns. Possible values are:A single color string referred to by name, RGB or RGBA code,for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For
instance [egreenf,fyellowf] each columnfs line will be filled in
green or yellow, alternatively. If there is only a single column to
be plotted, then only the first color from the color list will be
used.A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are calledaandb, then passing {eaf: egreenf, ebf: eredf} will color lines for
columnain green and lines for columnbin red.**kwargsAdditional keyword arguments are documented inDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with onematplotlib.axes.Axesper column whensubplots=True.See alsomatplotlib.pyplot.plotPlot y versus x as lines and/or markers.Examples>>>s=pd.Series([1,3,2])>>>s.plot.line()The following example shows the populations for some animals
over the years.>>>df=pd.DataFrame({...'pig':[20,18,489,675,1776],...'horse':[4,25,281,600,1900]...},index=[1990,1997,2003,2009,2014])>>>lines=df.plot.line()An example with subplots, so an array of axes is returned.>>>axes=df.plot.line(subplots=True)>>>type(axes)<class 'numpy.ndarray'>Letfs repeat the same example, but specifying colors for
each column (in this case, for each animal).>>>axes=df.plot.line(...subplots=True,color={""pig"":""pink"",""horse"":""#742802""}...)The following example shows the relationship between both
populations.>>>lines=df.plot.line(x='pig',y='horse')"
Pandas,DataFrame,pandas.DataFrame.plot.pie,"pandas.DataFrame.plot.pie#DataFrame.plot.pie(**kwargs)[source]#Generate a pie plot.A pie plot is a proportional representation of the numerical data in a
column. This function wrapsmatplotlib.pyplot.pie()for the
specified column. If no column reference is passed andsubplots=Truea pie plot is drawn for each numerical column
independently.Parameters:yint or label, optionalLabel or position of the column to plot.
If not provided,subplots=Trueargument must be passed.**kwargsKeyword arguments to pass on toDataFrame.plot().Returns:matplotlib.axes.Axes or np.ndarray of themA NumPy array is returned whensubplotsis True.See alsoSeries.plot.pieGenerate a pie plot for a Series.DataFrame.plotMake plots of a DataFrame.ExamplesIn the example below we have a DataFrame with the information about
planetfs mass and radius. We pass the emassf column to the
pie function to get a pie plot.>>>df=pd.DataFrame({'mass':[0.330,4.87,5.97],...'radius':[2439.7,6051.8,6378.1]},...index=['Mercury','Venus','Earth'])>>>plot=df.plot.pie(y='mass',figsize=(5,5))>>>plot=df.plot.pie(subplots=True,figsize=(11,6))"
Pandas,DataFrame,pandas.DataFrame.plot.scatter,"pandas.DataFrame.plot.scatter#DataFrame.plot.scatter(x,y,s=None,c=None,**kwargs)[source]#Create a scatter plot with varying marker point size and color.The coordinates of each point are defined by two dataframe columns and
filled circles are used to represent each point. This kind of plot is
useful to see complex correlations between two variables. Points could
be for instance natural 2D coordinates like longitude and latitude in
a map or, in general, any pair of metrics that can be plotted against
each other.Parameters:xint or strThe column name or column position to be used as horizontal
coordinates for each point.yint or strThe column name or column position to be used as vertical
coordinates for each point.sstr, scalar or array-like, optionalThe size of each point. Possible values are:A string with the name of the column to be used for markerfs size.A single scalar so all points have the same size.A sequence of scalars, which will be used for each pointfs size
recursively. For instance, when passing [2,14] all points size
will be either 2 or 14, alternatively.cstr, int or array-like, optionalThe color of each point. Possible values are:A single color string referred to by name, RGB or RGBA code,
for instance eredf or e#a98d19f.A sequence of color strings referred to by name, RGB or RGBA
code, which will be used for each pointfs color recursively. For
instance [egreenf,fyellowf] all points will be filled in green or
yellow, alternatively.A column name or position whose values will be used to color the
marker points according to a colormap.**kwargsKeyword arguments to pass on toDataFrame.plot().Returns:matplotlib.axes.Axesor numpy.ndarray of themSee alsomatplotlib.pyplot.scatterScatter plot using multiple input data formats.ExamplesLetfs see how to draw a scatter plot using coordinates from the values
in a DataFramefs columns.>>>df=pd.DataFrame([[5.1,3.5,0],[4.9,3.0,0],[7.0,3.2,1],...[6.4,3.2,1],[5.9,3.0,2]],...columns=['length','width','species'])>>>ax1=df.plot.scatter(x='length',...y='width',...c='DarkBlue')And now with the color determined by a column as well.>>>ax2=df.plot.scatter(x='length',...y='width',...c='species',...colormap='viridis')"
Pandas,DataFrame,pandas.DataFrame.boxplot,"pandas.DataFrame.boxplot#DataFrame.boxplot(column=None,by=None,ax=None,fontsize=None,rot=0,grid=True,figsize=None,layout=None,return_type=None,backend=None,**kwargs)[source]#Make a box plot from DataFrame columns.Make a box-and-whisker plot from DataFrame columns, optionally grouped
by some other columns. A box plot is a method for graphically depicting
groups of numerical data through their quartiles.
The box extends from the Q1 to Q3 quartile values of the data,
with a line at the median (Q2). The whiskers extend from the edges
of box to show the range of the data. By default, they extend no more than1.5 * IQR (IQR = Q3 - Q1)from the edges of the box, ending at the farthest
data point within that interval. Outliers are plotted as separate dots.For further details see
Wikipediafs entry forboxplot.Parameters:columnstr or list of str, optionalColumn name or list of names, or vector.
Can be any valid input topandas.DataFrame.groupby().bystr or array-like, optionalColumn in the DataFrame topandas.DataFrame.groupby().
One box-plot will be done per value of columns inby.axobject of class matplotlib.axes.Axes, optionalThe matplotlib axes to be used by boxplot.fontsizefloat or strTick label font size in points or as a string (e.g.,large).rotfloat, default 0The rotation angle of labels (in degrees)
with respect to the screen coordinate system.gridbool, default TrueSetting this to True will show the grid.figsizeA tuple (width, height) in inchesThe size of the figure to create in matplotlib.layouttuple (rows, columns), optionalFor example, (3, 5) will display the subplots
using 3 rows and 5 columns, starting from the top-left.return_type{eaxesf, edictf, ebothf} or None, default eaxesfThe kind of object to return. The default isaxes.eaxesf returns the matplotlib axes the boxplot is drawn on.edictf returns a dictionary whose values are the matplotlib
Lines of the boxplot.ebothf returns a namedtuple with the axes and dict.when grouping withby, a Series mapping columns toreturn_typeis returned.Ifreturn_typeisNone, a NumPy array
of axes with the same shape aslayoutis returned.backendstr, default NoneBackend to use instead of the backend specified in the optionplotting.backend. For instance, ematplotlibf. Alternatively, to
specify theplotting.backendfor the whole session, setpd.options.plotting.backend.**kwargsAll other plotting keyword arguments to be passed tomatplotlib.pyplot.boxplot().Returns:resultSee Notes.See alsopandas.Series.plot.histMake a histogram.matplotlib.pyplot.boxplotMatplotlib equivalent plot.NotesThe return type depends on thereturn_typeparameter:eaxesf : object of class matplotlib.axes.Axesedictf : dict of matplotlib.lines.Line2D objectsebothf : a namedtuple with structure (ax, lines)For data grouped withby, return a Series of the above or a numpy
array:Seriesarray(forreturn_type=None)Usereturn_type='dict'when you want to tweak the appearance
of the lines after plotting. In this case a dict containing the Lines
making up the boxes, caps, fliers, medians, and whiskers is returned.ExamplesBoxplots can be created for every column in the dataframe
bydf.boxplot()or indicating the columns to be used:>>>np.random.seed(1234)>>>df=pd.DataFrame(np.random.randn(10,4),...columns=['Col1','Col2','Col3','Col4'])>>>boxplot=df.boxplot(column=['Col1','Col2','Col3'])Boxplots of variables distributions grouped by the values of a third
variable can be created using the optionby. For instance:>>>df=pd.DataFrame(np.random.randn(10,2),...columns=['Col1','Col2'])>>>df['X']=pd.Series(['A','A','A','A','A',...'B','B','B','B','B'])>>>boxplot=df.boxplot(by='X')A list of strings (i.e.['X','Y']) can be passed to boxplot
in order to group the data by combination of the variables in the x-axis:>>>df=pd.DataFrame(np.random.randn(10,3),...columns=['Col1','Col2','Col3'])>>>df['X']=pd.Series(['A','A','A','A','A',...'B','B','B','B','B'])>>>df['Y']=pd.Series(['A','B','A','B','A',...'B','A','B','A','B'])>>>boxplot=df.boxplot(column=['Col1','Col2'],by=['X','Y'])The layout of boxplot can be adjusted giving a tuple tolayout:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...layout=(2,1))Additional formatting can be done to the boxplot, like suppressing the grid
(grid=False), rotating the labels in the x-axis (i.e.rot=45)
or changing the fontsize (i.e.fontsize=15):>>>boxplot=df.boxplot(grid=False,rot=45,fontsize=15)The parameterreturn_typecan be used to select the type of element
returned byboxplot. Whenreturn_type='axes'is selected,
the matplotlib axes on which the boxplot is drawn are returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],return_type='axes')>>>type(boxplot)<class 'matplotlib.axes._axes.Axes'>When grouping withby, a Series mapping columns toreturn_typeis returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...return_type='axes')>>>type(boxplot)<class 'pandas.core.series.Series'>Ifreturn_typeisNone, a NumPy array of axes with the same shape
aslayoutis returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...return_type=None)>>>type(boxplot)<class 'numpy.ndarray'>"
Pandas,DataFrame,pandas.DataFrame.hist,"pandas.DataFrame.hist#DataFrame.hist(column=None,by=None,grid=True,xlabelsize=None,xrot=None,ylabelsize=None,yrot=None,ax=None,sharex=False,sharey=False,figsize=None,layout=None,bins=10,backend=None,legend=False,**kwargs)[source]#Make a histogram of the DataFramefs columns.Ahistogramis a representation of the distribution of data.
This function callsmatplotlib.pyplot.hist(), on each series in
the DataFrame, resulting in one histogram per column.Parameters:dataDataFrameThe pandas object holding the data.columnstr or sequence, optionalIf passed, will be used to limit data to a subset of columns.byobject, optionalIf passed, then used to form histograms for separate groups.gridbool, default TrueWhether to show axis grid lines.xlabelsizeint, default NoneIf specified changes the x-axis label size.xrotfloat, default NoneRotation of x axis labels. For example, a value of 90 displays the
x labels rotated 90 degrees clockwise.ylabelsizeint, default NoneIf specified changes the y-axis label size.yrotfloat, default NoneRotation of y axis labels. For example, a value of 90 displays the
y labels rotated 90 degrees clockwise.axMatplotlib axes object, default NoneThe axes to plot the histogram on.sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to
invisible; defaults to True if ax is None otherwise False if an ax
is passed in.
Note that passing in both an ax and sharex=True will alter all x axis
labels for all subplots in a figure.shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to
invisible.figsizetuple, optionalThe size in inches of the figure to create. Uses the value inmatplotlib.rcParamsby default.layouttuple, optionalTuple of (rows, columns) for the layout of the histograms.binsint or sequence, default 10Number of histogram bins to be used. If an integer is given, bins + 1
bin edges are calculated and returned. If bins is a sequence, gives
bin edges, including left edge of first bin and right edge of last
bin. In this case, bins is returned unmodified.backendstr, default NoneBackend to use instead of the backend specified in the optionplotting.backend. For instance, ematplotlibf. Alternatively, to
specify theplotting.backendfor the whole session, setpd.options.plotting.backend.legendbool, default FalseWhether to show the legend.**kwargsAll other plotting keyword arguments to be passed tomatplotlib.pyplot.hist().Returns:matplotlib.AxesSubplot or numpy.ndarray of themSee alsomatplotlib.pyplot.histPlot a histogram using matplotlib.ExamplesThis example draws a histogram based on the length and width of
some animals, displayed in three bins>>>df=pd.DataFrame({...'length':[1.5,0.5,1.2,0.9,3],...'width':[0.7,0.2,0.15,0.2,1.1]...},index=['pig','rabbit','duck','chicken','horse'])>>>hist=df.hist(bins=3)"
Pandas,DataFrame,pandas.DataFrame.sparse.density,"pandas.DataFrame.sparse.density#DataFrame.sparse.density[source]#Ratio of non-sparse points to total (dense) data points.Examples>>>df=pd.DataFrame({""A"":pd.arrays.SparseArray([0,1,0,1])})>>>df.sparse.density0.5"
Pandas,DataFrame,pandas.DataFrame.sparse.from_spmatrix,"pandas.DataFrame.sparse.from_spmatrix#classmethodDataFrame.sparse.from_spmatrix(data,index=None,columns=None)[source]#Create a new DataFrame from a scipy sparse matrix.Parameters:datascipy.sparse.spmatrixMust be convertible to csc format.index, columnsIndex, optionalRow and column labels to use for the resulting DataFrame.
Defaults to a RangeIndex.Returns:DataFrameEach column of the DataFrame is stored as aarrays.SparseArray.Examples>>>importscipy.sparse>>>mat=scipy.sparse.eye(3)>>>pd.DataFrame.sparse.from_spmatrix(mat)0    1    20  1.0  0.0  0.01  0.0  1.0  0.02  0.0  0.0  1.0"
Pandas,DataFrame,pandas.DataFrame.sparse.to_coo,"pandas.DataFrame.sparse.to_coo#DataFrame.sparse.to_coo()[source]#Return the contents of the frame as a sparse SciPy COO matrix.Returns:scipy.sparse.spmatrixIf the caller is heterogeneous and contains booleans or objects,
the result will be of dtype=object. See Notes.NotesThe dtype will be the lowest-common-denominator type (implicit
upcasting); that is to say if the dtypes (even of numeric types)
are mixed, the one that accommodates all will be chosen.e.g. If the dtypes are float16 and float32, dtype will be upcast to
float32. By numpy.find_common_type convention, mixing int64 and
and uint64 will result in a float64 dtype.Examples>>>df=pd.DataFrame({""A"":pd.arrays.SparseArray([0,1,0,1])})>>>df.sparse.to_coo()<4x1 sparse matrix of type '<class 'numpy.int64'>'with 2 stored elements in COOrdinate format>"
Pandas,DataFrame,pandas.DataFrame.sparse.to_dense,"pandas.DataFrame.sparse.to_dense#DataFrame.sparse.to_dense()[source]#Convert a DataFrame with sparse values to dense.Returns:DataFrameA DataFrame with the same values stored as dense arrays.Examples>>>df=pd.DataFrame({""A"":pd.arrays.SparseArray([0,1,0])})>>>df.sparse.to_dense()A0  01  12  0"
Pandas,DataFrame,pandas.DataFrame.from_dict,"pandas.DataFrame.from_dict#classmethodDataFrame.from_dict(data,orient='columns',dtype=None,columns=None)[source]#Construct DataFrame from dict of array-like or dicts.Creates DataFrame object from dictionary by columns or by index
allowing dtype specification.Parameters:datadictOf the form {field : array-like} or {field : dict}.orient{ecolumnsf, eindexf, etightf}, default ecolumnsfThe gorientationh of the data. If the keys of the passed dict
should be the columns of the resulting DataFrame, pass ecolumnsf
(default). Otherwise if the keys should be rows, pass eindexf.
If etightf, assume a dict with keys [eindexf, ecolumnsf, edataf,
eindex_namesf, ecolumn_namesf].New in version 1.4.0:etightf as an allowed value for theorientargumentdtypedtype, default NoneData type to force after DataFrame construction, otherwise infer.columnslist, default NoneColumn labels to use whenorient='index'. Raises a ValueError
if used withorient='columns'ororient='tight'.Returns:DataFrameSee alsoDataFrame.from_recordsDataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame.DataFrameDataFrame object creation using constructor.DataFrame.to_dictConvert the DataFrame to a dictionary.ExamplesBy default the keys of the dict become the DataFrame columns:>>>data={'col_1':[3,2,1,0],'col_2':['a','b','c','d']}>>>pd.DataFrame.from_dict(data)col_1 col_20      3     a1      2     b2      1     c3      0     dSpecifyorient='index'to create the DataFrame using dictionary
keys as rows:>>>data={'row_1':[3,2,1,0],'row_2':['a','b','c','d']}>>>pd.DataFrame.from_dict(data,orient='index')0  1  2  3row_1  3  2  1  0row_2  a  b  c  dWhen using the eindexf orientation, the column names can be
specified manually:>>>pd.DataFrame.from_dict(data,orient='index',...columns=['A','B','C','D'])A  B  C  Drow_1  3  2  1  0row_2  a  b  c  dSpecifyorient='tight'to create the DataFrame using a etightf
format:>>>data={'index':[('a','b'),('a','c')],...'columns':[('x',1),('y',2)],...'data':[[1,3],[2,4]],...'index_names':['n1','n2'],...'column_names':['z1','z2']}>>>pd.DataFrame.from_dict(data,orient='tight')z1     x  yz2     1  2n1 n2a  b   1  3c   2  4"
Pandas,DataFrame,pandas.DataFrame.from_records,"pandas.DataFrame.from_records#classmethodDataFrame.from_records(data,index=None,exclude=None,columns=None,coerce_float=False,nrows=None)[source]#Convert structured or record ndarray to DataFrame.Creates a DataFrame object from a structured ndarray, sequence of
tuples or dicts, or DataFrame.Parameters:datastructured ndarray, sequence of tuples or dicts, or DataFrameStructured input data.Deprecated since version 2.1.0:Passing a DataFrame is deprecated.indexstr, list of fields, array-likeField of array to use as the index, alternately a specific set of
input labels to use.excludesequence, default NoneColumns or fields to exclude.columnssequence, default NoneColumn names to use. If the passed data do not have names
associated with them, this argument provides names for the
columns. Otherwise this argument indicates the order of the columns
in the result (any names not found in the data will become all-NA
columns).coerce_floatbool, default FalseAttempt to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point, useful for SQL result sets.nrowsint, default NoneNumber of rows to read if data is an iterator.Returns:DataFrameSee alsoDataFrame.from_dictDataFrame from dict of array-like or dicts.DataFrameDataFrame object creation using constructor.ExamplesData can be provided as a structured ndarray:>>>data=np.array([(3,'a'),(2,'b'),(1,'c'),(0,'d')],...dtype=[('col_1','i4'),('col_2','U1')])>>>pd.DataFrame.from_records(data)col_1 col_20      3     a1      2     b2      1     c3      0     dData can be provided as a list of dicts:>>>data=[{'col_1':3,'col_2':'a'},...{'col_1':2,'col_2':'b'},...{'col_1':1,'col_2':'c'},...{'col_1':0,'col_2':'d'}]>>>pd.DataFrame.from_records(data)col_1 col_20      3     a1      2     b2      1     c3      0     dData can be provided as a list of tuples with corresponding columns:>>>data=[(3,'a'),(2,'b'),(1,'c'),(0,'d')]>>>pd.DataFrame.from_records(data,columns=['col_1','col_2'])col_1 col_20      3     a1      2     b2      1     c3      0     d"
Pandas,DataFrame,pandas.DataFrame.to_orc,"pandas.DataFrame.to_orc#DataFrame.to_orc(path=None,*,engine='pyarrow',index=None,engine_kwargs=None)[source]#Write a DataFrame to the ORC format.New in version 1.5.0.Parameters:pathstr, file-like object or None, default NoneIf a string, it will be used as Root Directory path
when writing a partitioned dataset. By file-like object,
we refer to objects with a write() method, such as a file handle
(e.g. via builtin open function). If path is None,
a bytes object is returned.engine{epyarrowf}, default epyarrowfORC library to use. Pyarrow must be >= 7.0.0.indexbool, optionalIfTrue, include the dataframefs index(es) in the file output.
IfFalse, they will not be written to the file.
IfNone, similar toinferthe dataframefs index(es)
will be saved. However, instead of being saved as values,
the RangeIndex will be stored as a range in the metadata so it
doesnft require much space and is faster. Other indexes will
be included as columns in the file output.engine_kwargsdict[str, Any] or None, default NoneAdditional keyword arguments passed topyarrow.orc.write_table().Returns:bytes if no path argument is provided else NoneRaises:NotImplementedErrorDtype of one or more columns is category, unsigned integers, interval,
period or sparse.ValueErrorengine is not pyarrow.See alsoread_orcRead a ORC file.DataFrame.to_parquetWrite a parquet file.DataFrame.to_csvWrite a csv file.DataFrame.to_sqlWrite to a sql table.DataFrame.to_hdfWrite to hdf.NotesBefore using this function you should read theuser guide about
ORCandinstall optional dependencies.This function requirespyarrowlibrary.For supported dtypes please refer tosupported ORC features in Arrow.Currently timezones in datetime columns are not preserved when a
dataframe is converted into ORC files.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>df.to_orc('df.orc')>>>pd.read_orc('df.orc')col1  col20     1     41     2     3If you want to get a buffer to the orc content you can write it to io.BytesIO>>>importio>>>b=io.BytesIO(df.to_orc())>>>b.seek(0)0>>>content=b.read()"
Pandas,DataFrame,pandas.DataFrame.to_parquet,"pandas.DataFrame.to_parquet#DataFrame.to_parquet(path=None,engine='auto',compression='snappy',index=None,partition_cols=None,storage_options=None,**kwargs)[source]#Write a DataFrame to the binary parquet format.This function writes the dataframe as aparquet file. You can choose different parquet
backends, and have the option of compression. Seethe user guidefor more details.Parameters:pathstr, path object, file-like object, or None, default NoneString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. If None, the result is
returned as bytes. If a string or path, it will be used as Root Directory
path when writing a partitioned dataset.Changed in version 1.2.0.Previously this was gfnamehengine{eautof, epyarrowf, efastparquetf}, default eautofParquet library to use. If eautof, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try epyarrowf, falling back to efastparquetf if
epyarrowf is unavailable.compressionstr or None, default esnappyfName of the compression to use. UseNonefor no compression.
Supported options: esnappyf, egzipf, ebrotlif, elz4f, ezstdf.indexbool, default NoneIfTrue, include the dataframefs index(es) in the file output.
IfFalse, they will not be written to the file.
IfNone, similar toTruethe dataframefs index(es)
will be saved. However, instead of being saved as values,
the RangeIndex will be stored as a range in the metadata so it
doesnft require much space and is faster. Other indexes will
be included as columns in the file output.partition_colslist, optional, default NoneColumn names by which to partition the dataset.
Columns are partitioned in the order they are given.
Must be None if path is not a string.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.**kwargsAdditional arguments passed to the parquet library. Seepandas iofor more details.Returns:bytes if no path argument is provided else NoneSee alsoread_parquetRead a parquet file.DataFrame.to_orcWrite an orc file.DataFrame.to_csvWrite a csv file.DataFrame.to_sqlWrite to a sql table.DataFrame.to_hdfWrite to hdf.NotesThis function requires either thefastparquetorpyarrowlibrary.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[3,4]})>>>df.to_parquet('df.parquet.gzip',...compression='gzip')>>>pd.read_parquet('df.parquet.gzip')col1  col20     1     31     2     4If you want to get a buffer to the parquet content you can use a io.BytesIO
object, as long as you donft use partition_cols, which creates multiple files.>>>importio>>>f=io.BytesIO()>>>df.to_parquet(f)>>>f.seek(0)0>>>content=f.read()"
Pandas,DataFrame,pandas.DataFrame.to_pickle,"pandas.DataFrame.to_pickle#DataFrame.to_pickle(path,compression='infer',protocol=5,storage_options=None)[source]#Pickle (serialize) object to file.Parameters:pathstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. File path where
the pickled object will be stored.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epathf is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.protocolintInt which indicates which protocol should be used by the pickler,
default HIGHEST_PROTOCOL (see[1]paragraph 12.1.2). The possible
values are 0, 1, 2, 3, 4, 5. A negative value for the protocol
parameter is equivalent to setting its value to HIGHEST_PROTOCOL.[1]https://docs.python.org/3/library/pickle.html.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.See alsoread_pickleLoad pickled pandas object (or any object) from file.DataFrame.to_hdfWrite DataFrame to an HDF5 file.DataFrame.to_sqlWrite DataFrame to a SQL database.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.Examples>>>original_df=pd.DataFrame({""foo"":range(5),""bar"":range(5,10)})>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>original_df.to_pickle(""./dummy.pkl"")>>>unpickled_df=pd.read_pickle(""./dummy.pkl"")>>>unpickled_dffoo  bar0    0    51    1    62    2    73    3    84    4    9"
Pandas,DataFrame,pandas.DataFrame.to_csv,"pandas.DataFrame.to_csv#DataFrame.to_csv(path_or_buf=None,sep=',',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,mode='w',encoding=None,compression='infer',quoting=None,quotechar='""',lineterminator=None,chunksize=None,date_format=None,doublequote=True,escapechar=None,decimal='.',errors='strict',storage_options=None)[source]#Write object to a comma-separated values (csv) file.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string. If a non-binary file object is passed, it should
be opened withnewline=ff, disabling universal newlines. If a binary
file object is passed,modemight need to contain aebf.Changed in version 1.2.0:Support for binary file objects was introduced.sepstr, default e,fString of length 1. Field delimiter for the output file.na_repstr, default efMissing data representation.float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes
precedence over other numeric formatting parameters, like decimal.columnssequence, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, andheaderandindexare True, then the index names are used. A
sequence should be given if the object uses MultiIndex. If
False do not print fields for index names. Use index_label=False
for easier importing in R.mode{ewf, exf, eaf}, default ewfForwarded to eitheropen(mode=)orfsspec.open(mode=)to control
the file opening. Typical values include:ewf, truncate the file first.exf, exclusive creation, failing if the file already exists.eaf, append to the end of file if it exists.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to eutf-8f.encodingis not supported ifpath_or_bufis a non-binary file object.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epath_or_buff is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key emethodf as compression mode
and other entries as additional compression options if
compression mode is ezipf.Passing compression options as keys in dict is
supported for compression modes egzipf, ebz2f, ezstdf, and ezipf.Changed in version 1.2.0:Compression is supported for binary file objects.Changed in version 1.2.0:Previous versions forwarded dict entries for egzipf togzip.openinstead ofgzip.GzipFilewhich prevented
settingmtime.quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set afloat_formatthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC
will treat them as non-numeric.quotecharstr, default e""fString of length 1. Character used to quote fields.lineterminatorstr, optionalThe newline character or character sequence to use in the output
file. Defaults toos.linesep, which depends on the OS in which
this method is called (f\nf for linux, e\r\nf for Windows, i.e.).Changed in version 1.5.0:Previously was line_terminator, changed for consistency with
read_csv and the standard library ecsvf module.chunksizeint or NoneRows to write at a time.date_formatstr, default NoneFormat string for datetime objects.doublequotebool, default TrueControl quoting ofquotecharinside a field.escapecharstr, default NoneString of length 1. Character used to escapesepandquotecharwhen appropriate.decimalstr, default e.fCharacter recognized as decimal separator. E.g. use e,f for
European data.errorsstr, default estrictfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.Returns:None or strIf path_or_buf is None, returns the resulting csv format as a
string. Otherwise returns None.See alsoread_csvLoad a CSV file into a DataFrame.to_excelWrite DataFrame to an Excel file.Examples>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv(index=False)'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'Create eout.zipf containing eout.csvf>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first
need to create it using either Pathlib or os:>>>frompathlibimportPath>>>filepath=Path('folder/subfolder/out.csv')>>>filepath.parent.mkdir(parents=True,exist_ok=True)>>>df.to_csv(filepath)>>>importos>>>os.makedirs('folder/subfolder',exist_ok=True)>>>df.to_csv('folder/subfolder/out.csv')"
Pandas,DataFrame,pandas.DataFrame.to_hdf,"pandas.DataFrame.to_hdf#DataFrame.to_hdf(path_or_buf,key,mode='a',complevel=None,complib=None,append=False,format=None,index=True,min_itemsize=None,nan_rep=None,dropna=None,data_columns=None,errors='strict',encoding='UTF-8')[source]#Write the contained data to an HDF5 file using HDFStore.Hierarchical Data Format (HDF) is self-describing, allowing an
application to interpret the structure and contents of a file with
no outside information. One HDF file can hold a mix of related objects
which can be accessed as a group or as individual objects.In order to add another DataFrame or Series to an existing HDF file
please use append mode and a different a key.WarningOne can store a subclass ofDataFrameorSeriesto HDF5,
but the type of the subclass is lost upon storing.For more information see theuser guide.Parameters:path_or_bufstr or pandas.HDFStoreFile path or HDFStore object.keystrIdentifier for the group in the store.mode{eaf, ewf, er+f}, default eafMode to open file:ewf: write, a new file is created (an existing file with
the same name would be deleted).eaf: append, an existing file is opened for reading and
writing, and if the file does not exist it is created.er+f: similar to eaf, but the file must already exist.complevel{0-9}, default NoneSpecifies a compression level for data.
A value of 0 or None disables compression.complib{ezlibf, elzof, ebzip2f, ebloscf}, default ezlibfSpecifies the compression library to be used.
These additional compressors for Blosc are supported
(default if no compressor specified: eblosc:blosclzf):
{eblosc:blosclzf, eblosc:lz4f, eblosc:lz4hcf, eblosc:snappyf,
eblosc:zlibf, eblosc:zstdf}.
Specifying a compression library which is not available issues
a ValueError.appendbool, default FalseFor Table formats, append the input data to the existing.format{efixedf, etablef, None}, default efixedfPossible values:efixedf: Fixed format. Fast writing/reading. Not-appendable,
nor searchable.etablef: Table format. Write as a PyTables Table structure
which may perform worse but allow more flexible operations
like searching / selecting subsets of the data.If None, pd.get_option(eio.hdf.default_formatf) is checked,
followed by fallback to gfixedh.indexbool, default TrueWrite DataFrame index as a column.min_itemsizedict or int, optionalMap column names to minimum string sizes for columns.nan_repAny, optionalHow to represent null values as str.
Not allowed with append=True.dropnabool, default False, optionalRemove missing values.data_columnslist of columns or True, optionalList of columns to create as indexed data columns for on-disk
queries, or True to use all columns. By default only the axes
of the object are indexed. SeeQuery via data columns. for
more information.
Applicable only to format=ftablef.errorsstr, default estrictfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.encodingstr, default gUTF-8hSee alsoread_hdfRead from HDF file.DataFrame.to_orcWrite a DataFrame to the binary orc format.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.DataFrame.to_sqlWrite to a SQL table.DataFrame.to_featherWrite out feather-format for DataFrames.DataFrame.to_csvWrite out to a csv file.Examples>>>df=pd.DataFrame({'A':[1,2,3],'B':[4,5,6]},...index=['a','b','c'])>>>df.to_hdf('data.h5',key='df',mode='w')We can add another object to the same file:>>>s=pd.Series([1,2,3,4])>>>s.to_hdf('data.h5',key='s')Reading from HDF file:>>>pd.read_hdf('data.h5','df')A  Ba  1  4b  2  5c  3  6>>>pd.read_hdf('data.h5','s')0    11    22    33    4dtype: int64"
Pandas,DataFrame,pandas.DataFrame.to_sql,"pandas.DataFrame.to_sql#DataFrame.to_sql(name,con,*,schema=None,if_exists='fail',index=True,index_label=None,chunksize=None,dtype=None,method=None)[source]#Write records stored in a DataFrame to a SQL database.Databases supported by SQLAlchemy[1]are supported. Tables can be
newly created, appended to, or overwritten.Parameters:namestrName of SQL table.consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. Legacy support is provided for sqlite3.Connection objects. The user
is responsible for engine disposal and connection closure for the SQLAlchemy
connectable. Seehere.
If passing a sqlalchemy.engine.Connection which is already in a transaction,
the transaction will not be committed. If passing a sqlite3.Connection,
it will not be possible to roll back the record insertion.schemastr, optionalSpecify the schema (if database flavor supports this). If None, use
default schema.if_exists{efailf, ereplacef, eappendf}, default efailfHow to behave if the table already exists.fail: Raise a ValueError.replace: Drop the table before inserting new values.append: Insert new values to the existing table.indexbool, default TrueWrite DataFrame index as a column. Usesindex_labelas the column
name in the table.index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) andindexis True, then the index names are used.
A sequence should be given if the DataFrame uses MultiIndex.chunksizeint, optionalSpecify the number of rows in each batch to be written at a time.
By default, all rows will be written at once.dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the
keys should be the column names and the values should be the
SQLAlchemy types or strings for the sqlite3 legacy mode. If a
scalar is provided, it will be applied to all columns.method{None, emultif, callable}, optionalControls the SQL insertion clause used:None : Uses standard SQLINSERTclause (one per row).emultif: Pass multiple values in a singleINSERTclause.callable with signature(pd_table,conn,keys,data_iter).Details and a sample callable implementation can be found in the
sectioninsert method.Returns:None or intNumber of rows affected by to_sql. None is returned if the callable
passed intomethoddoes not return an integer number of rows.The number of returned rows affected is the sum of therowcountattribute ofsqlite3.Cursoror SQLAlchemy connectable which may not
reflect the exact number of written rows as stipulated in thesqlite3orSQLAlchemy.New in version 1.4.0.Raises:ValueErrorWhen the table already exists andif_existsis efailf (the
default).See alsoread_sqlRead a DataFrame from a table.NotesTimezone aware datetime columns will be written asTimestampwithtimezonetype with SQLAlchemy if supported by the
database. Otherwise, the datetimes will be stored as timezone unaware
timestamps local to the original timezone.References[1]https://docs.sqlalchemy.org[2]https://www.python.org/dev/peps/pep-0249/ExamplesCreate an in-memory SQLite database.>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine('sqlite://',echo=False)Create a table from scratch with 3 rows.>>>df=pd.DataFrame({'name':['User 1','User 2','User 3']})>>>dfname0  User 11  User 22  User 3>>>df.to_sql(name='users',con=engine)3>>>fromsqlalchemyimporttext>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]Ansqlalchemy.engine.Connectioncan also be passed tocon:>>>withengine.begin()asconnection:...df1=pd.DataFrame({'name':['User 4','User 5']})...df1.to_sql(name='users',con=connection,if_exists='append')2This is allowed to support operations that require that the same
DBAPI connection is used for the entire operation.>>>df2=pd.DataFrame({'name':['User 6','User 7']})>>>df2.to_sql(name='users',con=engine,if_exists='append')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),(0, 'User 4'), (1, 'User 5'), (0, 'User 6'),(1, 'User 7')]Overwrite the table with justdf2.>>>df2.to_sql(name='users',con=engine,if_exists='replace',...index_label='id')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 6'), (1, 'User 7')]Usemethodto define a callable insertion method to do nothing
if therefs a primary key conflict on a table in a PostgreSQL database.>>>fromsqlalchemy.dialects.postgresqlimportinsert>>>definsert_on_conflict_nothing(table,conn,keys,data_iter):...# ""a"" is the primary key in ""conflict_table""...data=[dict(zip(keys,row))forrowindata_iter]...stmt=insert(table.table).values(data).on_conflict_do_nothing(index_elements=[""a""])...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_nothing)0For MySQL, a callable to update columnsbandcif therefs a conflict
on a primary key.>>>fromsqlalchemy.dialects.mysqlimportinsert>>>definsert_on_conflict_update(table,conn,keys,data_iter):...# update columns ""b"" and ""c"" on primary key conflict...data=[dict(zip(keys,row))forrowindata_iter]...stmt=(...insert(table.table)....values(data)...)...stmt=stmt.on_duplicate_key_update(b=stmt.inserted.b,c=stmt.inserted.c)...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_update)2Specify the dtype (especially useful for integers with missing values).
Notice that while pandas is forced to store the data as floating point,
the database supports nullable integers. When fetching the data with
Python, we get back integer scalars.>>>df=pd.DataFrame({""A"":[1,None,2]})>>>dfA0  1.01  NaN2  2.0>>>fromsqlalchemy.typesimportInteger>>>df.to_sql(name='integers',con=engine,index=False,...dtype={""A"":Integer()})3>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM integers"")).fetchall()[(1,), (None,), (2,)]"
Pandas,DataFrame,pandas.DataFrame.to_dict,"pandas.DataFrame.to_dict#DataFrame.to_dict(orient='dict',into=<class'dict'>,index=True)[source]#Convert the DataFrame to a dictionary.The type of the key-value pairs can be customized with the parameters
(see below).Parameters:orientstr {edictf, elistf, eseriesf, esplitf, etightf, erecordsf, eindexf}Determines the type of the values of the dictionary.edictf (default) : dict like {column -> {index -> value}}elistf : dict like {column -> [values]}eseriesf : dict like {column -> Series(values)}esplitf : dict like
{eindexf -> [index], ecolumnsf -> [columns], edataf -> [values]}etightf : dict like
{eindexf -> [index], ecolumnsf -> [columns], edataf -> [values],
eindex_namesf -> [index.names], ecolumn_namesf -> [column.names]}erecordsf : list like
[{column -> value}, c , {column -> value}]eindexf : dict like {index -> {column -> value}}New in version 1.4.0:etightf as an allowed value for theorientargumentintoclass, default dictThe collections.abc.Mapping subclass used for all Mappings
in the return value. Can be the actual class or an empty
instance of the mapping type you want. If you want a
collections.defaultdict, you must pass it initialized.indexbool, default TrueWhether to include the index item (and index_names item iforientis etightf) in the returned dictionary. Can only beFalsewhenorientis esplitf or etightf.New in version 2.0.0.Returns:dict, list or collections.abc.MappingReturn a collections.abc.Mapping object representing the DataFrame.
The resulting transformation depends on theorientparameter.See alsoDataFrame.from_dictCreate a DataFrame from a dictionary.DataFrame.to_jsonConvert a DataFrame to JSON format.Examples>>>df=pd.DataFrame({'col1':[1,2],...'col2':[0.5,0.75]},...index=['row1','row2'])>>>dfcol1  col2row1     1  0.50row2     2  0.75>>>df.to_dict(){'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}You can specify the return orientation.>>>df.to_dict('series'){'col1': row1    1row2    2Name: col1, dtype: int64,'col2': row1    0.50row2    0.75Name: col2, dtype: float64}>>>df.to_dict('split'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]]}>>>df.to_dict('records')[{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]>>>df.to_dict('index'){'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}>>>df.to_dict('tight'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}You can also specify the mapping type.>>>fromcollectionsimportOrderedDict,defaultdict>>>df.to_dict(into=OrderedDict)OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])If you want adefaultdict, you need to initialize it:>>>dd=defaultdict(list)>>>df.to_dict('records',into=dd)[defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]"
Pandas,DataFrame,pandas.DataFrame.to_excel,"pandas.DataFrame.to_excel#DataFrame.to_excel(excel_writer,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to
specify a target file name. To write to multiple sheets it is necessary to
create anExcelWriterobject with a target file name, and specify a sheet
in the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.
With all data written to the file it is necessary to save the changes.
Note that creating anExcelWriterobject with a file name that already
exists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default eSheet1fName of sheet which will contain DataFrame.na_repstr, default efMissing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=""%.2f""will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A
sequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, eopenpyxlf or exlsxwriterf. You can also set this
via the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default einffRepresentation for infinity (there is no native representation for
infinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that
is to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),
to_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further
data without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(""output.xlsx"")To specify the sheet name:>>>df1.to_excel(""output.xlsx"",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is
necessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,
you can pass theenginekeyword (the default engine is
automatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')"
Pandas,DataFrame,pandas.DataFrame.to_json,"pandas.DataFrame.to_json#DataFrame.to_json(path_or_buf=None,orient=None,date_format=None,double_precision=10,force_ascii=True,date_unit='ms',default_handler=None,lines=False,compression='infer',index=None,indent=None,storage_options=None,mode='w')[source]#Convert the object to a JSON string.Note NaNfs and None will be converted to null and datetime objects
will be converted to UNIX timestamps.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string.orientstrIndication of expected JSON string format.Series:default is eindexfallowed values are: {esplitf, erecordsf, eindexf, etablef}.DataFrame:default is ecolumnsfallowed values are: {esplitf, erecordsf, eindexf, ecolumnsf,
evaluesf, etablef}.The format of the JSON string:esplitf : dict like {eindexf -> [index], ecolumnsf -> [columns],
edataf -> [values]}erecordsf : list like [{column -> value}, c , {column -> value}]eindexf : dict like {index -> {column -> value}}ecolumnsf : dict like {column -> {index -> value}}evaluesf : just the values arrayetablef : dict like {eschemaf: {schema}, edataf: {data}}Describing the data, where data component is likeorient='records'.date_format{None, eepochf, eisof}Type of date conversion. eepochf = epoch milliseconds,
eisof = ISO8601. The default depends on theorient. Fororient='table', the default is eisof. For all other orients,
the default is eepochf.double_precisionint, default 10The number of decimal places to use when encoding
floating point values. The possible maximal value is 15.
Passing double_precision greater than 15 will raise a ValueError.force_asciibool, default TrueForce encoded string to be ASCII.date_unitstr, default emsf (milliseconds)The time unit to encode to, governs timestamp and ISO8601
precision. One of esf, emsf, eusf, ensf for second, millisecond,
microsecond, and nanosecond respectively.default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a
suitable format for JSON. Should receive a single argument which is
the object to convert and return a serialisable object.linesbool, default FalseIf eorientf is erecordsf write out line-delimited json format. Will
throw ValueError if incorrect eorientf since others are not
list-like.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epath_or_buff is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.indexbool or None, default NoneThe index is only used when eorientf is esplitf, eindexf, ecolumnf,
or etablef. Of these, eindexf and ecolumnf do not supportindex=False.indentint, optionalLength of whitespace used to indent each record.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.modestr, default ewf (writing)Specify the IO mode for output when supplying a path_or_buf.
Accepted args are ewf (writing) and eaf (append) only.
mode=faf is only supported when lines is True and orient is erecordsf.Returns:None or strIf path_or_buf is None, returns the resulting json format as a
string. Otherwise returns None.See alsoread_jsonConvert a JSON string to pandas object.NotesThe behavior ofindent=0varies from the stdlib, which does not
indent the output but does insert newlines. Currently,indent=0and the defaultindent=Noneare equivalent in pandas, though this
may change in a future release.orient='table'contains a epandas_versionf field under eschemaf.
This stores the version ofpandasused in the latest revision of the
schema.Examples>>>fromjsonimportloads,dumps>>>df=pd.DataFrame(...[[""a"",""b""],[""c"",""d""]],...index=[""row 1"",""row 2""],...columns=[""col 1"",""col 2""],...)>>>result=df.to_json(orient=""split"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""columns"": [""col 1"",""col 2""],""index"": [""row 1"",""row 2""],""data"": [[""a"",""b""],[""c"",""d""]]}Encoding/decoding a Dataframe using'records'formatted JSON.
Note that index labels are not preserved with this encoding.>>>result=df.to_json(orient=""records"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[{""col 1"": ""a"",""col 2"": ""b""},{""col 1"": ""c"",""col 2"": ""d""}]Encoding/decoding a Dataframe using'index'formatted JSON:>>>result=df.to_json(orient=""index"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""row 1"": {""col 1"": ""a"",""col 2"": ""b""},""row 2"": {""col 1"": ""c"",""col 2"": ""d""}}Encoding/decoding a Dataframe using'columns'formatted JSON:>>>result=df.to_json(orient=""columns"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""col 1"": {""row 1"": ""a"",""row 2"": ""c""},""col 2"": {""row 1"": ""b"",""row 2"": ""d""}}Encoding/decoding a Dataframe using'values'formatted JSON:>>>result=df.to_json(orient=""values"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[[""a"",""b""],[""c"",""d""]]Encoding with Table Schema:>>>result=df.to_json(orient=""table"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""schema"": {""fields"": [{""name"": ""index"",""type"": ""string""},{""name"": ""col 1"",""type"": ""string""},{""name"": ""col 2"",""type"": ""string""}],""primaryKey"": [""index""],""pandas_version"": ""1.4.0""},""data"": [{""index"": ""row 1"",""col 1"": ""a"",""col 2"": ""b""},{""index"": ""row 2"",""col 1"": ""c"",""col 2"": ""d""}]}"
Pandas,DataFrame,pandas.DataFrame.to_html,"pandas.DataFrame.to_html#DataFrame.to_html(buf=None,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',bold_rows=True,classes=None,escape=True,notebook=False,border=None,table_id=None,render_links=False,encoding=None)[source]#Render a DataFrame as an HTML table.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spacestr or int, list or dict of int or str, optionalThe minimum width of each column in CSS length units. An int is assumed to be px units..headerbool, optionalWhether to print column labels, default True.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default eNaNfString representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columnsf elements by position or
name.
The result of each function must be a unicode string.
List/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columnsf elements if they are
floats. This function must return a unicode string and will be
applied only to the non-NaNelements, withNaNbeing
handled byna_rep.Changed in version 1.2.0.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from
the print configuration (controlled by set_option), erightf out
of the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default e.fCharacter recognized as decimal separator, e.g. e,f in Europe.bold_rowsbool, default TrueMake the row labels bold in the output.classesstr or list or tuple, default NoneCSS class(es) to apply to the resulting html table.escapebool, default TrueConvert the characters <, >, and & to HTML-safe sequences.notebook{True, False}, default FalseWhether the generated HTML is for IPython Notebook.borderintAborder=borderattribute is included in the opening<table>tag. Defaultpd.options.display.html.border.table_idstr, optionalA css id is included in the opening<table>tag if specified.render_linksbool, default FalseConvert URLs to HTML links.encodingstr, default gutf-8hSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns
None.See alsoto_stringConvert DataFrame to a string.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>html_string='''<table border=""1"" class=""dataframe"">...<thead>...<tr style=""text-align: right;"">...<th></th>...<th>col1</th>...<th>col2</th>...</tr>...</thead>...<tbody>...<tr>...<th>0</th>...<td>1</td>...<td>4</td>...</tr>...<tr>...<th>1</th>...<td>2</td>...<td>3</td>...</tr>...</tbody>...</table>'''>>>asserthtml_string==df.to_html()"
Pandas,DataFrame,pandas.DataFrame.to_feather,"pandas.DataFrame.to_feather#DataFrame.to_feather(path,**kwargs)[source]#Write a DataFrame to the binary Feather format.Parameters:pathstr, path object, file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. If a string or a path,
it will be used as Root Directory path when writing a partitioned dataset.**kwargsAdditional keywords passed topyarrow.feather.write_feather().
This includes thecompression,compression_level,chunksizeandversionkeywords.NotesThis function writes the dataframe as afeather file. Requires a default
index. For saving the DataFrame with your custom index use a method that
supports custom indices e.g.to_parquet.Examples>>>df=pd.DataFrame([[1,2,3],[4,5,6]])>>>df.to_feather(""file.feather"")"
Pandas,DataFrame,pandas.DataFrame.to_latex,"pandas.DataFrame.to_latex#DataFrame.to_latex(buf=None,columns=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,bold_rows=False,column_format=None,longtable=None,escape=None,encoding=None,decimal='.',multicolumn=None,multicolumn_format=None,multirow=None,caption=None,label=None,position=None)[source]#Render object to a LaTeX tabular, longtable, or nested table.Requires\usepackage{{booktabs}}. The output can be copy/pasted
into a main LaTeX document or read from an external file
with\input{{table.tex}}.Changed in version 1.2.0:Added position argument, changed meaning of caption argument.Changed in version 2.0.0:Refactored to use the Styler implementation via jinja2 templating.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnslist of label, optionalThe subset of columns to write. Writes all columns by default.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given,
it is assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).na_repstr, default eNaNfMissing data representation.formatterslist of functions or dict of {{str: function}}, optionalFormatter functions to apply to columnsf elements by position or
name. The result of each function must be a unicode string.
List must be of length equal to the number of columns.float_formatone-parameter function or str, optional, default NoneFormatter for floating point numbers. For examplefloat_format=""%.2f""andfloat_format=""{{:0.2f}}"".formatwill
both result in 0.1234 being formatted as 0.12.sparsifybool, optionalSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row. By default, the value will be
read from the config module.index_namesbool, default TruePrints the names of the indexes.bold_rowsbool, default FalseMake the row labels bold in the output.column_formatstr, optionalThe columns format as specified inLaTeX table formate.g. erclf for 3
columns. By default, elf will be used for all columns except
columns of numbers, which default to erf.longtablebool, optionalUse a longtable environment instead of tabular. Requires
adding a usepackage{{longtable}} to your LaTeX preamble.
By default, the value will be read from the pandas config
module, and set toTrueif the optionstyler.latex.environmentisglongtableh.Changed in version 2.0.0:The pandas option affecting this argument has changed.escapebool, optionalBy default, the value will be read from the pandas config
module and set toTrueif the optionstyler.format.escapeisglatexh. When set to False prevents from escaping latex special
characters in column names.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value toFalse.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to eutf-8f.decimalstr, default e.fCharacter recognized as decimal separator, e.g. e,f in Europe.multicolumnbool, default TrueUse multicolumn to enhance MultiIndex columns.
The default will be read from the config module, and is set
as the optionstyler.sparse.columns.Changed in version 2.0.0:The pandas option affecting this argument has changed.multicolumn_formatstr, default erfThe alignment for multicolumns, similar tocolumn_formatThe default will be read from the config module, and is set as the optionstyler.latex.multicol_align.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value to grh.multirowbool, default TrueUse multirow to enhance MultiIndex rows. Requires adding a
usepackage{{multirow}} to your LaTeX preamble. Will print
centered labels (instead of top-aligned) across the contained
rows, separating groups via clines. The default will be read
from the pandas config module, and is set as the optionstyler.sparse.index.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value toTrue.captionstr or tuple, optionalTuple (full_caption, short_caption),
which results in\caption[short_caption]{{full_caption}};
if a single string is passed, no short caption will be set.Changed in version 1.2.0:Optionally allow caption to be a tuple(full_caption,short_caption).labelstr, optionalThe LaTeX label to be placed inside\label{{}}in the output.
This is used with\ref{{}}in the main.texfile.positionstr, optionalThe LaTeX positional argument for tables, to be placed after\begin{{}}in the output.New in version 1.2.0.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns None.See alsoio.formats.style.Styler.to_latexRender a DataFrame to LaTeX with conditional formatting.DataFrame.to_stringRender a DataFrame to a console-friendly tabular output.DataFrame.to_htmlRender a DataFrame as an HTML table.NotesAs of v2.0.0 this method has changed to use the Styler implementation as
part ofStyler.to_latex()viajinja2templating. This means
thatjinja2is a requirement, and needs to be installed, for this method
to function. It is advised that users switch to using Styler, since that
implementation is more frequently updated and contains much more
flexibility with the output.ExamplesConvert a general DataFrame to LaTeX with formatting:>>>df=pd.DataFrame(dict(name=['Raphael','Donatello'],...age=[26,45],...height=[181.23,177.65]))>>>print(df.to_latex(index=False,...formatters={""name"":str.upper},...float_format=""{:.1f}"".format,...))\begin{tabular}{lrr}\toprulename & age & height \\\midruleRAPHAEL & 26 & 181.2 \\DONATELLO & 45 & 177.7 \\\bottomrule\end{tabular}"
Pandas,DataFrame,pandas.DataFrame.to_stata,"pandas.DataFrame.to_stata#DataFrame.to_stata(path,*,convert_dates=None,write_index=True,byteorder=None,time_stamp=None,data_label=None,variable_labels=None,version=114,convert_strl=None,compression='infer',storage_options=None,value_labels=None)[source]#Export DataFrame object to Stata dta format.Writes the DataFrame to a Stata dataset file.
gdtah files contain a Stata dataset.Parameters:pathstr, path object, or bufferString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function.convert_datesdictDictionary mapping columns containing datetime types to stata
internal format to use when writing the dates. Options are etcf,
etdf, etmf, etwf, ethf, etqf, etyf. Column can be either an integer
or a name. Datetime columns that do not have a conversion type
specified will be converted to etcf. Raises NotImplementedError if
a datetime column has timezone information.write_indexboolWrite the index to Stata dataset.byteorderstrCan be g>h, g<h, glittleh, or gbigh. default issys.byteorder.time_stampdatetimeA datetime to use as file creation date. Default is the current
time.data_labelstr, optionalA label for the data set. Must be 80 characters or smaller.variable_labelsdictDictionary containing columns as keys and variable labels as
values. Each label must be 80 characters or smaller.version{114, 117, 118, 119, None}, default 114Version to use in the output dta file. Set to None to let pandas
decide between 118 or 119 formats depending on the number of
columns in the frame. Version 114 can be read by Stata 10 and
later. Version 117 can be read by Stata 13 or later. Version 118
is supported in Stata 14 and later. Version 119 is supported in
Stata 15 and later. Version 114 limits string variables to 244
characters or fewer while versions 117 and later allow strings
with lengths up to 2,000,000 characters. Versions 118 and 119
support Unicode characters, and version 119 supports more than
32,767 variables.Version 119 should usually only be used when the number of
variables exceeds the capacity of dta format 118. Exporting
smaller datasets in format 119 may have unintended consequences,
and, as of November 2020, Stata SE cannot read version 119 files.convert_strllist, optionalList of column names to convert to string columns to Stata StrL
format. Only available if version is 117. Storing strings in the
StrL format can produce smaller dta files if strings have more than
8 characters and values are repeated.compressionstr or dict, default einferfFor on-the-fly compression of the output data. If einferf and epathf is
path-like, then detect compression from the following extensions: e.gzf,
e.bz2f, e.zipf, e.xzf, e.zstf, e.tarf, e.tar.gzf, e.tar.xzf or e.tar.bz2f
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.value_labelsdict of dictsDictionary containing columns as keys and dictionaries of column value
to labels as values. Labels for a single variable must be 32,000
characters or smaller.New in version 1.4.0.Raises:NotImplementedErrorIf datetimes contain timezone informationColumn dtype is not representable in StataValueErrorColumns listed in convert_dates are neither datetime64[ns]
or datetime.datetimeColumn listed in convert_dates is not in DataFrameCategorical label contains more than 32,000 charactersSee alsoread_stataImport Stata data files.io.stata.StataWriterLow-level writer for Stata data files.io.stata.StataWriter117Low-level writer for version 117 files.Examples>>>df=pd.DataFrame({'animal':['falcon','parrot','falcon',...'parrot'],...'speed':[350,18,361,15]})>>>df.to_stata('animals.dta')"
Pandas,DataFrame,pandas.DataFrame.to_gbq,"pandas.DataFrame.to_gbq#DataFrame.to_gbq(destination_table,project_id=None,chunksize=None,reauth=False,if_exists='fail',auth_local_webserver=True,table_schema=None,location=None,progress_bar=True,credentials=None)[source]#Write a DataFrame to a Google BigQuery table.This function requires thepandas-gbq package.See theHow to authenticate with Google BigQueryguide for authentication instructions.Parameters:destination_tablestrName of table to be written, in the formdataset.tablename.project_idstr, optionalGoogle BigQuery Account project ID. Optional when available from
the environment.chunksizeint, optionalNumber of rows to be inserted in each chunk from the dataframe.
Set toNoneto load the whole dataframe at once.reauthbool, default FalseForce Google BigQuery to re-authenticate the user. This is useful
if multiple accounts are used.if_existsstr, default efailfBehavior when the destination table exists. Value can be one of:'fail'If table exists raise pandas_gbq.gbq.TableCreationError.'replace'If table exists, drop it, recreate it, and insert data.'append'If table exists, insert data. Create if does not exist.auth_local_webserverbool, default TrueUse thelocal webserver flowinstead of theconsole flowwhen getting user credentials.New in version 0.2.0 of pandas-gbq.Changed in version 1.5.0:Default value is changed toTrue. Google has deprecated theauth_local_webserver=Falsegout of bandh (copy-paste)
flow.table_schemalist of dicts, optionalList of BigQuery table fields to which according DataFrame
columns conform to, e.g.[{'name':'col1','type':'STRING'},...]. If schema is not provided, it will be
generated according to dtypes of DataFrame columns. See
BigQuery API documentation on available names of a field.New in version 0.3.1 of pandas-gbq.locationstr, optionalLocation where the load job should run. See theBigQuery locations
documentationfor a
list of available locations. The location must match that of the
target dataset.New in version 0.5.0 of pandas-gbq.progress_barbool, default TrueUse the librarytqdmto show the progress bar for the upload,
chunk by chunk.New in version 0.5.0 of pandas-gbq.credentialsgoogle.auth.credentials.Credentials, optionalCredentials for accessing Google APIs. Use this parameter to
override default credentials, such as to use Compute Enginegoogle.auth.compute_engine.Credentialsor Service
Accountgoogle.oauth2.service_account.Credentialsdirectly.New in version 0.8.0 of pandas-gbq.See alsopandas_gbq.to_gbqThis function in the pandas-gbq library.read_gbqRead a DataFrame from Google BigQuery.ExamplesExample taken fromGoogle BigQuery documentation>>>project_id=""my-project"">>>table_id='my_dataset.my_table'>>>df=pd.DataFrame({...""my_string"":[""a"",""b"",""c""],...""my_int64"":[1,2,3],...""my_float64"":[4.0,5.0,6.0],...""my_bool1"":[True,False,True],...""my_bool2"":[False,True,False],...""my_dates"":pd.date_range(""now"",periods=3),...}...)>>>df.to_gbq(table_id,project_id=project_id)"
Pandas,DataFrame,pandas.DataFrame.to_records,"pandas.DataFrame.to_records#DataFrame.to_records(index=True,column_dtypes=None,index_dtypes=None)[source]#Convert DataFrame to a NumPy record array.Index will be included as the first field of the record array if
requested.Parameters:indexbool, default TrueInclude index in resulting record array, stored in eindexf
field or using the index label, if set.column_dtypesstr, type, dict, default NoneIf a string or type, the data type to store all columns. If
a dictionary, a mapping of column names and indices (zero-indexed)
to specific data types.index_dtypesstr, type, dict, default NoneIf a string or type, the data type to store all index levels. If
a dictionary, a mapping of index level names and indices
(zero-indexed) to specific data types.This mapping is applied only ifindex=True.Returns:numpy.rec.recarrayNumPy ndarray with the DataFrame labels as fields and each row
of the DataFrame as entries.See alsoDataFrame.from_recordsConvert structured or record ndarray to DataFrame.numpy.rec.recarrayAn ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet.Examples>>>df=pd.DataFrame({'A':[1,2],'B':[0.5,0.75]},...index=['a','b'])>>>dfA     Ba  1  0.50b  2  0.75>>>df.to_records()rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])If the DataFrame index has no label then the recarray field name
is set to eindexf. If the index has a label then this is used as the
field name:>>>df.index=df.index.rename(""I"")>>>df.to_records()rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])The index can be excluded from the record array:>>>df.to_records(index=False)rec.array([(1, 0.5 ), (2, 0.75)],dtype=[('A', '<i8'), ('B', '<f8')])Data types can be specified for the columns:>>>df.to_records(column_dtypes={""A"":""int32""})rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])As well as for the index:>>>df.to_records(index_dtypes=""<S2"")rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])>>>index_dtypes=f""<S{df.index.str.len().max()}"">>>df.to_records(index_dtypes=index_dtypes)rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])"
Pandas,DataFrame,pandas.DataFrame.to_string,"pandas.DataFrame.to_string#DataFrame.to_string(buf=None,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',line_width=None,min_rows=None,max_colwidth=None,encoding=None)[source]#Render a DataFrame to a console-friendly tabular output.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spaceint, list or dict of int, optionalThe minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..headerbool or list of str, optionalWrite out the column names. If a list of columns is given, it is assumed to be aliases for the column names.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default eNaNfString representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columnsf elements by position or
name.
The result of each function must be a unicode string.
List/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columnsf elements if they are
floats. This function must return a unicode string and will be
applied only to the non-NaNelements, withNaNbeing
handled byna_rep.Changed in version 1.2.0.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from
the print configuration (controlled by set_option), erightf out
of the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default e.fCharacter recognized as decimal separator, e.g. e,f in Europe.line_widthint, optionalWidth to wrap a line in characters.min_rowsint, optionalThe number of rows to display in the console in a truncated repr
(when number of rows is abovemax_rows).max_colwidthint, optionalMax width to truncate each column in characters. By default, no limit.encodingstr, default gutf-8hSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns
None.See alsoto_htmlConvert DataFrame to HTML.Examples>>>d={'col1':[1,2,3],'col2':[4,5,6]}>>>df=pd.DataFrame(d)>>>print(df.to_string())col1  col20     1     41     2     52     3     6"
Pandas,DataFrame,pandas.DataFrame.to_clipboard,"pandas.DataFrame.to_clipboard#DataFrame.to_clipboard(excel=True,sep=None,**kwargs)[source]#Copy object to the system clipboard.Write a text representation of object to the system clipboard.
This can be pasted into Excel, for example.Parameters:excelbool, default TrueProduce output in a csv format for easy pasting into excel.True, use the provided separator for csv pasting.False, write a string representation of the object to the clipboard.sepstr, default'\t'Field delimiter.**kwargsThese parameters will be passed to DataFrame.to_csv.See alsoDataFrame.to_csvWrite a DataFrame to a comma-separated values (csv) file.read_clipboardRead text from clipboard and pass to read_csv.NotesRequirements for your platform.Linux :xclip, orxsel(withPyQt4modules)Windows : nonemacOS : noneThis method uses the processes developed for the packagepyperclip. A
solution to render any output string format is given in the examples.ExamplesCopy the contents of a DataFrame to the clipboard.>>>df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['A','B','C'])>>>df.to_clipboard(sep=',')...# Wrote the following to the system clipboard:...# ,A,B,C...# 0,1,2,3...# 1,4,5,6We can omit the index by passing the keywordindexand setting
it to false.>>>df.to_clipboard(sep=',',index=False)...# Wrote the following to the system clipboard:...# A,B,C...# 1,2,3...# 4,5,6Using the originalpyperclippackage for any string output format.importpypercliphtml=df.style.to_html()pyperclip.copy(html)"
Pandas,DataFrame,pandas.DataFrame.to_markdown,"pandas.DataFrame.to_markdown#DataFrame.to_markdown(buf=None,mode='wt',index=True,storage_options=None,**kwargs)[source]#Print DataFrame in Markdown-friendly format.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.modestr, optionalMode in which file is opened, gwth by default.indexbool, optional, default TrueAdd index (row) labels.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with gs3://h, and ggcs://h) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.**kwargsThese parameters will be passed totabulate.Returns:strDataFrame in Markdown-friendly format.NotesRequires thetabulatepackage.Examples>>>df=pd.DataFrame(...data={""animal_1"":[""elk"",""pig""],""animal_2"":[""dog"",""quetzal""]}...)>>>print(df.to_markdown())|    | animal_1   | animal_2   ||---:|:-----------|:-----------||  0 | elk        | dog        ||  1 | pig        | quetzal    |Output markdown with a tabulate option.>>>print(df.to_markdown(tablefmt=""grid""))+----+------------+------------+|    | animal_1   | animal_2   |+====+============+============+|  0 | elk        | dog        |+----+------------+------------+|  1 | pig        | quetzal    |+----+------------+------------+"
Pandas,DataFrame,pandas.DataFrame.style,"pandas.DataFrame.style#propertyDataFrame.style[source]#Returns a Styler object.Contains methods for building a styled HTML representation of the DataFrame.See alsoio.formats.style.StylerHelps style a DataFrame or Series according to the data with HTML and CSS.Examples>>>df=pd.DataFrame({'A':[1,2,3]})>>>df.stylePlease seeTable Visualizationfor more examples."
Pandas,DataFrame,pandas.DataFrame.__dataframe__,"pandas.DataFrame.__dataframe__#DataFrame.__dataframe__(nan_as_null=False,allow_copy=True)[source]#Return the dataframe interchange object implementing the interchange protocol.Parameters:nan_as_nullbool, default FalseWhether to tell the DataFrame to overwrite null values in the data
withNaN(orNaT).allow_copybool, default TrueWhether to allow memory copying when exporting. If set to False
it would cause non-zero-copy exports to fail.Returns:DataFrame interchange objectThe object which consuming library can use to ingress the dataframe.NotesDetails on the interchange protocol:https://data-apis.org/dataframe-protocol/latest/index.htmlnan_as_nullcurrently has no effect; once support for nullable extension
dtypes is added, this value should be propagated to columns.Examples>>>df_not_necessarily_pandas=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>interchange_object=df_not_necessarily_pandas.__dataframe__()>>>interchange_object.column_names()Index(['A', 'B'], dtype='object')>>>df_pandas=(pd.api.interchange.from_dataframe...(interchange_object.select_columns_by_name(['A'])))>>>df_pandasA0    11    2These methods (column_names,select_columns_by_name) should work
for any dataframe library which implements the interchange protocol."
