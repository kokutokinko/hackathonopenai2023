Name,Chapter,Section,Content
Pandas,DataFrame,pandas.DataFrame,"The pandas.DataFrame class is a two-dimensional, size-mutable, potentially heterogeneous tabular data structure that can contain labeled axes (rows and columns). It can be thought of as a dict-like container for Series objects.The class constructor takes several parameters:- data: This can be an ndarray, a list of dicts, a dict of Series, or a DataFrame. The data can also be in the form of an iterable or a dataclass. If data is a dict, the column order follows insertion order. If a dict contains Series with defined indices, the data is aligned based on those indices.- index: This parameter specifies the index or array-like object to use for the resulting DataFrame. If no indexing information is part of the input data and no index is provided, a default RangeIndex will be used.- columns: This parameter specifies the column labels to use for the resulting DataFrame when the data does not have them. If the data already contains column labels, column selection will be performed based on these labels.- dtype: This parameter allows you to force a specific data type for the DataFrame. Only a single data type is allowed.- copy: This parameter specifies whether to copy the data from the inputs. The default behavior depends on the type of data. For dict data, the default is copy=True, while for DataFrame or 2d ndarray input, the default is copy=False.The DataFrame class provides various methods and attributes for data manipulation, analysis, and visualization. Some important methods include:- head(): Returns the first n rows of the DataFrame.- tail(): Returns the last n rows of the DataFrame.- describe(): Generates descriptive statistics of the DataFrame.- info(): Prints a concise summary of the DataFrame.- shape: Returns a tuple representing the dimensionality of the DataFrame.- dtype: Returns the data types of the DataFrame.- columns: Returns the column labels of the DataFrame.- index: Returns the index (row labels) of the DataFrame.- dtypes: Returns the data types in the DataFrame.- values: Returns a Numpy representation of the DataFrame.The DataFrame class also provides various arithmetic and logical operations, such as addition, subtraction, multiplication, division, and comparison. These operations align on both row and column labels.The DataFrame can be constructed from various data types, including dictionaries, numpy ndarrays, dataclasses, and Series/DataFrames. The class also provides methods for reading data from files such as CSV and clipboard, as well as methods for writing data to files in formats like CSV, Excel, HDF5, Parquet, and Stata.In addition to these methods, the DataFrame class also supports indexing and slicing, grouping and aggregation, merging and joining, reshaping and pivoting, and handling missing values.Overall, the pandas DataFrame class provides a powerful tool for manipulating and analyzing tabular data, with a wide range of functionality and flexibility."
Pandas,DataFrame,pandas.DataFrame.head,"pandas.DataFrame.head#DataFrame.head(n=5)[source]#Return the firstnrows.This function returns the firstnrows for the object based
on position. It is useful for quickly testing if your object
has the right type of data in it.For negative values ofn, this function returns all rows except
the last|n|rows, equivalent todf[:n].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:same type as callerThe firstnrows of the caller object.See alsoDataFrame.tailReturns the lastnrows.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the first 5 lines>>>df.head()animal0  alligator1        bee2     falcon3       lion4     monkeyViewing the firstnlines (three in this case)>>>df.head(3)animal0  alligator1        bee2     falconFor negative values ofn>>>df.head(-3)animal0  alligator1        bee2     falcon3       lion4     monkey5     parrot"
Pandas,DataFrame,pandas.DataFrame.tail,"pandas.DataFrame.tail#DataFrame.tail(n=5)[source]#Return the lastnrows.This function returns lastnrows from the object based on
position. It is useful for quickly verifying data, for example,
after sorting or appending rows.For negative values ofn, this function returns all rows except
the first|n|rows, equivalent todf[|n|:].If n is larger than the number of rows, this function returns all rows.Parameters:nint, default 5Number of rows to select.Returns:type of callerThe lastnrows of the caller object.See alsoDataFrame.headThe firstnrows of the caller object.Examples>>>df=pd.DataFrame({'animal':['alligator','bee','falcon','lion',...'monkey','parrot','shark','whale','zebra']})>>>dfanimal0  alligator1        bee2     falcon3       lion4     monkey5     parrot6      shark7      whale8      zebraViewing the last 5 lines>>>df.tail()animal4  monkey5  parrot6   shark7   whale8   zebraViewing the lastnlines (three in this case)>>>df.tail(3)animal6  shark7  whale8  zebraFor negative values ofn>>>df.tail(-3)animal3    lion4  monkey5  parrot6   shark7   whale8   zebra"
Pandas,DataFrame,pandas.DataFrame.describe,"pandas.DataFrame.describe#DataFrame.describe(percentiles=None,include=None,exclude=None)[source]#Generate descriptive statistics.Descriptive statistics include those that summarize the central
tendency, dispersion and shape of a
datasetÅfs distribution, excludingNaNvalues.Analyzes both numeric and object series, as well
asDataFramecolumn sets of mixed data types. The output
will vary depending on what is provided. Refer to the notes
below for more detail.Parameters:percentileslist-like of numbers, optionalThe percentiles to include in the output. All should
fall between 0 and 1. The default is[.25,.5,.75], which returns the 25th, 50th, and
75th percentiles.includeÅeallÅf, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored
forSeries. Here are the options:ÅeallÅf : All columns of the input will be included in the output.A list-like of dtypes : Limits the results to the
provided data types.
To limit the result to numeric types submitnumpy.number. To limit it instead to object columns submit
thenumpy.objectdata type. Strings
can also be used in the style ofselect_dtypes(e.g.df.describe(include=['O'])). To
select pandas categorical columns, use'category'None (default) : The result will include all numeric columns.excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored
forSeries. Here are the options:A list-like of dtypes : Excludes the provided data types
from the result. To exclude numeric types submitnumpy.number. To exclude object columns submit the data
typenumpy.object. Strings can also be used in the style ofselect_dtypes(e.g.df.describe(exclude=['O'])). To
exclude pandas categorical columns, use'category'None (default) : The result will exclude nothing.Returns:Series or DataFrameSummary statistics of the Series or Dataframe provided.See alsoDataFrame.countCount number of non-NA/null observations.DataFrame.maxMaximum of the values in the object.DataFrame.minMinimum of the values in the object.DataFrame.meanMean of the values.DataFrame.stdStandard deviation of the observations.DataFrame.select_dtypesSubset of a DataFrame including/excluding columns based on their dtype.NotesFor numeric data, the resultÅfs index will includecount,mean,std,min,maxas well as lower,50and
upper percentiles. By default the lower percentile is25and the
upper percentile is75. The50percentile is the
same as the median.For object data (e.g. strings or timestamps), the resultÅfs index
will includecount,unique,top, andfreq. Thetopis the most common value. Thefreqis the most common valueÅfs
frequency. Timestamps also include thefirstandlastitems.If multiple object values have the highest count, then thecountandtopresults will be arbitrarily chosen from
among those with the highest count.For mixed data types provided via aDataFrame, the default is to
return only an analysis of numeric columns. If the dataframe consists
only of object and categorical data without any numeric columns, the
default is to return an analysis of both the object and categorical
columns. Ifinclude='all'is provided as an option, the result
will include a union of attributes of each type.Theincludeandexcludeparameters can be used to limit
which columns in aDataFrameare analyzed for the output.
The parameters are ignored when analyzing aSeries.ExamplesDescribing a numericSeries.>>>s=pd.Series([1,2,3])>>>s.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0dtype: float64Describing a categoricalSeries.>>>s=pd.Series(['a','a','b','c'])>>>s.describe()count     4unique    3top       afreq      2dtype: objectDescribing a timestampSeries.>>>s=pd.Series([...np.datetime64(""2000-01-01""),...np.datetime64(""2010-01-01""),...np.datetime64(""2010-01-01"")...])>>>s.describe()count                      3mean     2006-09-01 08:00:00min      2000-01-01 00:00:0025%      2004-12-31 12:00:0050%      2010-01-01 00:00:0075%      2010-01-01 00:00:00max      2010-01-01 00:00:00dtype: objectDescribing aDataFrame. By default only numeric fields
are returned.>>>df=pd.DataFrame({'categorical':pd.Categorical(['d','e','f']),...'numeric':[1,2,3],...'object':['a','b','c']...})>>>df.describe()numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Describing all columns of aDataFrameregardless of data type.>>>df.describe(include='all')categorical  numeric objectcount            3      3.0      3unique           3      NaN      3top              f      NaN      afreq             1      NaN      1mean           NaN      2.0    NaNstd            NaN      1.0    NaNmin            NaN      1.0    NaN25%            NaN      1.5    NaN50%            NaN      2.0    NaN75%            NaN      2.5    NaNmax            NaN      3.0    NaNDescribing a column from aDataFrameby accessing it as
an attribute.>>>df.numeric.describe()count    3.0mean     2.0std      1.0min      1.025%      1.550%      2.075%      2.5max      3.0Name: numeric, dtype: float64Including only numeric columns in aDataFramedescription.>>>df.describe(include=[np.number])numericcount      3.0mean       2.0std        1.0min        1.025%        1.550%        2.075%        2.5max        3.0Including only string columns in aDataFramedescription.>>>df.describe(include=[object])objectcount       3unique      3top         afreq        1Including only categorical columns from aDataFramedescription.>>>df.describe(include=['category'])categoricalcount            3unique           3top              dfreq             1Excluding numeric columns from aDataFramedescription.>>>df.describe(exclude=[np.number])categorical objectcount            3      3unique           3      3top              f      afreq             1      1Excluding object columns from aDataFramedescription.>>>df.describe(exclude=[object])categorical  numericcount            3      3.0unique           3      NaNtop              f      NaNfreq             1      NaNmean           NaN      2.0std            NaN      1.0min            NaN      1.025%            NaN      1.550%            NaN      2.075%            NaN      2.5max            NaN      3.0"
Pandas,DataFrame,pandas.DataFrame.info,"pandas.DataFrame.info#DataFrame.info(verbose=None,buf=None,max_cols=None,memory_usage=None,show_counts=None)[source]#Print a concise summary of a DataFrame.This method prints information about a DataFrame including
the index dtype and columns, non-null values and memory usage.Parameters:verbosebool, optionalWhether to print the full summary. By default, the setting inpandas.options.display.max_info_columnsis followed.bufwritable buffer, defaults to sys.stdoutWhere to send the output. By default, the output is printed to
sys.stdout. Pass a writable buffer if you need to further process
the output.max_colsint, optionalWhen to switch from the verbose to the truncated output. If the
DataFrame has more thanmax_colscolumns, the truncated output
is used. By default, the setting inpandas.options.display.max_info_columnsis used.memory_usagebool, str, optionalSpecifies whether total memory usage of the DataFrame
elements (including the index) should be displayed. By default,
this follows thepandas.options.display.memory_usagesetting.True always show memory usage. False never shows memory usage.
A value of ÅedeepÅf is equivalent to ÅgTrue with deep introspectionÅh.
Memory usage is shown in human-readable units (base-2
representation). Without deep introspection a memory estimation is
made based in column dtype and number of rows assuming values
consume the same memory amount for corresponding dtypes. With deep
memory introspection, a real memory usage calculation is performed
at the cost of computational resources. See theFrequently Asked Questionsfor more
details.show_countsbool, optionalWhether to show the non-null counts. By default, this is shown
only if the DataFrame is smaller thanpandas.options.display.max_info_rowsandpandas.options.display.max_info_columns. A value of True always
shows the counts, and False never shows the counts.Returns:NoneThis method prints a summary of a DataFrame and returns None.See alsoDataFrame.describeGenerate descriptive statistics of DataFrame columns.DataFrame.memory_usageMemory usage of DataFrame columns.Examples>>>int_values=[1,2,3,4,5]>>>text_values=['alpha','beta','gamma','delta','epsilon']>>>float_values=[0.0,0.25,0.5,0.75,1.0]>>>df=pd.DataFrame({""int_col"":int_values,""text_col"":text_values,...""float_col"":float_values})>>>dfint_col text_col  float_col0        1    alpha       0.001        2     beta       0.252        3    gamma       0.503        4    delta       0.754        5  epsilon       1.00Prints information of all columns:>>>df.info(verbose=True)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Data columns (total 3 columns):#   Column     Non-Null Count  Dtype---  ------     --------------  -----0   int_col    5 non-null      int641   text_col   5 non-null      object2   float_col  5 non-null      float64dtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPrints a summary of columns count and its dtypes but not per column
information:>>>df.info(verbose=False)<class 'pandas.core.frame.DataFrame'>RangeIndex: 5 entries, 0 to 4Columns: 3 entries, int_col to float_coldtypes: float64(1), int64(1), object(1)memory usage: 248.0+ bytesPipe output of DataFrame.info to buffer instead of sys.stdout, get
buffer content and writes to a text file:>>>importio>>>buffer=io.StringIO()>>>df.info(buf=buffer)>>>s=buffer.getvalue()>>>withopen(""df_info.txt"",""w"",...encoding=""utf-8"")asf:...f.write(s)260Thememory_usageparameter allows deep introspection mode, specially
useful for big DataFrames and fine-tune memory optimization:>>>random_strings_array=np.random.choice(['a','b','c'],10**6)>>>df=pd.DataFrame({...'column_1':np.random.choice(['a','b','c'],10**6),...'column_2':np.random.choice(['a','b','c'],10**6),...'column_3':np.random.choice(['a','b','c'],10**6)...})>>>df.info()<class 'pandas.core.frame.DataFrame'>RangeIndex: 1000000 entries, 0 to 999999Data columns (total 3 columns):#   Column    Non-Null Count    Dtype---  ------    --------------    -----0   column_1  1000000 non-null  object1   column_2  1000000 non-null  object2   column_3  1000000 non-null  objectdtypes: object(3)memory usage: 22.9+ MB>>>df.info(memory_usage='deep')<class 'pandas.core.frame.DataFrame'>RangeIndex: 1000000 entries, 0 to 999999Data columns (total 3 columns):#   Column    Non-Null Count    Dtype---  ------    --------------    -----0   column_1  1000000 non-null  object1   column_2  1000000 non-null  object2   column_3  1000000 non-null  objectdtypes: object(3)memory usage: 165.9 MB"
Pandas,DataFrame,pandas.DataFrame.columns,"pandas.DataFrame.columns#DataFrame.columns#The column labels of the DataFrame.Examples>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>dfA  B0    1  31    2  4>>>df.columnsIndex(['A', 'B'], dtype='object')"
Pandas,DataFrame,pandas.DataFrame.index,"pandas.DataFrame.index#DataFrame.index#The index (row labels) of the DataFrame.The index of a DataFrame is a series of labels that identify each row.
The labels can be integers, strings, or any other hashable type. The index
is used for label-based access and alignment, and can be accessed or
modified using this attribute.Returns:pandas.IndexThe index labels of the DataFrame.See alsoDataFrame.columnsThe column labels of the DataFrame.DataFrame.to_numpyConvert the DataFrame to a NumPy array.Examples>>>df=pd.DataFrame({'Name':['Alice','Bob','Aritra'],...'Age':[25,30,35],...'Location':['Seattle','New York','Kona']},...index=([10,20,30]))>>>df.indexIndex([10, 20, 30], dtype='int64')In this example, we create a DataFrame with 3 rows and 3 columns,
including Name, Age, and Location information. We set the index labels to
be the integers 10, 20, and 30. We then access theindexattribute of the
DataFrame, which returns anIndexobject containing the index labels.>>>df.index=[100,200,300]>>>dfName  Age Location100  Alice   25  Seattle200    Bob   30 New York300  Aritra  35    KonaIn this example, we modify the index labels of the DataFrame by assigning
a new list of labels to theindexattribute. The DataFrame is then
updated with the new labels, and the output shows the modified DataFrame."
Pandas,DataFrame,pandas.DataFrame.dtypes,"pandas.DataFrame.dtypes#propertyDataFrame.dtypes[source]#Return the dtypes in the DataFrame.This returns a Series with the data type of each column.
The resultÅfs index is the original DataFrameÅfs columns. Columns
with mixed types are stored with theobjectdtype. Seethe User Guidefor more.Returns:pandas.SeriesThe data type of each column.Examples>>>df=pd.DataFrame({'float':[1.0],...'int':[1],...'datetime':[pd.Timestamp('20180310')],...'string':['foo']})>>>df.dtypesfloat              float64int                  int64datetime    datetime64[ns]string              objectdtype: object"
Pandas,DataFrame,pandas.DataFrame.shape,"pandas.DataFrame.shape#propertyDataFrame.shape[source]#Return a tuple representing the dimensionality of the DataFrame.See alsondarray.shapeTuple of array dimensions.Examples>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4]})>>>df.shape(2, 2)>>>df=pd.DataFrame({'col1':[1,2],'col2':[3,4],...'col3':[5,6]})>>>df.shape(2, 3)"
Pandas,DataFrame,pandas.DataFrame.groupby,"pandas.DataFrame.groupby#DataFrame.groupby(by=None,axis=_NoDefault.no_default,level=None,as_index=True,sort=True,group_keys=True,observed=_NoDefault.no_default,dropna=True)[source]#Group DataFrame using a mapper or by a Series of columns.A groupby operation involves some combination of splitting the
object, applying a function, and combining the results. This can be
used to group large amounts of data and compute operations on these
groups.Parameters:bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby.
Ifbyis a function, itÅfs called on each value of the objectÅfs
index. If a dict or Series is passed, the Series or dict VALUES
will be used to determine the groups (the SeriesÅf values are first
aligned; see.align()method). If a list or ndarray of length
equal to the selected axis is passed (see thegroupby user guide),
the values are used as-is to determine the groups. A label or list
of labels may be passed to group by the columns inself.
Notice that a tuple is interpreted as a (single) key.axis{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}, default 0Split along rows (0) or columns (1). ForSeriesthis parameter
is unused and defaults to 0.Deprecated since version 2.1.0:Will be removed and behave like axis=0 in a future version.
Foraxis=1, doframe.T.groupby(...)instead.levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular
level or levels. Do not specify bothbyandlevel.as_indexbool, default TrueReturn object with group labels as the
index. Only relevant for DataFrame input. as_index=False is
effectively ÅgSQL-styleÅh grouped output. This argument has no effect
on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).sortbool, default TrueSort group keys. Get better performance by turning this off.
Note this does not influence the order of observations within each
group. Groupby preserves the order of rows within each group. If False,
the groups will appear in the same order as they did in the original DataFrame.
This argument has no effect on filtrations (see thefiltrations in the user guide),
such ashead(),tail(),nth()and in transformations
(see thetransformations in the user guide).Changed in version 2.0.0:Specifyingsort=Falsewith an ordered categorical grouper will no
longer sort the values.group_keysbool, default TrueWhen calling apply and thebyargument produces a like-indexed
(i.e.a transform) result, add group keys to
index to identify pieces. By default group keys are not included
when the resultÅfs index (and column) labels match the inputs, and
are included otherwise.Changed in version 1.5.0:Warns thatgroup_keyswill no longer be ignored when the
result fromapplyis a like-indexed Series or DataFrame.
Specifygroup_keysexplicitly to include the group keys or
not.Changed in version 2.0.0:group_keysnow defaults toTrue.observedbool, default FalseThis only applies if any of the groupers are Categoricals.
If True: only show observed values for categorical groupers.
If False: show all values for categorical groupers.Deprecated since version 2.1.0:The default value will change to True in a future version of pandas.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together
with row/column will be dropped.
If False, NA values will also be treated as the key in groups.Returns:pandas.api.typing.DataFrameGroupByReturns a groupby object that contains information about the groups.See alsoresampleConvenience method for frequency conversion and resampling of time series.NotesSee theuser guidefor more
detailed usage and examples, including splitting an object into groups,
iterating through groups, selecting a group, aggregation, and more.Examples>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>dfAnimal  Max Speed0  Falcon      380.01  Falcon      370.02  Parrot       24.03  Parrot       26.0>>>df.groupby(['Animal']).mean()Max SpeedAnimalFalcon      375.0Parrot       25.0Hierarchical IndexesWe can groupby different levels of a hierarchical index
using thelevelparameter:>>>arrays=[['Falcon','Falcon','Parrot','Parrot'],...['Captive','Wild','Captive','Wild']]>>>index=pd.MultiIndex.from_arrays(arrays,names=('Animal','Type'))>>>df=pd.DataFrame({'Max Speed':[390.,350.,30.,20.]},...index=index)>>>dfMax SpeedAnimal TypeFalcon Captive      390.0Wild         350.0Parrot Captive       30.0Wild          20.0>>>df.groupby(level=0).mean()Max SpeedAnimalFalcon      370.0Parrot       25.0>>>df.groupby(level=""Type"").mean()Max SpeedTypeCaptive      210.0Wild         185.0We can also choose to include NA in group keys or not by settingdropnaparameter, the default setting isTrue.>>>l=[[1,2,3],[1,None,4],[2,1,3],[1,2,2]]>>>df=pd.DataFrame(l,columns=[""a"",""b"",""c""])>>>df.groupby(by=[""b""]).sum()a   cb1.0 2   32.0 2   5>>>df.groupby(by=[""b""],dropna=False).sum()a   cb1.0 2   32.0 2   5NaN 1   4>>>l=[[""a"",12,12],[None,12.3,33.],[""b"",12.3,123],[""a"",1,1]]>>>df=pd.DataFrame(l,columns=[""a"",""b"",""c""])>>>df.groupby(by=""a"").sum()b     caa   13.0   13.0b   12.3  123.0>>>df.groupby(by=""a"",dropna=False).sum()b     caa   13.0   13.0b   12.3  123.0NaN 12.3   33.0When using.apply(), usegroup_keysto include or exclude the
group keys. Thegroup_keysargument defaults toTrue(include).>>>df=pd.DataFrame({'Animal':['Falcon','Falcon',...'Parrot','Parrot'],...'Max Speed':[380.,370.,24.,26.]})>>>df.groupby(""Animal"",group_keys=True)[['Max Speed']].apply(lambdax:x)Max SpeedAnimalFalcon 0      380.01      370.0Parrot 2       24.03       26.0>>>df.groupby(""Animal"",group_keys=False)[['Max Speed']].apply(lambdax:x)Max Speed0      380.01      370.02       24.03       26.0"
Pandas,DataFrame,pandas.DataFrame.merge,"pandas.DataFrame.merge#DataFrame.merge(right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=None,indicator=False,validate=None)[source]#Merge DataFrame or named Series objects with a database-style join.A named Series object is treated as a DataFrame with a single named column.The join is done on columns or indexes. If joining columns on
columns, the DataFrame indexeswill be ignored. Otherwise if joining indexes
on indexes or indexes on a column or columns, the index will be passed on.
When performing a cross merge, no column specifications to merge on are
allowed.WarningIf both key columns contain rows where the key is a null value, those
rows will be matched against each other. This is different from usual SQL
join behaviour and can lead to unexpected results.Parameters:rightDataFrame or named SeriesObject to merge with.how{ÅeleftÅf, ÅerightÅf, ÅeouterÅf, ÅeinnerÅf, ÅecrossÅf}, default ÅeinnerÅfType of merge to be performed.left: use only keys from left frame, similar to a SQL left outer join;
preserve key order.right: use only keys from right frame, similar to a SQL right outer join;
preserve key order.outer: use union of keys from both frames, similar to a SQL full outer
join; sort keys lexicographically.inner: use intersection of keys from both frames, similar to a SQL inner
join; preserve the order of the left keys.cross: creates the cartesian product from both frames, preserves the order
of the left keys.onlabel or listColumn or index level names to join on. These must be found in both
DataFrames. Ifonis None and not merging on indexes then this defaults
to the intersection of the columns in both DataFrames.left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also
be an array or list of arrays of the length of the left DataFrame.
These arrays are treated as if they are columns.right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also
be an array or list of arrays of the length of the right DataFrame.
These arrays are treated as if they are columns.left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a
MultiIndex, the number of keys in the other DataFrame (either the index
or a number of columns) must match the number of levels.right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as
left_index.sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False,
the order of the join keys depends on the join type (how keyword).suffixeslist-like, default is (Åg_xÅh, Åg_yÅh)A length-2 sequence where each element is optionally a string
indicating the suffix to add to overlapping column names inleftandrightrespectively. Pass a value ofNoneinstead
of a string to indicate that the column name fromleftorrightshould be left as-is, with no suffix. At least one of the
values must not be None.copybool, default TrueIf False, avoid copy if possible.NoteThecopykeyword will change behavior in pandas 3.0.Copy-on-Writewill be enabled by default, which means that all methods with acopykeyword will use a lazy copy mechanism to defer the copy and
ignore thecopykeyword. Thecopykeyword will be removed in a
future version of pandas.You can already get the future behavior and improvements through
enabling copy on writepd.options.mode.copy_on_write=Trueindicatorbool or str, default FalseIf True, adds a column to the output DataFrame called Åg_mergeÅh with
information on the source of each row. The column can be given a different
name by providing a string argument. The column will have a Categorical
type with the value of Ågleft_onlyÅh for observations whose merge key only
appears in the left DataFrame, Ågright_onlyÅh for observations
whose merge key only appears in the right DataFrame, and ÅgbothÅh
if the observationÅfs merge key is found in both DataFrames.validatestr, optionalIf specified, checks if merge is of specified type.Ågone_to_oneÅh or Åg1:1Åh: check if merge keys are unique in both
left and right datasets.Ågone_to_manyÅh or Åg1:mÅh: check if merge keys are unique in left
dataset.Ågmany_to_oneÅh or Ågm:1Åh: check if merge keys are unique in right
dataset.Ågmany_to_manyÅh or Ågm:mÅh: allowed, but does not result in checks.Returns:DataFrameA DataFrame of the two merged objects.See alsomerge_orderedMerge with optional filling/interpolation.merge_asofMerge on nearest keys.DataFrame.joinSimilar method using indices.Examples>>>df1=pd.DataFrame({'lkey':['foo','bar','baz','foo'],...'value':[1,2,3,5]})>>>df2=pd.DataFrame({'rkey':['foo','bar','baz','foo'],...'value':[5,6,7,8]})>>>df1lkey value0   foo      11   bar      22   baz      33   foo      5>>>df2rkey value0   foo      51   bar      62   baz      73   foo      8Merge df1 and df2 on the lkey and rkey columns. The value columns have
the default suffixes, _x and _y, appended.>>>df1.merge(df2,left_on='lkey',right_on='rkey')lkey  value_x rkey  value_y0  foo        1  foo        51  foo        1  foo        82  bar        2  bar        63  baz        3  baz        74  foo        5  foo        55  foo        5  foo        8Merge DataFrames df1 and df2 with specified left and right suffixes
appended to any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',...suffixes=('_left','_right'))lkey  value_left rkey  value_right0  foo           1  foo            51  foo           1  foo            82  bar           2  bar            63  baz           3  baz            74  foo           5  foo            55  foo           5  foo            8Merge DataFrames df1 and df2, but raise an exception if the DataFrames have
any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',suffixes=(False,False))Traceback (most recent call last):...ValueError:columns overlap but no suffix specified:Index(['value'], dtype='object')>>>df1=pd.DataFrame({'a':['foo','bar'],'b':[1,2]})>>>df2=pd.DataFrame({'a':['foo','baz'],'c':[3,4]})>>>df1a  b0   foo  11   bar  2>>>df2a  c0   foo  31   baz  4>>>df1.merge(df2,how='inner',on='a')a  b  c0   foo  1  3>>>df1.merge(df2,how='left',on='a')a  b  c0   foo  1  3.01   bar  2  NaN>>>df1=pd.DataFrame({'left':['foo','bar']})>>>df2=pd.DataFrame({'right':[7,8]})>>>df1left0   foo1   bar>>>df2right0   71   8>>>df1.merge(df2,how='cross')left  right0   foo      71   foo      82   bar      73   bar      8"
Pandas,DataFrame,pandas.DataFrame.sort_values,"pandas.DataFrame.sort_values#DataFrame.sort_values(by,*,axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last',ignore_index=False,key=None)[source]#Sort by the values along either axis.Parameters:bystr or list of strName or list of names to sort by.ifaxisis 0 orÅeindexÅfthenbymay contain index
levels and/or column labels.ifaxisis 1 orÅecolumnsÅfthenbymay contain column
levels and/or index labels.axisÅg{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}Åh, default 0Axis to be sorted.ascendingbool or list of bool, default TrueSort ascending vs. descending. Specify list for multiple sort
orders. If this is a list of bools, must match the length of
the by.inplacebool, default FalseIf True, perform operation in-place.kind{ÅequicksortÅf, ÅemergesortÅf, ÅeheapsortÅf, ÅestableÅf}, default ÅequicksortÅfChoice of sorting algorithm. See alsonumpy.sort()for more
information.mergesortandstableare the only stable algorithms. For
DataFrames, this option is only applied when sorting on a single
column or label.na_position{ÅefirstÅf, ÅelastÅf}, default ÅelastÅfPuts NaNs at the beginning iffirst;lastputs NaNs at the
end.ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, Åc, n - 1.keycallable, optionalApply the key function to the values
before sorting. This is similar to thekeyargument in the
builtinsorted()function, with the notable difference that
thiskeyfunction should bevectorized. It should expect aSeriesand return a Series with the same shape as the input.
It will be applied to each column inbyindependently.Returns:DataFrame or NoneDataFrame with sorted values or None ifinplace=True.See alsoDataFrame.sort_indexSort a DataFrame by the index.Series.sort_valuesSimilar method for a Series.Examples>>>df=pd.DataFrame({...'col1':['A','A','B',np.nan,'D','C'],...'col2':[2,1,9,8,7,4],...'col3':[0,1,9,4,2,3],...'col4':['a','B','c','D','e','F']...})>>>dfcol1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FSort by col1>>>df.sort_values(by=['col1'])col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort by multiple columns>>>df.sort_values(by=['col1','col2'])col1  col2  col3 col41    A     1     1    B0    A     2     0    a2    B     9     9    c5    C     4     3    F4    D     7     2    e3  NaN     8     4    DSort Descending>>>df.sort_values(by='col1',ascending=False)col1  col2  col3 col44    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    B3  NaN     8     4    DPutting NAs first>>>df.sort_values(by='col1',ascending=False,na_position='first')col1  col2  col3 col43  NaN     8     4    D4    D     7     2    e5    C     4     3    F2    B     9     9    c0    A     2     0    a1    A     1     1    BSorting with a key function>>>df.sort_values(by='col4',key=lambdacol:col.str.lower())col1  col2  col3 col40    A     2     0    a1    A     1     1    B2    B     9     9    c3  NaN     8     4    D4    D     7     2    e5    C     4     3    FNatural sort with the key argument,
using thenatsort <https://github.com/SethMMorton/natsort>package.>>>df=pd.DataFrame({...""time"":['0hr','128hr','72hr','48hr','96hr'],...""value"":[10,20,30,40,50]...})>>>dftime  value0    0hr     101  128hr     202   72hr     303   48hr     404   96hr     50>>>fromnatsortimportindex_natsorted>>>df.sort_values(...by=""time"",...key=lambdax:np.argsort(index_natsorted(df[""time""]))...)time  value0    0hr     103   48hr     402   72hr     304   96hr     501  128hr     20"
Pandas,DataFrame,pandas.DataFrame.drop,"pandas.DataFrame.drop#DataFrame.drop(labels=None,*,axis=0,index=None,columns=None,level=None,inplace=False,errors='raise')[source]#Drop specified labels from rows or columns.Remove rows or columns by specifying label names and corresponding
axis, or by directly specifying index or column names. When using a
multi-index, labels on different levels can be removed by specifying
the level. See theuser guidefor more information about the now unused levels.Parameters:labelssingle label or list-likeIndex or column labels to drop. A tuple will be used as a single
label and not treated as a list-like.axis{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}, default 0Whether to drop labels from the index (0 or ÅeindexÅf) or
columns (1 or ÅecolumnsÅf).indexsingle label or list-likeAlternative to specifying axis (labels,axis=0is equivalent toindex=labels).columnssingle label or list-likeAlternative to specifying axis (labels,axis=1is equivalent tocolumns=labels).levelint or level name, optionalFor MultiIndex, level from which the labels will be removed.inplacebool, default FalseIf False, return a copy. Otherwise, do operation
in place and return None.errors{ÅeignoreÅf, ÅeraiseÅf}, default ÅeraiseÅfIf ÅeignoreÅf, suppress error and only existing labels are
dropped.Returns:DataFrame or NoneReturns DataFrame or None DataFrame with the specified
index or column labels removed or None if inplace=True.Raises:KeyErrorIf any of the labels is not found in the selected axis.See alsoDataFrame.locLabel-location based indexer for selection by label.DataFrame.dropnaReturn DataFrame with labels on given axis omitted where (all or any) data are missing.DataFrame.drop_duplicatesReturn DataFrame with duplicate rows removed, optionally only considering certain columns.Series.dropReturn Series with specified index labels removed.Examples>>>df=pd.DataFrame(np.arange(12).reshape(3,4),...columns=['A','B','C','D'])>>>dfA  B   C   D0  0  1   2   31  4  5   6   72  8  9  10  11Drop columns>>>df.drop(['B','C'],axis=1)A   D0  0   31  4   72  8  11>>>df.drop(columns=['B','C'])A   D0  0   31  4   72  8  11Drop a row by index>>>df.drop([0,1])A  B   C   D2  8  9  10  11Drop columns and/or rows of MultiIndex DataFrame>>>midx=pd.MultiIndex(levels=[['llama','cow','falcon'],...['speed','weight','length']],...codes=[[0,0,0,1,1,1,2,2,2],...[0,1,2,0,1,2,0,1,2]])>>>df=pd.DataFrame(index=midx,columns=['big','small'],...data=[[45,30],[200,100],[1.5,1],[30,20],...[250,150],[1.5,0.8],[320,250],...[1,0.8],[0.3,0.2]])>>>dfbig     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0weight  1.0     0.8length  0.3     0.2Drop a specific index combination from the MultiIndex
DataFrame, i.e., drop the combination'falcon'and'weight', which deletes only the corresponding row>>>df.drop(index=('falcon','weight'))big     smallllama   speed   45.0    30.0weight  200.0   100.0length  1.5     1.0cow     speed   30.0    20.0weight  250.0   150.0length  1.5     0.8falcon  speed   320.0   250.0length  0.3     0.2>>>df.drop(index='cow',columns='small')bigllama   speed   45.0weight  200.0length  1.5falcon  speed   320.0weight  1.0length  0.3>>>df.drop(index='length',level=1)big     smallllama   speed   45.0    30.0weight  200.0   100.0cow     speed   30.0    20.0weight  250.0   150.0falcon  speed   320.0   250.0weight  1.0     0.8"
Pandas,DataFrame,pandas.DataFrame.isnull,"pandas.DataFrame.isnull#DataFrame.isnull()[source]#DataFrame.isnull is an alias for DataFrame.isna.Detect missing values.Return a boolean same-sized object indicating if the values are NA.
NA values, such as None ornumpy.NaN, gets mapped to True
values.
Everything else gets mapped to False values. Characters such as empty
strings''ornumpy.infare not considered NA values
(unless you setpandas.options.mode.use_inf_as_na=True).Returns:DataFrameMask of bool values for each element in DataFrame that
indicates whether an element is an NA value.See alsoDataFrame.isnullAlias of isna.DataFrame.notnaBoolean inverse of isna.DataFrame.dropnaOmit axes labels with missing values.isnaTop-level isna.ExamplesShow which entries in a DataFrame are NA.>>>df=pd.DataFrame(dict(age=[5,6,np.nan],...born=[pd.NaT,pd.Timestamp('1939-05-27'),...pd.Timestamp('1940-04-25')],...name=['Alfred','Batman',''],...toy=[None,'Batmobile','Joker']))>>>dfage       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker>>>df.isna()age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  FalseShow which entries in a Series are NA.>>>ser=pd.Series([5,6,np.nan])>>>ser0    5.01    6.02    NaNdtype: float64>>>ser.isna()0    False1    False2     Truedtype: bool"
Pandas,DataFrame,pandas.DataFrame.fillna,"pandas.DataFrame.fillna#DataFrame.fillna(value=None,*,method=None,axis=None,inplace=False,limit=None,downcast=_NoDefault.no_default)[source]#Fill NA/NaN values using the specified method.Parameters:valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a
dict/Series/DataFrame of values specifying which value to use for
each index (for a Series) or column (for a DataFrame). Values not
in the dict/Series/DataFrame will not be filled. This value cannot
be a list.method{ÅebackfillÅf, ÅebfillÅf, ÅeffillÅf, None}, default NoneMethod to use for filling holes in reindexed Series:ffill: propagate last valid observation forward to next valid.backfill / bfill: use next valid observation to fill gap.Deprecated since version 2.1.0:Use ffill or bfill instead.axis{0 or ÅeindexÅf} for Series, {0 or ÅeindexÅf, 1 or ÅecolumnsÅf} for DataFrameAxis along which to fill missing values. ForSeriesthis parameter is unused and defaults to 0.inplacebool, default FalseIf True, fill in-place. Note: this will modify any
other views on this object (e.g., a no-copy slice for a column in a
DataFrame).limitint, default NoneIf method is specified, this is the maximum number of consecutive
NaN values to forward/backward fill. In other words, if there is
a gap with more than this number of consecutive NaNs, it will only
be partially filled. If method is not specified, this is the
maximum number of entries along the entire axis where NaNs will be
filled. Must be greater than 0 if not None.downcastdict, default is NoneA dict of item->dtype of what to downcast if possible,
or the string ÅeinferÅf which will try to downcast to an appropriate
equal type (e.g. float64 to int64 if possible).Deprecated since version 2.2.0.Returns:Series/DataFrame or NoneObject with missing values filled or None ifinplace=True.See alsoffillFill values by propagating the last valid observation to next valid.bfillFill values by using the next valid observation to fill the gap.interpolateFill NaN values using interpolation.reindexConform object to new index.asfreqConvert TimeSeries to specified frequency.Examples>>>df=pd.DataFrame([[np.nan,2,np.nan,0],...[3,4,np.nan,1],...[np.nan,np.nan,np.nan,np.nan],...[np.nan,3,np.nan,4]],...columns=list(""ABCD""))>>>dfA    B   C    D0  NaN  2.0 NaN  0.01  3.0  4.0 NaN  1.02  NaN  NaN NaN  NaN3  NaN  3.0 NaN  4.0Replace all NaN elements with 0s.>>>df.fillna(0)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  0.03  0.0  3.0  0.0  4.0Replace all NaN elements in column ÅeAÅf, ÅeBÅf, ÅeCÅf, and ÅeDÅf, with 0, 1,
2, and 3 respectively.>>>values={""A"":0,""B"":1,""C"":2,""D"":3}>>>df.fillna(value=values)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  2.0  1.02  0.0  1.0  2.0  3.03  0.0  3.0  2.0  4.0Only replace the first NaN element.>>>df.fillna(value=values,limit=1)A    B    C    D0  0.0  2.0  2.0  0.01  3.0  4.0  NaN  1.02  NaN  1.0  NaN  3.03  NaN  3.0  NaN  4.0When filling using a DataFrame, replacement happens along
the same column names and same indices>>>df2=pd.DataFrame(np.zeros((4,4)),columns=list(""ABCE""))>>>df.fillna(df2)A    B    C    D0  0.0  2.0  0.0  0.01  3.0  4.0  0.0  1.02  0.0  0.0  0.0  NaN3  0.0  3.0  0.0  4.0Note that column D is not affected since it is not present in df2."
Pandas,DataFrame,pandas.DataFrame.to_csv,"pandas.DataFrame.to_csv#DataFrame.to_csv(path_or_buf=None,*,sep=',',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,mode='w',encoding=None,compression='infer',quoting=None,quotechar='""',lineterminator=None,chunksize=None,date_format=None,doublequote=True,escapechar=None,decimal='.',errors='strict',storage_options=None)[source]#Write object to a comma-separated values (csv) file.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string. If a non-binary file object is passed, it should
be opened withnewline=ÅfÅf, disabling universal newlines. If a binary
file object is passed,modemight need to contain aÅebÅf.sepstr, default Åe,ÅfString of length 1. Field delimiter for the output file.na_repstr, default ÅeÅfMissing data representation.float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes
precedence over other numeric formatting parameters, like decimal.columnssequence, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, andheaderandindexare True, then the index names are used. A
sequence should be given if the object uses MultiIndex. If
False do not print fields for index names. Use index_label=False
for easier importing in R.mode{ÅewÅf, ÅexÅf, ÅeaÅf}, default ÅewÅfForwarded to eitheropen(mode=)orfsspec.open(mode=)to control
the file opening. Typical values include:ÅewÅf, truncate the file first.ÅexÅf, exclusive creation, failing if the file already exists.ÅeaÅf, append to the end of file if it exists.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to Åeutf-8Åf.encodingis not supported ifpath_or_bufis a non-binary file object.compressionstr or dict, default ÅeinferÅfFor on-the-fly compression of the output data. If ÅeinferÅf and Åepath_or_bufÅf is
path-like, then detect compression from the following extensions: Åe.gzÅf,
Åe.bz2Åf, Åe.zipÅf, Åe.xzÅf, Åe.zstÅf, Åe.tarÅf, Åe.tar.gzÅf, Åe.tar.xzÅf or Åe.tar.bz2Åf
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key ÅemethodÅf as compression mode
and other entries as additional compression options if
compression mode is ÅezipÅf.Passing compression options as keys in dict is
supported for compression modes ÅegzipÅf, Åebz2Åf, ÅezstdÅf, and ÅezipÅf.quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set afloat_formatthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC
will treat them as non-numeric.quotecharstr, default Åe""ÅfString of length 1. Character used to quote fields.lineterminatorstr, optionalThe newline character or character sequence to use in the output
file. Defaults toos.linesep, which depends on the OS in which
this method is called (Åf\nÅf for linux, Åe\r\nÅf for Windows, i.e.).Changed in version 1.5.0:Previously was line_terminator, changed for consistency with
read_csv and the standard library ÅecsvÅf module.chunksizeint or NoneRows to write at a time.date_formatstr, default NoneFormat string for datetime objects.doublequotebool, default TrueControl quoting ofquotecharinside a field.escapecharstr, default NoneString of length 1. Character used to escapesepandquotecharwhen appropriate.decimalstr, default Åe.ÅfCharacter recognized as decimal separator. E.g. use Åe,Åf for
European data.errorsstr, default ÅestrictÅfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.Returns:None or strIf path_or_buf is None, returns the resulting csv format as a
string. Otherwise returns None.See alsoread_csvLoad a CSV file into a DataFrame.to_excelWrite DataFrame to an Excel file.ExamplesCreate Åeout.csvÅf containing ÅedfÅf without indices>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv('out.csv',index=False)Create Åeout.zipÅf containing Åeout.csvÅf>>>df.to_csv(index=False)'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first
need to create it using either Pathlib or os:>>>frompathlibimportPath>>>filepath=Path('folder/subfolder/out.csv')>>>filepath.parent.mkdir(parents=True,exist_ok=True)>>>df.to_csv(filepath)>>>importos>>>os.makedirs('folder/subfolder',exist_ok=True)>>>df.to_csv('folder/subfolder/out.csv')"
Pandas,DataFrame,pandas.DataFrame.to_excel,"pandas.DataFrame.to_excel#DataFrame.to_excel(excel_writer,*,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to
specify a target file name. To write to multiple sheets it is necessary to
create anExcelWriterobject with a target file name, and specify a sheet
in the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.
With all data written to the file it is necessary to save the changes.
Note that creating anExcelWriterobject with a file name that already
exists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default ÅeSheet1ÅfName of sheet which will contain DataFrame.na_repstr, default ÅeÅfMissing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=""%.2f""will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A
sequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, ÅeopenpyxlÅf or ÅexlsxwriterÅf. You can also set this
via the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default ÅeinfÅfRepresentation for infinity (there is no native representation for
infinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that
is to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),
to_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further
data without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(""output.xlsx"")To specify the sheet name:>>>df1.to_excel(""output.xlsx"",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is
necessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,
you can pass theenginekeyword (the default engine is
automatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')"
Pandas,DataFrame,pandas.DataFrame.to_json,"pandas.DataFrame.to_json#DataFrame.to_json(path_or_buf=None,*,orient=None,date_format=None,double_precision=10,force_ascii=True,date_unit='ms',default_handler=None,lines=False,compression='infer',index=None,indent=None,storage_options=None,mode='w')[source]#Convert the object to a JSON string.Note NaNÅfs and None will be converted to null and datetime objects
will be converted to UNIX timestamps.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string.orientstrIndication of expected JSON string format.Series:default is ÅeindexÅfallowed values are: {ÅesplitÅf, ÅerecordsÅf, ÅeindexÅf, ÅetableÅf}.DataFrame:default is ÅecolumnsÅfallowed values are: {ÅesplitÅf, ÅerecordsÅf, ÅeindexÅf, ÅecolumnsÅf,
ÅevaluesÅf, ÅetableÅf}.The format of the JSON string:ÅesplitÅf : dict like {ÅeindexÅf -> [index], ÅecolumnsÅf -> [columns],
ÅedataÅf -> [values]}ÅerecordsÅf : list like [{column -> value}, Åc , {column -> value}]ÅeindexÅf : dict like {index -> {column -> value}}ÅecolumnsÅf : dict like {column -> {index -> value}}ÅevaluesÅf : just the values arrayÅetableÅf : dict like {ÅeschemaÅf: {schema}, ÅedataÅf: {data}}Describing the data, where data component is likeorient='records'.date_format{None, ÅeepochÅf, ÅeisoÅf}Type of date conversion. ÅeepochÅf = epoch milliseconds,
ÅeisoÅf = ISO8601. The default depends on theorient. Fororient='table', the default is ÅeisoÅf. For all other orients,
the default is ÅeepochÅf.double_precisionint, default 10The number of decimal places to use when encoding
floating point values. The possible maximal value is 15.
Passing double_precision greater than 15 will raise a ValueError.force_asciibool, default TrueForce encoded string to be ASCII.date_unitstr, default ÅemsÅf (milliseconds)The time unit to encode to, governs timestamp and ISO8601
precision. One of ÅesÅf, ÅemsÅf, ÅeusÅf, ÅensÅf for second, millisecond,
microsecond, and nanosecond respectively.default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a
suitable format for JSON. Should receive a single argument which is
the object to convert and return a serialisable object.linesbool, default FalseIf ÅeorientÅf is ÅerecordsÅf write out line-delimited json format. Will
throw ValueError if incorrect ÅeorientÅf since others are not
list-like.compressionstr or dict, default ÅeinferÅfFor on-the-fly compression of the output data. If ÅeinferÅf and Åepath_or_bufÅf is
path-like, then detect compression from the following extensions: Åe.gzÅf,
Åe.bz2Åf, Åe.zipÅf, Åe.xzÅf, Åe.zstÅf, Åe.tarÅf, Åe.tar.gzÅf, Åe.tar.xzÅf or Åe.tar.bz2Åf
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.indexbool or None, default NoneThe index is only used when ÅeorientÅf is ÅesplitÅf, ÅeindexÅf, ÅecolumnÅf,
or ÅetableÅf. Of these, ÅeindexÅf and ÅecolumnÅf do not supportindex=False.indentint, optionalLength of whitespace used to indent each record.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.modestr, default ÅewÅf (writing)Specify the IO mode for output when supplying a path_or_buf.
Accepted args are ÅewÅf (writing) and ÅeaÅf (append) only.
mode=ÅfaÅf is only supported when lines is True and orient is ÅerecordsÅf.Returns:None or strIf path_or_buf is None, returns the resulting json format as a
string. Otherwise returns None.See alsoread_jsonConvert a JSON string to pandas object.NotesThe behavior ofindent=0varies from the stdlib, which does not
indent the output but does insert newlines. Currently,indent=0and the defaultindent=Noneare equivalent in pandas, though this
may change in a future release.orient='table'contains a Åepandas_versionÅf field under ÅeschemaÅf.
This stores the version ofpandasused in the latest revision of the
schema.Examples>>>fromjsonimportloads,dumps>>>df=pd.DataFrame(...[[""a"",""b""],[""c"",""d""]],...index=[""row 1"",""row 2""],...columns=[""col 1"",""col 2""],...)>>>result=df.to_json(orient=""split"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""columns"": [""col 1"",""col 2""],""index"": [""row 1"",""row 2""],""data"": [[""a"",""b""],[""c"",""d""]]}Encoding/decoding a Dataframe using'records'formatted JSON.
Note that index labels are not preserved with this encoding.>>>result=df.to_json(orient=""records"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[{""col 1"": ""a"",""col 2"": ""b""},{""col 1"": ""c"",""col 2"": ""d""}]Encoding/decoding a Dataframe using'index'formatted JSON:>>>result=df.to_json(orient=""index"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""row 1"": {""col 1"": ""a"",""col 2"": ""b""},""row 2"": {""col 1"": ""c"",""col 2"": ""d""}}Encoding/decoding a Dataframe using'columns'formatted JSON:>>>result=df.to_json(orient=""columns"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""col 1"": {""row 1"": ""a"",""row 2"": ""c""},""col 2"": {""row 1"": ""b"",""row 2"": ""d""}}Encoding/decoding a Dataframe using'values'formatted JSON:>>>result=df.to_json(orient=""values"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[[""a"",""b""],[""c"",""d""]]Encoding with Table Schema:>>>result=df.to_json(orient=""table"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""schema"": {""fields"": [{""name"": ""index"",""type"": ""string""},{""name"": ""col 1"",""type"": ""string""},{""name"": ""col 2"",""type"": ""string""}],""primaryKey"": [""index""],""pandas_version"": ""1.4.0""},""data"": [{""index"": ""row 1"",""col 1"": ""a"",""col 2"": ""b""},{""index"": ""row 2"",""col 1"": ""c"",""col 2"": ""d""}]}"
Pandas,DataFrame,pandas.DataFrame.to_sql,"pandas.DataFrame.to_sql#DataFrame.to_sql(name,con,*,schema=None,if_exists='fail',index=True,index_label=None,chunksize=None,dtype=None,method=None)[source]#Write records stored in a DataFrame to a SQL database.Databases supported by SQLAlchemy[1]are supported. Tables can be
newly created, appended to, or overwritten.Parameters:namestrName of SQL table.consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. Legacy support is provided for sqlite3.Connection objects. The user
is responsible for engine disposal and connection closure for the SQLAlchemy
connectable. Seehere.
If passing a sqlalchemy.engine.Connection which is already in a transaction,
the transaction will not be committed. If passing a sqlite3.Connection,
it will not be possible to roll back the record insertion.schemastr, optionalSpecify the schema (if database flavor supports this). If None, use
default schema.if_exists{ÅefailÅf, ÅereplaceÅf, ÅeappendÅf}, default ÅefailÅfHow to behave if the table already exists.fail: Raise a ValueError.replace: Drop the table before inserting new values.append: Insert new values to the existing table.indexbool, default TrueWrite DataFrame index as a column. Usesindex_labelas the column
name in the table. Creates a table index for this column.index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) andindexis True, then the index names are used.
A sequence should be given if the DataFrame uses MultiIndex.chunksizeint, optionalSpecify the number of rows in each batch to be written at a time.
By default, all rows will be written at once.dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the
keys should be the column names and the values should be the
SQLAlchemy types or strings for the sqlite3 legacy mode. If a
scalar is provided, it will be applied to all columns.method{None, ÅemultiÅf, callable}, optionalControls the SQL insertion clause used:None : Uses standard SQLINSERTclause (one per row).ÅemultiÅf: Pass multiple values in a singleINSERTclause.callable with signature(pd_table,conn,keys,data_iter).Details and a sample callable implementation can be found in the
sectioninsert method.Returns:None or intNumber of rows affected by to_sql. None is returned if the callable
passed intomethoddoes not return an integer number of rows.The number of returned rows affected is the sum of therowcountattribute ofsqlite3.Cursoror SQLAlchemy connectable which may not
reflect the exact number of written rows as stipulated in thesqlite3orSQLAlchemy.New in version 1.4.0.Raises:ValueErrorWhen the table already exists andif_existsis ÅefailÅf (the
default).See alsoread_sqlRead a DataFrame from a table.NotesTimezone aware datetime columns will be written asTimestampwithtimezonetype with SQLAlchemy if supported by the
database. Otherwise, the datetimes will be stored as timezone unaware
timestamps local to the original timezone.References[1]https://docs.sqlalchemy.org[2]https://www.python.org/dev/peps/pep-0249/ExamplesCreate an in-memory SQLite database.>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine('sqlite://',echo=False)Create a table from scratch with 3 rows.>>>df=pd.DataFrame({'name':['User 1','User 2','User 3']})>>>dfname0  User 11  User 22  User 3>>>df.to_sql(name='users',con=engine)3>>>fromsqlalchemyimporttext>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]Ansqlalchemy.engine.Connectioncan also be passed tocon:>>>withengine.begin()asconnection:...df1=pd.DataFrame({'name':['User 4','User 5']})...df1.to_sql(name='users',con=connection,if_exists='append')2This is allowed to support operations that require that the same
DBAPI connection is used for the entire operation.>>>df2=pd.DataFrame({'name':['User 6','User 7']})>>>df2.to_sql(name='users',con=engine,if_exists='append')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),(0, 'User 4'), (1, 'User 5'), (0, 'User 6'),(1, 'User 7')]Overwrite the table with justdf2.>>>df2.to_sql(name='users',con=engine,if_exists='replace',...index_label='id')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 6'), (1, 'User 7')]Usemethodto define a callable insertion method to do nothing
if thereÅfs a primary key conflict on a table in a PostgreSQL database.>>>fromsqlalchemy.dialects.postgresqlimportinsert>>>definsert_on_conflict_nothing(table,conn,keys,data_iter):...# ""a"" is the primary key in ""conflict_table""...data=[dict(zip(keys,row))forrowindata_iter]...stmt=insert(table.table).values(data).on_conflict_do_nothing(index_elements=[""a""])...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_nothing)0For MySQL, a callable to update columnsbandcif thereÅfs a conflict
on a primary key.>>>fromsqlalchemy.dialects.mysqlimportinsert>>>definsert_on_conflict_update(table,conn,keys,data_iter):...# update columns ""b"" and ""c"" on primary key conflict...data=[dict(zip(keys,row))forrowindata_iter]...stmt=(...insert(table.table)....values(data)...)...stmt=stmt.on_duplicate_key_update(b=stmt.inserted.b,c=stmt.inserted.c)...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_update)2Specify the dtype (especially useful for integers with missing values).
Notice that while pandas is forced to store the data as floating point,
the database supports nullable integers. When fetching the data with
Python, we get back integer scalars.>>>df=pd.DataFrame({""A"":[1,None,2]})>>>dfA0  1.01  NaN2  2.0>>>fromsqlalchemy.typesimportInteger>>>df.to_sql(name='integers',con=engine,index=False,...dtype={""A"":Integer()})3>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM integers"")).fetchall()[(1,), (None,), (2,)]"
Pandas,DataFrame,pandas.DataFrame.to_dict,"pandas.DataFrame.to_dict#DataFrame.to_dict(orient='dict',*,into=<class'dict'>,index=True)[source]#Convert the DataFrame to a dictionary.The type of the key-value pairs can be customized with the parameters
(see below).Parameters:orientstr {ÅedictÅf, ÅelistÅf, ÅeseriesÅf, ÅesplitÅf, ÅetightÅf, ÅerecordsÅf, ÅeindexÅf}Determines the type of the values of the dictionary.ÅedictÅf (default) : dict like {column -> {index -> value}}ÅelistÅf : dict like {column -> [values]}ÅeseriesÅf : dict like {column -> Series(values)}ÅesplitÅf : dict like
{ÅeindexÅf -> [index], ÅecolumnsÅf -> [columns], ÅedataÅf -> [values]}ÅetightÅf : dict like
{ÅeindexÅf -> [index], ÅecolumnsÅf -> [columns], ÅedataÅf -> [values],
Åeindex_namesÅf -> [index.names], Åecolumn_namesÅf -> [column.names]}ÅerecordsÅf : list like
[{column -> value}, Åc , {column -> value}]ÅeindexÅf : dict like {index -> {column -> value}}New in version 1.4.0:ÅetightÅf as an allowed value for theorientargumentintoclass, default dictThe collections.abc.MutableMapping subclass used for all Mappings
in the return value. Can be the actual class or an empty
instance of the mapping type you want. If you want a
collections.defaultdict, you must pass it initialized.indexbool, default TrueWhether to include the index item (and index_names item iforientis ÅetightÅf) in the returned dictionary. Can only beFalsewhenorientis ÅesplitÅf or ÅetightÅf.New in version 2.0.0.Returns:dict, list or collections.abc.MutableMappingReturn a collections.abc.MutableMapping object representing the
DataFrame. The resulting transformation depends on theorientparameter.See alsoDataFrame.from_dictCreate a DataFrame from a dictionary.DataFrame.to_jsonConvert a DataFrame to JSON format.Examples>>>df=pd.DataFrame({'col1':[1,2],...'col2':[0.5,0.75]},...index=['row1','row2'])>>>dfcol1  col2row1     1  0.50row2     2  0.75>>>df.to_dict(){'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}You can specify the return orientation.>>>df.to_dict('series'){'col1': row1    1row2    2Name: col1, dtype: int64,'col2': row1    0.50row2    0.75Name: col2, dtype: float64}>>>df.to_dict('split'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]]}>>>df.to_dict('records')[{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]>>>df.to_dict('index'){'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}>>>df.to_dict('tight'){'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}You can also specify the mapping type.>>>fromcollectionsimportOrderedDict,defaultdict>>>df.to_dict(into=OrderedDict)OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])If you want adefaultdict, you need to initialize it:>>>dd=defaultdict(list)>>>df.to_dict('records',into=dd)[defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]"
Pandas,DataFrame,pandas.DataFrame.to_string,"pandas.DataFrame.to_string#DataFrame.to_string(buf=None,*,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',line_width=None,min_rows=None,max_colwidth=None,encoding=None)[source]#Render a DataFrame to a console-friendly tabular output.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spaceint, list or dict of int, optionalThe minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..headerbool or list of str, optionalWrite out the column names. If a list of columns is given, it is assumed to be aliases for the column names.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default ÅeNaNÅfString representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columnsÅf elements by position or
name.
The result of each function must be a unicode string.
List/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columnsÅf elements if they are
floats. This function must return a unicode string and will be
applied only to the non-NaNelements, withNaNbeing
handled byna_rep.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from
the print configuration (controlled by set_option), ÅerightÅf out
of the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default Åe.ÅfCharacter recognized as decimal separator, e.g. Åe,Åf in Europe.line_widthint, optionalWidth to wrap a line in characters.min_rowsint, optionalThe number of rows to display in the console in a truncated repr
(when number of rows is abovemax_rows).max_colwidthint, optionalMax width to truncate each column in characters. By default, no limit.encodingstr, default Ågutf-8ÅhSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns
None.See alsoto_htmlConvert DataFrame to HTML.Examples>>>d={'col1':[1,2,3],'col2':[4,5,6]}>>>df=pd.DataFrame(d)>>>print(df.to_string())col1  col20     1     41     2     52     3     6"
Pandas,Plotting,pandas.plotting.andrews_curves,"pandas.plotting.andrews_curves#pandas.plotting.andrews_curves(frame,class_column,ax=None,samples=200,color=None,colormap=None,**kwargs)[source]#Generate a matplotlib plot for visualizing clusters of multivariate data.Andrews curves have the functional form:\[f(t) = \frac{x_1}{\sqrt{2}} + x_2 \sin(t) + x_3 \cos(t) +
x_4 \sin(2t) + x_5 \cos(2t) + \cdots\]Where\(x\)coefficients correspond to the values of each dimension
and\(t\)is linearly spaced between\(-\pi\)and\(+\pi\).
Each row of frame then corresponds to a single curve.Parameters:frameDataFrameData to be plotted, preferably normalized to (0.0, 1.0).class_columnlabelName of the column containing class names.axaxes object, default NoneAxes to use.samplesintNumber of points to plot in each curve.colorstr, list[str] or tuple[str], optionalColors to use for the different classes. Colors can be strings
or 3-element floating point RGB values.colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If a string, load colormap with that
name from matplotlib.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.AxesExamples>>>df=pd.read_csv(...'https://raw.githubusercontent.com/pandas-dev/'...'pandas/main/pandas/tests/io/data/csv/iris.csv'...)>>>pd.plotting.andrews_curves(df,'Name')"
Pandas,Plotting,pandas.plotting.autocorrelation_plot,"pandas.plotting.autocorrelation_plot#pandas.plotting.autocorrelation_plot(series,ax=None,**kwargs)[source]#Autocorrelation plot for time series.Parameters:seriesSeriesThe time series to visualize.axMatplotlib axis object, optionalThe matplotlib axis object to use.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.AxesExamplesThe horizontal lines in the plot correspond to 95% and 99% confidence bands.The dashed line is 99% confidence band.>>>spacing=np.linspace(-9*np.pi,9*np.pi,num=1000)>>>s=pd.Series(0.7*np.random.rand(1000)+0.3*np.sin(spacing))>>>pd.plotting.autocorrelation_plot(s)"
Pandas,Plotting,pandas.plotting.bootstrap_plot,"pandas.plotting.bootstrap_plot#pandas.plotting.bootstrap_plot(series,fig=None,size=50,samples=500,**kwds)[source]#Bootstrap plot on mean, median and mid-range statistics.The bootstrap plot is used to estimate the uncertainty of a statistic
by relying on random sampling with replacement[1]. This function will
generate bootstrapping plots for mean, median and mid-range statistics
for the given number of samples of the given size.[1]ÅgBootstrapping (statistics)Åh inhttps://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29Parameters:seriespandas.SeriesSeries from where to get the samplings for the bootstrapping.figmatplotlib.figure.Figure, default NoneIf given, it will use thefigreference for plotting instead of
creating a new one with default parameters.sizeint, default 50Number of data points to consider during each sampling. It must be
less than or equal to the length of theseries.samplesint, default 500Number of times the bootstrap procedure is performed.**kwdsOptions to pass to matplotlib plotting method.Returns:matplotlib.figure.FigureMatplotlib figure.See alsopandas.DataFrame.plotBasic plotting for DataFrame objects.pandas.Series.plotBasic plotting for Series objects.ExamplesThis example draws a basic bootstrap plot for a Series.>>>s=pd.Series(np.random.uniform(size=100))>>>pd.plotting.bootstrap_plot(s)<Figure size 640x480 with 6 Axes>"
Pandas,Plotting,pandas.plotting.boxplot,"pandas.plotting.boxplot#pandas.plotting.boxplot(data,column=None,by=None,ax=None,fontsize=None,rot=0,grid=True,figsize=None,layout=None,return_type=None,**kwargs)[source]#Make a box plot from DataFrame columns.Make a box-and-whisker plot from DataFrame columns, optionally grouped
by some other columns. A box plot is a method for graphically depicting
groups of numerical data through their quartiles.
The box extends from the Q1 to Q3 quartile values of the data,
with a line at the median (Q2). The whiskers extend from the edges
of box to show the range of the data. By default, they extend no more than1.5 * IQR (IQR = Q3 - Q1)from the edges of the box, ending at the farthest
data point within that interval. Outliers are plotted as separate dots.For further details see
WikipediaÅfs entry forboxplot.Parameters:dataDataFrameThe data to visualize.columnstr or list of str, optionalColumn name or list of names, or vector.
Can be any valid input topandas.DataFrame.groupby().bystr or array-like, optionalColumn in the DataFrame topandas.DataFrame.groupby().
One box-plot will be done per value of columns inby.axobject of class matplotlib.axes.Axes, optionalThe matplotlib axes to be used by boxplot.fontsizefloat or strTick label font size in points or as a string (e.g.,large).rotfloat, default 0The rotation angle of labels (in degrees)
with respect to the screen coordinate system.gridbool, default TrueSetting this to True will show the grid.figsizeA tuple (width, height) in inchesThe size of the figure to create in matplotlib.layouttuple (rows, columns), optionalFor example, (3, 5) will display the subplots
using 3 rows and 5 columns, starting from the top-left.return_type{ÅeaxesÅf, ÅedictÅf, ÅebothÅf} or None, default ÅeaxesÅfThe kind of object to return. The default isaxes.ÅeaxesÅf returns the matplotlib axes the boxplot is drawn on.ÅedictÅf returns a dictionary whose values are the matplotlib
Lines of the boxplot.ÅebothÅf returns a namedtuple with the axes and dict.when grouping withby, a Series mapping columns toreturn_typeis returned.Ifreturn_typeisNone, a NumPy array
of axes with the same shape aslayoutis returned.**kwargsAll other plotting keyword arguments to be passed tomatplotlib.pyplot.boxplot().Returns:resultSee Notes.See alsopandas.Series.plot.histMake a histogram.matplotlib.pyplot.boxplotMatplotlib equivalent plot.NotesThe return type depends on thereturn_typeparameter:ÅeaxesÅf : object of class matplotlib.axes.AxesÅedictÅf : dict of matplotlib.lines.Line2D objectsÅebothÅf : a namedtuple with structure (ax, lines)For data grouped withby, return a Series of the above or a numpy
array:Seriesarray(forreturn_type=None)Usereturn_type='dict'when you want to tweak the appearance
of the lines after plotting. In this case a dict containing the Lines
making up the boxes, caps, fliers, medians, and whiskers is returned.ExamplesBoxplots can be created for every column in the dataframe
bydf.boxplot()or indicating the columns to be used:>>>np.random.seed(1234)>>>df=pd.DataFrame(np.random.randn(10,4),...columns=['Col1','Col2','Col3','Col4'])>>>boxplot=df.boxplot(column=['Col1','Col2','Col3'])Boxplots of variables distributions grouped by the values of a third
variable can be created using the optionby. For instance:>>>df=pd.DataFrame(np.random.randn(10,2),...columns=['Col1','Col2'])>>>df['X']=pd.Series(['A','A','A','A','A',...'B','B','B','B','B'])>>>boxplot=df.boxplot(by='X')A list of strings (i.e.['X','Y']) can be passed to boxplot
in order to group the data by combination of the variables in the x-axis:>>>df=pd.DataFrame(np.random.randn(10,3),...columns=['Col1','Col2','Col3'])>>>df['X']=pd.Series(['A','A','A','A','A',...'B','B','B','B','B'])>>>df['Y']=pd.Series(['A','B','A','B','A',...'B','A','B','A','B'])>>>boxplot=df.boxplot(column=['Col1','Col2'],by=['X','Y'])The layout of boxplot can be adjusted giving a tuple tolayout:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...layout=(2,1))Additional formatting can be done to the boxplot, like suppressing the grid
(grid=False), rotating the labels in the x-axis (i.e.rot=45)
or changing the fontsize (i.e.fontsize=15):>>>boxplot=df.boxplot(grid=False,rot=45,fontsize=15)The parameterreturn_typecan be used to select the type of element
returned byboxplot. Whenreturn_type='axes'is selected,
the matplotlib axes on which the boxplot is drawn are returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],return_type='axes')>>>type(boxplot)<class 'matplotlib.axes._axes.Axes'>When grouping withby, a Series mapping columns toreturn_typeis returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...return_type='axes')>>>type(boxplot)<class 'pandas.core.series.Series'>Ifreturn_typeisNone, a NumPy array of axes with the same shape
aslayoutis returned:>>>boxplot=df.boxplot(column=['Col1','Col2'],by='X',...return_type=None)>>>type(boxplot)<class 'numpy.ndarray'>"
Pandas,Plotting,pandas.plotting.deregister_matplotlib_converters,"pandas.plotting.deregister_matplotlib_converters#pandas.plotting.deregister_matplotlib_converters()[source]#Remove pandas formatters and converters.Removes the custom converters added byregister(). This
attempts to set the state of the registry back to the state before
pandas registered its own units. Converters for pandasÅf own types like
Timestamp and Period are removed completely. Converters for types
pandas overwrites, likedatetime.datetime, are restored to their
original value.See alsoregister_matplotlib_convertersRegister pandas formatters and converters with matplotlib.ExamplesThe following line is done automatically by pandas so
the plot can be rendered:>>>pd.plotting.register_matplotlib_converters()>>>df=pd.DataFrame({'ts':pd.period_range('2020',periods=2,freq='M'),...'y':[1,2]...})>>>plot=df.plot.line(x='ts',y='y')Unsetting the register manually an error will be raised:>>>pd.set_option(""plotting.matplotlib.register_converters"",...False)>>>df.plot.line(x='ts',y='y')Traceback (most recent call last):TypeError:float() argument must be a string or a real number, not 'Period'"
Pandas,Plotting,pandas.plotting.lag_plot,"pandas.plotting.lag_plot#pandas.plotting.lag_plot(series,lag=1,ax=None,**kwds)[source]#Lag plot for time series.Parameters:seriesSeriesThe time series to visualize.lagint, default 1Lag length of the scatter plot.axMatplotlib axis object, optionalThe matplotlib axis object to use.**kwdsMatplotlib scatter method keyword arguments.Returns:matplotlib.axes.AxesExamplesLag plots are most commonly used to look for patterns in time series data.Given the following time series>>>np.random.seed(5)>>>x=np.cumsum(np.random.normal(loc=1,scale=5,size=50))>>>s=pd.Series(x)>>>s.plot()A lag plot withlag=1returns>>>pd.plotting.lag_plot(s,lag=1)<Axes: xlabel='y(t)', ylabel='y(t + 1)'>"
Pandas,Plotting,pandas.plotting.parallel_coordinates,"pandas.plotting.parallel_coordinates#pandas.plotting.parallel_coordinates(frame,class_column,cols=None,ax=None,color=None,use_columns=False,xticks=None,colormap=None,axvlines=True,axvlines_kwds=None,sort_labels=False,**kwargs)[source]#Parallel coordinates plotting.Parameters:frameDataFrameclass_columnstrColumn name containing class names.colslist, optionalA list of column names to use.axmatplotlib.axis, optionalMatplotlib axis object.colorlist or tuple, optionalColors to use for the different classes.use_columnsbool, optionalIf true, columns will be used as xticks.xtickslist or tuple, optionalA list of values to use for xticks.colormapstr or matplotlib colormap, default NoneColormap to use for line colors.axvlinesbool, optionalIf true, vertical lines will be added at each xtick.axvlines_kwdskeywords, optionalOptions to be passed to axvline method for vertical lines.sort_labelsbool, default FalseSort class_column labels, useful when assigning colors.**kwargsOptions to pass to matplotlib plotting method.Returns:matplotlib.axes.AxesExamples>>>df=pd.read_csv(...'https://raw.githubusercontent.com/pandas-dev/'...'pandas/main/pandas/tests/io/data/csv/iris.csv'...)>>>pd.plotting.parallel_coordinates(...df,'Name',color=('#556270','#4ECDC4','#C7F464')...)"
Pandas,Plotting,pandas.plotting.plot_params,"pandas.plotting.plot_params#pandas.plotting.plot_params={'xaxis.compat':False}#Stores pandas plotting options.Allows for parameter aliasing so you can just use parameter names that are
the same as the plot function parameters, but is stored in a canonical
format that makes it easy to breakdown into groups later.Examples>>>np.random.seed(42)>>>df=pd.DataFrame({'A':np.random.randn(10),...'B':np.random.randn(10)},...index=pd.date_range(""1/1/2000"",...freq='4MS',periods=10))>>>withpd.plotting.plot_params.use(""x_compat"",True):..._=df[""A""].plot(color=""r"")..._=df[""B""].plot(color=""g"")"
Pandas,Plotting,pandas.plotting.radviz,"pandas.plotting.radviz#pandas.plotting.radviz(frame,class_column,ax=None,color=None,colormap=None,**kwds)[source]#Plot a multidimensional dataset in 2D.Each Series in the DataFrame is represented as a evenly distributed
slice on a circle. Each data point is rendered in the circle according to
the value on each Series. Highly correlatedSeriesin theDataFrameare placed closer on the unit circle.RadViz allow to project a N-dimensional data set into a 2D space where the
influence of each dimension can be interpreted as a balance between the
influence of all dimensions.More info available at theoriginal articledescribing RadViz.Parameters:frameDataFrameObject holding the data.class_columnstrColumn name containing the name of the data point category.axmatplotlib.axes.Axes, optionalA plot instance to which to add the information.colorlist[str] or tuple[str], optionalAssign a color to each category. Example: [ÅeblueÅf, ÅegreenÅf].colormapstr ormatplotlib.colors.Colormap, default NoneColormap to select colors from. If string, load colormap with that
name from matplotlib.**kwdsOptions to pass to matplotlib scatter plotting method.Returns:matplotlib.axes.AxesSee alsopandas.plotting.andrews_curvesPlot clustering visualization.Examples>>>df=pd.DataFrame(...{...'SepalLength':[6.5,7.7,5.1,5.8,7.6,5.0,5.4,4.6,6.7,4.6],...'SepalWidth':[3.0,3.8,3.8,2.7,3.0,2.3,3.0,3.2,3.3,3.6],...'PetalLength':[5.5,6.7,1.9,5.1,6.6,3.3,4.5,1.4,5.7,1.0],...'PetalWidth':[1.8,2.2,0.4,1.9,2.1,1.0,1.5,0.2,2.1,0.2],...'Category':[...'virginica',...'virginica',...'setosa',...'virginica',...'virginica',...'versicolor',...'versicolor',...'setosa',...'virginica',...'setosa'...]...}...)>>>pd.plotting.radviz(df,'Category')"
Pandas,Plotting,pandas.plotting.register_matplotlib_converters,"pandas.plotting.register_matplotlib_converters#pandas.plotting.register_matplotlib_converters()[source]#Register pandas formatters and converters with matplotlib.This function modifies the globalmatplotlib.units.registrydictionary. pandas adds custom converters forpd.Timestamppd.Periodnp.datetime64datetime.datetimedatetime.datedatetime.timeSee alsoderegister_matplotlib_convertersRemove pandas formatters and converters.ExamplesThe following line is done automatically by pandas so
the plot can be rendered:>>>pd.plotting.register_matplotlib_converters()>>>df=pd.DataFrame({'ts':pd.period_range('2020',periods=2,freq='M'),...'y':[1,2]...})>>>plot=df.plot.line(x='ts',y='y')Unsetting the register manually an error will be raised:>>>pd.set_option(""plotting.matplotlib.register_converters"",...False)>>>df.plot.line(x='ts',y='y')Traceback (most recent call last):TypeError:float() argument must be a string or a real number, not 'Period'"
Pandas,Plotting,pandas.plotting.scatter_matrix,"pandas.plotting.scatter_matrix#pandas.plotting.scatter_matrix(frame,alpha=0.5,figsize=None,ax=None,grid=False,diagonal='hist',marker='.',density_kwds=None,hist_kwds=None,range_padding=0.05,**kwargs)[source]#Draw a matrix of scatter plots.Parameters:frameDataFramealphafloat, optionalAmount of transparency applied.figsize(float,float), optionalA tuple (width, height) in inches.axMatplotlib axis object, optionalgridbool, optionalSetting this to True will show the grid.diagonal{ÅehistÅf, ÅekdeÅf}Pick between ÅekdeÅf and ÅehistÅf for either Kernel Density Estimation or
Histogram plot in the diagonal.markerstr, optionalMatplotlib marker type, default Åe.Åf.density_kwdskeywordsKeyword arguments to be passed to kernel density estimate plot.hist_kwdskeywordsKeyword arguments to be passed to hist function.range_paddingfloat, default 0.05Relative extension of axis range in x and y with respect to
(x_max - x_min) or (y_max - y_min).**kwargsKeyword arguments to be passed to scatter function.Returns:numpy.ndarrayA matrix of scatter plots.Examples>>>df=pd.DataFrame(np.random.randn(1000,4),columns=['A','B','C','D'])>>>pd.plotting.scatter_matrix(df,alpha=0.2)array([[<Axes: xlabel='A', ylabel='A'>, <Axes: xlabel='B', ylabel='A'>,<Axes: xlabel='C', ylabel='A'>, <Axes: xlabel='D', ylabel='A'>],[<Axes: xlabel='A', ylabel='B'>, <Axes: xlabel='B', ylabel='B'>,<Axes: xlabel='C', ylabel='B'>, <Axes: xlabel='D', ylabel='B'>],[<Axes: xlabel='A', ylabel='C'>, <Axes: xlabel='B', ylabel='C'>,<Axes: xlabel='C', ylabel='C'>, <Axes: xlabel='D', ylabel='C'>],[<Axes: xlabel='A', ylabel='D'>, <Axes: xlabel='B', ylabel='D'>,<Axes: xlabel='C', ylabel='D'>, <Axes: xlabel='D', ylabel='D'>]],dtype=object)"
Pandas,Plotting,pandas.plotting.table,"pandas.plotting.table#pandas.plotting.table(ax,data,**kwargs)[source]#Helper function to convert DataFrame and Series to matplotlib.table.Parameters:axMatplotlib axes objectdataDataFrame or SeriesData for table contents.**kwargsKeyword arguments to be passed to matplotlib.table.table.
IfrowLabelsorcolLabelsis not specified, data index or column
name will be used.Returns:matplotlib table objectExamples>>>importmatplotlib.pyplotasplt>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>fix,ax=plt.subplots()>>>ax.axis('off')(0.0, 1.0, 0.0, 1.0)>>>table=pd.plotting.table(ax,df,loc='center',...cellLoc='center',colWidths=list([.2,.2]))"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.groups,"pandas.core.groupby.DataFrameGroupBy.groups#propertyDataFrameGroupBy.groups[source]#Dict {group name -> group labels}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).groups{'a': ['a', 'a'], 'b': ['b']}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""])>>>dfa  b  c0  1  2  31  1  5  62  7  8  9>>>df.groupby(by=[""a""]).groups{1: [0, 1], 7: [2]}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').groups{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.groups,"pandas.core.groupby.SeriesGroupBy.groups#propertySeriesGroupBy.groups[source]#Dict {group name -> group labels}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).groups{'a': ['a', 'a'], 'b': ['b']}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""])>>>dfa  b  c0  1  2  31  1  5  62  7  8  9>>>df.groupby(by=[""a""]).groups{1: [0, 1], 7: [2]}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').groups{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.indices,"pandas.core.groupby.DataFrameGroupBy.indices#propertyDataFrameGroupBy.indices[source]#Dict {group name -> group indices}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).indices{'a': array([0, 1]), 'b': array([2])}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""owl"",""toucan"",""eagle""])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[""a""]).indices{1: array([0, 1]), 7: array([2])}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').indicesdefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],Timestamp('2023-02-01 00:00:00'): [2, 3]})"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.indices,"pandas.core.groupby.SeriesGroupBy.indices#propertySeriesGroupBy.indices[source]#Dict {group name -> group indices}.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).indices{'a': array([0, 1]), 'b': array([2])}For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""owl"",""toucan"",""eagle""])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[""a""]).indices{1: array([0, 1]), 7: array([2])}For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').indicesdefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],Timestamp('2023-02-01 00:00:00'): [2, 3]})"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.get_group,"pandas.core.groupby.DataFrameGroupBy.get_group#DataFrameGroupBy.get_group(name,obj=None)[source]#Construct DataFrame from group with provided name.Parameters:nameobjectThe name of the group to get as a DataFrame.objDataFrame, default NoneThe DataFrame to take the DataFrame out of. If
it is None, the object groupby was called on will
be used.Deprecated since version 2.1.0:The obj is deprecated and will be removed in a future version.
Dodf.iloc[gb.indices.get(name)]instead ofgb.get_group(name,obj=df).Returns:same type as objExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).get_group(""a"")a    1a    2dtype: int64For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""owl"",""toucan"",""eagle""])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[""a""]).get_group((1,))a  b  cowl     1  2  3toucan  1  5  6For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').get_group('2023-01-01')2023-01-01    12023-01-15    2dtype: int64"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.get_group,"pandas.core.groupby.SeriesGroupBy.get_group#SeriesGroupBy.get_group(name,obj=None)[source]#Construct DataFrame from group with provided name.Parameters:nameobjectThe name of the group to get as a DataFrame.objDataFrame, default NoneThe DataFrame to take the DataFrame out of. If
it is None, the object groupby was called on will
be used.Deprecated since version 2.1.0:The obj is deprecated and will be removed in a future version.
Dodf.iloc[gb.indices.get(name)]instead ofgb.get_group(name,obj=df).Returns:same type as objExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,3],index=lst)>>>sera    1a    2b    3dtype: int64>>>ser.groupby(level=0).get_group(""a"")a    1a    2dtype: int64For DataFrameGroupBy:>>>data=[[1,2,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""owl"",""toucan"",""eagle""])>>>dfa  b  cowl     1  2  3toucan  1  5  6eagle   7  8  9>>>df.groupby(by=[""a""]).get_group((1,))a  b  cowl     1  2  3toucan  1  5  6For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').get_group('2023-01-01')2023-01-01    12023-01-15    2dtype: int64"
Pandas,GroupBy,pandas.Grouper,"pandas.Grouper#classpandas.Grouper(*args,**kwargs)[source]#A Grouper allows the user to specify a groupby instruction for an object.This specification will select a column via the key parameter, or if the
level and/or axis parameters are given, a level of the index of the target
object.Ifaxisand/orlevelare passed as keywords to bothGrouperandgroupby, the values passed toGroupertake precedence.Parameters:keystr, defaults to NoneGroupby key, which selects the grouping column of the target.levelname/number, defaults to NoneThe level for the target index.freqstr / frequency object, defaults to NoneThis will groupby the specified frequency if the target selection
(via key or level) is a datetime-like object. For full specification
of available frequencies, please seehere.axisstr, int, defaults to 0Number/name of the axis.sortbool, default to FalseWhether to sort the resulting labels.closed{ÅeleftÅf or ÅerightÅf}Closed end of interval. Only whenfreqparameter is passed.label{ÅeleftÅf or ÅerightÅf}Interval boundary to use for labeling.
Only whenfreqparameter is passed.convention{ÅestartÅf, ÅeendÅf, ÅeeÅf, ÅesÅf}If grouper is PeriodIndex andfreqparameter is passed.originTimestamp or str, default Åestart_dayÅfThe timestamp on which to adjust the grouping. The timezone of origin must
match the timezone of the index.
If string, must be one of the following:ÅeepochÅf:originis 1970-01-01ÅestartÅf:originis the first value of the timeseriesÅestart_dayÅf:originis the first day at midnight of the timeseriesÅeendÅf:originis the last value of the timeseriesÅeend_dayÅf:originis the ceiling midnight of the last dayNew in version 1.3.0.offsetTimedelta or str, default is NoneAn offset timedelta added to the origin.dropnabool, default TrueIf True, and if group keys contain NA values, NA values together with
row/column will be dropped. If False, NA values will also be treated as
the key in groups.Returns:Grouper or pandas.api.typing.TimeGrouperA TimeGrouper is returned iffreqis notNone. Otherwise, a Grouper
is returned.Examplesdf.groupby(pd.Grouper(key=""Animal""))is equivalent todf.groupby('Animal')>>>df=pd.DataFrame(...{...""Animal"":[""Falcon"",""Parrot"",""Falcon"",""Falcon"",""Parrot""],...""Speed"":[100,5,200,300,15],...}...)>>>dfAnimal  Speed0  Falcon    1001  Parrot      52  Falcon    2003  Falcon    3004  Parrot     15>>>df.groupby(pd.Grouper(key=""Animal"")).mean()SpeedAnimalFalcon  200.0Parrot   10.0Specify a resample operation on the column ÅePublish dateÅf>>>df=pd.DataFrame(...{...""Publish date"":[...pd.Timestamp(""2000-01-02""),...pd.Timestamp(""2000-01-02""),...pd.Timestamp(""2000-01-09""),...pd.Timestamp(""2000-01-16"")...],...""ID"":[0,1,2,3],...""Price"":[10,20,30,40]...}...)>>>dfPublish date  ID  Price0   2000-01-02   0     101   2000-01-02   1     202   2000-01-09   2     303   2000-01-16   3     40>>>df.groupby(pd.Grouper(key=""Publish date"",freq=""1W"")).mean()ID  PricePublish date2000-01-02    0.5   15.02000-01-09    2.0   30.02000-01-16    3.0   40.0If you want to adjust the start of the bins based on a fixed timestamp:>>>start,end='2000-10-01 23:30:00','2000-10-02 00:30:00'>>>rng=pd.date_range(start,end,freq='7min')>>>ts=pd.Series(np.arange(len(rng))*3,index=rng)>>>ts2000-10-01 23:30:00     02000-10-01 23:37:00     32000-10-01 23:44:00     62000-10-01 23:51:00     92000-10-01 23:58:00    122000-10-02 00:05:00    152000-10-02 00:12:00    182000-10-02 00:19:00    212000-10-02 00:26:00    24Freq: 7min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min')).sum()2000-10-01 23:14:00     02000-10-01 23:31:00     92000-10-01 23:48:00    212000-10-02 00:05:00    542000-10-02 00:22:00    24Freq: 17min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min',origin='epoch')).sum()2000-10-01 23:18:00     02000-10-01 23:35:00    182000-10-01 23:52:00    272000-10-02 00:09:00    392000-10-02 00:26:00    24Freq: 17min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min',origin='2000-01-01')).sum()2000-10-01 23:24:00     32000-10-01 23:41:00    152000-10-01 23:58:00    452000-10-02 00:15:00    45Freq: 17min, dtype: int64If you want to adjust the start of the bins with anoffsetTimedelta, the two
following lines are equivalent:>>>ts.groupby(pd.Grouper(freq='17min',origin='start')).sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17min, dtype: int64>>>ts.groupby(pd.Grouper(freq='17min',offset='23h30min')).sum()2000-10-01 23:30:00     92000-10-01 23:47:00    212000-10-02 00:04:00    542000-10-02 00:21:00    24Freq: 17min, dtype: int64To replace the use of the deprecatedbaseargument, you can now useoffset,
in this example it is equivalent to havebase=2:>>>ts.groupby(pd.Grouper(freq='17min',offset='2min')).sum()2000-10-01 23:16:00     02000-10-01 23:33:00     92000-10-01 23:50:00    362000-10-02 00:07:00    392000-10-02 00:24:00    24Freq: 17min, dtype: int64"
Pandas,GroupBy,pandas.NamedAgg,"pandas.NamedAgg#classpandas.NamedAgg(column,aggfunc)[source]#Helper for column specific aggregation with control over output column names.Subclass of typing.NamedTuple.Parameters:columnHashableColumn label in the DataFrame to apply aggfunc.aggfuncfunction or strFunction to apply to the provided column. If string, the name of a built-in
pandas function.Examples>>>df=pd.DataFrame({""key"":[1,1,2],""a"":[-1,0,1],1:[10,11,12]})>>>agg_a=pd.NamedAgg(column=""a"",aggfunc=""min"")>>>agg_1=pd.NamedAgg(column=1,aggfunc=lambdax:np.mean(x))>>>df.groupby(""key"").agg(result_a=agg_a,result_1=agg_1)result_a  result_1key1          -1      10.52           1      12.0AttributesaggfuncAlias for field number 1columnAlias for field number 0Methodscount(value,/)Return number of occurrences of value.index(value[,start,stop])Return first index of value."
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.apply,"pandas.core.groupby.SeriesGroupBy.apply#SeriesGroupBy.apply(func,*args,**kwargs)[source]#Apply functionfuncgroup-wise and combine the results together.The function passed toapplymust take a series as its first
argument and return a DataFrame, Series or scalar.applywill
then take care of combining the results back together into a single
dataframe or series.applyis therefore a highly flexible
grouping method.Whileapplyis a very flexible method, its downside is that
using it can be quite a bit slower than using more specific methods
likeaggortransform. Pandas offers a wide range of method that will
be much faster than usingapplyfor their specific purposes, so try to
use them before reaching forapply.Parameters:funccallableA callable that takes a series as its first argument, and
returns a dataframe, a series or a scalar. In addition the
callable may take positional and keyword arguments.include_groupsbool, default TrueWhen True, will attempt to applyfuncto the groupings in
the case that they are columns of the DataFrame. If this raises a
TypeError, the result will be computed with the groupings excluded.
When False, the groupings will be excluded when applyingfunc.New in version 2.2.0.Deprecated since version 2.2.0:Setting include_groups to True is deprecated. Only the value
False will be allowed in a future version of pandas.args, kwargstuple and dictOptional positional and keyword arguments to pass tofunc.Returns:Series or DataFrameSee alsopipeApply function to the full GroupBy object instead of to each group.aggregateApply aggregate function to the GroupBy object.transformApply function column-by-column to the GroupBy object.Series.applyApply a function to a Series.DataFrame.applyApply a function to each row or column of a DataFrame.NotesChanged in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>s=pd.Series([0,1,2],index='a a b'.split())>>>g1=s.groupby(s.index,group_keys=False)>>>g2=s.groupby(s.index,group_keys=True)Fromsabove we can see thatghas two groups,aandb.
Notice thatg1haveg2have two groups,aandb, and only
differ in theirgroup_keysargument. Callingapplyin various ways,
we can get different grouping results:Example 1: The function passed toapplytakes a Series as
its argument and returns a Series.applycombines the result for
each group together into a new Series.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc.>>>g1.apply(lambdax:x*2ifx.name=='a'elsex/2)a    0.0a    2.0b    1.0dtype: float64In the above, the groups are not part of the index. We can have them included
by usingg2wheregroup_keys=True:>>>g2.apply(lambdax:x*2ifx.name=='a'elsex/2)a  a    0.0a    2.0b  b    1.0dtype: float64Example 2: The function passed toapplytakes a Series as
its argument and returns a scalar.applycombines the result for
each group together into a Series, including setting the index as
appropriate:>>>g1.apply(lambdax:x.max()-x.min())a    1b    0dtype: int64Thegroup_keysargument has no effect here because the result is not
like-indexed (i.e.a transform) when compared
to the input.>>>g2.apply(lambdax:x.max()-x.min())a    1b    0dtype: int64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.apply,"pandas.core.groupby.DataFrameGroupBy.apply#DataFrameGroupBy.apply(func,*args,include_groups=True,**kwargs)[source]#Apply functionfuncgroup-wise and combine the results together.The function passed toapplymust take a dataframe as its first
argument and return a DataFrame, Series or scalar.applywill
then take care of combining the results back together into a single
dataframe or series.applyis therefore a highly flexible
grouping method.Whileapplyis a very flexible method, its downside is that
using it can be quite a bit slower than using more specific methods
likeaggortransform. Pandas offers a wide range of method that will
be much faster than usingapplyfor their specific purposes, so try to
use them before reaching forapply.Parameters:funccallableA callable that takes a dataframe as its first argument, and
returns a dataframe, a series or a scalar. In addition the
callable may take positional and keyword arguments.include_groupsbool, default TrueWhen True, will attempt to applyfuncto the groupings in
the case that they are columns of the DataFrame. If this raises a
TypeError, the result will be computed with the groupings excluded.
When False, the groupings will be excluded when applyingfunc.New in version 2.2.0.Deprecated since version 2.2.0:Setting include_groups to True is deprecated. Only the value
False will be allowed in a future version of pandas.args, kwargstuple and dictOptional positional and keyword arguments to pass tofunc.Returns:Series or DataFrameSee alsopipeApply function to the full GroupBy object instead of to each group.aggregateApply aggregate function to the GroupBy object.transformApply function column-by-column to the GroupBy object.Series.applyApply a function to a Series.DataFrame.applyApply a function to each row or column of a DataFrame.NotesChanged in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':'a a b'.split(),...'B':[1,2,3],...'C':[4,6,5]})>>>g1=df.groupby('A',group_keys=False)>>>g2=df.groupby('A',group_keys=True)Notice thatg1andg2have two groups,aandb, and only
differ in theirgroup_keysargument. Callingapplyin various ways,
we can get different grouping results:Example 1: below the function passed toapplytakes a DataFrame as
its argument and returns a DataFrame.applycombines the result for
each group together into a new DataFrame:>>>g1[['B','C']].apply(lambdax:x/x.sum())B    C0  0.333333  0.41  0.666667  0.62  1.000000  1.0In the above, the groups are not part of the index. We can have them included
by usingg2wheregroup_keys=True:>>>g2[['B','C']].apply(lambdax:x/x.sum())B    CAa 0  0.333333  0.41  0.666667  0.6b 2  1.000000  1.0Example 2: The function passed toapplytakes a DataFrame as
its argument and returns a Series.applycombines the result for
each group together into a new DataFrame.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc.>>>g1[['B','C']].apply(lambdax:x.astype(float).max()-x.min())B    CAa  1.0  2.0b  0.0  0.0>>>g2[['B','C']].apply(lambdax:x.astype(float).max()-x.min())B    CAa  1.0  2.0b  0.0  0.0Thegroup_keysargument has no effect here because the result is not
like-indexed (i.e.a transform) when compared
to the input.Example 3: The function passed toapplytakes a DataFrame as
its argument and returns a scalar.applycombines the result for
each group together into a Series, including setting the index as
appropriate:>>>g1.apply(lambdax:x.C.max()-x.B.min(),include_groups=False)Aa    5b    2dtype: int64"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.agg,"pandas.core.groupby.SeriesGroupBy.agg#SeriesGroupBy.agg(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either
work when passed a Series or when passed to Series.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']None, in which case**kwargsare used with Named Aggregation. Here the
output has one column for each element in**kwargs. The name of the
column is keyword, whereas the value determines the aggregation used to compute
the values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported
with this engine.If the'numba'engine is chosen, the function must be
a user defined function withvaluesandindexas the
first and second arguments respectively in the function signature.
Each groupÅfs index will be passed to the user defined function
and optionally available for use.Deprecated since version 2.1.0:Passing a dictionary is deprecated and will raise in a future version
of pandas. Pass a list of aggregations instead.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to the function**kwargsIffuncis None,**kwargsare used to define the output names and
aggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:SeriesSee alsoSeries.groupby.applyApply function func group-wise and combine the results together.Series.groupby.transformTransforms the Series on each group based on the given function.Series.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no Ågfall backÅh behavior internally.
The group data and group index will be passed as numpy arrays to the JITed
user defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.groupby([1,1,2,2]).min()1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg('min')1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg(['min','max'])min  max1    1    22    3    4The output column names can be controlled by passing
the desired column names and aggregations as keyword arguments.>>>s.groupby([1,1,2,2]).agg(...minimum='min',...maximum='max',...)minimum  maximum1        1        22        3        4Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>s.groupby([1,1,2,2]).agg(lambdax:x.astype(float).min())1    1.02    3.0dtype: float64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.agg,"pandas.core.groupby.DataFrameGroupBy.agg#DataFrameGroupBy.agg(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either
work when passed a DataFrame or when passed to DataFrame.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.None, in which case**kwargsare used with Named Aggregation. Here the
output has one column for each element in**kwargs. The name of the
column is keyword, whereas the value determines the aggregation used to compute
the values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported
with this engine.If the'numba'engine is chosen, the function must be
a user defined function withvaluesandindexas the
first and second arguments respectively in the function signature.
Each groupÅfs index will be passed to the user defined function
and optionally available for use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to the function**kwargsIffuncis None,**kwargsare used to define the output names and
aggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:DataFrameSee alsoDataFrame.groupby.applyApply function func group-wise and combine the results together.DataFrame.groupby.transformTransforms the Series on each group based on the given function.DataFrame.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no Ågfall backÅh behavior internally.
The group data and group index will be passed as numpy arrays to the JITed
user defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Examples>>>data={""A"":[1,1,2,2],...""B"":[1,2,3,4],...""C"":[0.362838,0.227877,1.267767,-0.562860]}>>>df=pd.DataFrame(data)>>>dfA  B         C0  1  1  0.3628381  1  2  0.2278772  2  3  1.2677673  2  4 -0.562860The aggregation is for each column.>>>df.groupby('A').agg('min')B         CA1  1  0.2278772  3 -0.562860Multiple aggregations>>>df.groupby('A').agg(['min','max'])B             Cmin max       min       maxA1   1   2  0.227877  0.3628382   3   4 -0.562860  1.267767Select a column for aggregation>>>df.groupby('A').B.agg(['min','max'])min  maxA1    1    22    3    4User-defined function for aggregation>>>df.groupby('A').agg(lambdax:sum(x)+2)B          CA1       5       2.5907152       9       2.704907Different aggregations per column>>>df.groupby('A').agg({'B':['min','max'],'C':'sum'})B             Cmin max       sumA1   1   2  0.5907152   3   4  0.704907To control the output names with different aggregations per column,
pandas supports Ågnamed aggregationÅh>>>df.groupby(""A"").agg(...b_min=pd.NamedAgg(column=""B"",aggfunc=""min""),...c_sum=pd.NamedAgg(column=""C"",aggfunc=""sum"")...)b_min     c_sumA1      1  0.5907152      3  0.704907The keywords are theoutputcolumn namesThe values are tuples whose first element is the column to select
and the second element is the aggregation to apply to that column.
Pandas provides thepandas.NamedAggnamedtuple with the fields['column','aggfunc']to make it clearer what the arguments are.
As usual, the aggregation can be a callable or a string alias.SeeNamed aggregationfor more.Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>df.groupby(""A"")[[""B""]].agg(lambdax:x.astype(float).min())BA1   1.02   3.0"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.aggregate,"pandas.core.groupby.SeriesGroupBy.aggregate#SeriesGroupBy.aggregate(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either
work when passed a Series or when passed to Series.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']None, in which case**kwargsare used with Named Aggregation. Here the
output has one column for each element in**kwargs. The name of the
column is keyword, whereas the value determines the aggregation used to compute
the values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported
with this engine.If the'numba'engine is chosen, the function must be
a user defined function withvaluesandindexas the
first and second arguments respectively in the function signature.
Each groupÅfs index will be passed to the user defined function
and optionally available for use.Deprecated since version 2.1.0:Passing a dictionary is deprecated and will raise in a future version
of pandas. Pass a list of aggregations instead.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to the function**kwargsIffuncis None,**kwargsare used to define the output names and
aggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:SeriesSee alsoSeries.groupby.applyApply function func group-wise and combine the results together.Series.groupby.transformTransforms the Series on each group based on the given function.Series.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no Ågfall backÅh behavior internally.
The group data and group index will be passed as numpy arrays to the JITed
user defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Examples>>>s=pd.Series([1,2,3,4])>>>s0    11    22    33    4dtype: int64>>>s.groupby([1,1,2,2]).min()1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg('min')1    12    3dtype: int64>>>s.groupby([1,1,2,2]).agg(['min','max'])min  max1    1    22    3    4The output column names can be controlled by passing
the desired column names and aggregations as keyword arguments.>>>s.groupby([1,1,2,2]).agg(...minimum='min',...maximum='max',...)minimum  maximum1        1        22        3        4Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>s.groupby([1,1,2,2]).agg(lambdax:x.astype(float).min())1    1.02    3.0dtype: float64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.aggregate,"pandas.core.groupby.DataFrameGroupBy.aggregate#DataFrameGroupBy.aggregate(func=None,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Aggregate using one or more operations over the specified axis.Parameters:funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either
work when passed a DataFrame or when passed to DataFrame.apply.Accepted combinations are:functionstring function namelist of functions and/or function names, e.g.[np.sum,'mean']dict of axis labels -> functions, function names or list of such.None, in which case**kwargsare used with Named Aggregation. Here the
output has one column for each element in**kwargs. The name of the
column is keyword, whereas the value determines the aggregation used to compute
the values in the column.Can also accept a Numba JIT function withengine='numba'specified. Only passing a single function is supported
with this engine.If the'numba'engine is chosen, the function must be
a user defined function withvaluesandindexas the
first and second arguments respectively in the function signature.
Each groupÅfs index will be passed to the user defined function
and optionally available for use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or globally settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to the function**kwargsIffuncis None,**kwargsare used to define the output names and
aggregations via Named Aggregation. Seefuncentry.Otherwise, keyword arguments to be passed into func.Returns:DataFrameSee alsoDataFrame.groupby.applyApply function func group-wise and combine the results together.DataFrame.groupby.transformTransforms the Series on each group based on the given function.DataFrame.aggregateAggregate using one or more operations over the specified axis.NotesWhen usingengine='numba', there will be no Ågfall backÅh behavior internally.
The group data and group index will be passed as numpy arrays to the JITed
user defined function, and no alternative execution attempts will be tried.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Examples>>>data={""A"":[1,1,2,2],...""B"":[1,2,3,4],...""C"":[0.362838,0.227877,1.267767,-0.562860]}>>>df=pd.DataFrame(data)>>>dfA  B         C0  1  1  0.3628381  1  2  0.2278772  2  3  1.2677673  2  4 -0.562860The aggregation is for each column.>>>df.groupby('A').agg('min')B         CA1  1  0.2278772  3 -0.562860Multiple aggregations>>>df.groupby('A').agg(['min','max'])B             Cmin max       min       maxA1   1   2  0.227877  0.3628382   3   4 -0.562860  1.267767Select a column for aggregation>>>df.groupby('A').B.agg(['min','max'])min  maxA1    1    22    3    4User-defined function for aggregation>>>df.groupby('A').agg(lambdax:sum(x)+2)B          CA1       5       2.5907152       9       2.704907Different aggregations per column>>>df.groupby('A').agg({'B':['min','max'],'C':'sum'})B             Cmin max       sumA1   1   2  0.5907152   3   4  0.704907To control the output names with different aggregations per column,
pandas supports Ågnamed aggregationÅh>>>df.groupby(""A"").agg(...b_min=pd.NamedAgg(column=""B"",aggfunc=""min""),...c_sum=pd.NamedAgg(column=""C"",aggfunc=""sum"")...)b_min     c_sumA1      1  0.5907152      3  0.704907The keywords are theoutputcolumn namesThe values are tuples whose first element is the column to select
and the second element is the aggregation to apply to that column.
Pandas provides thepandas.NamedAggnamedtuple with the fields['column','aggfunc']to make it clearer what the arguments are.
As usual, the aggregation can be a callable or a string alias.SeeNamed aggregationfor more.Changed in version 1.3.0:The resulting dtype will reflect the return value of the aggregating function.>>>df.groupby(""A"")[[""B""]].agg(lambdax:x.astype(float).min())BA1   1.02   3.0"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.transform,"pandas.core.groupby.SeriesGroupBy.transform#SeriesGroupBy.transform(func,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Call function producing a same-indexed Series on each group.Returns a Series having the same indexes as the original object
filled with the transformed values.Parameters:ffunction, strFunction to apply to each group. See the Notes section below for requirements.Accepted inputs are:StringPython functionNumba JIT function withengine='numba'specified.Only passing a single function is supported with this engine.
If the'numba'engine is chosen, the function must be
a user defined function withvaluesandindexas the
first and second arguments respectively in the function signature.
Each groupÅfs index will be passed to the user defined function
and optionally available for use.If a string is chosen, then it needs to be the name
of the groupby method you want to use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or the global settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to the function**kwargsKeyword arguments to be passed into func.Returns:SeriesSee alsoSeries.groupby.applyApply functionfuncgroup-wise and combine the results together.Series.groupby.aggregateAggregate using one or more operations over the specified axis.Series.transformCallfuncon self producing a Series with the same axis shape as self.NotesEach group is endowed the attribute ÅenameÅf in case you need to know
which group you are working on.The current implementation imposes three requirements on f:f must return a value that either has the same shape as the input
subframe or can be broadcast to the shape of the input subframe.
For example, iffreturns a scalar it will be broadcast to have the
same shape as the input subframe.if this is a DataFrame, f must support application column-by-column
in the subframe. If f also supports application to the entire subframe,
then a fast path is used starting from the second chunk.f must not mutate groups. Mutation is not supported and may
produce unexpected results. SeeMutating with User Defined Function (UDF) methodsfor more details.When usingengine='numba', there will be no Ågfall backÅh behavior internally.
The group data and group index will be passed as numpy arrays to the JITed
user defined function, and no alternative execution attempts will be tried.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Changed in version 2.0.0:When using.transformon a grouped DataFrame and the transformation function
returns a DataFrame, pandas now aligns the resultÅfs index
with the inputÅfs index. You can call.to_numpy()on the
result of the transformation function to avoid alignment.Examples>>>ser=pd.Series([390.0,350.0,30.0,20.0],...index=[""Falcon"",""Falcon"",""Parrot"",""Parrot""],...name=""Max Speed"")>>>grouped=ser.groupby([1,1,2,2])>>>grouped.transform(lambdax:(x-x.mean())/x.std())Falcon    0.707107Falcon   -0.707107Parrot    0.707107Parrot   -0.707107Name: Max Speed, dtype: float64Broadcast result of the transformation>>>grouped.transform(lambdax:x.max()-x.min())Falcon    40.0Falcon    40.0Parrot    10.0Parrot    10.0Name: Max Speed, dtype: float64>>>grouped.transform(""mean"")Falcon    370.0Falcon    370.0Parrot     25.0Parrot     25.0Name: Max Speed, dtype: float64Changed in version 1.3.0.The resulting dtype will reflect the return value of the passedfunc,
for example:>>>grouped.transform(lambdax:x.astype(int).max())Falcon    390Falcon    390Parrot     30Parrot     30Name: Max Speed, dtype: int64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.transform,"pandas.core.groupby.DataFrameGroupBy.transform#DataFrameGroupBy.transform(func,*args,engine=None,engine_kwargs=None,**kwargs)[source]#Call function producing a same-indexed DataFrame on each group.Returns a DataFrame having the same indexes as the original object
filled with the transformed values.Parameters:ffunction, strFunction to apply to each group. See the Notes section below for requirements.Accepted inputs are:StringPython functionNumba JIT function withengine='numba'specified.Only passing a single function is supported with this engine.
If the'numba'engine is chosen, the function must be
a user defined function withvaluesandindexas the
first and second arguments respectively in the function signature.
Each groupÅfs index will be passed to the user defined function
and optionally available for use.If a string is chosen, then it needs to be the name
of the groupby method you want to use.*argsPositional arguments to pass to func.enginestr, default None'cython': Runs the function through C-extensions from cython.'numba': Runs the function through JIT compiled code from numba.None: Defaults to'cython'or the global settingcompute.use_numbaengine_kwargsdict, default NoneFor'cython'engine, there are no acceptedengine_kwargsFor'numba'engine, the engine can acceptnopython,nogilandparalleldictionary keys. The values must either beTrueorFalse. The defaultengine_kwargsfor the'numba'engine is{'nopython':True,'nogil':False,'parallel':False}and will be
applied to the function**kwargsKeyword arguments to be passed into func.Returns:DataFrameSee alsoDataFrame.groupby.applyApply functionfuncgroup-wise and combine the results together.DataFrame.groupby.aggregateAggregate using one or more operations over the specified axis.DataFrame.transformCallfuncon self producing a DataFrame with the same axis shape as self.NotesEach group is endowed the attribute ÅenameÅf in case you need to know
which group you are working on.The current implementation imposes three requirements on f:f must return a value that either has the same shape as the input
subframe or can be broadcast to the shape of the input subframe.
For example, iffreturns a scalar it will be broadcast to have the
same shape as the input subframe.if this is a DataFrame, f must support application column-by-column
in the subframe. If f also supports application to the entire subframe,
then a fast path is used starting from the second chunk.f must not mutate groups. Mutation is not supported and may
produce unexpected results. SeeMutating with User Defined Function (UDF) methodsfor more details.When usingengine='numba', there will be no Ågfall backÅh behavior internally.
The group data and group index will be passed as numpy arrays to the JITed
user defined function, and no alternative execution attempts will be tried.Changed in version 1.3.0:The resulting dtype will reflect the return value of the passedfunc,
see the examples below.Changed in version 2.0.0:When using.transformon a grouped DataFrame and the transformation function
returns a DataFrame, pandas now aligns the resultÅfs index
with the inputÅfs index. You can call.to_numpy()on the
result of the transformation function to avoid alignment.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':['one','one','two','three',...'two','two'],...'C':[1,5,5,2,5,5],...'D':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')[['C','D']]>>>grouped.transform(lambdax:(x-x.mean())/x.std())C         D0 -1.154701 -0.5773501  0.577350  0.0000002  0.577350  1.1547013 -1.154701 -1.0000004  0.577350 -0.5773505  0.577350  1.000000Broadcast result of the transformation>>>grouped.transform(lambdax:x.max()-x.min())C    D0  4.0  6.01  3.0  8.02  4.0  6.03  3.0  8.04  4.0  6.05  3.0  8.0>>>grouped.transform(""mean"")C    D0  3.666667  4.01  4.000000  5.02  3.666667  4.03  4.000000  5.04  3.666667  4.05  4.000000  5.0Changed in version 1.3.0.The resulting dtype will reflect the return value of the passedfunc,
for example:>>>grouped.transform(lambdax:x.astype(int).max())C  D0  5  81  5  92  5  83  5  94  5  85  5  9"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.pipe,"pandas.core.groupby.SeriesGroupBy.pipe#SeriesGroupBy.pipe(func,*args,**kwargs)[source]#Apply afuncwith arguments to this GroupBy object and return its result.Use.pipewhen you want to improve readability by chaining together
functions that expect Series, DataFrames, GroupBy or Resampler objects.
Instead of writing>>>h=lambdax,arg2,arg3:x+1-arg2*arg3>>>g=lambdax,arg1:x*5/arg1>>>f=lambdax:x**4>>>df=pd.DataFrame([[""a"",4],[""b"",5]],columns=[""group"",""value""])>>>h(g(f(df.groupby('group')),arg1=1),arg2=2,arg3=3)You can write>>>(df.groupby('group')....pipe(f)....pipe(g,arg1=1)....pipe(h,arg2=2,arg3=3))which is much more readable.Parameters:funccallable or tuple of (callable, str)Function to apply to this GroupBy object or, alternatively,
a(callable, data_keyword)tuple wheredata_keywordis a
string indicating the keyword ofcallablethat expects the
GroupBy object.argsiterable, optionalPositional arguments passed intofunc.kwargsdict, optionalA dictionary of keyword arguments passed intofunc.Returns:the return type offunc.See alsoSeries.pipeApply a function with arguments to a series.DataFrame.pipeApply a function with arguments to a dataframe.applyApply function to each group instead of to the full GroupBy object.NotesSee morehereExamples>>>df=pd.DataFrame({'A':'a b a b'.split(),'B':[1,2,3,4]})>>>dfA  B0  a  11  b  22  a  33  b  4To get the difference between each groups maximum and minimum value in one
pass, you can do>>>df.groupby('A').pipe(lambdax:x.max()-x.min())BAa  2b  2"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.pipe,"pandas.core.groupby.DataFrameGroupBy.pipe#DataFrameGroupBy.pipe(func,*args,**kwargs)[source]#Apply afuncwith arguments to this GroupBy object and return its result.Use.pipewhen you want to improve readability by chaining together
functions that expect Series, DataFrames, GroupBy or Resampler objects.
Instead of writing>>>h=lambdax,arg2,arg3:x+1-arg2*arg3>>>g=lambdax,arg1:x*5/arg1>>>f=lambdax:x**4>>>df=pd.DataFrame([[""a"",4],[""b"",5]],columns=[""group"",""value""])>>>h(g(f(df.groupby('group')),arg1=1),arg2=2,arg3=3)You can write>>>(df.groupby('group')....pipe(f)....pipe(g,arg1=1)....pipe(h,arg2=2,arg3=3))which is much more readable.Parameters:funccallable or tuple of (callable, str)Function to apply to this GroupBy object or, alternatively,
a(callable, data_keyword)tuple wheredata_keywordis a
string indicating the keyword ofcallablethat expects the
GroupBy object.argsiterable, optionalPositional arguments passed intofunc.kwargsdict, optionalA dictionary of keyword arguments passed intofunc.Returns:the return type offunc.See alsoSeries.pipeApply a function with arguments to a series.DataFrame.pipeApply a function with arguments to a dataframe.applyApply function to each group instead of to the full GroupBy object.NotesSee morehereExamples>>>df=pd.DataFrame({'A':'a b a b'.split(),'B':[1,2,3,4]})>>>dfA  B0  a  11  b  22  a  33  b  4To get the difference between each groups maximum and minimum value in one
pass, you can do>>>df.groupby('A').pipe(lambdax:x.max()-x.min())BAa  2b  2"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.filter,"pandas.core.groupby.DataFrameGroupBy.filter#DataFrameGroupBy.filter(func,dropna=True,*args,**kwargs)[source]#Filter elements from groups that donÅft satisfy a criterion.Elements from groups are filtered if they do not satisfy the
boolean criterion specified by func.Parameters:funcfunctionCriterion to apply to each group. Should return True or False.dropnaboolDrop groups that do not pass the filter. True by default; if False,
groups that evaluate False are filled with NaNs.Returns:DataFrameNotesEach subframe is endowed the attribute ÅenameÅf in case you need to know
which group you are working on.Functions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':[1,2,3,4,5,6],...'C':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')>>>grouped.filter(lambdax:x['B'].mean()>3.)A  B    C1  bar  2  5.03  bar  4  1.05  bar  6  9.0"
Pandas,GroupBy,pandas.core.groupby.SeriesGroupBy.filter,"pandas.core.groupby.SeriesGroupBy.filter#SeriesGroupBy.filter(func,dropna=True,*args,**kwargs)[source]#Filter elements from groups that donÅft satisfy a criterion.Elements from groups are filtered if they do not satisfy the
boolean criterion specified by func.Parameters:funcfunctionCriterion to apply to each group. Should return True or False.dropnaboolDrop groups that do not pass the filter. True by default; if False,
groups that evaluate False are filled with NaNs.Returns:SeriesNotesFunctions that mutate the passed object can produce unexpected
behavior or errors and are not supported. SeeMutating with User Defined Function (UDF) methodsfor more details.Examples>>>df=pd.DataFrame({'A':['foo','bar','foo','bar',...'foo','bar'],...'B':[1,2,3,4,5,6],...'C':[2.0,5.,8.,1.,2.,9.]})>>>grouped=df.groupby('A')>>>df.groupby('A').B.filter(lambdax:x.mean()>3.)1    23    45    6Name: B, dtype: int64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.all,"pandas.core.groupby.DataFrameGroupBy.all#DataFrameGroupBy.all(skipna=True)[source]#Return True if all values in the group are truthful, else False.Parameters:skipnabool, default TrueFlag to ignore nan values during truth testing.Returns:Series or DataFrameDataFrame or Series of boolean values, where a value is True if all elements
are True within its respective group, False otherwise.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,0],index=lst)>>>sera    1a    2b    0dtype: int64>>>ser.groupby(level=0).all()a     Trueb    Falsedtype: boolFor DataFrameGroupBy:>>>data=[[1,0,3],[1,5,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""ostrich"",""penguin"",""parrot""])>>>dfa  b  costrich  1  0  3penguin  1  5  6parrot   7  8  9>>>df.groupby(by=[""a""]).all()b      ca1  False   True7   True   True"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.any,"pandas.core.groupby.DataFrameGroupBy.any#DataFrameGroupBy.any(skipna=True)[source]#Return True if any value in the group is truthful, else False.Parameters:skipnabool, default TrueFlag to ignore nan values during truth testing.Returns:Series or DataFrameDataFrame or Series of boolean values, where a value is True if any element
is True within its respective group, False otherwise.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,0],index=lst)>>>sera    1a    2b    0dtype: int64>>>ser.groupby(level=0).any()a     Trueb    Falsedtype: boolFor DataFrameGroupBy:>>>data=[[1,0,3],[1,0,6],[7,1,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""ostrich"",""penguin"",""parrot""])>>>dfa  b  costrich  1  0  3penguin  1  0  6parrot   7  1  9>>>df.groupby(by=[""a""]).any()b      ca1  False   True7   True   True"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.bfill,"pandas.core.groupby.DataFrameGroupBy.bfill#DataFrameGroupBy.bfill(limit=None)[source]#Backward fill the values.Parameters:limitint, optionalLimit of how many values to fill.Returns:Series or DataFrameObject with missing values filled.See alsoSeries.bfillBackward fill the missing values in the dataset.DataFrame.bfillBackward fill the missing values in the dataset.Series.fillnaFill NaN values of a Series.DataFrame.fillnaFill NaN values of a DataFrame.ExamplesWith Series:>>>index=['Falcon','Falcon','Parrot','Parrot','Parrot']>>>s=pd.Series([None,1,None,None,3],index=index)>>>sFalcon    NaNFalcon    1.0Parrot    NaNParrot    NaNParrot    3.0dtype: float64>>>s.groupby(level=0).bfill()Falcon    1.0Falcon    1.0Parrot    3.0Parrot    3.0Parrot    3.0dtype: float64>>>s.groupby(level=0).bfill(limit=1)Falcon    1.0Falcon    1.0Parrot    NaNParrot    3.0Parrot    3.0dtype: float64With DataFrame:>>>df=pd.DataFrame({'A':[1,None,None,None,4],...'B':[None,None,5,None,7]},index=index)>>>dfA         BFalcon  1.0       NaNFalcon  NaN       NaNParrot  NaN       5.0Parrot  NaN       NaNParrot  4.0       7.0>>>df.groupby(level=0).bfill()A         BFalcon  1.0       NaNFalcon  NaN       NaNParrot  4.0       5.0Parrot  4.0       7.0Parrot  4.0       7.0>>>df.groupby(level=0).bfill(limit=1)A         BFalcon  1.0       NaNFalcon  NaN       NaNParrot  NaN       5.0Parrot  4.0       7.0Parrot  4.0       7.0"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.corr,"pandas.core.groupby.DataFrameGroupBy.corr#DataFrameGroupBy.corr(method='pearson',min_periods=1,numeric_only=False)[source]#Compute pairwise correlation of columns, excluding NA/null values.Parameters:method{ÅepearsonÅf, ÅekendallÅf, ÅespearmanÅf} or callableMethod of correlation:pearson : standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: callable with input two 1d ndarraysand returning a float. Note that the returned matrix from corr
will have 1 along the diagonals and will be symmetric
regardless of the callableÅfs behavior.min_periodsint, optionalMinimum number of observations required per pair of columns
to have a valid result. Currently only available for Pearson
and Spearman correlation.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:DataFrameCorrelation matrix.See alsoDataFrame.corrwithCompute pairwise correlation with another DataFrame or Series.Series.corrCompute the correlation between two Series.NotesPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.Pearson correlation coefficientKendall rank correlation coefficientSpearmanÅfs rank correlation coefficientExamples>>>defhistogram_intersection(a,b):...v=np.minimum(a,b).sum().round(decimals=1)...returnv>>>df=pd.DataFrame([(.2,.3),(.0,.6),(.6,.0),(.2,.1)],...columns=['dogs','cats'])>>>df.corr(method=histogram_intersection)dogs  catsdogs   1.0   0.3cats   0.3   1.0>>>df=pd.DataFrame([(1,1),(2,np.nan),(np.nan,3),(4,4)],...columns=['dogs','cats'])>>>df.corr(min_periods=3)dogs  catsdogs   1.0   NaNcats   NaN   1.0"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.corrwith,"pandas.core.groupby.DataFrameGroupBy.corrwith#DataFrameGroupBy.corrwith(other,axis=_NoDefault.no_default,drop=False,method='pearson',numeric_only=False)[source]#Compute pairwise correlation.Pairwise correlation is computed between rows or columns of
DataFrame with rows or columns of Series or DataFrame. DataFrames
are first aligned along both axes before computing the
correlations.Parameters:otherDataFrame, SeriesObject with which to compute correlations.axis{0 or ÅeindexÅf, 1 or ÅecolumnsÅf}, default 0The axis to use. 0 or ÅeindexÅf to compute row-wise, 1 or ÅecolumnsÅf for
column-wise.dropbool, default FalseDrop missing indices from result.method{ÅepearsonÅf, ÅekendallÅf, ÅespearmanÅf} or callableMethod of correlation:pearson : standard correlation coefficientkendall : Kendall Tau correlation coefficientspearman : Spearman rank correlationcallable: callable with input two 1d ndarraysand returning a float.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:SeriesPairwise correlations.See alsoDataFrame.corrCompute pairwise correlation of columns.Examples>>>index=[""a"",""b"",""c"",""d"",""e""]>>>columns=[""one"",""two"",""three"",""four""]>>>df1=pd.DataFrame(np.arange(20).reshape(5,4),index=index,columns=columns)>>>df2=pd.DataFrame(np.arange(16).reshape(4,4),index=index[:4],columns=columns)>>>df1.corrwith(df2)one      1.0two      1.0three    1.0four     1.0dtype: float64>>>df2.corrwith(df1,axis=1)a    1.0b    1.0c    1.0d    1.0e    NaNdtype: float64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.count,"pandas.core.groupby.DataFrameGroupBy.count#DataFrameGroupBy.count()[source]#Compute count of group, excluding missing values.Returns:Series or DataFrameCount of values within each group.See alsoSeries.groupbyApply a function groupby to a Series.DataFrame.groupbyApply a function groupby to each row or column of a DataFrame.ExamplesFor SeriesGroupBy:>>>lst=['a','a','b']>>>ser=pd.Series([1,2,np.nan],index=lst)>>>sera    1.0a    2.0b    NaNdtype: float64>>>ser.groupby(level=0).count()a    2b    0dtype: int64For DataFrameGroupBy:>>>data=[[1,np.nan,3],[1,np.nan,6],[7,8,9]]>>>df=pd.DataFrame(data,columns=[""a"",""b"",""c""],...index=[""cow"",""horse"",""bull""])>>>dfa         b     ccow     1       NaN     3horse   1       NaN     6bull    7       8.0     9>>>df.groupby(""a"").count()b   ca1   0   27   1   1For Resampler:>>>ser=pd.Series([1,2,3,4],index=pd.DatetimeIndex(...['2023-01-01','2023-01-15','2023-02-01','2023-02-15']))>>>ser2023-01-01    12023-01-15    22023-02-01    32023-02-15    4dtype: int64>>>ser.resample('MS').count()2023-01-01    22023-02-01    2Freq: MS, dtype: int64"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.cov,"pandas.core.groupby.DataFrameGroupBy.cov#DataFrameGroupBy.cov(min_periods=None,ddof=1,numeric_only=False)[source]#Compute pairwise covariance of columns, excluding NA/null values.Compute the pairwise covariance among the series of a DataFrame.
The returned data frame is thecovariance matrixof the columns
of the DataFrame.Both NA and null values are automatically excluded from the
calculation. (See the note below about bias from missing values.)
A threshold can be set for the minimum number of
observations for each value created. Comparisons with observations
below this threshold will be returned asNaN.This method is generally used for the analysis of time series data to
understand the relationship between different measures
across time.Parameters:min_periodsint, optionalMinimum number of observations required per pair of columns
to have a valid result.ddofint, default 1Delta degrees of freedom. The divisor used in calculations
isN-ddof, whereNrepresents the number of elements.
This argument is applicable only when nonanis in the dataframe.numeric_onlybool, default FalseInclude onlyfloat,intorbooleandata.New in version 1.5.0.Changed in version 2.0.0:The default value ofnumeric_onlyis nowFalse.Returns:DataFrameThe covariance matrix of the series of the DataFrame.See alsoSeries.covCompute covariance with another Series.core.window.ewm.ExponentialMovingWindow.covExponential weighted sample covariance.core.window.expanding.Expanding.covExpanding sample covariance.core.window.rolling.Rolling.covRolling sample covariance.NotesReturns the covariance matrix of the DataFrameÅfs time series.
The covariance is normalized by N-ddof.For DataFrames that have Series that are missing data (assuming that
data ismissing at random)
the returned covariance matrix will be an unbiased estimate
of the variance and covariance between the member Series.However, for many applications this estimate may not be acceptable
because the estimate covariance matrix is not guaranteed to be positive
semi-definite. This could lead to estimate correlations having
absolute values which are greater than one, and/or a non-invertible
covariance matrix. SeeEstimation of covariance matricesfor more details.Examples>>>df=pd.DataFrame([(1,2),(0,3),(2,0),(1,1)],...columns=['dogs','cats'])>>>df.cov()dogs      catsdogs  0.666667 -1.000000cats -1.000000  1.666667>>>np.random.seed(42)>>>df=pd.DataFrame(np.random.randn(1000,5),...columns=['a','b','c','d','e'])>>>df.cov()a         b         c         d         ea  0.998438 -0.020161  0.059277 -0.008943  0.014144b -0.020161  1.059352 -0.008543 -0.024738  0.009826c  0.059277 -0.008543  1.010670 -0.001486 -0.000271d -0.008943 -0.024738 -0.001486  0.921297 -0.013692e  0.014144  0.009826 -0.000271 -0.013692  0.977795Minimum number of periodsThis method also supports an optionalmin_periodskeyword
that specifies the required minimum number of non-NA observations for
each column pair in order to have a valid result:>>>np.random.seed(42)>>>df=pd.DataFrame(np.random.randn(20,3),...columns=['a','b','c'])>>>df.loc[df.index[:5],'a']=np.nan>>>df.loc[df.index[5:10],'b']=np.nan>>>df.cov(min_periods=12)a         b         ca  0.316741       NaN -0.150812b       NaN  1.248003  0.191417c -0.150812  0.191417  0.895202"
Pandas,GroupBy,pandas.core.groupby.DataFrameGroupBy.cumcount,"pandas.core.groupby.DataFrameGroupBy.cumcount#DataFrameGroupBy.cumcount(ascending=True)[source]#Number each item in each group from 0 to the length of that group - 1.Essentially this is equivalent toself.apply(lambdax:pd.Series(np.arange(len(x)),x.index))Parameters:ascendingbool, default TrueIf False, number in reverse, from length of group - 1 to 0.Returns:SeriesSequence number of each element within each group.See alsongroupNumber the groups themselves.Examples>>>df=pd.DataFrame([['a'],['a'],['a'],['b'],['b'],['a']],...columns=['A'])>>>dfA0  a1  a2  a3  b4  b5  a>>>df.groupby('A').cumcount()0    01    12    23    04    15    3dtype: int64>>>df.groupby('A').cumcount(ascending=False)0    31    22    13    14    05    0dtype: int64"
Pandas,Input Output,pandas.read_csv,"The `pandas.read_csv()` function is used to read a comma-separated values (csv) file into a DataFrame. It has multiple parameters that allow for customization of the import process.Here is a summary of the important parameters:- `filepath_or_buffer`: Accepts a string path or file-like object. Can also accept a URL.- `sep`: Specifies the character or regex pattern to treat as the delimiter. The default is ','.- `header`: Specifies the row number(s) containing column labels. By default, it will infer the column names from the first line of the file.- `names`: Specifies a sequence of column labels to apply. If the file has a header row, this parameter should be set to 0 to override the column names.- `index_col`: Specifies the column(s) to use as row labels. Can be specified by column labels or indices.- `usecols`: Specifies a subset of columns to select, either by column labels or indices.- `dtype`: Specifies the data type(s) to apply to the dataset or individual columns.- `skiprows`: Specifies line numbers to skip at the start of the file.- `na_values`: Specifies additional strings to recognize as NaN.- `parse_dates`: Specifies columns to parse as dates.- `encoding`: Specifies the encoding of the file.- `quotechar`: Specifies the character used to denote the start and end of a quoted item. Quoted items can include the delimiter, and it will be ignored.- `quoting`: Specifies the field quoting behavior. Can be one of four constants.- `skip_blank_lines`: Specifies whether to skip over blank lines rather than interpreting them as NaN values.- `float_precision`: Specifies the converter the C engine should use for floating-point values.The function returns a DataFrame, which is a two-dimensional data structure with labeled axes.Example usage:```import pandas as pddata = pd.read_csv('data.csv')print(data)```This will read the 'data.csv' file and store its contents into a DataFrame named 'data'."
Pandas,Input Output,pandas.DataFrame.to_csv,"The Numpy reference is describing the `to_csv` function in the pandas.DataFrame module. The `to_csv` function is used to write a pandas DataFrame object to a comma-separated values (csv) file. The function has several parameters that allow you to customize the output file.The first parameter, `path_or_buf`, specifies the path to the output file. It can be a string representing the file path, a path object, a file-like object, or None. If None, the result is returned as a string. The `sep` parameter specifies the field delimiter for the output file, which is a comma by default.The `na_rep` parameter specifies the representation for missing data, which is an empty string by default. The `float_format` parameter allows you to specify the format string for floating-point numbers. If a Callable is given, it takes precedence over other numeric formatting parameters.The `columns` parameter specifies the columns to write to the file, and the `header` parameter controls whether or not to write out the column names. The `index` parameter controls whether to write row names (index), and the `index_label` parameter specifies the column label for the index column(s) if desired.The `mode` parameter controls the file opening mode, such as 'w' for truncating the file first, 'x' for exclusive creation, or 'a' for appending to the end of the file. The `encoding` parameter specifies the encoding to use in the output file, and the `compression` parameter allows for on-the-fly compression of the output data.The `quoting` parameter specifies the quoting behavior, and the `quotechar` parameter specifies the character used to quote fields. The `lineterminator` parameter specifies the newline character or character sequence to use in the output file.Other parameters include `chunksize` to specify the number of rows to write at a time, `date_format` to specify the format string for datetime objects, `doublequote` to control the quoting of quotechar inside a field, `escapechar` to specify the character used to escape sep and quotechar when appropriate, `decimal` to specify the character recognized as the decimal separator, `errors` to specify how encoding and decoding errors are handled, and `storage_options` for extra options that make sense for a particular storage connection.The function returns None if the `path_or_buf` parameter is provided, or the resulting csv format as a string if it is None.The reference also includes examples of how to use the `to_csv` function to create a csv file and to write a csv file to a new folder or nested folder using pathlib or os.Overall, the `to_csv` function in the pandas.DataFrame module provides a flexible way to write DataFrame objects to csv files with customizable settings for various aspects of the output file."
Pandas,Input Output,pandas.read_excel,"The `pandas.read_excel` function is used to read an Excel file into a pandas DataFrame. It supports various file extensions including xls, xlsx, xlsm, xlsb, odf, ods, and odt. The function can read from a local file system or a URL.The `io` parameter specifies the file or file-like object to be read. It can be a string path, bytes, ExcelFile, xlrd.Book, path object, or file-like object. If a string path is provided, it can be a valid URL starting with http, ftp, s3, or file, or a local file path starting with file://localhost/.The `sheet_name` parameter is used to specify which sheet(s) of the Excel file to read. It can be a string, integer, list, or None. If a string is provided, it is interpreted as the sheet name. If an integer is provided, it is interpreted as the zero-indexed sheet position. If a list is provided, it is interpreted as a list of sheet names or positions. If None is provided, all worksheets are read.The `header` parameter specifies which row to use as the column labels of the parsed DataFrame. It can be an integer or a list of integers. If a list of integers is passed, those row positions will be combined into a MultiIndex. Use None if there is no header.The `names` parameter is a list of column names to use. If the file contains no header row, this parameter should be explicitly passed with header=None.The `index_col` parameter specifies which column to use as the row labels of the DataFrame. It can be an integer, string, or a list of integers or strings. If None is passed, there is no such column. If a list is passed, those columns will be combined into a MultiIndex. If a subset of data is selected with `usecols`, the `index_col` is based on the subset. Missing values will be forward-filled to allow roundtripping with `to_excel`, but you can avoid forward-filling by using `set_index` after reading the data instead of `index_col`.The `usecols` parameter specifies which columns to parse. It can be a string, list-like object, or a callable function. If None, all columns are parsed. If a string is provided, it indicates a comma-separated list of Excel column letters and column ranges. If a list of integers is provided, it indicates the column numbers to be parsed. If a list of strings is provided, it indicates the column names to be parsed. If a callable is provided, it evaluates each column name against it and parses the column if the callable returns True.The `dtype` parameter can be used to specify the data types for the data or columns in the DataFrame. It can be either a type name or a dictionary of column names to types. For example, {ÅeaÅf: np.float64, ÅebÅf: np.int32}. If None is used, it infers the dtype of each column based on the data. The `dtype` parameter is not applied if `converters` are specified.The `engine` parameter specifies the Excel engine to be used. It can be one of the following strings: 'openpyxl', 'calamine', 'odf', 'pyxlsb', or 'xlrd'. If the `engine` parameter is set to None, the function will determine the engine based on the file type.Other parameters include `converters` for specifying conversion functions for values in certain columns, `true_values` and `false_values` for specifying values to consider as True and False, `skiprows` for specifying rows to skip at the start of the file, `nrows` for specifying the number of rows to parse, `na_values` for specifying additional strings to recognize as NA/NaN, `keep_default_na` for specifying whether to include the default NaN values when parsing the data, `na_filter` for specifying whether to detect missing value markers, `verbose` for indicating the number of NA values placed in non-numeric columns, `parse_dates` for specifying how to parse date columns, `date_parser` for specifying a function for converting string columns to datetime instances, `date_format` for specifying the format of dates, `thousands` for specifying the thousands separator for parsing string columns to numeric, `decimal` for specifying the character to recognize as the decimal point, `comment` for indicating comments in the input file, `skipfooter` for specifying the number of rows to skip at the end of the file, `storage_options` for specifying extra options for a specific storage connection, `dtype_backend` for specifying the back-end data type applied to the resultant DataFrame, and `engine_kwargs` for specifying arbitrary keyword arguments passed to the Excel engine.The function returns a DataFrame or a dictionary of DataFrames, depending on the sheet_name argument. If a single sheet is read, a DataFrame is returned. If multiple sheets are read, a dictionary of DataFrames is returned, where the keys are the sheet names or positions.There are also examples provided in the documentation showing how to use the function with different parameters."
Pandas,Input Output,pandas.DataFrame.to_excel,"pandas.DataFrame.to_excel#DataFrame.to_excel(excel_writer,*,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to
specify a target file name. To write to multiple sheets it is necessary to
create anExcelWriterobject with a target file name, and specify a sheet
in the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.
With all data written to the file it is necessary to save the changes.
Note that creating anExcelWriterobject with a file name that already
exists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default ÅeSheet1ÅfName of sheet which will contain DataFrame.na_repstr, default ÅeÅfMissing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=""%.2f""will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A
sequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, ÅeopenpyxlÅf or ÅexlsxwriterÅf. You can also set this
via the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default ÅeinfÅfRepresentation for infinity (there is no native representation for
infinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that
is to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),
to_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further
data without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(""output.xlsx"")To specify the sheet name:>>>df1.to_excel(""output.xlsx"",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is
necessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,
you can pass theenginekeyword (the default engine is
automatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')"
Pandas,Input Output,pandas.ExcelWriter,"The Numpy reference is a description of the `pandas.ExcelWriter` class in the pandas library. This class is used for writing DataFrame objects into excel sheets. The default engine used is `xlsxwriter` for xlsx files, if it is installed, otherwise `openpyxl` for ods files. The class provides several parameters to customize the writing process.The `path` parameter specifies the path to the xls or xlsx or ods file. The `engine` parameter is optional and specifies the engine to use for writing. If not specified, it defaults to `io.excel.<extension>.writer`. The `date_format` and `datetime_format` parameters allow you to specify the format strings for dates and datetime objects written into Excel files.The `mode` parameter specifies the file mode to use, either 'w' for write or 'a' for append. The `storage_options` parameter is optional and provides extra options for a particular storage connection, such as host, port, username, password, etc. The `if_sheet_exists` parameter specifies how to behave when trying to write to a sheet that already exists in append mode. It can be set to 'error' to raise a ValueError, 'new' to create a new sheet, 'replace' to delete the contents of the sheet before writing to it, or 'overlay' to write contents to the existing sheet without removing the existing contents.The `engine_kwargs` parameter is optional and allows you to pass keyword arguments to the engine. These arguments are then passed to the respective engine functions.The class can be used as a context manager or you can manually call the `close()` method to save and close any opened file handles.The reference provides several examples of how to use the `pandas.ExcelWriter` class. It shows how to write DataFrames to separate sheets in a single file, set the date and datetime format, append to an existing Excel file, write multiple DataFrames to a single sheet, store the file in RAM, and pack the file into a zip archive. It also demonstrates how to specify additional arguments to the underlying engine.The reference also provides information about the attributes and methods of the `pandas.ExcelWriter` class. The `book` attribute is a Book instance, the `date_format` attribute is the format string for dates written into Excel files, the `datetime_format` attribute is the format string for datetime objects written into Excel files, the `engine` attribute is the name of the engine being used, the `if_sheet_exists` attribute specifies how to behave when writing to an existing sheet in append mode, and the `sheets` attribute is a mapping of sheet names to sheet objects. The class also provides the `supported_extensions` attribute, which is a list of extensions that the writer engine supports.In addition, the class provides the `check_extension(ext)` method, which checks the extension of the given path against the supported extensions of the writer. It also provides the `close()` method, which is a synonym for `save()` to make it more file-like."
Pandas,Input Output,pandas.read_json,"The function `pandas.read_json()` is used to convert a JSON string into a pandas object, such as a Series or DataFrame. It takes several parameters:- `path_or_buf`: A valid JSON string, path, or file-like object. It can be a file path, a URL, or a file-like object (e.g., a file handle or `StringIO`).- `orient`: An optional parameter indicating the expected JSON string format. It can be one of the following: 'split', 'records', 'index', 'columns', 'values', or 'table'. The default value depends on the `typ` parameter.- `typ`: The type of object to recover. It can be either 'frame' (default) or 'series'.- `dtype`: If True, infer dtypes; if a dictionary of column to dtype, use those; if False, don't infer dtypes at all.- `convert_axes`: Try to convert the axes to the proper dtypes.- `convert_dates`: If True, default datelike columns may be converted (depending on `keep_default_dates`). If False, no dates will be converted. If a list of column names is provided, then those columns will be converted, along with the default datelike columns.- `keep_default_dates`: If parsing dates (`convert_dates` is not False), try to parse the default datelike columns.- `precise_float`: Set to True to enable the usage of higher precision (strtod) function when decoding string to double values.- `date_unit`: The timestamp unit to detect if converting dates. The default behavior is to try and detect the correct precision, but you can pass one of 's', 'ms', 'us', or 'ns' to force parsing only seconds, milliseconds, microseconds, or nanoseconds, respectively.- `encoding`: The encoding to use to decode bytes. Default is 'utf-8'.- `encoding_errors`: How encoding errors are treated. Default is 'strict'.- `lines`: Read the file as a JSON object per line. Default is False.- `chunksize`: Return a JsonReader object for iteration.- `compression`: For on-the-fly decompression of on-disk data. Default is 'infer'.- `nrows`: The number of lines from the line-delimited JSON file to be read.- `storage_options`: Extra options that make sense for a particular storage connection, such as host, port, username, password, etc.- `dtype_backend`: The back-end data type applied to the resultant DataFrame. Default is 'numpy_nullable'.- `engine`: Parser engine to use. Default is 'ujson', and 'pyarrow' is also available when `lines` is True.The function returns either a Series, DataFrame, or a JsonReader object if `chunksize` is not 0 or None.The documentation provides multiple usage examples with different parameter combinations. It includes examples of encoding and decoding DataFrames using various formats, such as 'split', 'index', 'records', and 'table'. It also demonstrates the use of the `dtype_backend` parameter for specifying the data type of the resultant DataFrame."
Pandas,Input Output,pandas.DataFrame.to_json,"The `to_json()` function in pandas is used to convert a DataFrame object into a JSON string. It has several parameters that allow you to customize the output format and behavior.The `path_or_buf` parameter specifies the destination where the JSON string will be written. It can be a string representing a file path, a path object, a file-like object with a `write()` function, or None, in which case the result is returned as a string.The `orient` parameter indicates the format of the JSON string. For a Series object, the default is 'index', and allowed values are 'split', 'records', 'index', and 'table'. For a DataFrame object, the default is 'columns', and allowed values are 'split', 'records', 'index', 'columns', 'values', and 'table'.The `date_format` parameter determines the type of date conversion in the JSON string. It can be set to None, 'epoch' (epoch milliseconds), or 'iso' (ISO8601).The `double_precision` parameter specifies the number of decimal places to use when encoding floating-point values.The `force_ascii` parameter determines whether the encoded string should be forced to be ASCII.The `date_unit` parameter specifies the time unit to encode to, governing timestamp and ISO8601 precision. The options are 's', 'ms' (default), 'us', and 'ns' for second, millisecond, microsecond, and nanosecond, respectively.The `default_handler` parameter is a callable that is called if an object cannot be converted to a suitable format for JSON.The `lines` parameter is a boolean indicating whether to write out the JSON string in line-delimited format when the `orient` is set to 'records'.The `compression` parameter allows for on-the-fly compression of the output data. It can be set to 'infer' to detect compression from the file extension if a file path is provided, or it can be a dict with a key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'}, with other key-value pairs forwarded to the corresponding compression file object.The `index` parameter is only used when the `orient` is set to 'split', 'index', 'column', or 'table'. It specifies whether to include the index in the JSON string output.The `indent` parameter specifies the length of whitespace used to indent each record in the JSON string.The `storage_options` parameter is a dictionary of extra options that make sense for a particular storage connection, such as host, port, username, password, etc.The `mode` parameter specifies the IO mode for output when supplying a `path_or_buf`. Accepted values are 'w' (writing) and 'a' (append), with 'a' only supported when `lines` is True and `orient` is 'records'.The function returns None if `path_or_buf` is provided, or the resulting JSON format as a string if `path_or_buf` is None.The examples provided demonstrate how to encode and decode a DataFrame using different formats such as 'split', 'records', 'index', 'columns', 'values', and 'table'."
Pandas,Input Output,pandas.read_html,"The `pandas.read_html` function is used to read HTML tables into a list of DataFrame objects. It has the following parameters:- `io`: Takes a string, path object, or file-like object. The string can represent a URL or the HTML itself.- `match`: The set of tables containing text matching this regex or string will be returned. Defaults to Åe.+Åf (match any non-empty string).- `flavor`: The parsing engine (or list of parsing engines) to use. Defaults to None, which tries to use lxml to parse and falls back on bs4+html5lib if lxml fails.- `header`: The row (or list of rows for a MultiIndex) to use to make the columns headers. Defaults to None.- `index_col`: The column (or list of columns) to use to create the index. Defaults to None.- `skiprows`: Number of rows to skip after parsing the column integer. 0-based. Defaults to None.- `attrs`: This is a dictionary of attributes that you can pass to use to identify the table in the HTML. Defaults to None.- `parse_dates`: Specifies whether to parse dates in the DataFrame. Defaults to False.- `thousands`: Separator to use to parse thousands. Defaults to ','.- `encoding`: The encoding used to decode the web page. Defaults to None.- `decimal`: Character to recognize as decimal point. Defaults to '.'.- `converters`: Dict of functions for converting values in certain columns. Defaults to None.- `na_values`: Iterable of custom NA values. Defaults to None.- `keep_default_na`: Specifies whether to override default NaN values with custom NA values. Defaults to True.- `displayed_only`: Specifies whether elements with ""display: none"" should be parsed. Defaults to True.- `extract_links`: Specifies the section(s) to extract links from. Defaults to None.- `dtype_backend`: Back-end data type applied to the resulting DataFrame. Defaults to 'numpy_nullable'.- `storage_options`: Extra options that make sense for a particular storage connection. Defaults to None.The function returns a list of DataFrames.Some important notes about using this function:- Before using this function, it is recommended to read the gotchas about the HTML parsing libraries.- Expect to do some cleanup after calling this function, such as manually assigning column names if they are converted to NaN when passing the `header=0` argument.- This function searches for `<table>` elements and only for `<tr>` and `<th>` rows and `<td>` elements within each `<tr>` or `<th>` element in the table.- If the function has a `<thead>` argument, it is used to construct the header, otherwise it attempts to find the header within the body.- The `header` argument is applied after `skiprows` is applied.- This function will always return a list of DataFrame or it will fail, it will not return an empty list.The documentation provides examples of reading HTML tables using the `read_html` function."
Pandas,Input Output,pandas.DataFrame.to_html,"pandas.DataFrame.to_html#DataFrame.to_html(buf=None,*,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',bold_rows=True,classes=None,escape=True,notebook=False,border=None,table_id=None,render_links=False,encoding=None)[source]#Render a DataFrame as an HTML table.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spacestr or int, list or dict of int or str, optionalThe minimum width of each column in CSS length units. An int is assumed to be px units..headerbool, optionalWhether to print column labels, default True.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default ÅeNaNÅfString representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columnsÅf elements by position or
name.
The result of each function must be a unicode string.
List/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columnsÅf elements if they are
floats. This function must return a unicode string and will be
applied only to the non-NaNelements, withNaNbeing
handled byna_rep.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from
the print configuration (controlled by set_option), ÅerightÅf out
of the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default Åe.ÅfCharacter recognized as decimal separator, e.g. Åe,Åf in Europe.bold_rowsbool, default TrueMake the row labels bold in the output.classesstr or list or tuple, default NoneCSS class(es) to apply to the resulting html table.escapebool, default TrueConvert the characters <, >, and & to HTML-safe sequences.notebook{True, False}, default FalseWhether the generated HTML is for IPython Notebook.borderintAborder=borderattribute is included in the opening<table>tag. Defaultpd.options.display.html.border.table_idstr, optionalA css id is included in the opening<table>tag if specified.render_linksbool, default FalseConvert URLs to HTML links.encodingstr, default Ågutf-8ÅhSet character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns
None.See alsoto_stringConvert DataFrame to a string.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>html_string='''<table border=""1"" class=""dataframe"">...<thead>...<tr style=""text-align: right;"">...<th></th>...<th>col1</th>...<th>col2</th>...</tr>...</thead>...<tbody>...<tr>...<th>0</th>...<td>1</td>...<td>4</td>...</tr>...<tr>...<th>1</th>...<td>2</td>...<td>3</td>...</tr>...</tbody>...</table>'''>>>asserthtml_string==df.to_html()"
Pandas,Input Output,pandas.read_xml,"The `read_xml` function in the pandas library allows you to read XML documents and convert them into a DataFrame object. It was introduced in version 1.3.0 of pandas.The function has the following parameters:- `path_or_buffer` (required): It is a string, path object, or a file-like object that contains the XML document. It can also be a URL.- `xpath` (optional, default='./*'): The XPath used to parse the XML document. It should return a collection of elements.- `namespaces` (optional): A dictionary that contains the namespaces defined in the XML document.- `elems_only` (optional, default=False): If True, only the child elements at the specified XPath will be parsed.- `attrs_only` (optional, default=False): If True, only the attributes at the specified XPath will be parsed.- `names` (optional): A list-like object that contains the column names for the DataFrame.- `dtype` (optional): The data type for the data or columns. It can be a type name or a dictionary of column -> type.- `converters` (optional): A dictionary of functions for converting values in certain columns.- `parse_dates` (optional): Identifiers to parse the index or columns to datetime.- `encoding` (optional, default='utf-8'): The encoding of the XML document.- `parser` (optional, default='lxml'): The parser module to use for retrieval of data. Only 'lxml' and 'etree' are supported.- `stylesheet` (optional): An XSLT script used to flatten complex, deeply nested XML documents for easier parsing.- `iterparse` (optional): The nodes or attributes to retrieve in iterparsing of the XML document.- `compression` (optional, default='infer'): The compression method to use for on-the-fly decompression of on-disk data.- `storage_options` (optional): Extra options that make sense for a particular storage connection.- `dtype_backend` (optional, default='numpy_nullable'): The back-end data type applied to the resultant DataFrame.The function returns a DataFrame.The examples provided in the documentation show how to use the `read_xml` function with different XML documents. It demonstrates how to parse XML documents with different structures using the `xpath` parameter, how to handle namespaces, and how to specify data types and converters. It also provides examples of using the `parse_dates` parameter to parse dates in the XML document.Overall, the `read_xml` function is a convenient way to import XML documents into pandas DataFrames, particularly for shallow XML documents with a specific structure. However, for more complex XML documents, the function provides the `stylesheet` parameter for temporary redesign of the original document with XSLT."
Pandas,Input Output,pandas.DataFrame.to_xml,"The DataFrame.to_xml() function in the pandas library allows you to render a DataFrame into an XML document. This function was introduced in version 1.3.0 of pandas.The function takes several parameters:- path_or_buffer: This parameter specifies the output destination for the XML document. It can be a string representing a file path, a path object, a file-like object with a write() function, or None. If this parameter is None, the result is returned as a string.- index: This boolean parameter determines whether to include the index in the XML document. By default, it is set to True.- root_name: This parameter specifies the name of the root element in the XML document. The default name is 'data'.- row_name: This parameter specifies the name of the row element in the XML document. The default name is 'row'.- na_rep: This optional parameter specifies the representation for missing data. If not specified, missing values will be omitted from the XML document.- attr_cols: This optional parameter is a list-like object that specifies which columns should be written as attributes in the row element. If hierarchical columns are present, they will be flattened with underscores delimiting the different levels.- elem_cols: This optional parameter is a list-like object that specifies which columns should be written as children in the row element. By default, all columns are output as children of the row element. If hierarchical columns are present, they will be flattened with underscores delimiting the different levels.- namespaces: This optional parameter is a dictionary that specifies all namespaces to be defined in the root element. Keys of the dictionary should be prefix names and values should be corresponding URIs. Default namespaces should be given an empty string key.- prefix: This optional parameter specifies the namespace prefix to be used for every element and/or attribute in the document. This should be one of the keys in the namespaces dictionary.- encoding: This parameter specifies the encoding of the resulting document. The default encoding is 'utf-8'.- xml_declaration: This boolean parameter determines whether to include the XML declaration at the start of the document. By default, it is set to True.- pretty_print: This boolean parameter determines whether the output should be pretty printed with indentation and line breaks. By default, it is set to True.- parser: This parameter specifies the parser module to use for building the XML tree. Only 'lxml' and 'etree' are supported. By default, 'lxml' is used.- stylesheet: This optional parameter specifies an XSLT script used to transform the raw XML output. The script should use the layout of elements and attributes from the original output. This parameter requires the 'lxml' parser to be installed. Only XSLT 1.0 scripts are currently supported.- compression: This parameter allows for on-the-fly compression of the output data. The compression method can be automatically detected from the file extension in the path_or_buffer parameter. It can also be specified as a dictionary with additional compression options.- storage_options: This optional parameter allows for additional options that make sense for a particular storage connection, such as host, port, username, password, etc.The function returns None if the path_or_buffer parameter is specified. Otherwise, it returns the resulting XML format as a string.Here are some examples of using the DataFrame.to_xml() function:``` pythondf = pd.DataFrame({'shape': ['square', 'circle', 'triangle'],                   'degrees': [360, 360, 180],                   'sides': [4, np.nan, 3]})df.to_xml()```Output:``` xml<?xml version='1.0' encoding='utf-8'?><data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data>`````` pythondf.to_xml(attr_cols=['index', 'shape', 'degrees', 'sides'])```Output:``` xml<?xml version='1.0' encoding='utf-8'?><data><row index=""0"" shape=""square"" degrees=""360"" sides=""4.0""/><row index=""1"" shape=""circle"" degrees=""360""/><row index=""2"" shape=""triangle"" degrees=""180"" sides=""3.0""/></data>`````` pythondf.to_xml(namespaces={""doc"": ""https://example.com""}, prefix=""doc"")```Output:``` xml<?xml version='1.0' encoding='utf-8'?><doc:data xmlns:doc=""https://example.com""><doc:row><doc:index>0</doc:index><doc:shape>square</doc:shape><doc:degrees>360</doc:degrees><doc:sides>4.0</doc:sides></doc:row><doc:row><doc:index>1</doc:index><doc:shape>circle</doc:shape><doc:degrees>360</doc:degrees><doc:sides/></doc:row><doc:row><doc:index>2</doc:index><doc:shape>triangle</doc:shape><doc:degrees>180</doc:degrees><doc:sides>3.0</doc:sides></doc:row></doc:data>```"
Pandas,Input Output,pandas.read_hdf,"pandas.read_hdf#pandas.read_hdf(path_or_buf,key=None,mode='r',errors='strict',where=None,start=None,stop=None,columns=None,iterator=False,chunksize=None,**kwargs)[source]#Read from the store, close it if we opened it.Retrieve pandas object stored in file, optionally based on where
criteria.WarningPandas uses PyTables for reading and writing HDF5 files, which allows
serializing object-dtype data with pickle when using the ÅgfixedÅh format.
Loading pickled data received from untrusted sources can be unsafe.See:https://docs.python.org/3/library/pickle.htmlfor more.Parameters:path_or_bufstr, path object, pandas.HDFStoreAny valid string path is acceptable. Only supports the local file system,
remote URLs and file-like objects are not supported.If you want to pass in a path object, pandas accepts anyos.PathLike.Alternatively, pandas accepts an openpandas.HDFStoreobject.keyobject, optionalThe group identifier in the store. Can be omitted if the HDF file
contains a single pandas object.mode{ÅerÅf, Åer+Åf, ÅeaÅf}, default ÅerÅfMode to use when opening the file. Ignored if path_or_buf is apandas.HDFStore. Default is ÅerÅf.errorsstr, default ÅestrictÅfSpecifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.wherelist, optionalA list of Term (or convertible) objects.startint, optionalRow number to start selection.stopint, optionalRow number to stop selection.columnslist, optionalA list of columns names to return.iteratorbool, optionalReturn an iterator object.chunksizeint, optionalNumber of rows to include in an iteration when using an iterator.**kwargsAdditional keyword arguments passed to HDFStore.Returns:objectThe selected object. Return type depends on the object stored.See alsoDataFrame.to_hdfWrite a HDF file from a DataFrame.HDFStoreLow-level access to HDF files.Examples>>>df=pd.DataFrame([[1,1.0,'a']],columns=['x','y','z'])>>>df.to_hdf('./store.h5','data')>>>reread=pd.read_hdf('./store.h5')"
Pandas,Input Output,pandas.DataFrame.to_feather,"pandas.DataFrame.to_feather#DataFrame.to_feather(path,**kwargs)[source]#Write a DataFrame to the binary Feather format.Parameters:pathstr, path object, file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. If a string or a path,
it will be used as Root Directory path when writing a partitioned dataset.**kwargsAdditional keywords passed topyarrow.feather.write_feather().
This includes thecompression,compression_level,chunksizeandversionkeywords.NotesThis function writes the dataframe as afeather file. Requires a default
index. For saving the DataFrame with your custom index use a method that
supports custom indices e.g.to_parquet.Examples>>>df=pd.DataFrame([[1,2,3],[4,5,6]])>>>df.to_feather(""file.feather"")"
Pandas,Input Output,pandas.read_parquet,"pandas.read_parquet#pandas.read_parquet(path,engine='auto',columns=None,storage_options=None,use_nullable_dtypes=_NoDefault.no_default,dtype_backend=_NoDefault.no_default,filesystem=None,filters=None,**kwargs)[source]#Load a parquet object from the file path, returning a DataFrame.Parameters:pathstr, path object or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binaryread()function.
The string could be a URL. Valid URL schemes include http, ftp, s3,
gs, and file. For file URLs, a host is expected. A local file could be:file://localhost/path/to/table.parquet.
A file URL can also be a path to a directory that contains multiple
partitioned parquet files. Both pyarrow and fastparquet support
paths to directories as well as file URLs. A directory path could be:file://localhost/path/to/tablesors3://bucket/partition_dir.engine{ÅeautoÅf, ÅepyarrowÅf, ÅefastparquetÅf}, default ÅeautoÅfParquet library to use. If ÅeautoÅf, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try ÅepyarrowÅf, falling back to ÅefastparquetÅf if
ÅepyarrowÅf is unavailable.When using the'pyarrow'engine and no storage options are provided
and a filesystem is implemented by bothpyarrow.fsandfsspec(e.g. Ågs3://Åh), then thepyarrow.fsfilesystem is attempted first.
Use the filesystem keyword with an instantiated fsspec filesystem
if you wish to use its implementation.columnslist, default=NoneIf not None, only these columns will be read from the file.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.3.0.use_nullable_dtypesbool, default FalseIf True, use dtypes that usepd.NAas missing value indicator
for the resulting DataFrame. (only applicable for thepyarrowengine)
As new dtypes are added that supportpd.NAin the future, the
output with this option will change to use those dtypes.
Note: this is an experimental option, and behaviour (e.g. additional
support dtypes) may change without notice.Deprecated since version 2.0.dtype_backend{Åenumpy_nullableÅf, ÅepyarrowÅf}, default Åenumpy_nullableÅfBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file. Only implemented
forengine=""pyarrow"".New in version 2.1.0.filtersList[Tuple] or List[List[Tuple]], default NoneTo filter out data.
Filter syntax: [[(column, op, val), Åc],Åc]
where op is [==, =, >, >=, <, <=, !=, in, not in]
The innermost tuples are transposed into a set of filters applied
through anANDoperation.
The outer list combines these sets of filters through anORoperation.
A single list of tuples can also be used, meaning that noORoperation between set of filters is to be conducted.Using this argument will NOT result in row-wise filtering of the final
partitions unlessengine=""pyarrow""is also specified. For
other engines, filtering is only performed at the partition level, that is,
to prevent the loading of some row-groups and/or files.New in version 2.1.0.**kwargsAny additional kwargs are passed to the engine.Returns:DataFrameSee alsoDataFrame.to_parquetCreate a parquet object that serializes a DataFrame.Examples>>>original_df=pd.DataFrame(...{""foo"":range(5),""bar"":range(5,10)}...)>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>df_parquet_bytes=original_df.to_parquet()>>>fromioimportBytesIO>>>restored_df=pd.read_parquet(BytesIO(df_parquet_bytes))>>>restored_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>restored_df.equals(original_df)True>>>restored_bar=pd.read_parquet(BytesIO(df_parquet_bytes),columns=[""bar""])>>>restored_barbar0    51    62    73    84    9>>>restored_bar.equals(original_df[['bar']])TrueThe function useskwargsthat are passed directly to the engine.
In the following example, we use thefiltersargument of the pyarrow
engine to filter the rows of the DataFrame.Sincepyarrowis the default engine, we can omit theengineargument.
Note that thefiltersargument is implemented by thepyarrowengine,
which can benefit from multithreading and also potentially be more
economical in terms of memory.>>>sel=[(""foo"","">"",2)]>>>restored_part=pd.read_parquet(BytesIO(df_parquet_bytes),filters=sel)>>>restored_partfoo  bar0    3    81    4    9"
Pandas,Input Output,pandas.DataFrame.to_parquet,"pandas.DataFrame.to_parquet#DataFrame.to_parquet(path=None,*,engine='auto',compression='snappy',index=None,partition_cols=None,storage_options=None,**kwargs)[source]#Write a DataFrame to the binary parquet format.This function writes the dataframe as aparquet file. You can choose different parquet
backends, and have the option of compression. Seethe user guidefor more details.Parameters:pathstr, path object, file-like object, or None, default NoneString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. If None, the result is
returned as bytes. If a string or path, it will be used as Root Directory
path when writing a partitioned dataset.engine{ÅeautoÅf, ÅepyarrowÅf, ÅefastparquetÅf}, default ÅeautoÅfParquet library to use. If ÅeautoÅf, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try ÅepyarrowÅf, falling back to ÅefastparquetÅf if
ÅepyarrowÅf is unavailable.compressionstr or None, default ÅesnappyÅfName of the compression to use. UseNonefor no compression.
Supported options: ÅesnappyÅf, ÅegzipÅf, ÅebrotliÅf, Åelz4Åf, ÅezstdÅf.indexbool, default NoneIfTrue, include the dataframeÅfs index(es) in the file output.
IfFalse, they will not be written to the file.
IfNone, similar toTruethe dataframeÅfs index(es)
will be saved. However, instead of being saved as values,
the RangeIndex will be stored as a range in the metadata so it
doesnÅft require much space and is faster. Other indexes will
be included as columns in the file output.partition_colslist, optional, default NoneColumn names by which to partition the dataset.
Columns are partitioned in the order they are given.
Must be None if path is not a string.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with Ågs3://Åh, and Åggcs://Åh) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.**kwargsAdditional arguments passed to the parquet library. Seepandas iofor more details.Returns:bytes if no path argument is provided else NoneSee alsoread_parquetRead a parquet file.DataFrame.to_orcWrite an orc file.DataFrame.to_csvWrite a csv file.DataFrame.to_sqlWrite to a sql table.DataFrame.to_hdfWrite to hdf.NotesThis function requires either thefastparquetorpyarrowlibrary.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[3,4]})>>>df.to_parquet('df.parquet.gzip',...compression='gzip')>>>pd.read_parquet('df.parquet.gzip')col1  col20     1     31     2     4If you want to get a buffer to the parquet content you can use a io.BytesIO
object, as long as you donÅft use partition_cols, which creates multiple files.>>>importio>>>f=io.BytesIO()>>>df.to_parquet(f)>>>f.seek(0)0>>>content=f.read()"
Pandas,Input Output,pandas.read_sql_table,"pandas.read_sql_table#pandas.read_sql_table(table_name,con,schema=None,index_col=None,coerce_float=True,parse_dates=None,columns=None,chunksize=None,dtype_backend=_NoDefault.no_default)[source]#Read SQL database table into a DataFrame.Given a table name and a SQLAlchemy connectable, returns a DataFrame.
This function does not support DBAPI connections.Parameters:table_namestrName of SQL table in database.conSQLAlchemy connectable or strA database URI could be provided as str.
SQLite DBAPI connection mode not supported.schemastr, default NoneName of SQL schema in database to query (if database flavor
supports this). Uses default schema if None (default).index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point. Can result in loss of Precision.parse_dateslist or dict, default NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is
strftime compatible in case of parsing string times or is one of
(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds
to the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,
such as SQLite.columnslist, default NoneList of column names to select from SQL table.chunksizeint, default NoneIf specified, returns an iterator wherechunksizeis the number of
rows to include in each chunk.dtype_backend{Åenumpy_nullableÅf, ÅepyarrowÅf}, default Åenumpy_nullableÅfBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or Iterator[DataFrame]A SQL table is returned as two-dimensional data structure with labeled
axes.See alsoread_sql_queryRead SQL query into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.NotesAny datetime values with time zone information will be converted to UTC.Examples>>>pd.read_sql_table('table_name','postgres:///db_name')"
Pandas,Input Output,pandas.read_sql_query,"pandas.read_sql_query#pandas.read_sql_query(sql,con,index_col=None,coerce_float=True,params=None,parse_dates=None,chunksize=None,dtype=None,dtype_backend=_NoDefault.no_default)[source]#Read SQL query into a DataFrame.Returns a DataFrame corresponding to the result set of the query
string. Optionally provide anindex_colparameter to use one of the
columns as the index, otherwise default integer index will be used.Parameters:sqlstr SQL query or SQLAlchemy Selectable (select or text object)SQL query to be executed.conSQLAlchemy connectable, str, or sqlite3 connectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. If a DBAPI2 object, only sqlite3 is supported.index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point. Useful for SQL result sets.paramslist, tuple or mapping, optional, default: NoneList of parameters to pass to execute method. The syntax used
to pass parameters is database driver dependent. Check your
database driver documentation for which of the five syntax styles,
described in PEP 249Åfs paramstyle, is supported.
Eg. for psycopg2, uses %(name)s so use params={ÅenameÅf : ÅevalueÅf}.parse_dateslist or dict, default: NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is
strftime compatible in case of parsing string times, or is one of
(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds
to the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,
such as SQLite.chunksizeint, default NoneIf specified, return an iterator wherechunksizeis the number of
rows to include in each chunk.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or
{ÅeaÅf: np.float64, ÅebÅf: np.int32, ÅecÅf: ÅeInt64Åf}.New in version 1.3.0.dtype_backend{Åenumpy_nullableÅf, ÅepyarrowÅf}, default Åenumpy_nullableÅfBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.NotesAny datetime values with time zone information parsed via theparse_datesparameter will be converted to UTC.Examples>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine(""sqlite:///database.db"")>>>withengine.connect()asconn,conn.begin():...data=pd.read_sql_table(""data"",conn)"
Pandas,Input Output,pandas.read_sql,"pandas.read_sql#pandas.read_sql(sql,con,index_col=None,coerce_float=True,params=None,parse_dates=None,columns=None,chunksize=None,dtype_backend=_NoDefault.no_default,dtype=None)[source]#Read SQL query or database table into a DataFrame.This function is a convenience wrapper aroundread_sql_tableandread_sql_query(for backward compatibility). It will delegate
to the specific function depending on the provided input. A SQL query
will be routed toread_sql_query, while a database table name will
be routed toread_sql_table. Note that the delegated function might
have more specific notes about their functionality not listed here.Parameters:sqlstr or SQLAlchemy Selectable (select or text object)SQL query to be executed or a table name.conADBC Connection, SQLAlchemy connectable, str, or sqlite3 connectionADBC provides high performance I/O with native type support, where available.
Using SQLAlchemy makes it possible to use any DB supported by that
library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible
for engine disposal and connection closure for the ADBC connection and
SQLAlchemy connectable; str connections are closed automatically. Seehere.index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point, useful for SQL result sets.paramslist, tuple or dict, optional, default: NoneList of parameters to pass to execute method. The syntax used
to pass parameters is database driver dependent. Check your
database driver documentation for which of the five syntax styles,
described in PEP 249Åfs paramstyle, is supported.
Eg. for psycopg2, uses %(name)s so use params={ÅenameÅf : ÅevalueÅf}.parse_dateslist or dict, default: NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is
strftime compatible in case of parsing string times, or is one of
(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds
to the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,
such as SQLite.columnslist, default: NoneList of column names to select from SQL table (only used when reading
a table).chunksizeint, default NoneIf specified, return an iterator wherechunksizeis the
number of rows to include in each chunk.dtype_backend{Åenumpy_nullableÅf, ÅepyarrowÅf}, default Åenumpy_nullableÅfBack-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or
{ÅeaÅf: np.float64, ÅebÅf: np.int32, ÅecÅf: ÅeInt64Åf}.
The argument is ignored if a table is passed instead of a query.New in version 2.0.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sql_queryRead SQL query into a DataFrame.ExamplesRead data from SQL via either a SQL query or a SQL tablename.
When using a SQLite database only SQL queries are accepted,
providing only the SQL tablename will result in an error.>>>fromsqlite3importconnect>>>conn=connect(':memory:')>>>df=pd.DataFrame(data=[[0,'10/11/12'],[1,'12/11/10']],...columns=['int_column','date_column'])>>>df.to_sql(name='test_data',con=conn)2>>>pd.read_sql('SELECT int_column, date_column FROM test_data',conn)int_column date_column0           0    10/11/121           1    12/11/10>>>pd.read_sql('test_data','postgres:///db_name')Apply date parsing to columns through theparse_datesargument
Theparse_datesargument callspd.to_datetimeon the provided columns.
Custom argument values for applyingpd.to_datetimeon a column are specified
via a dictionary format:>>>pd.read_sql('SELECT int_column, date_column FROM test_data',...conn,...parse_dates={""date_column"":{""format"":""%d/%m/%y""}})int_column date_column0           0  2012-11-101           1  2010-11-12New in version 2.2.0:pandas now supports reading via ADBC drivers>>>fromadbc_driver_postgresqlimportdbapi>>>withdbapi.connect('postgres:///db_name')asconn:...pd.read_sql('SELECT int_column FROM test_data',conn)int_column0           01           1"
Pandas,Input Output,pandas.DataFrame.to_sql,"The text provides a reference for the method `to_sql()` in the pandas library. This method allows users to write records stored in a DataFrame to a SQL database. It supports various databases supported by SQLAlchemy.Here are the key parameters for the `to_sql()` method:1. `name` (str): Specifies the name of the SQL table.2. `con` (SQLAlchemy Engine, Connection, or sqlite3.Connection): Specifies the connection object to the database.3. `schema` (str, optional): Specifies the schema (if applicable).4. `if_exists` (str, default 'fail'): Specifies how to behave if the table already exists ('fail', 'replace', 'append').5. `index` (bool, default True): Specifies whether to write the DataFrame index as a column.6. `index_label` (str or sequence, default None): Specifies the label for the index column(s).7. `chunksize` (int, optional): Specifies the number of rows to be written at a time.8. `dtype` (dict or scalar, optional): Specifies the datatype for columns.9. `method` (None, 'multi', callable, optional): Controls the SQL insertion clause used.The method returns the number of rows affected by the operation or None if the callable passed into the `method` parameter does not return an integer number of rows.The text also provides examples of using the `to_sql()` method. It shows how to create an in-memory SQLite database, create a table from a DataFrame, append data to an existing table, overwrite a table, and use the `method` parameter to define a callable insertion method.Furthermore, the text mentions that timezone aware datetime columns will be written as Timestampwithtimezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone.Overall, the `to_sql()` method in pandas provides a convenient way to write DataFrame records to a SQL database, with support for various databases and customization options."
Pandas,Input Output,pandas.read_gbq,"pandas.read_gbq#pandas.read_gbq(query,project_id=None,index_col=None,col_order=None,reauth=False,auth_local_webserver=True,dialect=None,location=None,configuration=None,credentials=None,use_bqstorage_api=None,max_results=None,progress_bar_type=None)[source]#Load data from Google BigQuery.Deprecated since version 2.2.0:Please usepandas_gbq.read_gbqinstead.This function requires thepandas-gbq package.See theHow to authenticate with Google BigQueryguide for authentication instructions.Parameters:querystrSQL-Like Query to return data values.project_idstr, optionalGoogle BigQuery Account project ID. Optional when available from
the environment.index_colstr, optionalName of result column to use for index in results DataFrame.col_orderlist(str), optionalList of BigQuery column names in the desired order for results
DataFrame.reauthbool, default FalseForce Google BigQuery to re-authenticate the user. This is useful
if multiple accounts are used.auth_local_webserverbool, default TrueUse thelocal webserver flowinstead of theconsole flowwhen getting user credentials.New in version 0.2.0 of pandas-gbq.Changed in version 1.5.0:Default value is changed toTrue. Google has deprecated theauth_local_webserver=FalseÅgout of bandÅh (copy-paste)
flow.dialectstr, default ÅelegacyÅfNote: The default value is changing to ÅestandardÅf in a future version.SQL syntax dialect to use. Value can be one of:'legacy'Use BigQueryÅfs legacy SQL dialect. For more information seeBigQuery Legacy SQL Reference.'standard'Use BigQueryÅfs standard SQL, which is
compliant with the SQL 2011 standard. For more information
seeBigQuery Standard SQL Reference.locationstr, optionalLocation where the query job should run. See theBigQuery locations
documentationfor a
list of available locations. The location must match that of any
datasets used in the query.New in version 0.5.0 of pandas-gbq.configurationdict, optionalQuery config parameters for job processing.
For example:configuration = {ÅequeryÅf: {ÅeuseQueryCacheÅf: False}}For more information seeBigQuery REST API Reference.credentialsgoogle.auth.credentials.Credentials, optionalCredentials for accessing Google APIs. Use this parameter to override
default credentials, such as to use Compute Enginegoogle.auth.compute_engine.Credentialsor Service Accountgoogle.oauth2.service_account.Credentialsdirectly.New in version 0.8.0 of pandas-gbq.use_bqstorage_apibool, default FalseUse theBigQuery Storage APIto
download query results quickly, but at an increased cost. To use this
API, firstenable it in the Cloud Console.
You must also have thebigquery.readsessions.createpermission on the project you are billing queries to.This feature requires version 0.10.0 or later of thepandas-gbqpackage. It also requires thegoogle-cloud-bigquery-storageandfastavropackages.max_resultsint, optionalIf set, limit the maximum number of rows to fetch from the query
results.progress_bar_typeOptional, strIf set, use thetqdmlibrary to
display a progress bar while the data downloads. Install thetqdmpackage to use this feature.Possible values ofprogress_bar_typeinclude:NoneNo progress bar.'tqdm'Use thetqdm.tqdm()function to print a progress bar
tosys.stderr.'tqdm_notebook'Use thetqdm.tqdm_notebook()function to display a
progress bar as a Jupyter notebook widget.'tqdm_gui'Use thetqdm.tqdm_gui()function to display a
progress bar as a graphical dialog box.Returns:df: DataFrameDataFrame representing results of query.See alsopandas_gbq.read_gbqThis function in the pandas-gbq library.DataFrame.to_gbqWrite a DataFrame to Google BigQuery.ExamplesExample taken fromGoogle BigQuery documentation>>>sql=""SELECT name FROM table_name WHERE state = 'TX' LIMIT 100;"">>>df=pd.read_gbq(sql,dialect=""standard"")>>>project_id=""your-project-id"">>>df=pd.read_gbq(sql,...project_id=project_id,...dialect=""standard""...)"