{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d12ce70-3d52-4062-9e79-c69071ceae2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index import (\n",
    "    Document,\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    ServiceContext,\n",
    "    LangchainEmbedding\n",
    ")\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt\n",
    "\n",
    "# APIキーなどの設定\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b154ae0-0ed4-4d98-bf57-4505f518ccb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#service_contextの生成\n",
    "def create_service_context():\n",
    "    # LLM Predictor\n",
    "    llm_predictor = LLMPredictor(llm=AzureChatOpenAI(\n",
    "        deployment_name='GPT35TURBO',         #デプロイ名\n",
    "        max_tokens=3000,                        #最大トークン数\n",
    "        temperature=1,                          #出力のランダム度合い\n",
    "        openai_api_version=openai.api_version   #openaiのapiのバージョン情報\n",
    "    ))\n",
    "\n",
    "    # テキストの埋め込みベクトル変換(Embedding)に関する設定\n",
    "    embeddings = LangchainEmbedding(OpenAIEmbeddings(\n",
    "        engine=\"ADA\",        #エンベディングに使うモデル\n",
    "        chunk_size=1,                           #ここでのチャンクサイズはバッチサイズ\n",
    "        openai_api_version=openai.api_version   #openaiのapiのバージョン情報\n",
    "    ))\n",
    "\n",
    "    # Prompt Helper（テキスト分割に関する設定）\n",
    "    prompt_helper = PromptHelper(\n",
    "        max_input_size=3000,    # 最大入力サイズ\n",
    "        num_output=1000,        # LLMの出力サイズ\n",
    "        chunk_size_limit=1000,  # 使用する最大チャンクサイズ（チャンク：テキストを細かく分割したもの）\n",
    "        max_chunk_overlap=0,    # チャンクオーバーラップの最大トークン数\n",
    "        separator=\"。\"        # テキスト分割の区切り文字\n",
    "    )\n",
    "\n",
    "    # Service Contextの生成\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor,    # LLM Predictor\n",
    "        embed_model=embeddings,         # エンベディングについての設定\n",
    "        prompt_helper=prompt_helper     # Prompt Helper\n",
    "    )\n",
    "\n",
    "    return service_context, prompt_helper\n",
    "\n",
    "def llama_generate(index, query, top_k):\n",
    "    \"\"\"llama-indexによる回答の生成\"\"\"\n",
    "    # 与えるコンテキスト（商品リストのうちクエリとの類似度が高いもの）をもとに回答をもとめるようなプロンプト\n",
    "    QA_PROMPT_TMPL = (\n",
    "        \"私たちは以下の情報をコンテキスト情報として与えます。 \\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"この情報をもとに質問に日本語で回答してください。: {query_str}\\n\"\n",
    "    )\n",
    "    qa_prompt = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "    \n",
    "    # 回答生成\n",
    "    \n",
    "    # プロンプトと上位いくつまでの類似度を使用するか設定\n",
    "    query_engine = index.as_query_engine(\n",
    "        engine='GPT35TURBO',#←ここを変更\n",
    "        text_qa_template=qa_prompt, # 上記のプロンプトを与える（デフォルトは英語文）\n",
    "        similarity_top_k=top_k      # 参考情報（商品リスト）のうちクエリとの類似度上位何件を生成に利用するか\n",
    "    )\n",
    "    # 生成\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9143f877-754c-40fb-b678-6ade315ed89e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain/embeddings/openai.py:214: UserWarning: WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "service_context, prompt_helper = create_service_context()\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b31ddd-6dab-4280-bfed-c99142c9457e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc95ccf-1ea9-4da3-9d00-75c61c1e278d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.GPTVectorStoreIndex at 0x7fa602e17110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2163329a-5d68-448a-ae05-8c30c6510a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# クエリ （description：アップロードした顧客情報)\n",
    "query = f\"あなたは顧客に商品を推薦する営業です。\\\n",
    "以下の顧客情報に一番適しているものを提案してください。\\\n",
    "その理由も回答してください。\\\n",
    "また、推薦する際に顧客が購入したくなるような文章を生成してください。\\\n",
    "顧客情報： \\\n",
    "出力形式は以下のようにしてください。\\\n",
    "適しているもの：\\\n",
    "選んだ理由：\\\n",
    "推薦メッセージ：\"\n",
    "\n",
    "# llama-indexによる回答の生成\n",
    "result = llama_generate(index=index, query=query, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ac0529-c00c-422e-b6b0-576e7ac53949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='申し訳ございません。提供いただいた情報から顧客に適した商品を提案することはできません。情報が不足しているか、または提供された情報が理解不能である可能性があります。もし他に何か質問や情報があれば、お知らせください。それに基づいてお手伝いさせていただきます。', source_nodes=[NodeWithScore(node=Node(text=',   0.  +0.j        , # may vary0.  +0.j        ,   0.  +0.j        ],[-12.5+17.20477401j,   0.  +0.j        ,   0.  +0.j        ,0.  +0.j        ,   0.  +0.j        ],[-12.5 +4.0614962j ,   0.  +0.j        ,   0.  +0.j        ,0.  +0.j        ,   0.  +0.j        ],[-12.5 -4.0614962j ,   0.  +0.j        ,   0.  +0.j        ,0.  +0.j        ,   0.  +0.j        ],[-12.5-17.20477401j,   0.  +0.j        ,   0.  +0.j        ,0.  +0.j        ,   0.  +0.j        ]])', doc_id='93cb015a-0717-42cb-8871-38b0ee4dbab4', embedding=None, doc_hash='4b864b4818654d4111020e3913e605c37d84ec563a7fa573b709ac5d826baf17', extra_info=None, node_info={'start': 2667, 'end': 3130, '_node_type': '1'}, relationships={<DocumentRelationship.SOURCE: '1'>: '87035bfc-7f39-441b-afc4-58a631e1d235', <DocumentRelationship.PREVIOUS: '2'>: 'ffcca0f8-0938-4432-82d0-9ac619a30ff4'}), score=0.6218610025173462)], extra_info={'93cb015a-0717-42cb-8871-38b0ee4dbab4': None})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bb114-531a-4470-9b0c-0d73afe302d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
