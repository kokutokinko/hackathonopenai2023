{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c9f0d7-dbc4-4f92-9c8b-be7add7fe629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index import (\n",
    "    Document,\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    ServiceContext,\n",
    "    LangchainEmbedding\n",
    ")\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt\n",
    "\n",
    "\n",
    "\n",
    "# APIキーなどの設定\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15'\n",
    "\n",
    "\n",
    "\n",
    "def llama_index_generate(references):\n",
    "    \"\"\"llama-indexによるインデックスの生成\"\"\"\n",
    "    # LLM Predictor\n",
    "    llm_predictor = LLMPredictor(llm=AzureChatOpenAI(\n",
    "        deployment_name='GPT35TURBO',         # デプロイ名 #←ここを変更\n",
    "        max_tokens=1000,                      # 最大トークン数\n",
    "        temperature=0,                        # 出力のランダム度合い\n",
    "        openai_api_version=openai.api_version # openaiのapiのバージョン情報\n",
    "    ))\n",
    "\n",
    "    #テキストの埋め込みベクトル変換(Embedding)に関する設定\n",
    "    embeddings = LangchainEmbedding(OpenAIEmbeddings(\n",
    "        engine=\"ADA\",       # エンベディングに使うモデル\n",
    "        chunk_size=1,                         # ここでのチャンクサイズはバッチサイズ\n",
    "        openai_api_version=openai.api_version # openaiのapiのバージョン情報\n",
    "    ))\n",
    "\n",
    "    #  Prompt Helper（テキスト分割に関する設定）\n",
    "    prompt_helper = PromptHelper(\n",
    "        max_input_size=3000,    # 最大入力サイズ\n",
    "        num_output=1000,        # LLMの出力サイズ\n",
    "        chunk_size_limit=3000,  # 使用する最大チャンクサイズ（チャンク：テキストを細かく分割したもの）\n",
    "        max_chunk_overlap=0,    # チャンクオーバーラップの最大トークン数\n",
    "        separator=\"。\"          # テキスト分割の区切り文字\n",
    "    )\n",
    "\n",
    "    # Service Context（インデックスを作ったりクエリを実行する際に必要になるものをまとめたもの）\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor, # LLM Predictor\n",
    "        embed_model=embeddings,      # エンベディングについての設定\n",
    "        prompt_helper=prompt_helper  # Prompt Helper\n",
    "    )\n",
    "\n",
    "    # インデックスの生成\n",
    "    index = GPTVectorStoreIndex.from_documents(\n",
    "        references,                      # 参考として与えたデータ（商品リスト）\n",
    "        service_context=service_context, # Service Context\n",
    "        prompt_helper=prompt_helper      # Prompt Helper\n",
    "    )\n",
    "\n",
    "    return index\n",
    "\n",
    "def llama_index_getdocument(text_list):\n",
    "    \"\"\"テキストのリストをまとめたdocumentsを返す\"\"\"\n",
    "    documents = [Document(t) for t in text_list]\n",
    "\n",
    "    return documents\n",
    "\n",
    "def llama_generate(index, query, top_k):\n",
    "    \"\"\"llama-indexによる回答の生成\"\"\"\n",
    "    # 与えるコンテキスト（商品リストのうちクエリとの類似度が高いもの）をもとに回答をもとめるようなプロンプト\n",
    "    QA_PROMPT_TMPL = (\n",
    "        \"私たちは以下の情報をコンテキスト情報として与えます。 \\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"この情報をもとに質問に日本語で回答してください。: {query_str}\\n\"\n",
    "    )\n",
    "    qa_prompt = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "    \n",
    "    # 回答生成\n",
    "    with st.spinner(\"検索中（1分ほどかかります）...\"):\n",
    "        # プロンプトと上位いくつまでの類似度を使用するか設定\n",
    "        query_engine = index.as_query_engine(\n",
    "            engine='gpt-35-turbo',#←ここを変更\n",
    "            text_qa_template=qa_prompt, # 上記のプロンプトを与える（デフォルトは英語文）\n",
    "            similarity_top_k=top_k      # 参考情報（商品リスト）のうちクエリとの類似度上位何件を生成に利用するか\n",
    "        )\n",
    "        # 生成\n",
    "        response = query_engine.query(query)\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_chatgpt_response(past_messages):\n",
    "    \"\"\"ChatGPTにより回答生成\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"GPT35TURBO\", \n",
    "        messages=past_messages,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96ab916-1f60-41fb-a217-dc38e3fe88d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"output.csv\", encoding=\"shift-jis\")\n",
    "columns = df.columns\n",
    "df[\"_text\"] = \"\"\n",
    "for column in columns:\n",
    "    df[\"_text\"] = df[\"_text\"] + f\"【{column}】\" + df[column]\n",
    "document_list = df[\"_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eebeaa-5cb2-4a2a-ae0c-2bbb8790df1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = llama_index_getdocument(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c4da96-8b5d-4cbe-b417-311fecaa49ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain/embeddings/openai.py:214: UserWarning: WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "index = llama_index_generate(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d5cb2c-6e5f-4fd3-8012-2139354d9ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.GPTVectorStoreIndex at 0x7f29ecc23f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb5e700-2a7b-4004-955b-b3b9efe779a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#storageにindexを保存する\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f94c0-3089-4c31-8147-ea4c2f9532c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
