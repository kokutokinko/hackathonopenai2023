ライブラリ名,章,節,内容
Pandas,General functions,pandas.melt,"pandas.melt#pandas.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None,ignore_index=True)[source]#Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.This function is useful to massage a DataFrame into a format where one
or more columns are identifier variables (id_vars), while all other
columns, considered measured variables (value_vars), are “unpivoted” to
the row axis, leaving just two non-identifier columns, ‘variable’ and
‘value’.Parameters:id_varstuple, list, or ndarray, optionalColumn(s) to use as identifier variables.value_varstuple, list, or ndarray, optionalColumn(s) to unpivot. If not specified, uses all columns that
are not set asid_vars.var_namescalarName to use for the ‘variable’ column. If None it usesframe.columns.nameor ‘variable’.value_namescalar, default ‘value’Name to use for the ‘value’ column.col_levelint or str, optionalIf columns are a MultiIndex then use this level to melt.ignore_indexbool, default TrueIf True, original index is ignored. If False, the original index is retained.
Index labels will be repeated as necessary.Returns:DataFrameUnpivoted DataFrame.See alsoDataFrame.meltIdentical method.pivot_tableCreate a spreadsheet-style pivot table as a DataFrame.DataFrame.pivotReturn reshaped DataFrame organized by given index / column values.DataFrame.explodeExplode a DataFrame from list-like columns to long format.NotesReferencethe user guidefor more examples.Examples>>>df=pd.DataFrame({'A':{0:'a',1:'b',2:'c'},...'B':{0:1,1:3,2:5},...'C':{0:2,1:4,2:6}})>>>dfA  B  C0  a  1  21  b  3  42  c  5  6>>>pd.melt(df,id_vars=['A'],value_vars=['B'])A variable  value0  a        B      11  b        B      32  c        B      5>>>pd.melt(df,id_vars=['A'],value_vars=['B','C'])A variable  value0  a        B      11  b        B      32  c        B      53  a        C      24  b        C      45  c        C      6The names of ‘variable’ and ‘value’ columns can be customized:>>>pd.melt(df,id_vars=['A'],value_vars=['B'],...var_name='myVarname',value_name='myValname')A myVarname  myValname0  a         B          11  b         B          32  c         B          5Original index values can be kept around:>>>pd.melt(df,id_vars=['A'],value_vars=['B','C'],ignore_index=False)A variable  value0  a        B      11  b        B      32  c        B      50  a        C      21  b        C      42  c        C      6If you have multi-index columns:>>>df.columns=[list('ABC'),list('DEF')]>>>dfA  B  CD  E  F0  a  1  21  b  3  42  c  5  6>>>pd.melt(df,col_level=0,id_vars=['A'],value_vars=['B'])A variable  value0  a        B      11  b        B      32  c        B      5>>>pd.melt(df,id_vars=[('A','D')],value_vars=[('B','E')])(A, D) variable_0 variable_1  value0      a          B          E      11      b          B          E      32      c          B          E      5"
Pandas,General functions,pandas.pivot,"pandas.pivot#pandas.pivot(data,*,columns,index=_NoDefault.no_default,values=_NoDefault.no_default)[source]#Return reshaped DataFrame organized by given index / column values.Reshape data (produce a “pivot” table) based on column values. Uses
unique values from specifiedindex/columnsto form axes of the
resulting DataFrame. This function does not support data
aggregation, multiple values will result in a MultiIndex in the
columns. See theUser Guidefor more on reshaping.Parameters:dataDataFramecolumnsstr or object or a list of strColumn to use to make new frame’s columns.indexstr or object or a list of str, optionalColumn to use to make new frame’s index. If not given, uses existing index.valuesstr, object or a list of the previous, optionalColumn(s) to use for populating new frame’s values. If not
specified, all remaining columns will be used and the result will
have hierarchically indexed columns.Returns:DataFrameReturns reshaped DataFrame.Raises:ValueError:When there are anyindex,columnscombinations with multiple
values.DataFrame.pivot_tablewhen you need to aggregate.See alsoDataFrame.pivot_tableGeneralization of pivot that can handle duplicate values for one index/column pair.DataFrame.unstackPivot based on the index values instead of a column.wide_to_longWide panel to long format. Less flexible but more user-friendly than melt.NotesFor finer-tuned control, see hierarchical indexing documentation along
with the related stack/unstack methods.Referencethe user guidefor more examples.Examples>>>df=pd.DataFrame({'foo':['one','one','one','two','two',...'two'],...'bar':['A','B','C','A','B','C'],...'baz':[1,2,3,4,5,6],...'zoo':['x','y','z','q','w','t']})>>>dffoo   bar  baz  zoo0   one   A    1    x1   one   B    2    y2   one   C    3    z3   two   A    4    q4   two   B    5    w5   two   C    6    t>>>df.pivot(index='foo',columns='bar',values='baz')bar  A   B   Cfooone  1   2   3two  4   5   6>>>df.pivot(index='foo',columns='bar')['baz']bar  A   B   Cfooone  1   2   3two  4   5   6>>>df.pivot(index='foo',columns='bar',values=['baz','zoo'])baz       zoobar   A  B  C   A  B  Cfooone   1  2  3   x  y  ztwo   4  5  6   q  w  tYou could also assign a list of column names or a list of index names.>>>df=pd.DataFrame({...""lev1"":[1,1,1,2,2,2],...""lev2"":[1,1,2,1,1,2],...""lev3"":[1,2,1,2,1,2],...""lev4"":[1,2,3,4,5,6],...""values"":[0,1,2,3,4,5]})>>>dflev1 lev2 lev3 lev4 values0   1    1    1    1    01   1    1    2    2    12   1    2    1    3    23   2    1    2    4    34   2    1    1    5    45   2    2    2    6    5>>>df.pivot(index=""lev1"",columns=[""lev2"",""lev3""],values=""values"")lev2    1         2lev3    1    2    1    2lev11     0.0  1.0  2.0  NaN2     4.0  3.0  NaN  5.0>>>df.pivot(index=[""lev1"",""lev2""],columns=[""lev3""],values=""values"")lev3    1    2lev1  lev21     1  0.0  1.02  2.0  NaN2     1  4.0  3.02  NaN  5.0A ValueError is raised if there are any duplicates.>>>df=pd.DataFrame({""foo"":['one','one','two','two'],...""bar"":['A','A','B','C'],...""baz"":[1,2,3,4]})>>>dffoo bar  baz0  one   A    11  one   A    22  two   B    33  two   C    4Notice that the first two rows are the same for ourindexandcolumnsarguments.>>>df.pivot(index='foo',columns='bar',values='baz')Traceback (most recent call last):...ValueError:Index contains duplicate entries, cannot reshape"
Pandas,General functions,pandas.pivot_table,"pandas.pivot_table#pandas.pivot_table(data,values=None,index=None,columns=None,aggfunc='mean',fill_value=None,margins=False,dropna=True,margins_name='All',observed=False,sort=True)[source]#Create a spreadsheet-style pivot table as a DataFrame.The levels in the pivot table will be stored in MultiIndex objects
(hierarchical indexes) on the index and columns of the result DataFrame.Parameters:dataDataFramevalueslist-like or scalar, optionalColumn or columns to aggregate.indexcolumn, Grouper, array, or list of the previousKeys to group by on the pivot table index. If a list is passed,
it can contain any of the other types (except list). If an array is
passed, it must be the same length as the data and will be used in
the same manner as column values.columnscolumn, Grouper, array, or list of the previousKeys to group by on the pivot table column. If a list is passed,
it can contain any of the other types (except list). If an array is
passed, it must be the same length as the data and will be used in
the same manner as column values.aggfuncfunction, list of functions, dict, default “mean”If a list of functions is passed, the resulting pivot table will have
hierarchical columns whose top level are the function names
(inferred from the function objects themselves).
If a dict is passed, the key is column to aggregate and the value is
function or list of functions. Ifmargin=True, aggfunc will be
used to calculate the partial aggregates.fill_valuescalar, default NoneValue to replace missing values with (in the resulting pivot table,
after aggregation).marginsbool, default FalseIfmargins=True, specialAllcolumns and rows
will be added with partial group aggregates across the categories
on the rows and columns.dropnabool, default TrueDo not include columns whose entries are all NaN. If True,
rows with a NaN value in any column will be omitted before
computing margins.margins_namestr, default ‘All’Name of the row / column that will contain the totals
when margins is True.observedbool, default FalseThis only applies if any of the groupers are Categoricals.
If True: only show observed values for categorical groupers.
If False: show all values for categorical groupers.sortbool, default TrueSpecifies if the result should be sorted.New in version 1.3.0.Returns:DataFrameAn Excel style pivot table.See alsoDataFrame.pivotPivot without aggregation that can handle non-numeric data.DataFrame.meltUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.wide_to_longWide panel to long format. Less flexible but more user-friendly than melt.NotesReferencethe user guidefor more examples.Examples>>>df=pd.DataFrame({""A"":[""foo"",""foo"",""foo"",""foo"",""foo"",...""bar"",""bar"",""bar"",""bar""],...""B"":[""one"",""one"",""one"",""two"",""two"",...""one"",""one"",""two"",""two""],...""C"":[""small"",""large"",""large"",""small"",...""small"",""large"",""small"",""small"",...""large""],...""D"":[1,2,2,3,3,4,5,6,7],...""E"":[2,4,5,5,6,6,8,9,9]})>>>dfA    B      C  D  E0  foo  one  small  1  21  foo  one  large  2  42  foo  one  large  2  53  foo  two  small  3  54  foo  two  small  3  65  bar  one  large  4  66  bar  one  small  5  87  bar  two  small  6  98  bar  two  large  7  9This first example aggregates values by taking the sum.>>>table=pd.pivot_table(df,values='D',index=['A','B'],...columns=['C'],aggfunc=""sum"")>>>tableC        large  smallA   Bbar one    4.0    5.0two    7.0    6.0foo one    4.0    1.0two    NaN    6.0We can also fill missing values using thefill_valueparameter.>>>table=pd.pivot_table(df,values='D',index=['A','B'],...columns=['C'],aggfunc=""sum"",fill_value=0)>>>tableC        large  smallA   Bbar one      4      5two      7      6foo one      4      1two      0      6The next example aggregates by taking the mean across multiple columns.>>>table=pd.pivot_table(df,values=['D','E'],index=['A','C'],...aggfunc={'D':""mean"",'E':""mean""})>>>tableD         EA   Cbar large  5.500000  7.500000small  5.500000  8.500000foo large  2.000000  4.500000small  2.333333  4.333333We can also calculate multiple types of aggregations for any given
value column.>>>table=pd.pivot_table(df,values=['D','E'],index=['A','C'],...aggfunc={'D':""mean"",...'E':[""min"",""max"",""mean""]})>>>tableD   Emean max      mean  minA   Cbar large  5.500000   9  7.500000    6small  5.500000   9  8.500000    8foo large  2.000000   5  4.500000    4small  2.333333   6  4.333333    2"
Pandas,General functions,pandas.crosstab,"pandas.crosstab#pandas.crosstab(index,columns,values=None,rownames=None,colnames=None,aggfunc=None,margins=False,margins_name='All',dropna=True,normalize=False)[source]#Compute a simple cross tabulation of two (or more) factors.By default, computes a frequency table of the factors unless an
array of values and an aggregation function are passed.Parameters:indexarray-like, Series, or list of arrays/SeriesValues to group by in the rows.columnsarray-like, Series, or list of arrays/SeriesValues to group by in the columns.valuesarray-like, optionalArray of values to aggregate according to the factors.
Requiresaggfuncbe specified.rownamessequence, default NoneIf passed, must match number of row arrays passed.colnamessequence, default NoneIf passed, must match number of column arrays passed.aggfuncfunction, optionalIf specified, requiresvaluesbe specified as well.marginsbool, default FalseAdd row/column margins (subtotals).margins_namestr, default ‘All’Name of the row/column that will contain the totals
when margins is True.dropnabool, default TrueDo not include columns whose entries are all NaN.normalizebool, {‘all’, ‘index’, ‘columns’}, or {0,1}, default FalseNormalize by dividing all values by the sum of values.If passed ‘all’ orTrue, will normalize over all values.If passed ‘index’ will normalize over each row.If passed ‘columns’ will normalize over each column.If margins isTrue, will also normalize margin values.Returns:DataFrameCross tabulation of the data.See alsoDataFrame.pivotReshape data based on column values.pivot_tableCreate a pivot table as a DataFrame.NotesAny Series passed will have their name attributes used unless row or column
names for the cross-tabulation are specified.Any input passed containing Categorical data will haveallof its
categories included in the cross-tabulation, even if the actual data does
not contain any instances of a particular category.In the event that there aren’t overlapping indexes an empty DataFrame will
be returned.Referencethe user guidefor more examples.Examples>>>a=np.array([""foo"",""foo"",""foo"",""foo"",""bar"",""bar"",...""bar"",""bar"",""foo"",""foo"",""foo""],dtype=object)>>>b=np.array([""one"",""one"",""one"",""two"",""one"",""one"",...""one"",""two"",""two"",""two"",""one""],dtype=object)>>>c=np.array([""dull"",""dull"",""shiny"",""dull"",""dull"",""shiny"",...""shiny"",""dull"",""shiny"",""shiny"",""shiny""],...dtype=object)>>>pd.crosstab(a,[b,c],rownames=['a'],colnames=['b','c'])b   one        twoc   dull shiny dull shinyabar    1     2    1     0foo    2     2    1     2Here ‘c’ and ‘f’ are not represented in the data and will not be
shown in the output because dropna is True by default. Set
dropna=False to preserve categories with no data.>>>foo=pd.Categorical(['a','b'],categories=['a','b','c'])>>>bar=pd.Categorical(['d','e'],categories=['d','e','f'])>>>pd.crosstab(foo,bar)col_0  d  erow_0a      1  0b      0  1>>>pd.crosstab(foo,bar,dropna=False)col_0  d  e  frow_0a      1  0  0b      0  1  0c      0  0  0"
Pandas,General functions,pandas.cut,"pandas.cut#pandas.cut(x,bins,right=True,labels=None,retbins=False,precision=3,include_lowest=False,duplicates='raise',ordered=True)[source]#Bin values into discrete intervals.Usecutwhen you need to segment and sort data values into bins. This
function is also useful for going from a continuous variable to a
categorical variable. For example,cutcould convert ages to groups of
age ranges. Supports binning into an equal number of bins, or a
pre-specified array of bins.Parameters:xarray-likeThe input array to be binned. Must be 1-dimensional.binsint, sequence of scalars, or IntervalIndexThe criteria to bin by.int : Defines the number of equal-width bins in the range ofx. The
range ofxis extended by .1% on each side to include the minimum
and maximum values ofx.sequence of scalars : Defines the bin edges allowing for non-uniform
width. No extension of the range ofxis done.IntervalIndex : Defines the exact bins to be used. Note that
IntervalIndex forbinsmust be non-overlapping.rightbool, default TrueIndicates whetherbinsincludes the rightmost edge or not. Ifright==True(the default), then thebins[1,2,3,4]indicate (1,2], (2,3], (3,4]. This argument is ignored whenbinsis an IntervalIndex.labelsarray or False, default NoneSpecifies the labels for the returned bins. Must be the same length as
the resulting bins. If False, returns only integer indicators of the
bins. This affects the type of the output container (see below).
This argument is ignored whenbinsis an IntervalIndex. If True,
raises an error. Whenordered=False, labels must be provided.retbinsbool, default FalseWhether to return the bins or not. Useful when bins is provided
as a scalar.precisionint, default 3The precision at which to store and display the bins labels.include_lowestbool, default FalseWhether the first interval should be left-inclusive or not.duplicates{default ‘raise’, ‘drop’}, optionalIf bin edges are not unique, raise ValueError or drop non-uniques.orderedbool, default TrueWhether the labels are ordered or not. Applies to returned types
Categorical and Series (with Categorical dtype). If True,
the resulting categorical will be ordered. If False, the resulting
categorical will be unordered (labels must be provided).Returns:outCategorical, Series, or ndarrayAn array-like object representing the respective bin for each value
ofx. The type depends on the value oflabels.None (default) : returns a Series for Seriesxor a
Categorical for all other inputs. The values stored within
are Interval dtype.sequence of scalars : returns a Series for Seriesxor a
Categorical for all other inputs. The values stored within
are whatever the type in the sequence is.False : returns an ndarray of integers.binsnumpy.ndarray or IntervalIndex.The computed or specified bins. Only returned whenretbins=True.
For scalar or sequencebins, this is an ndarray with the computed
bins. If setduplicates=drop,binswill drop non-unique bin. For
an IntervalIndexbins, this is equal tobins.See alsoqcutDiscretize variable into equal-sized buckets based on rank or based on sample quantiles.CategoricalArray type for storing data that come from a fixed set of values.SeriesOne-dimensional array with axis labels (including time series).IntervalIndexImmutable Index implementing an ordered, sliceable set.NotesAny NA values will be NA in the result. Out of bounds values will be NA in
the resulting Series or Categorical object.Referencethe user guidefor more examples.ExamplesDiscretize into three equal-sized bins.>>>pd.cut(np.array([1,7,5,4,6,3]),3)...[(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...>>>pd.cut(np.array([1,7,5,4,6,3]),3,retbins=True)...([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...array([0.994, 3.   , 5.   , 7.   ]))Discovers the same bins, but assign them specific labels. Notice that
the returned Categorical’s categories arelabelsand is ordered.>>>pd.cut(np.array([1,7,5,4,6,3]),...3,labels=[""bad"",""medium"",""good""])['bad', 'good', 'medium', 'medium', 'good', 'bad']Categories (3, object): ['bad' < 'medium' < 'good']ordered=Falsewill result in unordered categories when labels are passed.
This parameter can be used to allow non-unique labels:>>>pd.cut(np.array([1,7,5,4,6,3]),3,...labels=[""B"",""A"",""B""],ordered=False)['B', 'B', 'A', 'A', 'B', 'B']Categories (2, object): ['A', 'B']labels=Falseimplies you just want the bins back.>>>pd.cut([0,1,1,2],bins=4,labels=False)array([0, 1, 1, 3])Passing a Series as an input returns a Series with categorical dtype:>>>s=pd.Series(np.array([2,4,6,8,10]),...index=['a','b','c','d','e'])>>>pd.cut(s,3)...a    (1.992, 4.667]b    (1.992, 4.667]c    (4.667, 7.333]d     (7.333, 10.0]e     (7.333, 10.0]dtype: categoryCategories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ...Passing a Series as an input returns a Series with mapping value.
It is used to map numerically to intervals based on bins.>>>s=pd.Series(np.array([2,4,6,8,10]),...index=['a','b','c','d','e'])>>>pd.cut(s,[0,2,4,6,8,10],labels=False,retbins=True,right=False)...(a    1.0b    2.0c    3.0d    4.0e    NaNdtype: float64,array([ 0,  2,  4,  6,  8, 10]))Usedropoptional when bins is not unique>>>pd.cut(s,[0,2,4,6,10,10],labels=False,retbins=True,...right=False,duplicates='drop')...(a    1.0b    2.0c    3.0d    3.0e    NaNdtype: float64,array([ 0,  2,  4,  6, 10]))Passing an IntervalIndex forbinsresults in those categories exactly.
Notice that values not covered by the IntervalIndex are set to NaN. 0
is to the left of the first bin (which is closed on the right), and 1.5
falls between two bins.>>>bins=pd.IntervalIndex.from_tuples([(0,1),(2,3),(4,5)])>>>pd.cut([0,0.5,1.5,2.5,4.5],bins)[NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]]Categories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]]"
Pandas,General functions,pandas.qcut,"pandas.qcut#pandas.qcut(x,q,labels=None,retbins=False,precision=3,duplicates='raise')[source]#Quantile-based discretization function.Discretize variable into equal-sized buckets based on rank or based
on sample quantiles. For example 1000 values for 10 quantiles would
produce a Categorical object indicating quantile membership for each data point.Parameters:x1d ndarray or Seriesqint or list-like of floatNumber of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately
array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.labelsarray or False, default NoneUsed as labels for the resulting bins. Must be of the same length as
the resulting bins. If False, return only integer indicators of the
bins. If True, raises an error.retbinsbool, optionalWhether to return the (bins, labels) or not. Can be useful if bins
is given as a scalar.precisionint, optionalThe precision at which to store and display the bins labels.duplicates{default ‘raise’, ‘drop’}, optionalIf bin edges are not unique, raise ValueError or drop non-uniques.Returns:outCategorical or Series or array of integers if labels is FalseThe return type (Categorical or Series) depends on the input: a Series
of type category if input is a Series else Categorical. Bins are
represented as categories when categorical data is returned.binsndarray of floatsReturned only ifretbinsis True.NotesOut of bounds values will be NA in the resulting Categorical objectExamples>>>pd.qcut(range(5),4)...[(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]]Categories (4, interval[float64, right]): [(-0.001, 1.0] < (1.0, 2.0] ...>>>pd.qcut(range(5),3,labels=[""good"",""medium"",""bad""])...[good, good, medium, bad, bad]Categories (3, object): [good < medium < bad]>>>pd.qcut(range(5),4,labels=False)array([0, 0, 1, 2, 3])"
Pandas,General functions,pandas.merge,"pandas.merge#pandas.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,sort=False,suffixes=('_x','_y'),copy=None,indicator=False,validate=None)[source]#Merge DataFrame or named Series objects with a database-style join.A named Series object is treated as a DataFrame with a single named column.The join is done on columns or indexes. If joining columns on
columns, the DataFrame indexeswill be ignored. Otherwise if joining indexes
on indexes or indexes on a column or columns, the index will be passed on.
When performing a cross merge, no column specifications to merge on are
allowed.WarningIf both key columns contain rows where the key is a null value, those
rows will be matched against each other. This is different from usual SQL
join behaviour and can lead to unexpected results.Parameters:leftDataFrame or named SeriesrightDataFrame or named SeriesObject to merge with.how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’Type of merge to be performed.left: use only keys from left frame, similar to a SQL left outer join;
preserve key order.right: use only keys from right frame, similar to a SQL right outer join;
preserve key order.outer: use union of keys from both frames, similar to a SQL full outer
join; sort keys lexicographically.inner: use intersection of keys from both frames, similar to a SQL inner
join; preserve the order of the left keys.cross: creates the cartesian product from both frames, preserves the order
of the left keys.New in version 1.2.0.onlabel or listColumn or index level names to join on. These must be found in both
DataFrames. Ifonis None and not merging on indexes then this defaults
to the intersection of the columns in both DataFrames.left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also
be an array or list of arrays of the length of the left DataFrame.
These arrays are treated as if they are columns.right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also
be an array or list of arrays of the length of the right DataFrame.
These arrays are treated as if they are columns.left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a
MultiIndex, the number of keys in the other DataFrame (either the index
or a number of columns) must match the number of levels.right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as
left_index.sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False,
the order of the join keys depends on the join type (how keyword).suffixeslist-like, default is (“_x”, “_y”)A length-2 sequence where each element is optionally a string
indicating the suffix to add to overlapping column names inleftandrightrespectively. Pass a value ofNoneinstead
of a string to indicate that the column name fromleftorrightshould be left as-is, with no suffix. At least one of the
values must not be None.copybool, default TrueIf False, avoid copy if possible.indicatorbool or str, default FalseIf True, adds a column to the output DataFrame called “_merge” with
information on the source of each row. The column can be given a different
name by providing a string argument. The column will have a Categorical
type with the value of “left_only” for observations whose merge key only
appears in the left DataFrame, “right_only” for observations
whose merge key only appears in the right DataFrame, and “both”
if the observation’s merge key is found in both DataFrames.validatestr, optionalIf specified, checks if merge is of specified type.“one_to_one” or “1:1”: check if merge keys are unique in both
left and right datasets.“one_to_many” or “1:m”: check if merge keys are unique in left
dataset.“many_to_one” or “m:1”: check if merge keys are unique in right
dataset.“many_to_many” or “m:m”: allowed, but does not result in checks.Returns:DataFrameA DataFrame of the two merged objects.See alsomerge_orderedMerge with optional filling/interpolation.merge_asofMerge on nearest keys.DataFrame.joinSimilar method using indices.Examples>>>df1=pd.DataFrame({'lkey':['foo','bar','baz','foo'],...'value':[1,2,3,5]})>>>df2=pd.DataFrame({'rkey':['foo','bar','baz','foo'],...'value':[5,6,7,8]})>>>df1lkey value0   foo      11   bar      22   baz      33   foo      5>>>df2rkey value0   foo      51   bar      62   baz      73   foo      8Merge df1 and df2 on the lkey and rkey columns. The value columns have
the default suffixes, _x and _y, appended.>>>df1.merge(df2,left_on='lkey',right_on='rkey')lkey  value_x rkey  value_y0  foo        1  foo        51  foo        1  foo        82  foo        5  foo        53  foo        5  foo        84  bar        2  bar        65  baz        3  baz        7Merge DataFrames df1 and df2 with specified left and right suffixes
appended to any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',...suffixes=('_left','_right'))lkey  value_left rkey  value_right0  foo           1  foo            51  foo           1  foo            82  foo           5  foo            53  foo           5  foo            84  bar           2  bar            65  baz           3  baz            7Merge DataFrames df1 and df2, but raise an exception if the DataFrames have
any overlapping columns.>>>df1.merge(df2,left_on='lkey',right_on='rkey',suffixes=(False,False))Traceback (most recent call last):...ValueError:columns overlap but no suffix specified:Index(['value'], dtype='object')>>>df1=pd.DataFrame({'a':['foo','bar'],'b':[1,2]})>>>df2=pd.DataFrame({'a':['foo','baz'],'c':[3,4]})>>>df1a  b0   foo  11   bar  2>>>df2a  c0   foo  31   baz  4>>>df1.merge(df2,how='inner',on='a')a  b  c0   foo  1  3>>>df1.merge(df2,how='left',on='a')a  b  c0   foo  1  3.01   bar  2  NaN>>>df1=pd.DataFrame({'left':['foo','bar']})>>>df2=pd.DataFrame({'right':[7,8]})>>>df1left0   foo1   bar>>>df2right0   71   8>>>df1.merge(df2,how='cross')left  right0   foo      71   foo      82   bar      73   bar      8"
Pandas,General functions,pandas.merge_ordered,"pandas.merge_ordered#pandas.merge_ordered(left,right,on=None,left_on=None,right_on=None,left_by=None,right_by=None,fill_method=None,suffixes=('_x','_y'),how='outer')[source]#Perform a merge for ordered data with optional filling/interpolation.Designed for ordered data like time series data. Optionally
perform group-wise merge (see examples).Parameters:leftDataFrame or named SeriesrightDataFrame or named Seriesonlabel or listField names to join on. Must be found in both DataFrames.left_onlabel or list, or array-likeField names to join on in left DataFrame. Can be a vector or list of
vectors of the length of the DataFrame to use a particular vector as
the join key instead of columns.right_onlabel or list, or array-likeField names to join on in right DataFrame or vector/list of vectors per
left_on docs.left_bycolumn name or list of column namesGroup left DataFrame by group columns and merge piece by piece with
right DataFrame. Must be None if either left or right are a Series.right_bycolumn name or list of column namesGroup right DataFrame by group columns and merge piece by piece with
left DataFrame. Must be None if either left or right are a Series.fill_method{‘ffill’, None}, default NoneInterpolation method for data.suffixeslist-like, default is (“_x”, “_y”)A length-2 sequence where each element is optionally a string
indicating the suffix to add to overlapping column names inleftandrightrespectively. Pass a value ofNoneinstead
of a string to indicate that the column name fromleftorrightshould be left as-is, with no suffix. At least one of the
values must not be None.how{‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘outer’left: use only keys from left frame (SQL: left outer join)right: use only keys from right frame (SQL: right outer join)outer: use union of keys from both frames (SQL: full outer join)inner: use intersection of keys from both frames (SQL: inner join).Returns:DataFrameThe merged DataFrame output type will be the same as
‘left’, if it is a subclass of DataFrame.See alsomergeMerge with a database-style join.merge_asofMerge on nearest keys.Examples>>>frompandasimportmerge_ordered>>>df1=pd.DataFrame(...{...""key"":[""a"",""c"",""e"",""a"",""c"",""e""],...""lvalue"":[1,2,3,1,2,3],...""group"":[""a"",""a"",""a"",""b"",""b"",""b""]...}...)>>>df1key  lvalue group0   a       1     a1   c       2     a2   e       3     a3   a       1     b4   c       2     b5   e       3     b>>>df2=pd.DataFrame({""key"":[""b"",""c"",""d""],""rvalue"":[1,2,3]})>>>df2key  rvalue0   b       11   c       22   d       3>>>merge_ordered(df1,df2,fill_method=""ffill"",left_by=""group"")key  lvalue group  rvalue0   a       1     a     NaN1   b       1     a     1.02   c       2     a     2.03   d       2     a     3.04   e       3     a     3.05   a       1     b     NaN6   b       1     b     1.07   c       2     b     2.08   d       2     b     3.09   e       3     b     3.0"
Pandas,General functions,pandas.merge_asof,"pandas.merge_asof#pandas.merge_asof(left,right,on=None,left_on=None,right_on=None,left_index=False,right_index=False,by=None,left_by=None,right_by=None,suffixes=('_x','_y'),tolerance=None,allow_exact_matches=True,direction='backward')[source]#Perform a merge by key distance.This is similar to a left-join except that we match on nearest
key rather than equal keys. Both DataFrames must be sorted by the key.For each row in the left DataFrame:A “backward” search selects the last row in the right DataFrame whose
‘on’ key is less than or equal to the left’s key.A “forward” search selects the first row in the right DataFrame whose
‘on’ key is greater than or equal to the left’s key.A “nearest” search selects the row in the right DataFrame whose ‘on’
key is closest in absolute distance to the left’s key.Optionally match on equivalent keys with ‘by’ before searching with ‘on’.Parameters:leftDataFrame or named SeriesrightDataFrame or named SeriesonlabelField name to join on. Must be found in both DataFrames.
The data MUST be ordered. Furthermore this must be a numeric column,
such as datetimelike, integer, or float. On or left_on/right_on
must be given.left_onlabelField name to join on in left DataFrame.right_onlabelField name to join on in right DataFrame.left_indexboolUse the index of the left DataFrame as the join key.right_indexboolUse the index of the right DataFrame as the join key.bycolumn name or list of column namesMatch on these columns before performing merge operation.left_bycolumn nameField names to match on in the left DataFrame.right_bycolumn nameField names to match on in the right DataFrame.suffixes2-length sequence (tuple, list, …)Suffix to apply to overlapping column names in the left and right
side, respectively.toleranceint or Timedelta, optional, default NoneSelect asof tolerance within this range; must be compatible
with the merge index.allow_exact_matchesbool, default TrueIf True, allow matching with the same ‘on’ value
(i.e. less-than-or-equal-to / greater-than-or-equal-to)If False, don’t match the same ‘on’ value
(i.e., strictly less-than / strictly greater-than).direction‘backward’ (default), ‘forward’, or ‘nearest’Whether to search for prior, subsequent, or closest matches.Returns:DataFrameSee alsomergeMerge with a database-style join.merge_orderedMerge with optional filling/interpolation.Examples>>>left=pd.DataFrame({""a"":[1,5,10],""left_val"":[""a"",""b"",""c""]})>>>lefta left_val0   1        a1   5        b2  10        c>>>right=pd.DataFrame({""a"":[1,2,3,6,7],""right_val"":[1,2,3,6,7]})>>>righta  right_val0  1          11  2          22  3          33  6          64  7          7>>>pd.merge_asof(left,right,on=""a"")a left_val  right_val0   1        a          11   5        b          32  10        c          7>>>pd.merge_asof(left,right,on=""a"",allow_exact_matches=False)a left_val  right_val0   1        a        NaN1   5        b        3.02  10        c        7.0>>>pd.merge_asof(left,right,on=""a"",direction=""forward"")a left_val  right_val0   1        a        1.01   5        b        6.02  10        c        NaN>>>pd.merge_asof(left,right,on=""a"",direction=""nearest"")a left_val  right_val0   1        a          11   5        b          62  10        c          7We can use indexed DataFrames as well.>>>left=pd.DataFrame({""left_val"":[""a"",""b"",""c""]},index=[1,5,10])>>>leftleft_val1         a5         b10        c>>>right=pd.DataFrame({""right_val"":[1,2,3,6,7]},index=[1,2,3,6,7])>>>rightright_val1          12          23          36          67          7>>>pd.merge_asof(left,right,left_index=True,right_index=True)left_val  right_val1         a          15         b          310        c          7Here is a real-world times-series example>>>quotes=pd.DataFrame(...{...""time"":[...pd.Timestamp(""2016-05-25 13:30:00.023""),...pd.Timestamp(""2016-05-25 13:30:00.023""),...pd.Timestamp(""2016-05-25 13:30:00.030""),...pd.Timestamp(""2016-05-25 13:30:00.041""),...pd.Timestamp(""2016-05-25 13:30:00.048""),...pd.Timestamp(""2016-05-25 13:30:00.049""),...pd.Timestamp(""2016-05-25 13:30:00.072""),...pd.Timestamp(""2016-05-25 13:30:00.075"")...],...""ticker"":[...""GOOG"",...""MSFT"",...""MSFT"",...""MSFT"",...""GOOG"",...""AAPL"",...""GOOG"",...""MSFT""...],...""bid"":[720.50,51.95,51.97,51.99,720.50,97.99,720.50,52.01],...""ask"":[720.93,51.96,51.98,52.00,720.93,98.01,720.88,52.03]...}...)>>>quotestime ticker     bid     ask0 2016-05-25 13:30:00.023   GOOG  720.50  720.931 2016-05-25 13:30:00.023   MSFT   51.95   51.962 2016-05-25 13:30:00.030   MSFT   51.97   51.983 2016-05-25 13:30:00.041   MSFT   51.99   52.004 2016-05-25 13:30:00.048   GOOG  720.50  720.935 2016-05-25 13:30:00.049   AAPL   97.99   98.016 2016-05-25 13:30:00.072   GOOG  720.50  720.887 2016-05-25 13:30:00.075   MSFT   52.01   52.03>>>trades=pd.DataFrame(...{...""time"":[...pd.Timestamp(""2016-05-25 13:30:00.023""),...pd.Timestamp(""2016-05-25 13:30:00.038""),...pd.Timestamp(""2016-05-25 13:30:00.048""),...pd.Timestamp(""2016-05-25 13:30:00.048""),...pd.Timestamp(""2016-05-25 13:30:00.048"")...],...""ticker"":[""MSFT"",""MSFT"",""GOOG"",""GOOG"",""AAPL""],...""price"":[51.95,51.95,720.77,720.92,98.0],...""quantity"":[75,155,100,100,100]...}...)>>>tradestime ticker   price  quantity0 2016-05-25 13:30:00.023   MSFT   51.95        751 2016-05-25 13:30:00.038   MSFT   51.95       1552 2016-05-25 13:30:00.048   GOOG  720.77       1003 2016-05-25 13:30:00.048   GOOG  720.92       1004 2016-05-25 13:30:00.048   AAPL   98.00       100By default we are taking the asof of the quotes>>>pd.merge_asof(trades,quotes,on=""time"",by=""ticker"")time ticker   price  quantity     bid     ask0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.961 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.982 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.933 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.934 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaNWe only asof within 2ms between the quote time and the trade time>>>pd.merge_asof(...trades,quotes,on=""time"",by=""ticker"",tolerance=pd.Timedelta(""2ms"")...)time ticker   price  quantity     bid     ask0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.961 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.933 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.934 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaNWe only asof within 10ms between the quote time and the trade time
and we exclude exact matches on time. Howeverpriordata will
propagate forward>>>pd.merge_asof(...trades,...quotes,...on=""time"",...by=""ticker"",...tolerance=pd.Timedelta(""10ms""),...allow_exact_matches=False...)time ticker   price  quantity     bid     ask0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.982 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN"
Pandas,General functions,pandas.concat,"pandas.concat#pandas.concat(objs,*,axis=0,join='outer',ignore_index=False,keys=None,levels=None,names=None,verify_integrity=False,sort=False,copy=None)[source]#Concatenate pandas objects along a particular axis.Allows optional set logic along the other axes.Can also add a layer of hierarchical indexing on the concatenation axis,
which may be useful if the labels are the same (or overlapping) on
the passed axis number.Parameters:objsa sequence or mapping of Series or DataFrame objectsIf a mapping is passed, the sorted keys will be used as thekeysargument, unless it is passed, in which case the values will be
selected (see below). Any None objects will be dropped silently unless
they are all None in which case a ValueError will be raised.axis{0/’index’, 1/’columns’}, default 0The axis to concatenate along.join{‘inner’, ‘outer’}, default ‘outer’How to handle indexes on other axis (or axes).ignore_indexbool, default FalseIf True, do not use the index values along the concatenation axis. The
resulting axis will be labeled 0, …, n - 1. This is useful if you are
concatenating objects where the concatenation axis does not have
meaningful indexing information. Note the index values on the other
axes are still respected in the join.keyssequence, default NoneIf multiple levels passed, should contain tuples. Construct
hierarchical index using the passed keys as the outermost level.levelslist of sequences, default NoneSpecific levels (unique values) to use for constructing a
MultiIndex. Otherwise they will be inferred from the keys.nameslist, default NoneNames for the levels in the resulting hierarchical index.verify_integritybool, default FalseCheck whether the new concatenated axis contains duplicates. This can
be very expensive relative to the actual data concatenation.sortbool, default FalseSort non-concatenation axis if it is not already aligned.copybool, default TrueIf False, do not copy data unnecessarily.Returns:object, type of objsWhen concatenating allSeriesalong the index (axis=0), aSeriesis returned. Whenobjscontains at least oneDataFrame, aDataFrameis returned. When concatenating along
the columns (axis=1), aDataFrameis returned.See alsoDataFrame.joinJoin DataFrames using indexes.DataFrame.mergeMerge DataFrames by indexes or columns.NotesThe keys, levels, and names arguments are all optional.A walkthrough of how this method fits in with other tools for combining
pandas objects can be foundhere.It is not recommended to build DataFrames by adding single rows in a
for loop. Build a list of rows and make a DataFrame in a single concat.ExamplesCombine twoSeries.>>>s1=pd.Series(['a','b'])>>>s2=pd.Series(['c','d'])>>>pd.concat([s1,s2])0    a1    b0    c1    ddtype: objectClear the existing index and reset it in the result
by setting theignore_indexoption toTrue.>>>pd.concat([s1,s2],ignore_index=True)0    a1    b2    c3    ddtype: objectAdd a hierarchical index at the outermost level of
the data with thekeysoption.>>>pd.concat([s1,s2],keys=['s1','s2'])s1  0    a1    bs2  0    c1    ddtype: objectLabel the index keys you create with thenamesoption.>>>pd.concat([s1,s2],keys=['s1','s2'],...names=['Series name','Row ID'])Series name  Row IDs1           0         a1         bs2           0         c1         ddtype: objectCombine twoDataFrameobjects with identical columns.>>>df1=pd.DataFrame([['a',1],['b',2]],...columns=['letter','number'])>>>df1letter  number0      a       11      b       2>>>df2=pd.DataFrame([['c',3],['d',4]],...columns=['letter','number'])>>>df2letter  number0      c       31      d       4>>>pd.concat([df1,df2])letter  number0      a       11      b       20      c       31      d       4CombineDataFrameobjects with overlapping columns
and return everything. Columns outside the intersection will
be filled withNaNvalues.>>>df3=pd.DataFrame([['c',3,'cat'],['d',4,'dog']],...columns=['letter','number','animal'])>>>df3letter  number animal0      c       3    cat1      d       4    dog>>>pd.concat([df1,df3],sort=False)letter  number animal0      a       1    NaN1      b       2    NaN0      c       3    cat1      d       4    dogCombineDataFrameobjects with overlapping columns
and return only those that are shared by passinginnerto
thejoinkeyword argument.>>>pd.concat([df1,df3],join=""inner"")letter  number0      a       11      b       20      c       31      d       4CombineDataFrameobjects horizontally along the x axis by
passing inaxis=1.>>>df4=pd.DataFrame([['bird','polly'],['monkey','george']],...columns=['animal','name'])>>>pd.concat([df1,df4],axis=1)letter  number  animal    name0      a       1    bird   polly1      b       2  monkey  georgePrevent the result from including duplicate index values with theverify_integrityoption.>>>df5=pd.DataFrame([1],index=['a'])>>>df50a  1>>>df6=pd.DataFrame([2],index=['a'])>>>df60a  2>>>pd.concat([df5,df6],verify_integrity=True)Traceback (most recent call last):...ValueError:Indexes have overlapping values: ['a']Append a single row to the end of aDataFrameobject.>>>df7=pd.DataFrame({'a':1,'b':2},index=[0])>>>df7a   b0   1   2>>>new_row=pd.Series({'a':3,'b':4})>>>new_rowa    3b    4dtype: int64>>>pd.concat([df7,new_row.to_frame().T],ignore_index=True)a   b0   1   21   3   4"
Pandas,General functions,pandas.get_dummies,"pandas.get_dummies#pandas.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False,columns=None,sparse=False,drop_first=False,dtype=None)[source]#Convert categorical variable into dummy/indicator variables.Each variable is converted in as many 0/1 variables as there are different
values. Columns in the output are each named after a value; if the input is
a DataFrame, the name of the original variable is prepended to the value.Parameters:dataarray-like, Series, or DataFrameData of which to get dummy indicators.prefixstr, list of str, or dict of str, default NoneString to append DataFrame column names.
Pass a list with length equal to the number of columns
when calling get_dummies on a DataFrame. Alternatively,prefixcan be a dictionary mapping column names to prefixes.prefix_sepstr, default ‘_’If appending prefix, separator/delimiter to use. Or pass a
list or dictionary as withprefix.dummy_nabool, default FalseAdd a column to indicate NaNs, if False NaNs are ignored.columnslist-like, default NoneColumn names in the DataFrame to be encoded.
Ifcolumnsis None then all the columns withobject,string, orcategorydtype will be converted.sparsebool, default FalseWhether the dummy-encoded columns should be backed by
aSparseArray(True) or a regular NumPy array (False).drop_firstbool, default FalseWhether to get k-1 dummies out of k categorical levels by removing the
first level.dtypedtype, default boolData type for new columns. Only a single dtype is allowed.Returns:DataFrameDummy-coded data. Ifdatacontains other columns than the
dummy-coded one(s), these will be prepended, unaltered, to the result.See alsoSeries.str.get_dummiesConvert Series of strings to dummy codes.from_dummies()Convert dummy codes to categoricalDataFrame.NotesReferencethe user guidefor more examples.Examples>>>s=pd.Series(list('abca'))>>>pd.get_dummies(s)a      b      c0   True  False  False1  False   True  False2  False  False   True3   True  False  False>>>s1=['a','b',np.nan]>>>pd.get_dummies(s1)a      b0   True  False1  False   True2  False  False>>>pd.get_dummies(s1,dummy_na=True)a      b    NaN0   True  False  False1  False   True  False2  False  False   True>>>df=pd.DataFrame({'A':['a','b','a'],'B':['b','a','c'],...'C':[1,2,3]})>>>pd.get_dummies(df,prefix=['col1','col2'])C  col1_a  col1_b  col2_a  col2_b  col2_c0  1    True   False   False    True   False1  2   False    True    True   False   False2  3    True   False   False   False    True>>>pd.get_dummies(pd.Series(list('abcaa')))a      b      c0   True  False  False1  False   True  False2  False  False   True3   True  False  False4   True  False  False>>>pd.get_dummies(pd.Series(list('abcaa')),drop_first=True)b      c0  False  False1   True  False2  False   True3  False  False4  False  False>>>pd.get_dummies(pd.Series(list('abc')),dtype=float)a    b    c0  1.0  0.0  0.01  0.0  1.0  0.02  0.0  0.0  1.0"
Pandas,General functions,pandas.from_dummies,"pandas.from_dummies#pandas.from_dummies(data,sep=None,default_category=None)[source]#Create a categoricalDataFramefrom aDataFrameof dummy variables.Inverts the operation performed byget_dummies().New in version 1.5.0.Parameters:dataDataFrameData which contains dummy-coded variables in form of integer columns of
1’s and 0’s.sepstr, default NoneSeparator used in the column names of the dummy categories they are
character indicating the separation of the categorical names from the prefixes.
For example, if your column names are ‘prefix_A’ and ‘prefix_B’,
you can strip the underscore by specifying sep=’_’.default_categoryNone, Hashable or dict of Hashables, default NoneThe default category is the implied category when a value has none of the
listed categories specified with a one, i.e. if all dummies in a row are
zero. Can be a single value for all variables or a dict directly mapping
the default categories to a prefix of a variable.Returns:DataFrameCategorical data decoded from the dummy input-data.Raises:ValueErrorWhen the inputDataFramedatacontains NA values.When the inputDataFramedatacontains column names with separators
that do not match the separator specified withsep.When adictpassed todefault_categorydoes not include an implied
category for each prefix.When a value indatahas more than one category assigned to it.Whendefault_category=Noneand a value indatahas no category
assigned to it.TypeErrorWhen the inputdatais not of typeDataFrame.When the inputDataFramedatacontains non-dummy data.When the passedsepis of a wrong data type.When the passeddefault_categoryis of a wrong data type.See alsoget_dummies()ConvertSeriesorDataFrameto dummy codes.CategoricalRepresent a categorical variable in classic.NotesThe columns of the passed dummy data should only include 1’s and 0’s,
or boolean values.Examples>>>df=pd.DataFrame({""a"":[1,0,0,1],""b"":[0,1,0,0],...""c"":[0,0,1,0]})>>>dfa  b  c0  1  0  01  0  1  02  0  0  13  1  0  0>>>pd.from_dummies(df)0     a1     b2     c3     a>>>df=pd.DataFrame({""col1_a"":[1,0,1],""col1_b"":[0,1,0],...""col2_a"":[0,1,0],""col2_b"":[1,0,0],...""col2_c"":[0,0,1]})>>>dfcol1_a  col1_b  col2_a  col2_b  col2_c0       1       0       0       1       01       0       1       1       0       02       1       0       0       0       1>>>pd.from_dummies(df,sep=""_"")col1    col20    a       b1    b       a2    a       c>>>df=pd.DataFrame({""col1_a"":[1,0,0],""col1_b"":[0,1,0],...""col2_a"":[0,1,0],""col2_b"":[1,0,0],...""col2_c"":[0,0,0]})>>>dfcol1_a  col1_b  col2_a  col2_b  col2_c0       1       0       0       1       01       0       1       1       0       02       0       0       0       0       0>>>pd.from_dummies(df,sep=""_"",default_category={""col1"":""d"",""col2"":""e""})col1    col20    a       b1    b       a2    d       e"
Pandas,General functions,pandas.factorize,"pandas.factorize#pandas.factorize(values,sort=False,use_na_sentinel=True,size_hint=None)[source]#Encode the object as an enumerated type or categorical variable.This method is useful for obtaining a numeric representation of an
array when all that matters is identifying distinct values.factorizeis available as both a top-level functionpandas.factorize(),
and as a methodSeries.factorize()andIndex.factorize().Parameters:valuessequenceA 1-D sequence. Sequences that aren’t pandas objects are
coerced to ndarrays before factorization.sortbool, default FalseSortuniquesand shufflecodesto maintain the
relationship.use_na_sentinelbool, default TrueIf True, the sentinel -1 will be used for NaN values. If False,
NaN values will be encoded as non-negative integers and will not drop the
NaN from the uniques of the values.New in version 1.5.0.size_hintint, optionalHint to the hashtable sizer.Returns:codesndarrayAn integer ndarray that’s an indexer intouniques.uniques.take(codes)will have the same values asvalues.uniquesndarray, Index, or CategoricalThe unique valid values. Whenvaluesis Categorical,uniquesis a Categorical. Whenvaluesis some other pandas object, anIndexis returned. Otherwise, a 1-D ndarray is returned.NoteEven if there’s a missing value invalues,uniqueswillnotcontain an entry for it.See alsocutDiscretize continuous-valued array.uniqueFind the unique value in an array.NotesReferencethe user guidefor more examples.ExamplesThese examples all show factorize as a top-level method likepd.factorize(values). The results are identical for methods likeSeries.factorize().>>>codes,uniques=pd.factorize(np.array(['b','b','a','c','b'],dtype=""O""))>>>codesarray([0, 0, 1, 2, 0])>>>uniquesarray(['b', 'a', 'c'], dtype=object)Withsort=True, theuniqueswill be sorted, andcodeswill be
shuffled so that the relationship is the maintained.>>>codes,uniques=pd.factorize(np.array(['b','b','a','c','b'],dtype=""O""),...sort=True)>>>codesarray([1, 1, 0, 2, 1])>>>uniquesarray(['a', 'b', 'c'], dtype=object)Whenuse_na_sentinel=True(the default), missing values are indicated in
thecodeswith the sentinel value-1and missing values are not
included inuniques.>>>codes,uniques=pd.factorize(np.array(['b',None,'a','c','b'],dtype=""O""))>>>codesarray([ 0, -1,  1,  2,  0])>>>uniquesarray(['b', 'a', 'c'], dtype=object)Thus far, we’ve only factorized lists (which are internally coerced to
NumPy arrays). When factorizing pandas objects, the type ofuniqueswill differ. For Categoricals, aCategoricalis returned.>>>cat=pd.Categorical(['a','a','c'],categories=['a','b','c'])>>>codes,uniques=pd.factorize(cat)>>>codesarray([0, 0, 1])>>>uniques['a', 'c']Categories (3, object): ['a', 'b', 'c']Notice that'b'is inuniques.categories, despite not being
present incat.values.For all other pandas objects, an Index of the appropriate type is
returned.>>>cat=pd.Series(['a','a','c'])>>>codes,uniques=pd.factorize(cat)>>>codesarray([0, 0, 1])>>>uniquesIndex(['a', 'c'], dtype='object')If NaN is in the values, and we want to include NaN in the uniques of the
values, it can be achieved by settinguse_na_sentinel=False.>>>values=np.array([1,2,1,np.nan])>>>codes,uniques=pd.factorize(values)# default: use_na_sentinel=True>>>codesarray([ 0,  1,  0, -1])>>>uniquesarray([1., 2.])>>>codes,uniques=pd.factorize(values,use_na_sentinel=False)>>>codesarray([0, 1, 0, 2])>>>uniquesarray([ 1.,  2., nan])"
Pandas,General functions,pandas.unique,"pandas.unique#pandas.unique(values)[source]#Return unique values based on a hash table.Uniques are returned in order of appearance. This does NOT sort.Significantly faster than numpy.unique for long enough sequences.
Includes NA values.Parameters:values1d array-likeReturns:numpy.ndarray or ExtensionArrayThe return can be:Index : when the input is an IndexCategorical : when the input is a Categorical dtypendarray : when the input is a Series/ndarrayReturn numpy.ndarray or ExtensionArray.See alsoIndex.uniqueReturn unique values from an Index.Series.uniqueReturn unique values of Series object.Examples>>>pd.unique(pd.Series([2,1,3,3]))array([2, 1, 3])>>>pd.unique(pd.Series([2]+[1]*5))array([2, 1])>>>pd.unique(pd.Series([pd.Timestamp(""20160101""),pd.Timestamp(""20160101"")]))array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')>>>pd.unique(...pd.Series(...[...pd.Timestamp(""20160101"",tz=""US/Eastern""),...pd.Timestamp(""20160101"",tz=""US/Eastern""),...]...)...)<DatetimeArray>['2016-01-01 00:00:00-05:00']Length: 1, dtype: datetime64[ns, US/Eastern]>>>pd.unique(...pd.Index(...[...pd.Timestamp(""20160101"",tz=""US/Eastern""),...pd.Timestamp(""20160101"",tz=""US/Eastern""),...]...)...)DatetimeIndex(['2016-01-01 00:00:00-05:00'],dtype='datetime64[ns, US/Eastern]',freq=None)>>>pd.unique(np.array(list(""baabc""),dtype=""O""))array(['b', 'a', 'c'], dtype=object)An unordered Categorical will return categories in the
order of appearance.>>>pd.unique(pd.Series(pd.Categorical(list(""baabc""))))['b', 'a', 'c']Categories (3, object): ['a', 'b', 'c']>>>pd.unique(pd.Series(pd.Categorical(list(""baabc""),categories=list(""abc""))))['b', 'a', 'c']Categories (3, object): ['a', 'b', 'c']An ordered Categorical preserves the category ordering.>>>pd.unique(...pd.Series(...pd.Categorical(list(""baabc""),categories=list(""abc""),ordered=True)...)...)['b', 'a', 'c']Categories (3, object): ['a' < 'b' < 'c']An array of tuples>>>pd.unique(pd.Series([(""a"",""b""),(""b"",""a""),(""a"",""c""),(""b"",""a"")]).values)array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)"
Pandas,General functions,pandas.lreshape,"pandas.lreshape#pandas.lreshape(data,groups,dropna=True)[source]#Reshape wide-format data to long. Generalized inverse of DataFrame.pivot.Accepts a dictionary,groups, in which each key is a new column name
and each value is a list of old column names that will be “melted” under
the new column name as part of the reshape.Parameters:dataDataFrameThe wide-format DataFrame.groupsdict{new_name : list_of_columns}.dropnabool, default TrueDo not include columns whose entries are all NaN.Returns:DataFrameReshaped DataFrame.See alsomeltUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.pivotCreate a spreadsheet-style pivot table as a DataFrame.DataFrame.pivotPivot without aggregation that can handle non-numeric data.DataFrame.pivot_tableGeneralization of pivot that can handle duplicate values for one index/column pair.DataFrame.unstackPivot based on the index values instead of a column.wide_to_longWide panel to long format. Less flexible but more user-friendly than melt.Examples>>>data=pd.DataFrame({'hr1':[514,573],'hr2':[545,526],...'team':['Red Sox','Yankees'],...'year1':[2007,2007],'year2':[2008,2008]})>>>datahr1  hr2     team  year1  year20  514  545  Red Sox   2007   20081  573  526  Yankees   2007   2008>>>pd.lreshape(data,{'year':['year1','year2'],'hr':['hr1','hr2']})team  year   hr0  Red Sox  2007  5141  Yankees  2007  5732  Red Sox  2008  5453  Yankees  2008  526"
Pandas,General functions,pandas.wide_to_long,"pandas.wide_to_long#pandas.wide_to_long(df,stubnames,i,j,sep='',suffix='\\d+')[source]#Unpivot a DataFrame from wide to long format.Less flexible but more user-friendly than melt.With stubnames [‘A’, ‘B’], this function expects to find one or more
group of columns with format
A-suffix1, A-suffix2,…, B-suffix1, B-suffix2,…
You specify what you want to call this suffix in the resulting long format
withj(for examplej=’year’)Each row of these wide variables are assumed to be uniquely identified byi(can be a single column name or a list of column names)All remaining variables in the data frame are left intact.Parameters:dfDataFrameThe wide-format DataFrame.stubnamesstr or list-likeThe stub name(s). The wide format variables are assumed to
start with the stub names.istr or list-likeColumn(s) to use as id variable(s).jstrThe name of the sub-observation variable. What you wish to name your
suffix in the long format.sepstr, default “”A character indicating the separation of the variable names
in the wide format, to be stripped from the names in the long format.
For example, if your column names are A-suffix1, A-suffix2, you
can strip the hyphen by specifyingsep=’-’.suffixstr, default ‘\d+’A regular expression capturing the wanted suffixes. ‘\d+’ captures
numeric suffixes. Suffixes with no numbers could be specified with the
negated character class ‘\D+’. You can also further disambiguate
suffixes, for example, if your wide variables are of the form A-one,
B-two,.., and you have an unrelated column A-rating, you can ignore the
last one by specifyingsuffix=’(!?one|two)’. When all suffixes are
numeric, they are cast to int64/float64.Returns:DataFrameA DataFrame that contains each stub name as a variable, with new index
(i, j).See alsomeltUnpivot a DataFrame from wide to long format, optionally leaving identifiers set.pivotCreate a spreadsheet-style pivot table as a DataFrame.DataFrame.pivotPivot without aggregation that can handle non-numeric data.DataFrame.pivot_tableGeneralization of pivot that can handle duplicate values for one index/column pair.DataFrame.unstackPivot based on the index values instead of a column.NotesAll extra variables are left untouched. This simply usespandas.meltunder the hood, but is hard-coded to “do the right thing”
in a typical case.Examples>>>np.random.seed(123)>>>df=pd.DataFrame({""A1970"":{0:""a"",1:""b"",2:""c""},...""A1980"":{0:""d"",1:""e"",2:""f""},...""B1970"":{0:2.5,1:1.2,2:.7},...""B1980"":{0:3.2,1:1.3,2:.1},...""X"":dict(zip(range(3),np.random.randn(3)))...})>>>df[""id""]=df.index>>>dfA1970 A1980  B1970  B1980         X  id0     a     d    2.5    3.2 -1.085631   01     b     e    1.2    1.3  0.997345   12     c     f    0.7    0.1  0.282978   2>>>pd.wide_to_long(df,[""A"",""B""],i=""id"",j=""year"")...X  A    Bid year0  1970 -1.085631  a  2.51  1970  0.997345  b  1.22  1970  0.282978  c  0.70  1980 -1.085631  d  3.21  1980  0.997345  e  1.32  1980  0.282978  f  0.1With multiple id columns>>>df=pd.DataFrame({...'famid':[1,1,1,2,2,2,3,3,3],...'birth':[1,2,3,1,2,3,1,2,3],...'ht1':[2.8,2.9,2.2,2,1.8,1.9,2.2,2.3,2.1],...'ht2':[3.4,3.8,2.9,3.2,2.8,2.4,3.3,3.4,2.9]...})>>>dffamid  birth  ht1  ht20      1      1  2.8  3.41      1      2  2.9  3.82      1      3  2.2  2.93      2      1  2.0  3.24      2      2  1.8  2.85      2      3  1.9  2.46      3      1  2.2  3.37      3      2  2.3  3.48      3      3  2.1  2.9>>>l=pd.wide_to_long(df,stubnames='ht',i=['famid','birth'],j='age')>>>l...htfamid birth age1     1     1    2.82    3.42     1    2.92    3.83     1    2.22    2.92     1     1    2.02    3.22     1    1.82    2.83     1    1.92    2.43     1     1    2.22    3.32     1    2.32    3.43     1    2.12    2.9Going from long back to wide just takes some creative use ofunstack>>>w=l.unstack()>>>w.columns=w.columns.map('{0[0]}{0[1]}'.format)>>>w.reset_index()famid  birth  ht1  ht20      1      1  2.8  3.41      1      2  2.9  3.82      1      3  2.2  2.93      2      1  2.0  3.24      2      2  1.8  2.85      2      3  1.9  2.46      3      1  2.2  3.37      3      2  2.3  3.48      3      3  2.1  2.9Less wieldy column names are also handled>>>np.random.seed(0)>>>df=pd.DataFrame({'A(weekly)-2010':np.random.rand(3),...'A(weekly)-2011':np.random.rand(3),...'B(weekly)-2010':np.random.rand(3),...'B(weekly)-2011':np.random.rand(3),...'X':np.random.randint(3,size=3)})>>>df['id']=df.index>>>dfA(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id0        0.548814        0.544883        0.437587        0.383442  0   01        0.715189        0.423655        0.891773        0.791725  1   12        0.602763        0.645894        0.963663        0.528895  1   2>>>pd.wide_to_long(df,['A(weekly)','B(weekly)'],i='id',...j='year',sep='-')...X  A(weekly)  B(weekly)id year0  2010  0   0.548814   0.4375871  2010  1   0.715189   0.8917732  2010  1   0.602763   0.9636630  2011  0   0.544883   0.3834421  2011  1   0.423655   0.7917252  2011  1   0.645894   0.528895If we have many columns, we could also use a regex to find our
stubnames and pass that list on to wide_to_long>>>stubnames=sorted(...set([match[0]formatchindf.columns.str.findall(...r'[A-B]\(.*\)').valuesifmatch!=[]])...)>>>list(stubnames)['A(weekly)', 'B(weekly)']All of the above examples have integers as suffixes. It is possible to
have non-integers as suffixes.>>>df=pd.DataFrame({...'famid':[1,1,1,2,2,2,3,3,3],...'birth':[1,2,3,1,2,3,1,2,3],...'ht_one':[2.8,2.9,2.2,2,1.8,1.9,2.2,2.3,2.1],...'ht_two':[3.4,3.8,2.9,3.2,2.8,2.4,3.3,3.4,2.9]...})>>>dffamid  birth  ht_one  ht_two0      1      1     2.8     3.41      1      2     2.9     3.82      1      3     2.2     2.93      2      1     2.0     3.24      2      2     1.8     2.85      2      3     1.9     2.46      3      1     2.2     3.37      3      2     2.3     3.48      3      3     2.1     2.9>>>l=pd.wide_to_long(df,stubnames='ht',i=['famid','birth'],j='age',...sep='_',suffix=r'\w+')>>>l...htfamid birth age1     1     one  2.8two  3.42     one  2.9two  3.83     one  2.2two  2.92     1     one  2.0two  3.22     one  1.8two  2.83     one  1.9two  2.43     1     one  2.2two  3.32     one  2.3two  3.43     one  2.1two  2.9"
Pandas,General functions,pandas.isna,"pandas.isna#pandas.isna(obj)[source]#Detect missing values for an array-like object.This function takes a scalar or array-like object and indicates
whether values are missing (NaNin numeric arrays,NoneorNaNin object arrays,NaTin datetimelike).Parameters:objscalar or array-likeObject to check for null or missing values.Returns:bool or array-like of boolFor scalar input, returns a scalar boolean.
For array input, returns an array of boolean indicating whether each
corresponding element is missing.See alsonotnaBoolean inverse of pandas.isna.Series.isnaDetect missing values in a Series.DataFrame.isnaDetect missing values in a DataFrame.Index.isnaDetect missing values in an Index.ExamplesScalar arguments (including strings) result in a scalar boolean.>>>pd.isna('dog')False>>>pd.isna(pd.NA)True>>>pd.isna(np.nan)Truendarrays result in an ndarray of booleans.>>>array=np.array([[1,np.nan,3],[4,5,np.nan]])>>>arrayarray([[ 1., nan,  3.],[ 4.,  5., nan]])>>>pd.isna(array)array([[False,  True, False],[False, False,  True]])For indexes, an ndarray of booleans is returned.>>>index=pd.DatetimeIndex([""2017-07-05"",""2017-07-06"",None,...""2017-07-08""])>>>indexDatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],dtype='datetime64[ns]', freq=None)>>>pd.isna(index)array([False, False,  True, False])For Series and DataFrame, the same type is returned, containing booleans.>>>df=pd.DataFrame([['ant','bee','cat'],['dog',None,'fly']])>>>df0     1    20  ant   bee  cat1  dog  None  fly>>>pd.isna(df)0      1      20  False  False  False1  False   True  False>>>pd.isna(df[1])0    False1     TrueName: 1, dtype: bool"
Pandas,General functions,pandas.isnull,"pandas.isnull#pandas.isnull(obj)[source]#Detect missing values for an array-like object.This function takes a scalar or array-like object and indicates
whether values are missing (NaNin numeric arrays,NoneorNaNin object arrays,NaTin datetimelike).Parameters:objscalar or array-likeObject to check for null or missing values.Returns:bool or array-like of boolFor scalar input, returns a scalar boolean.
For array input, returns an array of boolean indicating whether each
corresponding element is missing.See alsonotnaBoolean inverse of pandas.isna.Series.isnaDetect missing values in a Series.DataFrame.isnaDetect missing values in a DataFrame.Index.isnaDetect missing values in an Index.ExamplesScalar arguments (including strings) result in a scalar boolean.>>>pd.isna('dog')False>>>pd.isna(pd.NA)True>>>pd.isna(np.nan)Truendarrays result in an ndarray of booleans.>>>array=np.array([[1,np.nan,3],[4,5,np.nan]])>>>arrayarray([[ 1., nan,  3.],[ 4.,  5., nan]])>>>pd.isna(array)array([[False,  True, False],[False, False,  True]])For indexes, an ndarray of booleans is returned.>>>index=pd.DatetimeIndex([""2017-07-05"",""2017-07-06"",None,...""2017-07-08""])>>>indexDatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],dtype='datetime64[ns]', freq=None)>>>pd.isna(index)array([False, False,  True, False])For Series and DataFrame, the same type is returned, containing booleans.>>>df=pd.DataFrame([['ant','bee','cat'],['dog',None,'fly']])>>>df0     1    20  ant   bee  cat1  dog  None  fly>>>pd.isna(df)0      1      20  False  False  False1  False   True  False>>>pd.isna(df[1])0    False1     TrueName: 1, dtype: bool"
Pandas,General functions,pandas.notna,"pandas.notna#pandas.notna(obj)[source]#Detect non-missing values for an array-like object.This function takes a scalar or array-like object and indicates
whether values are valid (not missing, which isNaNin numeric
arrays,NoneorNaNin object arrays,NaTin datetimelike).Parameters:objarray-like or object valueObject to check fornotnull ornon-missing values.Returns:bool or array-like of boolFor scalar input, returns a scalar boolean.
For array input, returns an array of boolean indicating whether each
corresponding element is valid.See alsoisnaBoolean inverse of pandas.notna.Series.notnaDetect valid values in a Series.DataFrame.notnaDetect valid values in a DataFrame.Index.notnaDetect valid values in an Index.ExamplesScalar arguments (including strings) result in a scalar boolean.>>>pd.notna('dog')True>>>pd.notna(pd.NA)False>>>pd.notna(np.nan)Falsendarrays result in an ndarray of booleans.>>>array=np.array([[1,np.nan,3],[4,5,np.nan]])>>>arrayarray([[ 1., nan,  3.],[ 4.,  5., nan]])>>>pd.notna(array)array([[ True, False,  True],[ True,  True, False]])For indexes, an ndarray of booleans is returned.>>>index=pd.DatetimeIndex([""2017-07-05"",""2017-07-06"",None,...""2017-07-08""])>>>indexDatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],dtype='datetime64[ns]', freq=None)>>>pd.notna(index)array([ True,  True, False,  True])For Series and DataFrame, the same type is returned, containing booleans.>>>df=pd.DataFrame([['ant','bee','cat'],['dog',None,'fly']])>>>df0     1    20  ant   bee  cat1  dog  None  fly>>>pd.notna(df)0      1     20  True   True  True1  True  False  True>>>pd.notna(df[1])0     True1    FalseName: 1, dtype: bool"
Pandas,General functions,pandas.notnull,"pandas.notnull#pandas.notnull(obj)[source]#Detect non-missing values for an array-like object.This function takes a scalar or array-like object and indicates
whether values are valid (not missing, which isNaNin numeric
arrays,NoneorNaNin object arrays,NaTin datetimelike).Parameters:objarray-like or object valueObject to check fornotnull ornon-missing values.Returns:bool or array-like of boolFor scalar input, returns a scalar boolean.
For array input, returns an array of boolean indicating whether each
corresponding element is valid.See alsoisnaBoolean inverse of pandas.notna.Series.notnaDetect valid values in a Series.DataFrame.notnaDetect valid values in a DataFrame.Index.notnaDetect valid values in an Index.ExamplesScalar arguments (including strings) result in a scalar boolean.>>>pd.notna('dog')True>>>pd.notna(pd.NA)False>>>pd.notna(np.nan)Falsendarrays result in an ndarray of booleans.>>>array=np.array([[1,np.nan,3],[4,5,np.nan]])>>>arrayarray([[ 1., nan,  3.],[ 4.,  5., nan]])>>>pd.notna(array)array([[ True, False,  True],[ True,  True, False]])For indexes, an ndarray of booleans is returned.>>>index=pd.DatetimeIndex([""2017-07-05"",""2017-07-06"",None,...""2017-07-08""])>>>indexDatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],dtype='datetime64[ns]', freq=None)>>>pd.notna(index)array([ True,  True, False,  True])For Series and DataFrame, the same type is returned, containing booleans.>>>df=pd.DataFrame([['ant','bee','cat'],['dog',None,'fly']])>>>df0     1    20  ant   bee  cat1  dog  None  fly>>>pd.notna(df)0      1     20  True   True  True1  True  False  True>>>pd.notna(df[1])0     True1    FalseName: 1, dtype: bool"
Pandas,General functions,pandas.to_numeric,"pandas.to_numeric#pandas.to_numeric(arg,errors='raise',downcast=None,dtype_backend=_NoDefault.no_default)[source]#Convert argument to a numeric type.The default return dtype isfloat64orint64depending on the data supplied. Use thedowncastparameter
to obtain other dtypes.Please note that precision loss may occur if really large numbers
are passed in. Due to the internal limitations ofndarray, if
numbers smaller than-9223372036854775808(np.iinfo(np.int64).min)
or larger than18446744073709551615(np.iinfo(np.uint64).max) are
passed in, it is very likely they will be converted to float so that
they can be stored in anndarray. These warnings apply similarly toSeriessince it internally leveragesndarray.Parameters:argscalar, list, tuple, 1-d array, or SeriesArgument to be converted.errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’If ‘raise’, then invalid parsing will raise an exception.If ‘coerce’, then invalid parsing will be set as NaN.If ‘ignore’, then invalid parsing will return the input.downcaststr, default NoneCan be ‘integer’, ‘signed’, ‘unsigned’, or ‘float’.
If not None, and if the data has been successfully cast to a
numerical dtype (or if the data was numeric to begin with),
downcast that resulting data to the smallest numerical dtype
possible according to the following rules:‘integer’ or ‘signed’: smallest signed int dtype (min.: np.int8)‘unsigned’: smallest unsigned int dtype (min.: np.uint8)‘float’: smallest float dtype (min.: np.float32)As this behaviour is separate from the core conversion to
numeric values, any errors raised during the downcasting
will be surfaced regardless of the value of the ‘errors’ input.In addition, downcasting will only occur if the size
of the resulting data’s dtype is strictly larger than
the dtype it is to be cast to, so if none of the dtypes
checked satisfy that specification, no downcasting will be
performed on the data.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:retNumeric if parsing succeeded.
Return type depends on input. Series if Series, otherwise ndarray.See alsoDataFrame.astypeCast argument to a specified dtype.to_datetimeConvert argument to datetime.to_timedeltaConvert argument to timedelta.numpy.ndarray.astypeCast a numpy array to a specified type.DataFrame.convert_dtypesConvert dtypes.ExamplesTake separate series and convert to numeric, coercing when told to>>>s=pd.Series(['1.0','2',-3])>>>pd.to_numeric(s)0    1.01    2.02   -3.0dtype: float64>>>pd.to_numeric(s,downcast='float')0    1.01    2.02   -3.0dtype: float32>>>pd.to_numeric(s,downcast='signed')0    11    22   -3dtype: int8>>>s=pd.Series(['apple','1.0','2',-3])>>>pd.to_numeric(s,errors='ignore')0    apple1      1.02        23       -3dtype: object>>>pd.to_numeric(s,errors='coerce')0    NaN1    1.02    2.03   -3.0dtype: float64Downcasting of nullable integer and floating dtypes is supported:>>>s=pd.Series([1,2,3],dtype=""Int64"")>>>pd.to_numeric(s,downcast=""integer"")0    11    22    3dtype: Int8>>>s=pd.Series([1.0,2.1,3.0],dtype=""Float64"")>>>pd.to_numeric(s,downcast=""float"")0    1.01    2.12    3.0dtype: Float32"
Pandas,General functions,pandas.to_datetime,"pandas.to_datetime#pandas.to_datetime(arg,errors='raise',dayfirst=False,yearfirst=False,utc=False,format=None,exact=_NoDefault.no_default,unit=None,infer_datetime_format=_NoDefault.no_default,origin='unix',cache=True)[source]#Convert argument to datetime.This function converts a scalar, array-like,SeriesorDataFrame/dict-like to a pandas datetime object.Parameters:argint, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-likeThe object to convert to a datetime. If aDataFrameis provided, the
method expects minimally the following columns:""year"",""month"",""day"". The column “year”
must be specified in 4-digit format.errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’If'raise', then invalid parsing will raise an exception.If'coerce', then invalid parsing will be set asNaT.If'ignore', then invalid parsing will return the input.dayfirstbool, default FalseSpecify a date parse order ifargis str or is list-like.
IfTrue, parses dates with the day first, e.g.""10/11/12""is parsed as2012-11-10.Warningdayfirst=Trueis not strict, but will prefer to parse
with day first.yearfirstbool, default FalseSpecify a date parse order ifargis str or is list-like.IfTrueparses dates with the year first, e.g.""10/11/12""is parsed as2010-11-12.If bothdayfirstandyearfirstareTrue,yearfirstis
preceded (same asdateutil).Warningyearfirst=Trueis not strict, but will prefer to parse
with year first.utcbool, default FalseControl timezone-related parsing, localization and conversion.IfTrue, the functionalwaysreturns a timezone-aware
UTC-localizedTimestamp,SeriesorDatetimeIndex. To do this, timezone-naive inputs arelocalizedas UTC, while timezone-aware inputs areconvertedto UTC.IfFalse(default), inputs will not be coerced to UTC.
Timezone-naive inputs will remain naive, while timezone-aware ones
will keep their time offsets. Limitations exist for mixed
offsets (typically, daylight savings), seeExamplessection for details.WarningIn a future version of pandas, parsing datetimes with mixed time
zones will raise an error unlessutc=True.
Please specifyutc=Trueto opt in to the new behaviour
and silence this warning. To create aSerieswith mixed offsets andobjectdtype, please useapplyanddatetime.datetime.strptime.See also: pandas general documentation abouttimezone conversion and
localization.formatstr, default NoneThe strftime to parse time, e.g.""%d/%m/%Y"". Seestrftime documentationfor more information on choices, though
note that""%f""will parse all the way up to nanoseconds.
You can also pass:“ISO8601”, to parse anyISO8601time string (not necessarily in exactly the same format);“mixed”, to infer the format for each element individually. This is risky,
and you should probably use it along withdayfirst.NoteIf aDataFrameis passed, thenformathas no effect.exactbool, default TrueControl howformatis used:IfTrue, require an exactformatmatch.IfFalse, allow theformatto match anywhere in the target
string.Cannot be used alongsideformat='ISO8601'orformat='mixed'.unitstr, default ‘ns’The unit of the arg (D,s,ms,us,ns) denote the unit, which is an
integer or float number. This will be based off the origin.
Example, withunit='ms'andorigin='unix', this would calculate
the number of milliseconds to the unix epoch start.infer_datetime_formatbool, default FalseIfTrueand noformatis given, attempt to infer the format
of the datetime strings based on the first non-NaN element,
and if it can be inferred, switch to a faster method of parsing them.
In some cases this can increase the parsing speed by ~5-10x.Deprecated since version 2.0.0:A strict version of this argument is now the default, passing it has
no effect.originscalar, default ‘unix’Define the reference date. The numeric values would be parsed as number
of units (defined byunit) since this reference date.If'unix'(or POSIX) time; origin is set to 1970-01-01.If'julian', unit must be'D', and origin is set to
beginning of Julian Calendar. Julian day number0is assigned
to the day starting at noon on January 1, 4713 BC.If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date
string), origin is set to Timestamp identified by origin.If a float or integer, origin is the millisecond difference
relative to 1970-01-01.cachebool, default TrueIfTrue, use a cache of unique, converted dates to apply the
datetime conversion. May produce significant speed-up when parsing
duplicate date strings, especially ones with timezone offsets. The cache
is only used when there are at least 50 values. The presence of
out-of-bounds values will render the cache unusable and may slow down
parsing.Returns:datetimeIf parsing succeeded.
Return type depends on input (types in parenthesis correspond to
fallback in case of unsuccessful timezone or out-of-range timestamp
parsing):scalar:Timestamp(ordatetime.datetime)array-like:DatetimeIndex(orSerieswithobjectdtype containingdatetime.datetime)Series:Seriesofdatetime64dtype (orSeriesofobjectdtype containingdatetime.datetime)DataFrame:Seriesofdatetime64dtype (orSeriesofobjectdtype containingdatetime.datetime)Raises:ParserErrorWhen parsing a date from string fails.ValueErrorWhen another datetime conversion error happens. For example when one
of ‘year’, ‘month’, day’ columns is missing in aDataFrame, or
when a Timezone-awaredatetime.datetimeis found in an array-like
of mixed time offsets, andutc=False.See alsoDataFrame.astypeCast argument to a specified dtype.to_timedeltaConvert argument to timedelta.convert_dtypesConvert dtypes.NotesMany input types are supported, and lead to different output types:scalarscan be int, float, str, datetime object (from stdlibdatetimemodule ornumpy). They are converted toTimestampwhen
possible, otherwise they are converted todatetime.datetime.
None/NaN/null scalars are converted toNaT.array-likecan contain int, float, str, datetime objects. They are
converted toDatetimeIndexwhen possible, otherwise they are
converted toIndexwithobjectdtype, containingdatetime.datetime. None/NaN/null entries are converted toNaTin both cases.Seriesare converted toSerieswithdatetime64dtype when possible, otherwise they are converted toSerieswithobjectdtype, containingdatetime.datetime. None/NaN/null
entries are converted toNaTin both cases.DataFrame/dict-likeare converted toSerieswithdatetime64dtype. For each row a datetime is created from assembling
the various dataframe columns. Column keys can be common abbreviations
like [‘year’, ‘month’, ‘day’, ‘minute’, ‘second’, ‘ms’, ‘us’, ‘ns’]) or
plurals of the same.The following causes are responsible fordatetime.datetimeobjects
being returned (possibly inside anIndexor aSerieswithobjectdtype) instead of a proper pandas designated type
(Timestamp,DatetimeIndexorSerieswithdatetime64dtype):when any input element is beforeTimestamp.minor afterTimestamp.max, seetimestamp limitations.whenutc=False(default) and the input is an array-like orSeriescontaining mixed naive/aware datetime, or aware with mixed
time offsets. Note that this happens in the (quite frequent) situation when
the timezone has a daylight savings policy. In that case you may wish to
useutc=True.ExamplesHandling various input formatsAssembling a datetime from multiple columns of aDataFrame. The keys
can be common abbreviations like [‘year’, ‘month’, ‘day’, ‘minute’, ‘second’,
‘ms’, ‘us’, ‘ns’]) or plurals of the same>>>df=pd.DataFrame({'year':[2015,2016],...'month':[2,3],...'day':[4,5]})>>>pd.to_datetime(df)0   2015-02-041   2016-03-05dtype: datetime64[ns]Using a unix epoch time>>>pd.to_datetime(1490195805,unit='s')Timestamp('2017-03-22 15:16:45')>>>pd.to_datetime(1490195805433502912,unit='ns')Timestamp('2017-03-22 15:16:45.433502912')WarningFor float arg, precision rounding might happen. To prevent
unexpected behavior use a fixed-width exact type.Using a non-unix epoch origin>>>pd.to_datetime([1,2,3],unit='D',...origin=pd.Timestamp('1960-01-01'))DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],dtype='datetime64[ns]', freq=None)Differences with strptime behavior""%f""will parse all the way up to nanoseconds.>>>pd.to_datetime('2018-10-26 12:00:00.0000000011',...format='%Y-%m-%d%H:%M:%S.%f')Timestamp('2018-10-26 12:00:00.000000001')Non-convertible date/timesIf a date does not meet thetimestamp limitations, passingerrors='ignore'will return the original input instead of raising any exception.Passingerrors='coerce'will force an out-of-bounds date toNaT,
in addition to forcing non-dates (or non-parseable dates) toNaT.>>>pd.to_datetime('13000101',format='%Y%m%d',errors='ignore')'13000101'>>>pd.to_datetime('13000101',format='%Y%m%d',errors='coerce')NaTTimezones and time offsetsThe default behaviour (utc=False) is as follows:Timezone-naive inputs are converted to timezone-naiveDatetimeIndex:>>>pd.to_datetime(['2018-10-26 12:00:00','2018-10-26 13:00:15'])DatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],dtype='datetime64[ns]', freq=None)Timezone-aware inputswith constant time offsetare converted to
timezone-awareDatetimeIndex:>>>pd.to_datetime(['2018-10-26 12:00 -0500','2018-10-26 13:00 -0500'])DatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],dtype='datetime64[ns, UTC-05:00]', freq=None)However, timezone-aware inputswith mixed time offsets(for example
issued from a timezone with daylight savings, such as Europe/Paris)
arenot successfully convertedto aDatetimeIndex.
Parsing datetimes with mixed time zones will show a warning unlessutc=True. If you specifyutc=Falsethe warning below will be shown
and a simpleIndexcontainingdatetime.datetimeobjects will be returned:>>>pd.to_datetime(['2020-10-25 02:00 +0200',...'2020-10-25 04:00 +0100'])FutureWarning: In a future version of pandas, parsing datetimes with mixedtime zones will raise an error unless `utc=True`. Please specify `utc=True`to opt in to the new behaviour and silence this warning. To create a `Series`with mixed offsets and `object` dtype, please use `apply` and`datetime.datetime.strptime`.Index([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],dtype='object')A mix of timezone-aware and timezone-naive inputs is also converted to
a simpleIndexcontainingdatetime.datetimeobjects:>>>fromdatetimeimportdatetime>>>pd.to_datetime([""2020-01-01 01:00:00-01:00"",...datetime(2020,1,1,3,0)])FutureWarning: In a future version of pandas, parsing datetimes with mixedtime zones will raise an error unless `utc=True`. Please specify `utc=True`to opt in to the new behaviour and silence this warning. To create a `Series`with mixed offsets and `object` dtype, please use `apply` and`datetime.datetime.strptime`.Index([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')Settingutc=Truesolves most of the above issues:Timezone-naive inputs arelocalizedas UTC>>>pd.to_datetime(['2018-10-26 12:00','2018-10-26 13:00'],utc=True)DatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],dtype='datetime64[ns, UTC]', freq=None)Timezone-aware inputs areconvertedto UTC (the output represents the
exact same datetime, but viewed from the UTC time offset+00:00).>>>pd.to_datetime(['2018-10-26 12:00 -0530','2018-10-26 12:00 -0500'],...utc=True)DatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],dtype='datetime64[ns, UTC]', freq=None)Inputs can contain both string or datetime, the above
rules still apply>>>pd.to_datetime(['2018-10-26 12:00',datetime(2020,1,1,18)],utc=True)DatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],dtype='datetime64[ns, UTC]', freq=None)"
Pandas,General functions,pandas.to_timedelta,"pandas.to_timedelta#pandas.to_timedelta(arg,unit=None,errors='raise')[source]#Convert argument to timedelta.Timedeltas are absolute differences in times, expressed in difference
units (e.g. days, hours, minutes, seconds). This method converts
an argument from a recognized timedelta format / value into
a Timedelta type.Parameters:argstr, timedelta, list-like or SeriesThe data to be converted to timedelta.Changed in version 2.0:Strings with units ‘M’, ‘Y’ and ‘y’ do not represent
unambiguous timedelta values and will raise an exception.unitstr, optionalDenotes the unit of the arg for numericarg. Defaults to""ns"".Possible values:‘W’‘D’ / ‘days’ / ‘day’‘hours’ / ‘hour’ / ‘hr’ / ‘h’‘m’ / ‘minute’ / ‘min’ / ‘minutes’ / ‘T’‘S’ / ‘seconds’ / ‘sec’ / ‘second’‘ms’ / ‘milliseconds’ / ‘millisecond’ / ‘milli’ / ‘millis’ / ‘L’‘us’ / ‘microseconds’ / ‘microsecond’ / ‘micro’ / ‘micros’ / ‘U’‘ns’ / ‘nanoseconds’ / ‘nano’ / ‘nanos’ / ‘nanosecond’ / ‘N’Must not be specified whenargcontext strings anderrors=""raise"".Deprecated since version 2.1.0:Units ‘T’ and ‘L’ are deprecated and will be removed in a future version.errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’If ‘raise’, then invalid parsing will raise an exception.If ‘coerce’, then invalid parsing will be set as NaT.If ‘ignore’, then invalid parsing will return the input.Returns:timedeltaIf parsing succeeded.
Return type depends on input:list-like: TimedeltaIndex of timedelta64 dtypeSeries: Series of timedelta64 dtypescalar: TimedeltaSee alsoDataFrame.astypeCast argument to a specified dtype.to_datetimeConvert argument to datetime.convert_dtypesConvert dtypes.NotesIf the precision is higher than nanoseconds, the precision of the duration is
truncated to nanoseconds for string inputs.ExamplesParsing a single string to a Timedelta:>>>pd.to_timedelta('1 days 06:05:01.00003')Timedelta('1 days 06:05:01.000030')>>>pd.to_timedelta('15.5us')Timedelta('0 days 00:00:00.000015500')Parsing a list or array of strings:>>>pd.to_timedelta(['1 days 06:05:01.00003','15.5us','nan'])TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT],dtype='timedelta64[ns]', freq=None)Converting numbers by specifying theunitkeyword argument:>>>pd.to_timedelta(np.arange(5),unit='s')TimedeltaIndex(['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02','0 days 00:00:03', '0 days 00:00:04'],dtype='timedelta64[ns]', freq=None)>>>pd.to_timedelta(np.arange(5),unit='d')TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],dtype='timedelta64[ns]', freq=None)"
Pandas,General functions,pandas.date_range,"pandas.date_range#pandas.date_range(start=None,end=None,periods=None,freq=None,tz=None,normalize=False,name=None,inclusive='both',*,unit=None,**kwargs)[source]#Return a fixed frequency DatetimeIndex.Returns the range of equally spaced time points (where the difference between any
two adjacent points is specified by the given frequency) such that they all
satisfystart <[=] x <[=] end, where the first one and the last one are, resp.,
the first and last time points in that range that fall on the boundary offreq(if given as a frequency string) or that are valid forfreq(if given as apandas.tseries.offsets.DateOffset). (If exactly one ofstart,end, orfreqisnotspecified, this missing parameter can be computed
givenperiods, the number of timesteps in the range. See the note below.)Parameters:startstr or datetime-like, optionalLeft bound for generating dates.endstr or datetime-like, optionalRight bound for generating dates.periodsint, optionalNumber of periods to generate.freqstr, Timedelta, datetime.timedelta, or DateOffset, default ‘D’Frequency strings can have multiples, e.g. ‘5H’. Seeherefor a list of
frequency aliases.tzstr or tzinfo, optionalTime zone name for returning localized DatetimeIndex, for example
‘Asia/Hong_Kong’. By default, the resulting DatetimeIndex is
timezone-naive unless timezone-aware datetime-likes are passed.normalizebool, default FalseNormalize start/end dates to midnight before generating date range.namestr, default NoneName of the resulting DatetimeIndex.inclusive{“both”, “neither”, “left”, “right”}, default “both”Include boundaries; Whether to set each bound as closed or open.New in version 1.4.0.unitstr, default NoneSpecify the desired resolution of the result.New in version 2.0.0.**kwargsFor compatibility. Has no effect on the result.Returns:DatetimeIndexSee alsoDatetimeIndexAn immutable container for datetimes.timedelta_rangeReturn a fixed frequency TimedeltaIndex.period_rangeReturn a fixed frequency PeriodIndex.interval_rangeReturn a fixed frequency IntervalIndex.NotesOf the four parametersstart,end,periods, andfreq,
exactly three must be specified. Iffreqis omitted, the resultingDatetimeIndexwill haveperiodslinearly spaced elements betweenstartandend(closed on both sides).To learn more about the frequency strings, please seethis link.ExamplesSpecifying the valuesThe next four examples generate the sameDatetimeIndex, but vary
the combination ofstart,endandperiods.Specifystartandend, with the default daily frequency.>>>pd.date_range(start='1/1/2018',end='1/08/2018')DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04','2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],dtype='datetime64[ns]', freq='D')Specify timezone-awarestartandend, with the default daily frequency.>>>pd.date_range(...start=pd.to_datetime(""1/1/2018"").tz_localize(""Europe/Berlin""),...end=pd.to_datetime(""1/08/2018"").tz_localize(""Europe/Berlin""),...)DatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00','2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00','2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00','2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],dtype='datetime64[ns, Europe/Berlin]', freq='D')Specifystartandperiods, the number of periods (days).>>>pd.date_range(start='1/1/2018',periods=8)DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04','2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],dtype='datetime64[ns]', freq='D')Specifyendandperiods, the number of periods (days).>>>pd.date_range(end='1/1/2018',periods=8)DatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28','2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],dtype='datetime64[ns]', freq='D')Specifystart,end, andperiods; the frequency is generated
automatically (linearly spaced).>>>pd.date_range(start='2018-04-24',end='2018-04-27',periods=3)DatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00','2018-04-27 00:00:00'],dtype='datetime64[ns]', freq=None)Other ParametersChanged thefreq(frequency) to'M'(month end frequency).>>>pd.date_range(start='1/1/2018',periods=5,freq='M')DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30','2018-05-31'],dtype='datetime64[ns]', freq='M')Multiples are allowed>>>pd.date_range(start='1/1/2018',periods=5,freq='3M')DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31','2019-01-31'],dtype='datetime64[ns]', freq='3M')freqcan also be specified as an Offset object.>>>pd.date_range(start='1/1/2018',periods=5,freq=pd.offsets.MonthEnd(3))DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31','2019-01-31'],dtype='datetime64[ns]', freq='3M')Specifytzto set the timezone.>>>pd.date_range(start='1/1/2018',periods=5,tz='Asia/Tokyo')DatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00','2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00','2018-01-05 00:00:00+09:00'],dtype='datetime64[ns, Asia/Tokyo]', freq='D')inclusivecontrols whether to includestartandendthat are on the
boundary. The default, “both”, includes boundary points on either end.>>>pd.date_range(start='2017-01-01',end='2017-01-04',inclusive=""both"")DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],dtype='datetime64[ns]', freq='D')Useinclusive='left'to excludeendif it falls on the boundary.>>>pd.date_range(start='2017-01-01',end='2017-01-04',inclusive='left')DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],dtype='datetime64[ns]', freq='D')Useinclusive='right'to excludestartif it falls on the boundary, and
similarlyinclusive='neither'will exclude bothstartandend.>>>pd.date_range(start='2017-01-01',end='2017-01-04',inclusive='right')DatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],dtype='datetime64[ns]', freq='D')Specify a unit>>>pd.date_range(start=""2017-01-01"",periods=10,freq=""100AS"",unit=""s"")DatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01','2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01','2817-01-01', '2917-01-01'],dtype='datetime64[s]', freq='100AS-JAN')"
Pandas,General functions,pandas.bdate_range,"pandas.bdate_range#pandas.bdate_range(start=None,end=None,periods=None,freq='B',tz=None,normalize=True,name=None,weekmask=None,holidays=None,inclusive='both',**kwargs)[source]#Return a fixed frequency DatetimeIndex with business day as the default.Parameters:startstr or datetime-like, default NoneLeft bound for generating dates.endstr or datetime-like, default NoneRight bound for generating dates.periodsint, default NoneNumber of periods to generate.freqstr, Timedelta, datetime.timedelta, or DateOffset, default ‘B’Frequency strings can have multiples, e.g. ‘5H’. The default is
business daily (‘B’).tzstr or NoneTime zone name for returning localized DatetimeIndex, for example
Asia/Beijing.normalizebool, default FalseNormalize start/end dates to midnight before generating date range.namestr, default NoneName of the resulting DatetimeIndex.weekmaskstr or None, default NoneWeekmask of valid business days, passed tonumpy.busdaycalendar,
only used when custom frequency strings are passed. The default
value None is equivalent to ‘Mon Tue Wed Thu Fri’.holidayslist-like or None, default NoneDates to exclude from the set of valid business days, passed tonumpy.busdaycalendar, only used when custom frequency strings
are passed.inclusive{“both”, “neither”, “left”, “right”}, default “both”Include boundaries; Whether to set each bound as closed or open.New in version 1.4.0.**kwargsFor compatibility. Has no effect on the result.Returns:DatetimeIndexNotesOf the four parameters:start,end,periods, andfreq,
exactly three must be specified. Specifyingfreqis a requirement
forbdate_range. Usedate_rangeif specifyingfreqis not
desired.To learn more about the frequency strings, please seethis link.ExamplesNote how the two weekend days are skipped in the result.>>>pd.bdate_range(start='1/1/2018',end='1/08/2018')DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04','2018-01-05', '2018-01-08'],dtype='datetime64[ns]', freq='B')"
Pandas,General functions,pandas.period_range,"pandas.period_range#pandas.period_range(start=None,end=None,periods=None,freq=None,name=None)[source]#Return a fixed frequency PeriodIndex.The day (calendar) is the default frequency.Parameters:startstr, datetime, date, pandas.Timestamp, or period-like, default NoneLeft bound for generating periods.endstr, datetime, date, pandas.Timestamp, or period-like, default NoneRight bound for generating periods.periodsint, default NoneNumber of periods to generate.freqstr or DateOffset, optionalFrequency alias. By default the freq is taken fromstartorendif those are Period objects. Otherwise, the default is""D""for
daily frequency.namestr, default NoneName of the resulting PeriodIndex.Returns:PeriodIndexNotesOf the three parameters:start,end, andperiods, exactly two
must be specified.To learn more about the frequency strings, please seethis link.Examples>>>pd.period_range(start='2017-01-01',end='2018-01-01',freq='M')PeriodIndex(['2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06','2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12','2018-01'],dtype='period[M]')IfstartorendarePeriodobjects, they will be used as anchor
endpoints for aPeriodIndexwith frequency matching that of theperiod_rangeconstructor.>>>pd.period_range(start=pd.Period('2017Q1',freq='Q'),...end=pd.Period('2017Q2',freq='Q'),freq='M')PeriodIndex(['2017-03', '2017-04', '2017-05', '2017-06'],dtype='period[M]')"
Pandas,General functions,pandas.timedelta_range,"pandas.timedelta_range#pandas.timedelta_range(start=None,end=None,periods=None,freq=None,name=None,closed=None,*,unit=None)[source]#Return a fixed frequency TimedeltaIndex with day as the default.Parameters:startstr or timedelta-like, default NoneLeft bound for generating timedeltas.endstr or timedelta-like, default NoneRight bound for generating timedeltas.periodsint, default NoneNumber of periods to generate.freqstr, Timedelta, datetime.timedelta, or DateOffset, default ‘D’Frequency strings can have multiples, e.g. ‘5H’.namestr, default NoneName of the resulting TimedeltaIndex.closedstr, default NoneMake the interval closed with respect to the given frequency to
the ‘left’, ‘right’, or both sides (None).unitstr, default NoneSpecify the desired resolution of the result.New in version 2.0.0.Returns:TimedeltaIndexNotesOf the four parametersstart,end,periods, andfreq,
exactly three must be specified. Iffreqis omitted, the resultingTimedeltaIndexwill haveperiodslinearly spaced elements betweenstartandend(closed on both sides).To learn more about the frequency strings, please seethis link.Examples>>>pd.timedelta_range(start='1 day',periods=4)TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],dtype='timedelta64[ns]', freq='D')Theclosedparameter specifies which endpoint is included. The default
behavior is to include both endpoints.>>>pd.timedelta_range(start='1 day',periods=4,closed='right')TimedeltaIndex(['2 days', '3 days', '4 days'],dtype='timedelta64[ns]', freq='D')Thefreqparameter specifies the frequency of the TimedeltaIndex.
Only fixed frequencies can be passed, non-fixed frequencies such as
‘M’ (month end) will raise.>>>pd.timedelta_range(start='1 day',end='2 days',freq='6H')TimedeltaIndex(['1 days 00:00:00', '1 days 06:00:00', '1 days 12:00:00','1 days 18:00:00', '2 days 00:00:00'],dtype='timedelta64[ns]', freq='6H')Specifystart,end, andperiods; the frequency is generated
automatically (linearly spaced).>>>pd.timedelta_range(start='1 day',end='5 days',periods=4)TimedeltaIndex(['1 days 00:00:00', '2 days 08:00:00', '3 days 16:00:00','5 days 00:00:00'],dtype='timedelta64[ns]', freq=None)Specify a unit>>>pd.timedelta_range(""1 Day"",periods=3,freq=""100000D"",unit=""s"")TimedeltaIndex(['1 days 00:00:00', '100001 days 00:00:00','200001 days 00:00:00'],dtype='timedelta64[s]', freq='100000D')"
Pandas,General functions,pandas.infer_freq,"pandas.infer_freq#pandas.infer_freq(index)[source]#Infer the most likely frequency given the input index.Parameters:indexDatetimeIndex, TimedeltaIndex, Series or array-likeIf passed a Series will use the values of the series (NOT THE INDEX).Returns:str or NoneNone if no discernible frequency.Raises:TypeErrorIf the index is not datetime-like.ValueErrorIf there are fewer than three values.Examples>>>idx=pd.date_range(start='2020/12/01',end='2020/12/30',periods=30)>>>pd.infer_freq(idx)'D'"
Pandas,General functions,pandas.interval_range,"pandas.interval_range#pandas.interval_range(start=None,end=None,periods=None,freq=None,name=None,closed='right')[source]#Return a fixed frequency IntervalIndex.Parameters:startnumeric or datetime-like, default NoneLeft bound for generating intervals.endnumeric or datetime-like, default NoneRight bound for generating intervals.periodsint, default NoneNumber of periods to generate.freqnumeric, str, Timedelta, datetime.timedelta, or DateOffset, default NoneThe length of each interval. Must be consistent with the type of start
and end, e.g. 2 for numeric, or ‘5H’ for datetime-like. Default is 1
for numeric and ‘D’ for datetime-like.namestr, default NoneName of the resulting IntervalIndex.closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both
or neither.Returns:IntervalIndexSee alsoIntervalIndexAn Index of intervals that are all closed on the same side.NotesOf the four parametersstart,end,periods, andfreq,
exactly three must be specified. Iffreqis omitted, the resultingIntervalIndexwill haveperiodslinearly spaced elements betweenstartandend, inclusively.To learn more about datetime-like frequency strings, please seethis link.ExamplesNumericstartandendis supported.>>>pd.interval_range(start=0,end=5)IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],dtype='interval[int64, right]')Additionally, datetime-like input is also supported.>>>pd.interval_range(start=pd.Timestamp('2017-01-01'),...end=pd.Timestamp('2017-01-04'))IntervalIndex([(2017-01-01, 2017-01-02], (2017-01-02, 2017-01-03],(2017-01-03, 2017-01-04]],dtype='interval[datetime64[ns], right]')Thefreqparameter specifies the frequency between the left and right.
endpoints of the individual intervals within theIntervalIndex. For
numericstartandend, the frequency must also be numeric.>>>pd.interval_range(start=0,periods=4,freq=1.5)IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],dtype='interval[float64, right]')Similarly, for datetime-likestartandend, the frequency must be
convertible to a DateOffset.>>>pd.interval_range(start=pd.Timestamp('2017-01-01'),...periods=3,freq='MS')IntervalIndex([(2017-01-01, 2017-02-01], (2017-02-01, 2017-03-01],(2017-03-01, 2017-04-01]],dtype='interval[datetime64[ns], right]')Specifystart,end, andperiods; the frequency is generated
automatically (linearly spaced).>>>pd.interval_range(start=0,end=6,periods=4)IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],dtype='interval[float64, right]')Theclosedparameter specifies which endpoints of the individual
intervals within theIntervalIndexare closed.>>>pd.interval_range(end=5,periods=4,closed='both')IntervalIndex([[1, 2], [2, 3], [3, 4], [4, 5]],dtype='interval[int64, both]')"
Pandas,General functions,pandas.eval,"pandas.eval#pandas.eval(expr,parser='pandas',engine=None,local_dict=None,global_dict=None,resolvers=(),level=0,target=None,inplace=False)[source]#Evaluate a Python expression as a string using various backends.The following arithmetic operations are supported:+,-,*,/,**,%,//(python engine only) along with the following
boolean operations:|(or),&(and), and~(not).
Additionally, the'pandas'parser allows the use ofand,or, andnotwith the same semantics as the
corresponding bitwise operators.SeriesandDataFrameobjects are supported and behave as they would
with plain ol’ Python evaluation.Parameters:exprstrThe expression to evaluate. This string cannot contain any Pythonstatements,
only Pythonexpressions.parser{‘pandas’, ‘python’}, default ‘pandas’The parser to use to construct the syntax tree from the expression. The
default of'pandas'parses code slightly different than standard
Python. Alternatively, you can parse an expression using the'python'parser to retain strict Python semantics. See theenhancing performancedocumentation for
more details.engine{‘python’, ‘numexpr’}, default ‘numexpr’The engine used to evaluate the expression. Supported engines areNone : tries to usenumexpr, falls back topython'numexpr': This default engine evaluates pandas objects using
numexpr for large speed ups in complex expressions with large frames.'python': Performs operations as if you hadeval’d in top
level python. This engine is generally not that useful.More backends may be available in the future.local_dictdict or None, optionalA dictionary of local variables, taken from locals() by default.global_dictdict or None, optionalA dictionary of global variables, taken from globals() by default.resolverslist of dict-like or None, optionalA list of objects implementing the__getitem__special method that
you can use to inject an additional collection of namespaces to use for
variable lookup. For example, this is used in thequery()method to inject theDataFrame.indexandDataFrame.columnsvariables that refer to their respectiveDataFrameinstance attributes.levelint, optionalThe number of prior stack frames to traverse and add to the current
scope. Most users willnotneed to change this parameter.targetobject, optional, default NoneThis is the target object for assignment. It is used when there is
variable assignment in the expression. If so, thentargetmust
support item assignment with string keys, and if a copy is being
returned, it must also support.copy().inplacebool, default FalseIftargetis provided, and the expression mutatestarget, whether
to modifytargetinplace. Otherwise, return a copy oftargetwith
the mutation.Returns:ndarray, numeric scalar, DataFrame, Series, or NoneThe completion value of evaluating the given code or None ifinplace=True.Raises:ValueErrorThere are many instances where such an error can be raised:target=None, but the expression is multiline.The expression is multiline, but not all them have item assignment.
An example of such an arrangement is this:a = b + 1
a + 2Here, there are expressions on different lines, making it multiline,
but the last line has no variable assigned to the output ofa + 2.inplace=True, but the expression is missing item assignment.Item assignment is provided, but thetargetdoes not support
string item assignment.Item assignment is provided andinplace=False, but thetargetdoes not support the.copy()methodSee alsoDataFrame.queryEvaluates a boolean expression to query the columns of a frame.DataFrame.evalEvaluate a string describing operations on DataFrame columns.NotesThedtypeof any objects involved in an arithmetic%operation are
recursively cast tofloat64.See theenhancing performancedocumentation for
more details.Examples>>>df=pd.DataFrame({""animal"":[""dog"",""pig""],""age"":[10,20]})>>>dfanimal  age0    dog   101    pig   20We can add a new column usingpd.eval:>>>pd.eval(""double_age = df.age * 2"",target=df)animal  age  double_age0    dog   10          201    pig   20          40"
Pandas,General functions,pandas.util.hash_array,"pandas.util.hash_array#pandas.util.hash_array(vals,encoding='utf8',hash_key='0123456789123456',categorize=True)[source]#Given a 1d array, return an array of deterministic integers.Parameters:valsndarray or ExtensionArrayencodingstr, default ‘utf8’Encoding for data & key when strings.hash_keystr, default _default_hash_keyHash_key for string key to encode.categorizebool, default TrueWhether to first categorize object arrays before hashing. This is more
efficient when the array contains duplicate values.Returns:ndarray[np.uint64, ndim=1]Hashed values, same length as the vals.Examples>>>pd.util.hash_array(np.array([1,2,3]))array([ 6238072747940578789, 15839785061582574730,  2185194620014831856],dtype=uint64)"
Pandas,General functions,pandas.util.hash_pandas_object,"pandas.util.hash_pandas_object#pandas.util.hash_pandas_object(obj,index=True,encoding='utf8',hash_key='0123456789123456',categorize=True)[source]#Return a data hash of the Index/Series/DataFrame.Parameters:objIndex, Series, or DataFrameindexbool, default TrueInclude the index in the hash (if Series/DataFrame).encodingstr, default ‘utf8’Encoding for data & key when strings.hash_keystr, default _default_hash_keyHash_key for string key to encode.categorizebool, default TrueWhether to first categorize object arrays before hashing. This is more
efficient when the array contains duplicate values.Returns:Series of uint64, same length as the objectExamples>>>pd.util.hash_pandas_object(pd.Series([1,2,3]))0    146390536861580357801     38695632792125307282      393322362522515241dtype: uint64"
Pandas,General functions,pandas.api.interchange.from_dataframe,"pandas.api.interchange.from_dataframe#pandas.api.interchange.from_dataframe(df,allow_copy=True)[source]#Build apd.DataFramefrom any DataFrame supporting the interchange protocol.Parameters:dfDataFrameXchgObject supporting the interchange protocol, i.e.__dataframe__method.allow_copybool, default: TrueWhether to allow copying the memory to perform the conversion
(if false then zero-copy approach is requested).Returns:pd.DataFrameExamples>>>df_not_necessarily_pandas=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>interchange_object=df_not_necessarily_pandas.__dataframe__()>>>interchange_object.column_names()Index(['A', 'B'], dtype='object')>>>df_pandas=(pd.api.interchange.from_dataframe...(interchange_object.select_columns_by_name(['A'])))>>>df_pandasA0    11    2These methods (column_names,select_columns_by_name) should work
for any dataframe library which implements the interchange protocol."
