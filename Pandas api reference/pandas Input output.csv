ライブラリ名,章,節,内容
Pandas,Input output,pandas.read_pickle,"pandas.read_pickle#pandas.read_pickle(filepath_or_buffer,compression='infer',storage_options=None)[source]#Load pickled pandas object (or any object) from file.WarningLoading pickled data received from untrusted sources can be
unsafe. Seehere.Parameters:filepath_or_bufferstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binaryreadlines()function.
Also accepts URL. URL is not limited to S3 and GCS.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.Returns:same type as object stored in fileSee alsoDataFrame.to_picklePickle (serialize) DataFrame object to file.Series.to_picklePickle (serialize) Series object to file.read_hdfRead HDF5 file into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.read_parquetLoad a parquet object, returning a DataFrame.Notesread_pickle is only guaranteed to be backwards compatible to pandas 0.20.3
provided the object was serialized with to_pickle.Examples>>>original_df=pd.DataFrame(...{""foo"":range(5),""bar"":range(5,10)}...)>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>pd.to_pickle(original_df,""./dummy.pkl"")>>>unpickled_df=pd.read_pickle(""./dummy.pkl"")>>>unpickled_dffoo  bar0    0    51    1    62    2    73    3    84    4    9"
Pandas,Input output,pandas.DataFrame.to_pickle,"pandas.DataFrame.to_pickle#DataFrame.to_pickle(path,compression='infer',protocol=5,storage_options=None)[source]#Pickle (serialize) object to file.Parameters:pathstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. File path where
the pickled object will be stored.compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.protocolintInt which indicates which protocol should be used by the pickler,
default HIGHEST_PROTOCOL (see[1]paragraph 12.1.2). The possible
values are 0, 1, 2, 3, 4, 5. A negative value for the protocol
parameter is equivalent to setting its value to HIGHEST_PROTOCOL.[1]https://docs.python.org/3/library/pickle.html.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.See alsoread_pickleLoad pickled pandas object (or any object) from file.DataFrame.to_hdfWrite DataFrame to an HDF5 file.DataFrame.to_sqlWrite DataFrame to a SQL database.DataFrame.to_parquetWrite a DataFrame to the binary parquet format.Examples>>>original_df=pd.DataFrame({""foo"":range(5),""bar"":range(5,10)})>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>original_df.to_pickle(""./dummy.pkl"")>>>unpickled_df=pd.read_pickle(""./dummy.pkl"")>>>unpickled_dffoo  bar0    0    51    1    62    2    73    3    84    4    9"
Pandas,Input output,pandas.read_table,"pandas.read_table#pandas.read_table(filepath_or_buffer,*,sep=_NoDefault.no_default,delimiter=None,header='infer',names=_NoDefault.no_default,index_col=None,usecols=None,dtype=None,engine=None,converters=None,true_values=None,false_values=None,skipinitialspace=False,skiprows=None,skipfooter=0,nrows=None,na_values=None,keep_default_na=True,na_filter=True,verbose=False,skip_blank_lines=True,parse_dates=False,infer_datetime_format=_NoDefault.no_default,keep_date_col=False,date_parser=_NoDefault.no_default,date_format=None,dayfirst=False,cache_dates=True,iterator=False,chunksize=None,compression='infer',thousands=None,decimal='.',lineterminator=None,quotechar='""',quoting=0,doublequote=True,escapechar=None,comment=None,encoding=None,encoding_errors='strict',dialect=None,on_bad_lines='error',delim_whitespace=False,low_memory=True,memory_map=False,float_precision=None,storage_options=None,dtype_backend=_NoDefault.no_default)[source]#Read general delimited file into DataFrame.Also supports optionally iterating or breaking of the file
into chunks.Additional help can be found in the online docs forIO Tools.Parameters:filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.csv.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method, such as
a file handle (e.g. via builtinopenfunction) orStringIO.sepstr, default ‘\t’ (tab-stop)Character or regex pattern to treat as the delimiter. Ifsep=None, the
C engine cannot automatically detect
the separator, but the Python parsing engine can, meaning the latter will
be used and automatically detect the separator from only the first valid
row of the file by Python’s builtin sniffer tool,csv.Sniffer.
In addition, separators longer than 1 character and different from'\s+'will be interpreted as regular expressions and will also force
the use of the Python parsing engine. Note that regex delimiters are prone
to ignoring quoted data. Regex example:'\r\t'.delimiterstr, optionalAlias forsep.headerint, Sequence of int, ‘infer’ or None, default ‘infer’Row number(s) containing column labels and marking the start of the
data (zero-indexed). Default behavior is to infer the column names: if nonamesare passed the behavior is identical toheader=0and column
names are inferred from the first line of the file, if column
names are passed explicitly tonamesthen the behavior is identical toheader=None. Explicitly passheader=0to be able to
replace existing names. The header can be a list of integers that
specify row locations for aMultiIndexon the columns
e.g.[0,1,3]. Intervening rows that are not specified will be
skipped (e.g. 2 in this example is skipped). Note that this
parameter ignores commented lines and empty lines ifskip_blank_lines=True, soheader=0denotes the first line of
data rather than the first line of the file.namesSequence of Hashable, optionalSequence of column labels to apply. If the file contains a header row,
then you should explicitly passheader=0to override the column names.
Duplicates in this list are not allowed.index_colHashable, Sequence of Hashable or False, optionalColumn(s) to use as row label(s), denoted either by column labels or column
indices. If a sequence of labels or indices is given,MultiIndexwill be formed for the row labels.Note:index_col=Falsecan be used to force pandas tonotuse the first
column as the index, e.g., when you have a malformed file with delimiters at
the end of each line.usecolslist of Hashable or Callable, optionalSubset of columns to select, denoted either by column labels or column indices.
If list-like, all elements must either
be positional (i.e. integer indices into the document columns) or strings
that correspond to column names provided either by the user innamesor
inferred from the document header row(s). Ifnamesare given, the document
header row(s) are not taken into account. For example, a valid list-likeusecolsparameter would be[0,1,2]or['foo','bar','baz'].
Element order is ignored, sousecols=[0,1]is the same as[1,0].
To instantiate aDataFramefromdatawith element order
preserved usepd.read_csv(data,usecols=['foo','bar'])[['foo','bar']]for columns in['foo','bar']order orpd.read_csv(data,usecols=['foo','bar'])[['bar','foo']]for['bar','foo']order.If callable, the callable function will be evaluated against the column
names, returning names where the callable function evaluates toTrue. An
example of a valid callable argument would belambdax:x.upper()in['AAA','BBB','DDD']. Using this parameter results in much faster
parsing time and lower memory usage.dtypedtype or dict of {Hashabledtype}, optionalData type(s) to apply to either the whole dataset or individual columns.
E.g.,{'a':np.float64,'b':np.int32,'c':'Int64'}Usestrorobjecttogether with suitablena_valuessettings
to preserve and not interpretdtype.
Ifconvertersare specified, they will be applied INSTEAD
ofdtypeconversion.New in version 1.5.0:Support fordefaultdictwas added. Specify adefaultdictas input where
the default determines thedtypeof the columns which are not explicitly
listed.engine{‘c’, ‘python’, ‘pyarrow’}, optionalParser engine to use. The C and pyarrow engines are faster, while the python engine
is currently more feature-complete. Multithreading is currently only supported by
the pyarrow engine.New in version 1.4.0:The ‘pyarrow’ engine was added as anexperimentalengine, and some features
are unsupported, or may not work correctly, with this engine.convertersdict of {HashableCallable}, optionalFunctions for converting values in specified columns. Keys can either
be column labels or column indices.true_valueslist, optionalValues to consider asTruein addition to case-insensitive variants of ‘True’.false_valueslist, optionalValues to consider asFalsein addition to case-insensitive variants of ‘False’.skipinitialspacebool, default FalseSkip spaces after delimiter.skiprowsint, list of int or Callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int)
at the start of the file.If callable, the callable function will be evaluated against the row
indices, returningTrueif the row should be skipped andFalseotherwise.
An example of a valid callable argument would belambdax:xin[0,2].skipfooterint, default 0Number of lines at bottom of file to skip (Unsupported withengine='c').nrowsint, optionalNumber of rows of file to read. Useful for reading pieces of large files.na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optionalAdditional strings to recognize asNA/NaN. Ifdictpassed, specific
per-columnNAvalues. By default the following values are interpreted asNaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”,
“1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”,
“n/a”, “nan”, “null “.keep_default_nabool, default TrueWhether or not to include the defaultNaNvalues when parsing the data.
Depending on whetherna_valuesis passed in, the behavior is as follows:Ifkeep_default_naisTrue, andna_valuesare specified,na_valuesis appended to the defaultNaNvalues used for parsing.Ifkeep_default_naisTrue, andna_valuesare not specified, only
the defaultNaNvalues are used for parsing.Ifkeep_default_naisFalse, andna_valuesare specified, only
theNaNvalues specifiedna_valuesare used for parsing.Ifkeep_default_naisFalse, andna_valuesare not specified, no
strings will be parsed asNaN.Note that ifna_filteris passed in asFalse, thekeep_default_naandna_valuesparameters will be ignored.na_filterbool, default TrueDetect missing value markers (empty strings and the value ofna_values). In
data without anyNAvalues, passingna_filter=Falsecan improve the
performance of reading a large file.verbosebool, default FalseIndicate number ofNAvalues placed in non-numeric columns.skip_blank_linesbool, default TrueIfTrue, skip over blank lines rather than interpreting asNaNvalues.parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default FalseThe behavior is as follows:bool. IfTrue-> try parsing the index.listofintor names. e.g. If[1,2,3]-> try parsing columns 1, 2, 3
each as a separate date column.listoflist. e.g. If[[1,3]]-> combine columns 1 and 3 and parse
as a single date column.dict, e.g.{'foo':[1,3]}-> parse columns 1, 3 as date and call
result ‘foo’If a column or index cannot be represented as an array ofdatetime,
say because of an unparsable value or a mixture of timezones, the column
or index will be returned unaltered as anobjectdata type. For
non-standarddatetimeparsing, useto_datetime()afterread_csv().Note: A fast-path exists for iso8601-formatted dates.infer_datetime_formatbool, default FalseIfTrueandparse_datesis enabled, pandas will attempt to infer the
format of thedatetimestrings in the columns, and if it can be inferred,
switch to a faster method of parsing them. In some cases this can increase
the parsing speed by 5-10x.Deprecated since version 2.0.0:A strict version of this argument is now the default, passing it has no effect.keep_date_colbool, default FalseIfTrueandparse_datesspecifies combining multiple columns then
keep the original columns.date_parserCallable, optionalFunction to use for converting a sequence of string columns to an array ofdatetimeinstances. The default usesdateutil.parser.parserto do the
conversion. pandas will try to calldate_parserin three different ways,
advancing to the next if an exception occurs: 1) Pass one or more arrays
(as defined byparse_dates) as arguments; 2) concatenate (row-wise) the
string values from the columns defined byparse_datesinto a single array
and pass that; and 3) calldate_parseronce for each row using one or
more strings (corresponding to the columns defined byparse_dates) as
arguments.Deprecated since version 2.0.0:Usedate_formatinstead, or read in asobjectand then applyto_datetime()as-needed.date_formatstr or dict of column -> format, optionalFormat to use for parsing dates when used in conjunction withparse_dates.
For anything more complex, please read in asobjectand then applyto_datetime()as-needed.New in version 2.0.0.dayfirstbool, default FalseDD/MM format dates, international and European format.cache_datesbool, default TrueIfTrue, use a cache of unique, converted dates to apply thedatetimeconversion. May produce significant speed-up when parsing duplicate
date strings, especially ones with timezone offsets.iteratorbool, default FalseReturnTextFileReaderobject for iteration or getting chunks withget_chunk().Changed in version 1.2:TextFileReaderis a context manager.chunksizeint, optionalNumber of lines to read from the file per chunk. Passing a value will cause the
function to return aTextFileReaderobject for iteration.
See theIO Tools docsfor more information oniteratorandchunksize.Changed in version 1.2:TextFileReaderis a context manager.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.thousandsstr (length 1), optionalCharacter acting as the thousands separator in numerical values.decimalstr (length 1), default ‘.’Character to recognize as decimal point (e.g., use ‘,’ for European data).lineterminatorstr (length 1), optionalCharacter used to denote a line break. Only valid with C parser.quotecharstr (length 1), optionalCharacter used to denote the start and end of a quoted item. Quoted
items can include thedelimiterand it will be ignored.quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMALControl field quoting behavior percsv.QUOTE_*constants. Default iscsv.QUOTE_MINIMAL(i.e., 0) which implies that only fields containing special
characters are quoted (e.g., characters defined inquotechar,delimiter,
orlineterminator.doublequotebool, default TrueWhenquotecharis specified andquotingis notQUOTE_NONE, indicate
whether or not to interpret two consecutivequotecharelements INSIDE a
field as a singlequotecharelement.escapecharstr (length 1), optionalCharacter used to escape other characters.commentstr (length 1), optionalCharacter indicating that the remainder of line should not be parsed.
If found at the beginning
of a line, the line will be ignored altogether. This parameter must be a
single character. Like empty lines (as long asskip_blank_lines=True),
fully commented lines are ignored by the parameterheaderbut not byskiprows. For example, ifcomment='#', parsing#empty\na,b,c\n1,2,3withheader=0will result in'a,b,c'being
treated as the header.encodingstr, optional, default ‘utf-8’Encoding to use for UTF when reading/writing (ex.'utf-8').List of Python
standard encodings.Changed in version 1.2:WhenencodingisNone,errors='replace'is passed toopen(). Otherwise,errors='strict'is passed toopen().
This behavior was previously only the case forengine='python'.Changed in version 1.3.0:encoding_errorsis a new argument.encodinghas no longer an
influence on how encoding errors are handled.encoding_errorsstr, optional, default ‘strict’How encoding errors are treated.List of possible values.New in version 1.3.0.dialectstr or csv.Dialect, optionalIf provided, this parameter will override values (default or not) for the
following parameters:delimiter,doublequote,escapechar,skipinitialspace,quotechar, andquoting. If it is necessary to
override values, aParserWarningwill be issued. Seecsv.Dialectdocumentation for more details.on_bad_lines{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’Specifies what to do upon encountering a bad line (a line with too many fields).
Allowed values are :'error', raise an Exception when a bad line is encountered.'warn', raise a warning when a bad line is encountered and skip that line.'skip', skip bad lines without raising or warning when they are encountered.New in version 1.3.0.New in version 1.4.0:Callable, function with signature(bad_line:list[str])->list[str]|Nonethat will process a single
bad line.bad_lineis a list of strings split by thesep.
If the function returnsNone, the bad line will be ignored.
If the function returns a newlistof strings with more elements than
expected, aParserWarningwill be emitted while dropping extra elements.
Only supported whenengine='python'delim_whitespacebool, default FalseSpecifies whether or not whitespace (e.g.''or'\t') will be
used as thesepdelimiter. Equivalent to settingsep='\s+'. If this option
is set toTrue, nothing should be passed in for thedelimiterparameter.low_memorybool, default TrueInternally process the file in chunks, resulting in lower memory use
while parsing, but possibly mixed type inference. To ensure no mixed
types either setFalse, or specify the type with thedtypeparameter.
Note that the entire file is read into a singleDataFrameregardless, use thechunksizeoriteratorparameter to return the data in
chunks. (Only valid with C parser).memory_mapbool, default FalseIf a filepath is provided forfilepath_or_buffer, map the file object
directly onto memory and access the data directly from there. Using this
option can improve performance because there is no longer any I/O overhead.float_precision{‘high’, ‘legacy’, ‘round_trip’}, optionalSpecifies which converter the C engine should use for floating-point
values. The options areNoneor'high'for the ordinary converter,'legacy'for the original lower precision pandas converter, and'round_trip'for the round-trip converter.Changed in version 1.2.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional
data structure with labeled axes.See alsoDataFrame.to_csvWrite DataFrame to a comma-separated values (csv) file.read_csvRead a comma-separated values (csv) file into DataFrame.read_fwfRead a table of fixed-width formatted lines into DataFrame.Examples>>>pd.read_table('data.csv')"
Pandas,Input output,pandas.read_csv,"pandas.read_csv#pandas.read_csv(filepath_or_buffer,*,sep=_NoDefault.no_default,delimiter=None,header='infer',names=_NoDefault.no_default,index_col=None,usecols=None,dtype=None,engine=None,converters=None,true_values=None,false_values=None,skipinitialspace=False,skiprows=None,skipfooter=0,nrows=None,na_values=None,keep_default_na=True,na_filter=True,verbose=False,skip_blank_lines=True,parse_dates=None,infer_datetime_format=_NoDefault.no_default,keep_date_col=False,date_parser=_NoDefault.no_default,date_format=None,dayfirst=False,cache_dates=True,iterator=False,chunksize=None,compression='infer',thousands=None,decimal='.',lineterminator=None,quotechar='""',quoting=0,doublequote=True,escapechar=None,comment=None,encoding=None,encoding_errors='strict',dialect=None,on_bad_lines='error',delim_whitespace=False,low_memory=True,memory_map=False,float_precision=None,storage_options=None,dtype_backend=_NoDefault.no_default)[source]#Read a comma-separated values (csv) file into DataFrame.Also supports optionally iterating or breaking of the file
into chunks.Additional help can be found in the online docs forIO Tools.Parameters:filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.csv.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method, such as
a file handle (e.g. via builtinopenfunction) orStringIO.sepstr, default ‘,’Character or regex pattern to treat as the delimiter. Ifsep=None, the
C engine cannot automatically detect
the separator, but the Python parsing engine can, meaning the latter will
be used and automatically detect the separator from only the first valid
row of the file by Python’s builtin sniffer tool,csv.Sniffer.
In addition, separators longer than 1 character and different from'\s+'will be interpreted as regular expressions and will also force
the use of the Python parsing engine. Note that regex delimiters are prone
to ignoring quoted data. Regex example:'\r\t'.delimiterstr, optionalAlias forsep.headerint, Sequence of int, ‘infer’ or None, default ‘infer’Row number(s) containing column labels and marking the start of the
data (zero-indexed). Default behavior is to infer the column names: if nonamesare passed the behavior is identical toheader=0and column
names are inferred from the first line of the file, if column
names are passed explicitly tonamesthen the behavior is identical toheader=None. Explicitly passheader=0to be able to
replace existing names. The header can be a list of integers that
specify row locations for aMultiIndexon the columns
e.g.[0,1,3]. Intervening rows that are not specified will be
skipped (e.g. 2 in this example is skipped). Note that this
parameter ignores commented lines and empty lines ifskip_blank_lines=True, soheader=0denotes the first line of
data rather than the first line of the file.namesSequence of Hashable, optionalSequence of column labels to apply. If the file contains a header row,
then you should explicitly passheader=0to override the column names.
Duplicates in this list are not allowed.index_colHashable, Sequence of Hashable or False, optionalColumn(s) to use as row label(s), denoted either by column labels or column
indices. If a sequence of labels or indices is given,MultiIndexwill be formed for the row labels.Note:index_col=Falsecan be used to force pandas tonotuse the first
column as the index, e.g., when you have a malformed file with delimiters at
the end of each line.usecolslist of Hashable or Callable, optionalSubset of columns to select, denoted either by column labels or column indices.
If list-like, all elements must either
be positional (i.e. integer indices into the document columns) or strings
that correspond to column names provided either by the user innamesor
inferred from the document header row(s). Ifnamesare given, the document
header row(s) are not taken into account. For example, a valid list-likeusecolsparameter would be[0,1,2]or['foo','bar','baz'].
Element order is ignored, sousecols=[0,1]is the same as[1,0].
To instantiate aDataFramefromdatawith element order
preserved usepd.read_csv(data,usecols=['foo','bar'])[['foo','bar']]for columns in['foo','bar']order orpd.read_csv(data,usecols=['foo','bar'])[['bar','foo']]for['bar','foo']order.If callable, the callable function will be evaluated against the column
names, returning names where the callable function evaluates toTrue. An
example of a valid callable argument would belambdax:x.upper()in['AAA','BBB','DDD']. Using this parameter results in much faster
parsing time and lower memory usage.dtypedtype or dict of {Hashabledtype}, optionalData type(s) to apply to either the whole dataset or individual columns.
E.g.,{'a':np.float64,'b':np.int32,'c':'Int64'}Usestrorobjecttogether with suitablena_valuessettings
to preserve and not interpretdtype.
Ifconvertersare specified, they will be applied INSTEAD
ofdtypeconversion.New in version 1.5.0:Support fordefaultdictwas added. Specify adefaultdictas input where
the default determines thedtypeof the columns which are not explicitly
listed.engine{‘c’, ‘python’, ‘pyarrow’}, optionalParser engine to use. The C and pyarrow engines are faster, while the python engine
is currently more feature-complete. Multithreading is currently only supported by
the pyarrow engine.New in version 1.4.0:The ‘pyarrow’ engine was added as anexperimentalengine, and some features
are unsupported, or may not work correctly, with this engine.convertersdict of {HashableCallable}, optionalFunctions for converting values in specified columns. Keys can either
be column labels or column indices.true_valueslist, optionalValues to consider asTruein addition to case-insensitive variants of ‘True’.false_valueslist, optionalValues to consider asFalsein addition to case-insensitive variants of ‘False’.skipinitialspacebool, default FalseSkip spaces after delimiter.skiprowsint, list of int or Callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int)
at the start of the file.If callable, the callable function will be evaluated against the row
indices, returningTrueif the row should be skipped andFalseotherwise.
An example of a valid callable argument would belambdax:xin[0,2].skipfooterint, default 0Number of lines at bottom of file to skip (Unsupported withengine='c').nrowsint, optionalNumber of rows of file to read. Useful for reading pieces of large files.na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optionalAdditional strings to recognize asNA/NaN. Ifdictpassed, specific
per-columnNAvalues. By default the following values are interpreted asNaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”,
“1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”,
“n/a”, “nan”, “null “.keep_default_nabool, default TrueWhether or not to include the defaultNaNvalues when parsing the data.
Depending on whetherna_valuesis passed in, the behavior is as follows:Ifkeep_default_naisTrue, andna_valuesare specified,na_valuesis appended to the defaultNaNvalues used for parsing.Ifkeep_default_naisTrue, andna_valuesare not specified, only
the defaultNaNvalues are used for parsing.Ifkeep_default_naisFalse, andna_valuesare specified, only
theNaNvalues specifiedna_valuesare used for parsing.Ifkeep_default_naisFalse, andna_valuesare not specified, no
strings will be parsed asNaN.Note that ifna_filteris passed in asFalse, thekeep_default_naandna_valuesparameters will be ignored.na_filterbool, default TrueDetect missing value markers (empty strings and the value ofna_values). In
data without anyNAvalues, passingna_filter=Falsecan improve the
performance of reading a large file.verbosebool, default FalseIndicate number ofNAvalues placed in non-numeric columns.skip_blank_linesbool, default TrueIfTrue, skip over blank lines rather than interpreting asNaNvalues.parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default FalseThe behavior is as follows:bool. IfTrue-> try parsing the index.listofintor names. e.g. If[1,2,3]-> try parsing columns 1, 2, 3
each as a separate date column.listoflist. e.g. If[[1,3]]-> combine columns 1 and 3 and parse
as a single date column.dict, e.g.{'foo':[1,3]}-> parse columns 1, 3 as date and call
result ‘foo’If a column or index cannot be represented as an array ofdatetime,
say because of an unparsable value or a mixture of timezones, the column
or index will be returned unaltered as anobjectdata type. For
non-standarddatetimeparsing, useto_datetime()afterread_csv().Note: A fast-path exists for iso8601-formatted dates.infer_datetime_formatbool, default FalseIfTrueandparse_datesis enabled, pandas will attempt to infer the
format of thedatetimestrings in the columns, and if it can be inferred,
switch to a faster method of parsing them. In some cases this can increase
the parsing speed by 5-10x.Deprecated since version 2.0.0:A strict version of this argument is now the default, passing it has no effect.keep_date_colbool, default FalseIfTrueandparse_datesspecifies combining multiple columns then
keep the original columns.date_parserCallable, optionalFunction to use for converting a sequence of string columns to an array ofdatetimeinstances. The default usesdateutil.parser.parserto do the
conversion. pandas will try to calldate_parserin three different ways,
advancing to the next if an exception occurs: 1) Pass one or more arrays
(as defined byparse_dates) as arguments; 2) concatenate (row-wise) the
string values from the columns defined byparse_datesinto a single array
and pass that; and 3) calldate_parseronce for each row using one or
more strings (corresponding to the columns defined byparse_dates) as
arguments.Deprecated since version 2.0.0:Usedate_formatinstead, or read in asobjectand then applyto_datetime()as-needed.date_formatstr or dict of column -> format, optionalFormat to use for parsing dates when used in conjunction withparse_dates.
For anything more complex, please read in asobjectand then applyto_datetime()as-needed.New in version 2.0.0.dayfirstbool, default FalseDD/MM format dates, international and European format.cache_datesbool, default TrueIfTrue, use a cache of unique, converted dates to apply thedatetimeconversion. May produce significant speed-up when parsing duplicate
date strings, especially ones with timezone offsets.iteratorbool, default FalseReturnTextFileReaderobject for iteration or getting chunks withget_chunk().Changed in version 1.2:TextFileReaderis a context manager.chunksizeint, optionalNumber of lines to read from the file per chunk. Passing a value will cause the
function to return aTextFileReaderobject for iteration.
See theIO Tools docsfor more information oniteratorandchunksize.Changed in version 1.2:TextFileReaderis a context manager.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.thousandsstr (length 1), optionalCharacter acting as the thousands separator in numerical values.decimalstr (length 1), default ‘.’Character to recognize as decimal point (e.g., use ‘,’ for European data).lineterminatorstr (length 1), optionalCharacter used to denote a line break. Only valid with C parser.quotecharstr (length 1), optionalCharacter used to denote the start and end of a quoted item. Quoted
items can include thedelimiterand it will be ignored.quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMALControl field quoting behavior percsv.QUOTE_*constants. Default iscsv.QUOTE_MINIMAL(i.e., 0) which implies that only fields containing special
characters are quoted (e.g., characters defined inquotechar,delimiter,
orlineterminator.doublequotebool, default TrueWhenquotecharis specified andquotingis notQUOTE_NONE, indicate
whether or not to interpret two consecutivequotecharelements INSIDE a
field as a singlequotecharelement.escapecharstr (length 1), optionalCharacter used to escape other characters.commentstr (length 1), optionalCharacter indicating that the remainder of line should not be parsed.
If found at the beginning
of a line, the line will be ignored altogether. This parameter must be a
single character. Like empty lines (as long asskip_blank_lines=True),
fully commented lines are ignored by the parameterheaderbut not byskiprows. For example, ifcomment='#', parsing#empty\na,b,c\n1,2,3withheader=0will result in'a,b,c'being
treated as the header.encodingstr, optional, default ‘utf-8’Encoding to use for UTF when reading/writing (ex.'utf-8').List of Python
standard encodings.Changed in version 1.2:WhenencodingisNone,errors='replace'is passed toopen(). Otherwise,errors='strict'is passed toopen().
This behavior was previously only the case forengine='python'.Changed in version 1.3.0:encoding_errorsis a new argument.encodinghas no longer an
influence on how encoding errors are handled.encoding_errorsstr, optional, default ‘strict’How encoding errors are treated.List of possible values.New in version 1.3.0.dialectstr or csv.Dialect, optionalIf provided, this parameter will override values (default or not) for the
following parameters:delimiter,doublequote,escapechar,skipinitialspace,quotechar, andquoting. If it is necessary to
override values, aParserWarningwill be issued. Seecsv.Dialectdocumentation for more details.on_bad_lines{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’Specifies what to do upon encountering a bad line (a line with too many fields).
Allowed values are :'error', raise an Exception when a bad line is encountered.'warn', raise a warning when a bad line is encountered and skip that line.'skip', skip bad lines without raising or warning when they are encountered.New in version 1.3.0.New in version 1.4.0:Callable, function with signature(bad_line:list[str])->list[str]|Nonethat will process a single
bad line.bad_lineis a list of strings split by thesep.
If the function returnsNone, the bad line will be ignored.
If the function returns a newlistof strings with more elements than
expected, aParserWarningwill be emitted while dropping extra elements.
Only supported whenengine='python'delim_whitespacebool, default FalseSpecifies whether or not whitespace (e.g.''or'\t') will be
used as thesepdelimiter. Equivalent to settingsep='\s+'. If this option
is set toTrue, nothing should be passed in for thedelimiterparameter.low_memorybool, default TrueInternally process the file in chunks, resulting in lower memory use
while parsing, but possibly mixed type inference. To ensure no mixed
types either setFalse, or specify the type with thedtypeparameter.
Note that the entire file is read into a singleDataFrameregardless, use thechunksizeoriteratorparameter to return the data in
chunks. (Only valid with C parser).memory_mapbool, default FalseIf a filepath is provided forfilepath_or_buffer, map the file object
directly onto memory and access the data directly from there. Using this
option can improve performance because there is no longer any I/O overhead.float_precision{‘high’, ‘legacy’, ‘round_trip’}, optionalSpecifies which converter the C engine should use for floating-point
values. The options areNoneor'high'for the ordinary converter,'legacy'for the original lower precision pandas converter, and'round_trip'for the round-trip converter.Changed in version 1.2.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional
data structure with labeled axes.See alsoDataFrame.to_csvWrite DataFrame to a comma-separated values (csv) file.read_tableRead general delimited file into DataFrame.read_fwfRead a table of fixed-width formatted lines into DataFrame.Examples>>>pd.read_csv('data.csv')"
Pandas,Input output,pandas.DataFrame.to_csv,"pandas.DataFrame.to_csv#DataFrame.to_csv(path_or_buf=None,sep=',',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,mode='w',encoding=None,compression='infer',quoting=None,quotechar='""',lineterminator=None,chunksize=None,date_format=None,doublequote=True,escapechar=None,decimal='.',errors='strict',storage_options=None)[source]#Write object to a comma-separated values (csv) file.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string. If a non-binary file object is passed, it should
be opened withnewline=’’, disabling universal newlines. If a binary
file object is passed,modemight need to contain a‘b’.Changed in version 1.2.0:Support for binary file objects was introduced.sepstr, default ‘,’String of length 1. Field delimiter for the output file.na_repstr, default ‘’Missing data representation.float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes
precedence over other numeric formatting parameters, like decimal.columnssequence, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, andheaderandindexare True, then the index names are used. A
sequence should be given if the object uses MultiIndex. If
False do not print fields for index names. Use index_label=False
for easier importing in R.mode{‘w’, ‘x’, ‘a’}, default ‘w’Forwarded to eitheropen(mode=)orfsspec.open(mode=)to control
the file opening. Typical values include:‘w’, truncate the file first.‘x’, exclusive creation, failing if the file already exists.‘a’, append to the end of file if it exists.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to ‘utf-8’.encodingis not supported ifpath_or_bufis a non-binary file object.compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.May be a dict with key ‘method’ as compression mode
and other entries as additional compression options if
compression mode is ‘zip’.Passing compression options as keys in dict is
supported for compression modes ‘gzip’, ‘bz2’, ‘zstd’, and ‘zip’.Changed in version 1.2.0:Compression is supported for binary file objects.Changed in version 1.2.0:Previous versions forwarded dict entries for ‘gzip’ togzip.openinstead ofgzip.GzipFilewhich prevented
settingmtime.quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set afloat_formatthen floats are converted to strings and thus csv.QUOTE_NONNUMERIC
will treat them as non-numeric.quotecharstr, default ‘""’String of length 1. Character used to quote fields.lineterminatorstr, optionalThe newline character or character sequence to use in the output
file. Defaults toos.linesep, which depends on the OS in which
this method is called (’\n’ for linux, ‘\r\n’ for Windows, i.e.).Changed in version 1.5.0:Previously was line_terminator, changed for consistency with
read_csv and the standard library ‘csv’ module.chunksizeint or NoneRows to write at a time.date_formatstr, default NoneFormat string for datetime objects.doublequotebool, default TrueControl quoting ofquotecharinside a field.escapecharstr, default NoneString of length 1. Character used to escapesepandquotecharwhen appropriate.decimalstr, default ‘.’Character recognized as decimal separator. E.g. use ‘,’ for
European data.errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.Returns:None or strIf path_or_buf is None, returns the resulting csv format as a
string. Otherwise returns None.See alsoread_csvLoad a CSV file into a DataFrame.to_excelWrite DataFrame to an Excel file.Examples>>>df=pd.DataFrame({'name':['Raphael','Donatello'],...'mask':['red','purple'],...'weapon':['sai','bo staff']})>>>df.to_csv(index=False)'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'Create ‘out.zip’ containing ‘out.csv’>>>compression_opts=dict(method='zip',...archive_name='out.csv')>>>df.to_csv('out.zip',index=False,...compression=compression_opts)To write a csv file to a new folder or nested folder you will first
need to create it using either Pathlib or os:>>>frompathlibimportPath>>>filepath=Path('folder/subfolder/out.csv')>>>filepath.parent.mkdir(parents=True,exist_ok=True)>>>df.to_csv(filepath)>>>importos>>>os.makedirs('folder/subfolder',exist_ok=True)>>>df.to_csv('folder/subfolder/out.csv')"
Pandas,Input output,pandas.read_fwf,"pandas.read_fwf#pandas.read_fwf(filepath_or_buffer,*,colspecs='infer',widths=None,infer_nrows=100,dtype_backend=_NoDefault.no_default,**kwds)[source]#Read a table of fixed-width formatted lines into DataFrame.Also supports optionally iterating or breaking of the file
into chunks.Additional help can be found in theonline docs for IO Tools.Parameters:filepath_or_bufferstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a textread()function.The string could be a URL.
Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.csv.colspecslist of tuple (int, int) or ‘infer’. optionalA list of tuples giving the extents of the fixed-width
fields of each line as half-open intervals (i.e., [from, to[ ).
String value ‘infer’ can be used to instruct the parser to try
detecting the column specifications from the first 100 rows of
the data which are not being skipped via skiprows (default=’infer’).widthslist of int, optionalA list of field widths which can be used instead of ‘colspecs’ if
the intervals are contiguous.infer_nrowsint, default 100The number of rows to consider when letting the parser determine thecolspecs.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.**kwdsoptionalOptional keyword arguments can be passed toTextFileReader.Returns:DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional
data structure with labeled axes.See alsoDataFrame.to_csvWrite DataFrame to a comma-separated values (csv) file.read_csvRead a comma-separated values (csv) file into DataFrame.Examples>>>pd.read_fwf('data.csv')"
Pandas,Input output,pandas.read_clipboard,"pandas.read_clipboard#pandas.read_clipboard(sep='\\s+',dtype_backend=_NoDefault.no_default,**kwargs)[source]#Read text from clipboard and pass toread_csv().Parses clipboard contents similar to how CSV files are parsed
usingread_csv().Parameters:sepstr, default ‘\s+’A string or regex delimiter. The default of'\\s+'denotes
one or more whitespace characters.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.**kwargsSeeread_csv()for the full argument list.Returns:DataFrameA parsedDataFrameobject.See alsoDataFrame.to_clipboardCopy object to the system clipboard.read_csvRead a comma-separated values (csv) file into DataFrame.read_fwfRead a table of fixed-width formatted lines into DataFrame.Examples>>>df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['A','B','C'])>>>df.to_clipboard()>>>pd.read_clipboard()A  B  C0    1  2  31    4  5  6"
Pandas,Input output,pandas.DataFrame.to_clipboard,"pandas.DataFrame.to_clipboard#DataFrame.to_clipboard(excel=True,sep=None,**kwargs)[source]#Copy object to the system clipboard.Write a text representation of object to the system clipboard.
This can be pasted into Excel, for example.Parameters:excelbool, default TrueProduce output in a csv format for easy pasting into excel.True, use the provided separator for csv pasting.False, write a string representation of the object to the clipboard.sepstr, default'\t'Field delimiter.**kwargsThese parameters will be passed to DataFrame.to_csv.See alsoDataFrame.to_csvWrite a DataFrame to a comma-separated values (csv) file.read_clipboardRead text from clipboard and pass to read_csv.NotesRequirements for your platform.Linux :xclip, orxsel(withPyQt4modules)Windows : nonemacOS : noneThis method uses the processes developed for the packagepyperclip. A
solution to render any output string format is given in the examples.ExamplesCopy the contents of a DataFrame to the clipboard.>>>df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['A','B','C'])>>>df.to_clipboard(sep=',')...# Wrote the following to the system clipboard:...# ,A,B,C...# 0,1,2,3...# 1,4,5,6We can omit the index by passing the keywordindexand setting
it to false.>>>df.to_clipboard(sep=',',index=False)...# Wrote the following to the system clipboard:...# A,B,C...# 1,2,3...# 4,5,6Using the originalpyperclippackage for any string output format.importpypercliphtml=df.style.to_html()pyperclip.copy(html)"
Pandas,Input output,pandas.read_excel,"pandas.read_excel#pandas.read_excel(io,sheet_name=0,*,header=0,names=None,index_col=None,usecols=None,dtype=None,engine=None,converters=None,true_values=None,false_values=None,skiprows=None,nrows=None,na_values=None,keep_default_na=True,na_filter=True,verbose=False,parse_dates=False,date_parser=_NoDefault.no_default,date_format=None,thousands=None,decimal='.',comment=None,skipfooter=0,storage_options=None,dtype_backend=_NoDefault.no_default,engine_kwargs=None)[source]#Read an Excel file into a pandas DataFrame.Supportsxls,xlsx,xlsm,xlsb,odf,odsandodtfile extensions
read from a local filesystem or URL. Supports an option to read
a single sheet or a list of sheets.Parameters:iostr, bytes, ExcelFile, xlrd.Book, path object, or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.xlsx.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method,
such as a file handle (e.g. via builtinopenfunction)
orStringIO.Deprecated since version 2.1.0:Passing byte strings is deprecated. To read from a
byte string, wrap it in aBytesIOobject.sheet_namestr, int, list, or None, default 0Strings are used for sheet names. Integers are used in zero-indexed
sheet positions (chart sheets do not count as a sheet position).
Lists of strings/integers are used to request multiple sheets.
Specify None to get all worksheets.Available cases:Defaults to0: 1st sheet as aDataFrame1: 2nd sheet as aDataFrame""Sheet1"": Load sheet with name “Sheet1”[0,1,""Sheet5""]: Load first, second and sheet named “Sheet5”
as a dict ofDataFrameNone: All worksheets.headerint, list of int, default 0Row (0-indexed) to use for the column labels of the parsed
DataFrame. If a list of integers is passed those row positions will
be combined into aMultiIndex. Use None if there is no header.namesarray-like, default NoneList of column names to use. If file contains no header row,
then you should explicitly pass header=None.index_colint, str, list of int, default NoneColumn (0-indexed) to use as the row labels of the DataFrame.
Pass None if there is no such column. If a list is passed,
those columns will be combined into aMultiIndex. If a
subset of data is selected withusecols, index_col
is based on the subset.Missing values will be forward filled to allow roundtripping withto_excelformerged_cells=True. To avoid forward filling the
missing values useset_indexafter reading the data instead ofindex_col.usecolsstr, list-like, or callable, default NoneIf None, then parse all columns.If str, then indicates comma separated list of Excel column letters
and column ranges (e.g. “A:E” or “A,C,E:F”). Ranges are inclusive of
both sides.If list of int, then indicates list of column numbers to be parsed
(0-indexed).If list of string, then indicates list of column names to be parsed.If callable, then evaluate each column name against it and parse the
column if the callable returnsTrue.Returns a subset of the columns according to behavior above.dtypeType name or dict of column -> type, default NoneData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32}
Useobjectto preserve data as stored in Excel and not interpret dtype.
If converters are specified, they will be applied INSTEAD
of dtype conversion.enginestr, default NoneIf io is not a buffer or path, this must be set to identify io.
Supported engines: “xlrd”, “openpyxl”, “odf”, “pyxlsb”.
Engine compatibility :“xlrd” supports old-style Excel files (.xls).“openpyxl” supports newer Excel file formats.“odf” supports OpenDocument file formats (.odf, .ods, .odt).“pyxlsb” supports Binary Excel files.Changed in version 1.2.0:The enginexlrdnow only supports old-style.xlsfiles.
Whenengine=None, the following logic will be
used to determine the engine:Ifpath_or_bufferis an OpenDocument format (.odf, .ods, .odt),
thenodfwill be used.Otherwise ifpath_or_bufferis an xls format,xlrdwill be used.Otherwise ifpath_or_bufferis in xlsb format,pyxlsbwill be used.New in version 1.3.0.Otherwiseopenpyxlwill be used.Changed in version 1.3.0.convertersdict, default NoneDict of functions for converting values in certain columns. Keys can
either be integers or column labels, values are functions that take one
input argument, the Excel cell content, and return the transformed
content.true_valueslist, default NoneValues to consider as True.false_valueslist, default NoneValues to consider as False.skiprowslist-like, int, or callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int) at the
start of the file. If callable, the callable function will be evaluated
against the row indices, returning True if the row should be skipped and
False otherwise. An example of a valid callable argument would belambdax:xin[0,2].nrowsint, default NoneNumber of rows to parse.na_valuesscalar, str, list-like, or dict, default NoneAdditional strings to recognize as NA/NaN. If dict passed, specific
per-column NA values. By default the following values are interpreted
as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’,
‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘None’,
‘n/a’, ‘nan’, ‘null’.keep_default_nabool, default TrueWhether or not to include the default NaN values when parsing the data.
Depending on whetherna_valuesis passed in, the behavior is as follows:Ifkeep_default_nais True, andna_valuesare specified,na_valuesis appended to the default NaN values used for parsing.Ifkeep_default_nais True, andna_valuesare not specified, only
the default NaN values are used for parsing.Ifkeep_default_nais False, andna_valuesare specified, only
the NaN values specifiedna_valuesare used for parsing.Ifkeep_default_nais False, andna_valuesare not specified, no
strings will be parsed as NaN.Note that ifna_filteris passed in as False, thekeep_default_naandna_valuesparameters will be ignored.na_filterbool, default TrueDetect missing value markers (empty strings and the value of na_values). In
data without any NAs, passing na_filter=False can improve the performance
of reading a large file.verbosebool, default FalseIndicate number of NA values placed in non-numeric columns.parse_datesbool, list-like, or dict, default FalseThe behavior is as follows:bool. If True -> try parsing the index.list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3
each as a separate date column.list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as
a single date column.dict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call
result ‘foo’If a column or index contains an unparsable date, the entire column or
index will be returned unaltered as an object data type. If you don`t want to
parse some cells as date just change their type in Excel to “Text”.
For non-standard datetime parsing, usepd.to_datetimeafterpd.read_excel.Note: A fast-path exists for iso8601-formatted dates.date_parserfunction, optionalFunction to use for converting a sequence of string columns to an array of
datetime instances. The default usesdateutil.parser.parserto do the
conversion. Pandas will try to calldate_parserin three different ways,
advancing to the next if an exception occurs: 1) Pass one or more arrays
(as defined byparse_dates) as arguments; 2) concatenate (row-wise) the
string values from the columns defined byparse_datesinto a single array
and pass that; and 3) calldate_parseronce for each row using one or
more strings (corresponding to the columns defined byparse_dates) as
arguments.Deprecated since version 2.0.0:Usedate_formatinstead, or read in asobjectand then applyto_datetime()as-needed.date_formatstr or dict of column -> format, defaultNoneIf used in conjunction withparse_dates, will parse dates according to this
format. For anything more complex,
please read in asobjectand then applyto_datetime()as-needed.New in version 2.0.0.thousandsstr, default NoneThousands separator for parsing string columns to numeric. Note that
this parameter is only necessary for columns stored as TEXT in Excel,
any numeric columns will automatically be parsed, regardless of display
format.decimalstr, default ‘.’Character to recognize as decimal point for parsing string columns to numeric.
Note that this parameter is only necessary for columns stored as TEXT in Excel,
any numeric columns will automatically be parsed, regardless of display
format.(e.g. use ‘,’ for European data).New in version 1.4.0.commentstr, default NoneComments out remainder of line. Pass a character or characters to this
argument to indicate comments in the input file. Any data between the
comment string and the end of the current line is ignored.skipfooterint, default 0Rows at the end to skip (0-indexed).storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.Returns:DataFrame or dict of DataFramesDataFrame from the passed in Excel file. See notes in sheet_name
argument for more information on when a dict of DataFrames is returned.See alsoDataFrame.to_excelWrite DataFrame to an Excel file.DataFrame.to_csvWrite DataFrame to a comma-separated values (csv) file.read_csvRead a comma-separated values (csv) file into DataFrame.read_fwfRead a table of fixed-width formatted lines into DataFrame.NotesFor specific information on the methods used for each Excel engine, refer to the pandasuser guideExamplesThe file can be read using the file name as string or an open file object:>>>pd.read_excel('tmp.xlsx',index_col=0)Name  Value0   string1      11   string2      22  #Comment      3>>>pd.read_excel(open('tmp.xlsx','rb'),...sheet_name='Sheet3')Unnamed: 0      Name  Value0           0   string1      11           1   string2      22           2  #Comment      3Index and header can be specified via theindex_colandheaderarguments>>>pd.read_excel('tmp.xlsx',index_col=None,header=None)0         1      20  NaN      Name  Value1  0.0   string1      12  1.0   string2      23  2.0  #Comment      3Column types are inferred but can be explicitly specified>>>pd.read_excel('tmp.xlsx',index_col=0,...dtype={'Name':str,'Value':float})Name  Value0   string1    1.01   string2    2.02  #Comment    3.0True, False, and NA values, and thousands separators have defaults,
but can be explicitly specified, too. Supply the values you would like
as strings or lists of strings!>>>pd.read_excel('tmp.xlsx',index_col=0,...na_values=['string1','string2'])Name  Value0       NaN      11       NaN      22  #Comment      3Comment lines in the excel input file can be skipped using thecommentkwarg>>>pd.read_excel('tmp.xlsx',index_col=0,comment='#')Name  Value0  string1    1.01  string2    2.02     None    NaN"
Pandas,Input output,pandas.DataFrame.to_excel,"pandas.DataFrame.to_excel#DataFrame.to_excel(excel_writer,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,inf_rep='inf',freeze_panes=None,storage_options=None,engine_kwargs=None)[source]#Write object to an Excel sheet.To write a single object to an Excel .xlsx file it is only necessary to
specify a target file name. To write to multiple sheets it is necessary to
create anExcelWriterobject with a target file name, and specify a sheet
in the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.
With all data written to the file it is necessary to save the changes.
Note that creating anExcelWriterobject with a file name that already
exists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default ‘Sheet1’Name of sheet which will contain DataFrame.na_repstr, default ‘’Missing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=""%.2f""will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A
sequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this
via the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default ‘inf’Representation for infinity (there is no native representation for
infinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that
is to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),
to_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further
data without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(""output.xlsx"")To specify the sheet name:>>>df1.to_excel(""output.xlsx"",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is
necessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,
you can pass theenginekeyword (the default engine is
automatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')"
Pandas,Input output,pandas.ExcelFile,"pandas.ExcelFile#classpandas.ExcelFile(path_or_buffer,engine=None,storage_options=None,engine_kwargs=None)[source]#Class for parsing tabular Excel sheets into DataFrame objects.See read_excel for more documentation.Parameters:path_or_bufferstr, bytes, path object (pathlib.Path or py._path.local.LocalPath),A file-like object, xlrd workbook or openpyxl workbook.
If a string or path object, expected to be a path to a
.xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.enginestr, default NoneIf io is not a buffer or path, this must be set to identify io.
Supported engines:xlrd,openpyxl,odf,pyxlsbEngine compatibility :xlrdsupports old-style Excel files (.xls).openpyxlsupports newer Excel file formats.odfsupports OpenDocument file formats (.odf, .ods, .odt).pyxlsbsupports Binary Excel files.Changed in version 1.2.0:The enginexlrdnow only supports old-style.xlsfiles.
Whenengine=None, the following logic will be
used to determine the engine:Ifpath_or_bufferis an OpenDocument format (.odf, .ods, .odt),
thenodfwill be used.Otherwise ifpath_or_bufferis an xls format,xlrdwill be used.Otherwise ifpath_or_bufferis in xlsb format,pyxlsbwill be used.New in version 1.3.0.Otherwise ifopenpyxlis installed,
thenopenpyxlwill be used.Otherwise ifxlrd>=2.0is installed, aValueErrorwill be raised.WarningPlease do not report issues when usingxlrdto read.xlsxfiles.
This is not supported, switch to usingopenpyxlinstead.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.Examples>>>file=pd.ExcelFile('myfile.xlsx')>>>withpd.ExcelFile(""myfile.xls"")asxls:...df1=pd.read_excel(xls,""Sheet1"")Attributesbooksheet_namesMethodsclose()close io if necessaryparse([sheet_name, header, names, ...])Parse specified sheet(s) into a DataFrame.ODFReaderOpenpyxlReaderPyxlsbReaderXlrdReader"
Pandas,Input output,pandas.ExcelFile.book,pandas.ExcelFile.book#propertyExcelFile.book[source]#
Pandas,Input output,pandas.ExcelFile.sheet_names,pandas.ExcelFile.sheet_names#propertyExcelFile.sheet_names[source]#
Pandas,Input output,pandas.ExcelFile.parse,"pandas.ExcelFile.parse#ExcelFile.parse(sheet_name=0,header=0,names=None,index_col=None,usecols=None,converters=None,true_values=None,false_values=None,skiprows=None,nrows=None,na_values=None,parse_dates=False,date_parser=_NoDefault.no_default,date_format=None,thousands=None,comment=None,skipfooter=0,dtype_backend=_NoDefault.no_default,**kwds)[source]#Parse specified sheet(s) into a DataFrame.Equivalent to read_excel(ExcelFile, …) See the read_excel
docstring for more info on accepted parameters.Returns:DataFrame or dict of DataFramesDataFrame from the passed in Excel file.Examples>>>df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['A','B','C'])>>>df.to_excel('myfile.xlsx')>>>file=pd.ExcelFile('myfile.xlsx')>>>file.parse()"
Pandas,Input output,pandas.io.formats.style.Styler.to_excel,"pandas.io.formats.style.Styler.to_excel#Styler.to_excel(excel_writer,sheet_name='Sheet1',na_rep='',float_format=None,columns=None,header=True,index=True,index_label=None,startrow=0,startcol=0,engine=None,merge_cells=True,encoding=None,inf_rep='inf',verbose=True,freeze_panes=None,storage_options=None)[source]#Write Styler to an Excel sheet.To write a single Styler to an Excel .xlsx file it is only necessary to
specify a target file name. To write to multiple sheets it is necessary to
create anExcelWriterobject with a target file name, and specify a sheet
in the file to write to.Multiple sheets may be written to by specifying uniquesheet_name.
With all data written to the file it is necessary to save the changes.
Note that creating anExcelWriterobject with a file name that already
exists will result in the contents of the existing file being erased.Parameters:excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter.sheet_namestr, default ‘Sheet1’Name of sheet which will contain DataFrame.na_repstr, default ‘’Missing data representation.float_formatstr, optionalFormat string for floating point numbers. For examplefloat_format=""%.2f""will format 0.1234 to 0.12.columnssequence or list of str, optionalColumns to write.headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is
assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, andheaderandindexare True, then the index names are used. A
sequence should be given if the DataFrame uses MultiIndex.startrowint, default 0Upper left cell row to dump data frame.startcolint, default 0Upper left cell column to dump data frame.enginestr, optionalWrite engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this
via the optionsio.excel.xlsx.writerorio.excel.xlsm.writer.merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells.inf_repstr, default ‘inf’Representation for infinity (there is no native representation for
infinity in Excel).freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that
is to be frozen.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.5.0.engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.See alsoto_csvWrite DataFrame to a comma-separated values (csv) file.ExcelWriterClass for writing DataFrame objects into excel sheets.read_excelRead an Excel file into a pandas DataFrame.read_csvRead a comma-separated values (csv) file into DataFrame.io.formats.style.Styler.to_excelAdd styles to Excel sheet.NotesFor compatibility withto_csv(),
to_excel serializes lists and dicts to strings before writing.Once a workbook has been saved it is not possible to write further
data without rewriting the whole workbook.ExamplesCreate, write to and save a workbook:>>>df1=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])>>>df1.to_excel(""output.xlsx"")To specify the sheet name:>>>df1.to_excel(""output.xlsx"",...sheet_name='Sheet_name_1')If you wish to write to more than one sheet in the workbook, it is
necessary to specify an ExcelWriter object:>>>df2=df1.copy()>>>withpd.ExcelWriter('output.xlsx')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_1')...df2.to_excel(writer,sheet_name='Sheet_name_2')ExcelWriter can also be used to append to an existing Excel file:>>>withpd.ExcelWriter('output.xlsx',...mode='a')aswriter:...df1.to_excel(writer,sheet_name='Sheet_name_3')To set the library that is used to write the Excel file,
you can pass theenginekeyword (the default engine is
automatically chosen depending on the file extension):>>>df1.to_excel('output1.xlsx',engine='xlsxwriter')"
Pandas,Input output,pandas.ExcelWriter,"pandas.ExcelWriter#classpandas.ExcelWriter(path,engine=None,date_format=None,datetime_format=None,mode='w',storage_options=None,if_sheet_exists=None,engine_kwargs=None)[source]#Class for writing DataFrame objects into excel sheets.Default is to use:xlsxwriterfor xlsx files if xlsxwriter
is installed otherwiseopenpyxlodswriterfor ods filesSeeDataFrame.to_excelfor typical usage.The writer should be used as a context manager. Otherwise, callclose()to save
and close any opened file handles.Parameters:pathstr or typing.BinaryIOPath to xls or xlsx or ods file.enginestr (optional)Engine to use for writing. If None, defaults toio.excel.<extension>.writer. NOTE: can only be passed as a keyword
argument.date_formatstr, default NoneFormat string for dates written into Excel files (e.g. ‘YYYY-MM-DD’).datetime_formatstr, default NoneFormat string for datetime objects written into Excel files.
(e.g. ‘YYYY-MM-DD HH:MM:SS’).mode{‘w’, ‘a’}, default ‘w’File mode to use (write or append). Append does not work with fsspec URLs.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.if_sheet_exists{‘error’, ‘new’, ‘replace’, ‘overlay’}, default ‘error’How to behave when trying to write to a sheet that already
exists (append mode only).error: raise a ValueError.new: Create a new sheet, with a name determined by the engine.replace: Delete the contents of the sheet before writing to it.overlay: Write contents to the existing sheet without first removing,
but possibly over top of, the existing contents.New in version 1.3.0.Changed in version 1.4.0:Addedoverlayoptionengine_kwargsdict, optionalKeyword arguments to be passed into the engine. These will be passed to
the following functions of the respective engines:xlsxwriter:xlsxwriter.Workbook(file,**engine_kwargs)openpyxl (write mode):openpyxl.Workbook(**engine_kwargs)openpyxl (append mode):openpyxl.load_workbook(file,**engine_kwargs)odswriter:odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)New in version 1.3.0.NotesFor compatibility with CSV writers, ExcelWriter serializes lists
and dicts to strings before writing.ExamplesDefault usage:>>>df=pd.DataFrame([[""ABC"",""XYZ""]],columns=[""Foo"",""Bar""])>>>withpd.ExcelWriter(""path_to_file.xlsx"")aswriter:...df.to_excel(writer)To write to separate sheets in a single file:>>>df1=pd.DataFrame([[""AAA"",""BBB""]],columns=[""Spam"",""Egg""])>>>df2=pd.DataFrame([[""ABC"",""XYZ""]],columns=[""Foo"",""Bar""])>>>withpd.ExcelWriter(""path_to_file.xlsx"")aswriter:...df1.to_excel(writer,sheet_name=""Sheet1"")...df2.to_excel(writer,sheet_name=""Sheet2"")You can set the date format or datetime format:>>>fromdatetimeimportdate,datetime>>>df=pd.DataFrame(...[...[date(2014,1,31),date(1999,9,24)],...[datetime(1998,5,26,23,33,4),datetime(2014,2,28,13,5,13)],...],...index=[""Date"",""Datetime""],...columns=[""X"",""Y""],...)>>>withpd.ExcelWriter(...""path_to_file.xlsx"",...date_format=""YYYY-MM-DD"",...datetime_format=""YYYY-MM-DD HH:MM:SS""...)aswriter:...df.to_excel(writer)You can also append to an existing Excel file:>>>withpd.ExcelWriter(""path_to_file.xlsx"",mode=""a"",engine=""openpyxl"")aswriter:...df.to_excel(writer,sheet_name=""Sheet3"")Here, theif_sheet_existsparameter can be set to replace a sheet if it
already exists:>>>withExcelWriter(...""path_to_file.xlsx"",...mode=""a"",...engine=""openpyxl"",...if_sheet_exists=""replace"",...)aswriter:...df.to_excel(writer,sheet_name=""Sheet1"")You can also write multiple DataFrames to a single sheet. Note that theif_sheet_existsparameter needs to be set tooverlay:>>>withExcelWriter(""path_to_file.xlsx"",...mode=""a"",...engine=""openpyxl"",...if_sheet_exists=""overlay"",...)aswriter:...df1.to_excel(writer,sheet_name=""Sheet1"")...df2.to_excel(writer,sheet_name=""Sheet1"",startcol=3)You can store Excel file in RAM:>>>importio>>>df=pd.DataFrame([[""ABC"",""XYZ""]],columns=[""Foo"",""Bar""])>>>buffer=io.BytesIO()>>>withpd.ExcelWriter(buffer)aswriter:...df.to_excel(writer)You can pack Excel file into zip archive:>>>importzipfile>>>df=pd.DataFrame([[""ABC"",""XYZ""]],columns=[""Foo"",""Bar""])>>>withzipfile.ZipFile(""path_to_file.zip"",""w"")aszf:...withzf.open(""filename.xlsx"",""w"")asbuffer:...withpd.ExcelWriter(buffer)aswriter:...df.to_excel(writer)You can specify additional arguments to the underlying engine:>>>withpd.ExcelWriter(...""path_to_file.xlsx"",...engine=""xlsxwriter"",...engine_kwargs={""options"":{""nan_inf_to_errors"":True}}...)aswriter:...df.to_excel(writer)In append mode,engine_kwargsare passed through to
openpyxl’sload_workbook:>>>withpd.ExcelWriter(...""path_to_file.xlsx"",...engine=""openpyxl"",...mode=""a"",...engine_kwargs={""keep_vba"":True}...)aswriter:...df.to_excel(writer,sheet_name=""Sheet2"")AttributesbookBook instance.date_formatFormat string for dates written into Excel files (e.g.datetime_formatFormat string for dates written into Excel files (e.g.engineName of engine.if_sheet_existsHow to behave when writing to a sheet that already exists in append mode.sheetsMapping of sheet names to sheet objects.supported_extensionsExtensions that writer engine supports.Methodscheck_extension(ext)checks that path's extension against the Writer's supported extensions.close()synonym for save, to make it more file-like"
Pandas,Input output,pandas.read_json,"pandas.read_json#pandas.read_json(path_or_buf,*,orient=None,typ='frame',dtype=None,convert_axes=None,convert_dates=True,keep_default_dates=True,precise_float=False,date_unit=None,encoding=None,encoding_errors='strict',lines=False,chunksize=None,compression='infer',nrows=None,storage_options=None,dtype_backend=_NoDefault.no_default,engine='ujson')[source]#Convert a JSON string to pandas object.Parameters:path_or_bufa valid JSON str, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.json.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method,
such as a file handle (e.g. via builtinopenfunction)
orStringIO.Deprecated since version 2.1.0:Passing json literal strings is deprecated.orientstr, optionalIndication of expected JSON string format.
Compatible JSON strings can be produced byto_json()with a
corresponding orient value.
The set of possible orients is:'split': dict like{index->[index],columns->[columns],data->[values]}'records': list like[{column->value},...,{column->value}]'index': dict like{index->{column->value}}'columns': dict like{column->{index->value}}'values': just the values array'table': dict like{'schema':{schema},'data':{data}}The allowed and default values depend on the value
of thetypparameter.whentyp=='series',allowed orients are{'split','records','index'}default is'index'The Series index must be unique for orient'index'.whentyp=='frame',allowed orients are{'split','records','index','columns','values','table'}default is'columns'The DataFrame index must be unique for orients'index'and'columns'.The DataFrame columns must be unique for orients'index','columns', and'records'.typ{‘frame’, ‘series’}, default ‘frame’The type of object to recover.dtypebool or dict, default NoneIf True, infer dtypes; if a dict of column to dtype, then use those;
if False, then don’t infer dtypes at all, applies only to the data.For allorientvalues except'table', default is True.convert_axesbool, default NoneTry to convert the axes to the proper dtypes.For allorientvalues except'table', default is True.convert_datesbool or list of str, default TrueIf True then default datelike columns may be converted (depending on
keep_default_dates).
If False, no dates will be converted.
If a list of column names, then those columns will be converted and
default datelike columns may also be converted (depending on
keep_default_dates).keep_default_datesbool, default TrueIf parsing dates (convert_dates is not False), then try to parse the
default datelike columns.
A column label is datelike ifit ends with'_at',it ends with'_time',it begins with'timestamp',it is'modified', orit is'date'.precise_floatbool, default FalseSet to enable usage of higher precision (strtod) function when
decoding string to double values. Default (False) is to use fast but
less precise builtin functionality.date_unitstr, default NoneThe timestamp unit to detect if converting dates. The default behaviour
is to try and detect the correct precision, but if this is not desired
then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force parsing only seconds,
milliseconds, microseconds or nanoseconds respectively.encodingstr, default is ‘utf-8’The encoding to use to decode py3 bytes.encoding_errorsstr, optional, default “strict”How encoding errors are treated.List of possible values.New in version 1.3.0.linesbool, default FalseRead the file as a json object per line.chunksizeint, optionalReturn JsonReader object for iteration.
See theline-delimited json docsfor more information onchunksize.
This can only be passed iflines=True.
If this is None, the file will be read into memory all at once.Changed in version 1.2:JsonReaderis a context manager.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buf’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.nrowsint, optionalThe number of lines from the line-delimited jsonfile that has to be read.
This can only be passed iflines=True.
If this is None, all the rows will be returned.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.engine{“ujson”, “pyarrow”}, default “ujson”Parser engine to use. The""pyarrow""engine is only available whenlines=True.New in version 2.0.Returns:Series, DataFrame, or pandas.api.typing.JsonReaderA JsonReader is returned whenchunksizeis not0orNone.
Otherwise, the type returned depends on the value oftyp.See alsoDataFrame.to_jsonConvert a DataFrame to a JSON string.Series.to_jsonConvert a Series to a JSON string.json_normalizeNormalize semi-structured JSON data into a flat table.NotesSpecific toorient='table', if aDataFramewith a literalIndexname ofindexgets written withto_json(), the
subsequent read operation will incorrectly set theIndexname toNone. This is becauseindexis also used byDataFrame.to_json()to denote a missingIndexname, and the subsequentread_json()operation cannot distinguish between the two. The same
limitation is encountered with aMultiIndexand any names
beginning with'level_'.Examples>>>fromioimportStringIO>>>df=pd.DataFrame([['a','b'],['c','d']],...index=['row 1','row 2'],...columns=['col 1','col 2'])Encoding/decoding a Dataframe using'split'formatted JSON:>>>df.to_json(orient='split')'{""columns"":[""col 1"",""col 2""],""index"":[""row 1"",""row 2""],""data"":[[""a"",""b""],[""c"",""d""]]}'>>>pd.read_json(StringIO(_),orient='split')col 1 col 2row 1     a     brow 2     c     dEncoding/decoding a Dataframe using'index'formatted JSON:>>>df.to_json(orient='index')'{""row 1"":{""col 1"":""a"",""col 2"":""b""},""row 2"":{""col 1"":""c"",""col 2"":""d""}}'>>>pd.read_json(StringIO(_),orient='index')col 1 col 2row 1     a     brow 2     c     dEncoding/decoding a Dataframe using'records'formatted JSON.
Note that index labels are not preserved with this encoding.>>>df.to_json(orient='records')'[{""col 1"":""a"",""col 2"":""b""},{""col 1"":""c"",""col 2"":""d""}]'>>>pd.read_json(StringIO(_),orient='records')col 1 col 20     a     b1     c     dEncoding with Table Schema>>>df.to_json(orient='table')'{""schema"":{""fields"":[{""name"":""index"",""type"":""string""},{""name"":""col 1"",""type"":""string""},{""name"":""col 2"",""type"":""string""}],""primaryKey"":[""index""],""pandas_version"":""1.4.0""},""data"":[{""index"":""row 1"",""col 1"":""a"",""col 2"":""b""},{""index"":""row 2"",""col 1"":""c"",""col 2"":""d""}]}'"
Pandas,Input output,pandas.json_normalize,"pandas.json_normalize#pandas.json_normalize(data,record_path=None,meta=None,meta_prefix=None,record_prefix=None,errors='raise',sep='.',max_level=None)[source]#Normalize semi-structured JSON data into a flat table.Parameters:datadict or list of dictsUnserialized JSON objects.record_pathstr or list of str, default NonePath in each object to list of records. If not passed, data will be
assumed to be an array of records.metalist of paths (str or list of str), default NoneFields to use as metadata for each record in resulting table.meta_prefixstr, default NoneIf True, prefix records with dotted (?) path, e.g. foo.bar.field if
meta is [‘foo’, ‘bar’].record_prefixstr, default NoneIf True, prefix records with dotted (?) path, e.g. foo.bar.field if
path to records is [‘foo’, ‘bar’].errors{‘raise’, ‘ignore’}, default ‘raise’Configures error handling.‘ignore’ : will ignore KeyError if keys listed in meta are not
always present.‘raise’ : will raise KeyError if keys listed in meta are not
always present.sepstr, default ‘.’Nested records will generate names separated by sep.
e.g., for sep=’.’, {‘foo’: {‘bar’: 0}} -> foo.bar.max_levelint, default NoneMax number of levels(depth of dict) to normalize.
if None, normalizes all levels.Returns:frameDataFrameNormalize semi-structured JSON data into a flat table.Examples>>>data=[...{""id"":1,""name"":{""first"":""Coleen"",""last"":""Volk""}},...{""name"":{""given"":""Mark"",""family"":""Regner""}},...{""id"":2,""name"":""Faye Raker""},...]>>>pd.json_normalize(data)id name.first name.last name.given name.family        name0  1.0     Coleen      Volk        NaN         NaN         NaN1  NaN        NaN       NaN       Mark      Regner         NaN2  2.0        NaN       NaN        NaN         NaN  Faye Raker>>>data=[...{...""id"":1,...""name"":""Cole Volk"",...""fitness"":{""height"":130,""weight"":60},...},...{""name"":""Mark Reg"",""fitness"":{""height"":130,""weight"":60}},...{...""id"":2,...""name"":""Faye Raker"",...""fitness"":{""height"":130,""weight"":60},...},...]>>>pd.json_normalize(data,max_level=0)id        name                        fitness0  1.0   Cole Volk  {'height': 130, 'weight': 60}1  NaN    Mark Reg  {'height': 130, 'weight': 60}2  2.0  Faye Raker  {'height': 130, 'weight': 60}Normalizes nested data up to level 1.>>>data=[...{...""id"":1,...""name"":""Cole Volk"",...""fitness"":{""height"":130,""weight"":60},...},...{""name"":""Mark Reg"",""fitness"":{""height"":130,""weight"":60}},...{...""id"":2,...""name"":""Faye Raker"",...""fitness"":{""height"":130,""weight"":60},...},...]>>>pd.json_normalize(data,max_level=1)id        name  fitness.height  fitness.weight0  1.0   Cole Volk             130              601  NaN    Mark Reg             130              602  2.0  Faye Raker             130              60>>>data=[...{...""state"":""Florida"",...""shortname"":""FL"",...""info"":{""governor"":""Rick Scott""},...""counties"":[...{""name"":""Dade"",""population"":12345},...{""name"":""Broward"",""population"":40000},...{""name"":""Palm Beach"",""population"":60000},...],...},...{...""state"":""Ohio"",...""shortname"":""OH"",...""info"":{""governor"":""John Kasich""},...""counties"":[...{""name"":""Summit"",""population"":1234},...{""name"":""Cuyahoga"",""population"":1337},...],...},...]>>>result=pd.json_normalize(...data,""counties"",[""state"",""shortname"",[""info"",""governor""]]...)>>>resultname  population    state shortname info.governor0        Dade       12345   Florida    FL    Rick Scott1     Broward       40000   Florida    FL    Rick Scott2  Palm Beach       60000   Florida    FL    Rick Scott3      Summit        1234   Ohio       OH    John Kasich4    Cuyahoga        1337   Ohio       OH    John Kasich>>>data={""A"":[1,2]}>>>pd.json_normalize(data,""A"",record_prefix=""Prefix."")Prefix.00          11          2Returns normalized data with columns prefixed with the given string."
Pandas,Input output,pandas.DataFrame.to_json,"pandas.DataFrame.to_json#DataFrame.to_json(path_or_buf=None,orient=None,date_format=None,double_precision=10,force_ascii=True,date_unit='ms',default_handler=None,lines=False,compression='infer',index=None,indent=None,storage_options=None,mode='w')[source]#Convert the object to a JSON string.Note NaN’s and None will be converted to null and datetime objects
will be converted to UNIX timestamps.Parameters:path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like
object implementing a write() function. If None, the result is
returned as a string.orientstrIndication of expected JSON string format.Series:default is ‘index’allowed values are: {‘split’, ‘records’, ‘index’, ‘table’}.DataFrame:default is ‘columns’allowed values are: {‘split’, ‘records’, ‘index’, ‘columns’,
‘values’, ‘table’}.The format of the JSON string:‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns],
‘data’ -> [values]}‘records’ : list like [{column -> value}, … , {column -> value}]‘index’ : dict like {index -> {column -> value}}‘columns’ : dict like {column -> {index -> value}}‘values’ : just the values array‘table’ : dict like {‘schema’: {schema}, ‘data’: {data}}Describing the data, where data component is likeorient='records'.date_format{None, ‘epoch’, ‘iso’}Type of date conversion. ‘epoch’ = epoch milliseconds,
‘iso’ = ISO8601. The default depends on theorient. Fororient='table', the default is ‘iso’. For all other orients,
the default is ‘epoch’.double_precisionint, default 10The number of decimal places to use when encoding
floating point values. The possible maximal value is 15.
Passing double_precision greater than 15 will raise a ValueError.force_asciibool, default TrueForce encoded string to be ASCII.date_unitstr, default ‘ms’ (milliseconds)The time unit to encode to, governs timestamp and ISO8601
precision. One of ‘s’, ‘ms’, ‘us’, ‘ns’ for second, millisecond,
microsecond, and nanosecond respectively.default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a
suitable format for JSON. Should receive a single argument which is
the object to convert and return a serialisable object.linesbool, default FalseIf ‘orient’ is ‘records’ write out line-delimited json format. Will
throw ValueError if incorrect ‘orient’ since others are not
list-like.compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.indexbool or None, default NoneThe index is only used when ‘orient’ is ‘split’, ‘index’, ‘column’,
or ‘table’. Of these, ‘index’ and ‘column’ do not supportindex=False.indentint, optionalLength of whitespace used to indent each record.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.modestr, default ‘w’ (writing)Specify the IO mode for output when supplying a path_or_buf.
Accepted args are ‘w’ (writing) and ‘a’ (append) only.
mode=’a’ is only supported when lines is True and orient is ‘records’.Returns:None or strIf path_or_buf is None, returns the resulting json format as a
string. Otherwise returns None.See alsoread_jsonConvert a JSON string to pandas object.NotesThe behavior ofindent=0varies from the stdlib, which does not
indent the output but does insert newlines. Currently,indent=0and the defaultindent=Noneare equivalent in pandas, though this
may change in a future release.orient='table'contains a ‘pandas_version’ field under ‘schema’.
This stores the version ofpandasused in the latest revision of the
schema.Examples>>>fromjsonimportloads,dumps>>>df=pd.DataFrame(...[[""a"",""b""],[""c"",""d""]],...index=[""row 1"",""row 2""],...columns=[""col 1"",""col 2""],...)>>>result=df.to_json(orient=""split"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""columns"": [""col 1"",""col 2""],""index"": [""row 1"",""row 2""],""data"": [[""a"",""b""],[""c"",""d""]]}Encoding/decoding a Dataframe using'records'formatted JSON.
Note that index labels are not preserved with this encoding.>>>result=df.to_json(orient=""records"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[{""col 1"": ""a"",""col 2"": ""b""},{""col 1"": ""c"",""col 2"": ""d""}]Encoding/decoding a Dataframe using'index'formatted JSON:>>>result=df.to_json(orient=""index"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""row 1"": {""col 1"": ""a"",""col 2"": ""b""},""row 2"": {""col 1"": ""c"",""col 2"": ""d""}}Encoding/decoding a Dataframe using'columns'formatted JSON:>>>result=df.to_json(orient=""columns"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""col 1"": {""row 1"": ""a"",""row 2"": ""c""},""col 2"": {""row 1"": ""b"",""row 2"": ""d""}}Encoding/decoding a Dataframe using'values'formatted JSON:>>>result=df.to_json(orient=""values"")>>>parsed=loads(result)>>>dumps(parsed,indent=4)[[""a"",""b""],[""c"",""d""]]Encoding with Table Schema:>>>result=df.to_json(orient=""table"")>>>parsed=loads(result)>>>dumps(parsed,indent=4){""schema"": {""fields"": [{""name"": ""index"",""type"": ""string""},{""name"": ""col 1"",""type"": ""string""},{""name"": ""col 2"",""type"": ""string""}],""primaryKey"": [""index""],""pandas_version"": ""1.4.0""},""data"": [{""index"": ""row 1"",""col 1"": ""a"",""col 2"": ""b""},{""index"": ""row 2"",""col 1"": ""c"",""col 2"": ""d""}]}"
Pandas,Input output,pandas.io.json.build_table_schema,"pandas.io.json.build_table_schema#pandas.io.json.build_table_schema(data,index=True,primary_key=None,version=True)[source]#Create a Table schema fromdata.Parameters:dataSeries, DataFrameindexbool, default TrueWhether to includedata.indexin the schema.primary_keybool or None, default TrueColumn names to designate as the primary key.
The defaultNonewill set‘primaryKey’to the index
level or levels if the index is unique.versionbool, default TrueWhether to include a fieldpandas_versionwith the version
of pandas that last revised the table schema. This version
can be different from the installed pandas version.Returns:dictNotesSeeTable Schemafor
conversion types.
Timedeltas as converted to ISO8601 duration format with
9 decimal places after the seconds field for nanosecond precision.Categoricals are converted to theanydtype, and use theenumfield
constraint to list the allowed values. Theorderedattribute is included
in anorderedfield.Examples>>>frompandas.io.json._table_schemaimportbuild_table_schema>>>df=pd.DataFrame(...{'A':[1,2,3],...'B':['a','b','c'],...'C':pd.date_range('2016-01-01',freq='d',periods=3),...},index=pd.Index(range(3),name='idx'))>>>build_table_schema(df){'fields': [{'name': 'idx', 'type': 'integer'}, {'name': 'A', 'type': 'integer'}, {'name': 'B', 'type': 'string'}, {'name': 'C', 'type': 'datetime'}], 'primaryKey': ['idx'], 'pandas_version': '1.4.0'}"
Pandas,Input output,pandas.read_html,"pandas.read_html#pandas.read_html(io,*,match='.+',flavor=None,header=None,index_col=None,skiprows=None,attrs=None,parse_dates=False,thousands=',',encoding=None,decimal='.',converters=None,na_values=None,keep_default_na=True,displayed_only=True,extract_links=None,dtype_backend=_NoDefault.no_default,storage_options=None)[source]#Read HTML tables into alistofDataFrameobjects.Parameters:iostr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a stringread()function.
The string can represent a URL or the HTML itself. Note that
lxml only accepts the http, ftp and file url protocols. If you have a
URL that starts with'https'you might try removing the's'.Deprecated since version 2.1.0:Passing html literal strings is deprecated.
Wrap literal string/bytes input inio.StringIO/io.BytesIOinstead.matchstr or compiled regular expression, optionalThe set of tables containing text matching this regex or string will be
returned. Unless the HTML is extremely simple you will probably need to
pass a non-empty string here. Defaults to ‘.+’ (match any non-empty
string). The default value will return all tables contained on a page.
This value is converted to a regular expression so that there is
consistent behavior between Beautiful Soup and lxml.flavorstr, optionalThe parsing engine to use. ‘bs4’ and ‘html5lib’ are synonymous with
each other, they are both there for backwards compatibility. The
default ofNonetries to uselxmlto parse and if that fails it
falls back onbs4+html5lib.headerint or list-like, optionalThe row (or list of rows for aMultiIndex) to use to
make the columns headers.index_colint or list-like, optionalThe column (or list of columns) to use to create the index.skiprowsint, list-like or slice, optionalNumber of rows to skip after parsing the column integer. 0-based. If a
sequence of integers or a slice is given, will skip the rows indexed by
that sequence. Note that a single element sequence means ‘skip the nth
row’ whereas an integer means ‘skip n rows’.attrsdict, optionalThis is a dictionary of attributes that you can pass to use to identify
the table in the HTML. These are not checked for validity before being
passed to lxml or Beautiful Soup. However, these attributes must be
valid HTML table attributes to work correctly. For example,attrs={'id':'table'}is a valid attribute dictionary because the ‘id’ HTML tag attribute is
a valid HTML attribute foranyHTML tag as perthis document.attrs={'asdf':'table'}isnota valid attribute dictionary because ‘asdf’ is not a valid
HTML attribute even if it is a valid XML attribute. Valid HTML 4.01
table attributes can be foundhere. A
working draft of the HTML 5 spec can be foundhere. It contains the
latest information on table attributes for the modern web.parse_datesbool, optionalSeeread_csv()for more details.thousandsstr, optionalSeparator to use to parse thousands. Defaults to','.encodingstr, optionalThe encoding used to decode the web page. Defaults toNone.``None``
preserves the previous encoding behavior, which depends on the
underlying parser library (e.g., the parser library will try to use
the encoding provided by the document).decimalstr, default ‘.’Character to recognize as decimal point (e.g. use ‘,’ for European
data).convertersdict, default NoneDict of functions for converting values in certain columns. Keys can
either be integers or column labels, values are functions that take one
input argument, the cell (not column) content, and return the
transformed content.na_valuesiterable, default NoneCustom NA values.keep_default_nabool, default TrueIf na_values are specified and keep_default_na is False the default NaN
values are overridden, otherwise they’re appended to.displayed_onlybool, default TrueWhether elements with “display: none” should be parsed.extract_links{None, “all”, “header”, “body”, “footer”}Table elements in the specified section(s) with <a> tags will have their
href extracted.New in version 1.5.0.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 2.1.0.Returns:dfsA list of DataFrames.See alsoread_csvRead a comma-separated values (csv) file into DataFrame.NotesBefore using this function you should read thegotchas about the
HTML parsing libraries.Expect to do some cleanup after you call this function. For example, you
might need to manually assign column names if the column names are
converted to NaN when you pass theheader=0argument. We try to assume as
little as possible about the structure of the table and push the
idiosyncrasies of the HTML contained in the table to the user.This function searches for<table>elements and only for<tr>and<th>rows and<td>elements within each<tr>or<th>element in the table.<td>stands for “table data”. This function
attempts to properly handlecolspanandrowspanattributes.
If the function has a<thead>argument, it is used to construct
the header, otherwise the function attempts to find the header within
the body (by putting rows with only<th>elements into the header).Similar toread_csv()theheaderargument is appliedafterskiprowsis applied.This function willalwaysreturn a list ofDataFrameorit will fail, e.g., it willnotreturn an empty list.ExamplesSee theread_html documentation in the IO section of the docsfor some examples of reading in HTML tables."
Pandas,Input output,pandas.DataFrame.to_html,"pandas.DataFrame.to_html#DataFrame.to_html(buf=None,columns=None,col_space=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,justify=None,max_rows=None,max_cols=None,show_dimensions=False,decimal='.',bold_rows=True,classes=None,escape=True,notebook=False,border=None,table_id=None,render_links=False,encoding=None)[source]#Render a DataFrame as an HTML table.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default.col_spacestr or int, list or dict of int or str, optionalThe minimum width of each column in CSS length units. An int is assumed to be px units..headerbool, optionalWhether to print column labels, default True.indexbool, optional, default TrueWhether to print index (row) labels.na_repstr, optional, default ‘NaN’String representation ofNaNto use.formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columns’ elements by position or
name.
The result of each function must be a unicode string.
List/tuple must be of length equal to the number of columns.float_formatone-parameter function, optional, default NoneFormatter function to apply to columns’ elements if they are
floats. This function must return a unicode string and will be
applied only to the non-NaNelements, withNaNbeing
handled byna_rep.Changed in version 1.2.0.sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row.index_namesbool, optional, default TruePrints the names of the indexes.justifystr, default NoneHow to justify the column labels. If None uses the option from
the print configuration (controlled by set_option), ‘right’ out
of the box. Valid values areleftrightcenterjustifyjustify-allstartendinheritmatch-parentinitialunset.max_rowsint, optionalMaximum number of rows to display in the console.max_colsint, optionalMaximum number of columns to display in the console.show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns).decimalstr, default ‘.’Character recognized as decimal separator, e.g. ‘,’ in Europe.bold_rowsbool, default TrueMake the row labels bold in the output.classesstr or list or tuple, default NoneCSS class(es) to apply to the resulting html table.escapebool, default TrueConvert the characters <, >, and & to HTML-safe sequences.notebook{True, False}, default FalseWhether the generated HTML is for IPython Notebook.borderintAborder=borderattribute is included in the opening<table>tag. Defaultpd.options.display.html.border.table_idstr, optionalA css id is included in the opening<table>tag if specified.render_linksbool, default FalseConvert URLs to HTML links.encodingstr, default “utf-8”Set character encoding.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns
None.See alsoto_stringConvert DataFrame to a string.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>html_string='''<table border=""1"" class=""dataframe"">...<thead>...<tr style=""text-align: right;"">...<th></th>...<th>col1</th>...<th>col2</th>...</tr>...</thead>...<tbody>...<tr>...<th>0</th>...<td>1</td>...<td>4</td>...</tr>...<tr>...<th>1</th>...<td>2</td>...<td>3</td>...</tr>...</tbody>...</table>'''>>>asserthtml_string==df.to_html()"
Pandas,Input output,pandas.io.formats.style.Styler.to_html,"pandas.io.formats.style.Styler.to_html#Styler.to_html(buf=None,*,table_uuid=None,table_attributes=None,sparse_index=None,sparse_columns=None,bold_headers=False,caption=None,max_rows=None,max_columns=None,encoding=None,doctype_html=False,exclude_styles=False,**kwargs)[source]#Write Styler to a file, buffer or string in HTML-CSS format.New in version 1.3.0.Parameters:bufstr, path object, file-like object, optionalString, path object (implementingos.PathLike[str]), or file-like
object implementing a stringwrite()function. IfNone, the result is
returned as a string.table_uuidstr, optionalId attribute assigned to the <table> HTML element in the format:<tableid=""T_<table_uuid>""..>If not given uses Styler’s initially assigned value.table_attributesstr, optionalAttributes to assign within the<table>HTML element in the format:<table..<table_attributes>>If not given defaults to Styler’s preexisting value.sparse_indexbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False
will display each explicit level element in a hierarchical key for each row.
Defaults topandas.options.styler.sparse.indexvalue.New in version 1.4.0.sparse_columnsbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False
will display each explicit level element in a hierarchical key for each
column. Defaults topandas.options.styler.sparse.columnsvalue.New in version 1.4.0.bold_headersbool, optionalAdds “font-weight: bold;” as a CSS property to table style header cells.New in version 1.4.0.captionstr, optionalSet, or overwrite, the caption on Styler before rendering.New in version 1.4.0.max_rowsint, optionalThe maximum number of rows that will be rendered. Defaults topandas.options.styler.render.max_rows/max_columns.New in version 1.4.0.max_columnsint, optionalThe maximum number of columns that will be rendered. Defaults topandas.options.styler.render.max_columns, which is None.Rows and columns may be reduced if the number of total elements is
large. This value is set topandas.options.styler.render.max_elements,
which is 262144 (18 bit browser rendering).New in version 1.4.0.encodingstr, optionalCharacter encoding setting for file output (and meta tags if available).
Defaults topandas.options.styler.render.encodingvalue of “utf-8”.doctype_htmlbool, default FalseWhether to output a fully structured HTML file including all
HTML elements, or just the core<style>and<table>elements.exclude_stylesbool, default FalseWhether to include the<style>element and all associated elementclassandididentifiers, or solely the<table>element without
styling identifiers.**kwargsAny additional keyword arguments are passed through to the jinja2self.template.renderprocess. This is useful when you need to provide
additional variables for a custom template.Returns:str or NoneIfbufis None, returns the result as a string. Otherwise returnsNone.See alsoDataFrame.to_htmlWrite a DataFrame to a file, buffer or string in HTML format.Examples>>>df=pd.DataFrame({'A':[1,2],'B':[3,4]})>>>print(df.style.to_html())<style type=""text/css""></style><table id=""T_1e78e""><thead><tr><th class=""blank level0"" >&nbsp;</th><th id=""T_1e78e_level0_col0"" class=""col_heading level0 col0"" >A</th><th id=""T_1e78e_level0_col1"" class=""col_heading level0 col1"" >B</th></tr>..."
Pandas,Input output,pandas.read_xml,"pandas.read_xml#pandas.read_xml(path_or_buffer,*,xpath='./*',namespaces=None,elems_only=False,attrs_only=False,names=None,dtype=None,converters=None,parse_dates=None,encoding='utf-8',parser='lxml',stylesheet=None,iterparse=None,compression='infer',storage_options=None,dtype_backend=_NoDefault.no_default)[source]#Read XML document into aDataFrameobject.New in version 1.3.0.Parameters:path_or_bufferstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing aread()function. The string can be any valid XML
string or a path. The string can further be a URL. Valid URL schemes
include http, ftp, s3, and file.Deprecated since version 2.1.0:Passing xml literal strings is deprecated.
Wrap literal xml input inio.StringIOorio.BytesIOinstead.xpathstr, optional, default ‘./*’TheXPathto parse required set of nodes for migration toDataFrame.``XPath`` should return a collection of elements
and not a single element. Note: Theetreeparser supports limitedXPathexpressions. For more complexXPath, uselxmlwhich requires
installation.namespacesdict, optionalThe namespaces defined in XML document as dicts with key being
namespace prefix and value the URI. There is no need to include all
namespaces in XML, only the ones used inxpathexpression.
Note: if XML document uses default namespace denoted asxmlns=’<URI>’without a prefix, you must assign any temporary
namespace prefix such as ‘doc’ to the URI in order to parse
underlying nodes and/or attributes. For example,namespaces={""doc"":""https://example.com""}elems_onlybool, optional, default FalseParse only the child elements at the specifiedxpath. By default,
all child elements and non-empty text nodes are returned.attrs_onlybool, optional, default FalseParse only the attributes at the specifiedxpath.
By default, all attributes are returned.nameslist-like, optionalColumn names for DataFrame of parsed XML data. Use this parameter to
rename original element names and distinguish same named elements and
attributes.dtypeType name or dict of column -> type, optionalData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32,
‘c’: ‘Int64’}
Usestrorobjecttogether with suitablena_valuessettings
to preserve and not interpret dtype.
If converters are specified, they will be applied INSTEAD
of dtype conversion.New in version 1.5.0.convertersdict, optionalDict of functions for converting values in certain columns. Keys can either
be integers or column labels.New in version 1.5.0.parse_datesbool or list of int or names or list of lists or dict, default FalseIdentifiers to parse index or columns to datetime. The behavior is as follows:boolean. If True -> try parsing the index.list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3
each as a separate date column.list of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as
a single date column.dict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call
result ‘foo’New in version 1.5.0.encodingstr, optional, default ‘utf-8’Encoding of XML document.parser{‘lxml’,’etree’}, default ‘lxml’Parser module to use for retrieval of data. Only ‘lxml’ and
‘etree’ are supported. With ‘lxml’ more complexXPathsearches
and ability to use XSLT stylesheet are supported.stylesheetstr, path object or file-like objectA URL, file-like object, or a raw string containing an XSLT script.
This stylesheet should flatten complex, deeply nested XML documents
for easier parsing. To use this feature you must havelxmlmodule
installed and specify ‘lxml’ asparser. Thexpathmust
reference nodes of transformed XML document generated after XSLT
transformation and not the original XML document. Only XSLT 1.0
scripts and not later versions is currently supported.iterparsedict, optionalThe nodes or attributes to retrieve in iterparsing of XML document
as a dict with key being the name of repeating element and value being
list of elements or attribute names that are descendants of the repeated
element. Note: If this option is used, it will replacexpathparsing
and unlikexpath, descendants do not need to relate to each other but can
exist any where in document under the repeating element. This memory-
efficient method should be used for very large XML files (500MB, 1GB, or 5GB+).
For example,iterparse={""row_element"":[""child_elem"",""attr"",""grandchild_elem""]}New in version 1.5.0.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:dfA DataFrame.See alsoread_jsonConvert a JSON string to pandas object.read_htmlRead HTML tables into a list of DataFrame objects.NotesThis method is best designed to import shallow XML documents in
following format which is the ideal fit for the two-dimensions of aDataFrame(row by column).<root><row><column1>data</column1><column2>data</column2><column3>data</column3>...</row><row>...</row>...</root>As a file format, XML documents can be designed any way including
layout of elements and attributes as long as it conforms to W3C
specifications. Therefore, this method is a convenience handler for
a specific flatter design and not all possible XML structures.However, for more complex XML documents,stylesheetallows you to
temporarily redesign original document with XSLT (a special purpose
language) for a flatter version for migration to a DataFrame.This function willalwaysreturn a singleDataFrameor raise
exceptions due to issues with XML document,xpath, or other
parameters.See theread_xml documentation in the IO section of the docsfor more information in using this method to parse XML
files to DataFrames.Examples>>>importio>>>xml='''<?xml version='1.0' encoding='utf-8'?>...<data xmlns=""http://example.com"">...<row>...<shape>square</shape>...<degrees>360</degrees>...<sides>4.0</sides>...</row>...<row>...<shape>circle</shape>...<degrees>360</degrees>...<sides/>...</row>...<row>...<shape>triangle</shape>...<degrees>180</degrees>...<sides>3.0</sides>...</row>...</data>'''>>>df=pd.read_xml(io.StringIO(xml))>>>dfshape  degrees  sides0    square      360    4.01    circle      360    NaN2  triangle      180    3.0>>>xml='''<?xml version='1.0' encoding='utf-8'?>...<data>...<row shape=""square"" degrees=""360"" sides=""4.0""/>...<row shape=""circle"" degrees=""360""/>...<row shape=""triangle"" degrees=""180"" sides=""3.0""/>...</data>'''>>>df=pd.read_xml(io.StringIO(xml),xpath="".//row"")>>>dfshape  degrees  sides0    square      360    4.01    circle      360    NaN2  triangle      180    3.0>>>xml='''<?xml version='1.0' encoding='utf-8'?>...<doc:data xmlns:doc=""https://example.com"">...<doc:row>...<doc:shape>square</doc:shape>...<doc:degrees>360</doc:degrees>...<doc:sides>4.0</doc:sides>...</doc:row>...<doc:row>...<doc:shape>circle</doc:shape>...<doc:degrees>360</doc:degrees>...<doc:sides/>...</doc:row>...<doc:row>...<doc:shape>triangle</doc:shape>...<doc:degrees>180</doc:degrees>...<doc:sides>3.0</doc:sides>...</doc:row>...</doc:data>'''>>>df=pd.read_xml(io.StringIO(xml),...xpath=""//doc:row"",...namespaces={""doc"":""https://example.com""})>>>dfshape  degrees  sides0    square      360    4.01    circle      360    NaN2  triangle      180    3.0"
Pandas,Input output,pandas.DataFrame.to_xml,"pandas.DataFrame.to_xml#DataFrame.to_xml(path_or_buffer=None,index=True,root_name='data',row_name='row',na_rep=None,attr_cols=None,elem_cols=None,namespaces=None,prefix=None,encoding='utf-8',xml_declaration=True,pretty_print=True,parser='lxml',stylesheet=None,compression='infer',storage_options=None)[source]#Render a DataFrame to an XML document.New in version 1.3.0.Parameters:path_or_bufferstr, path object, file-like object, or None, default NoneString, path object (implementingos.PathLike[str]), or file-like
object implementing awrite()function. If None, the result is returned
as a string.indexbool, default TrueWhether to include index in XML document.root_namestr, default ‘data’The name of root element in XML document.row_namestr, default ‘row’The name of row element in XML document.na_repstr, optionalMissing data representation.attr_colslist-like, optionalList of columns to write as attributes in row element.
Hierarchical columns will be flattened with underscore
delimiting the different levels.elem_colslist-like, optionalList of columns to write as children in row element. By default,
all columns output as children of row element. Hierarchical
columns will be flattened with underscore delimiting the
different levels.namespacesdict, optionalAll namespaces to be defined in root element. Keys of dict
should be prefix names and values of dict corresponding URIs.
Default namespaces should be given empty string key. For
example,namespaces={"""":""https://example.com""}prefixstr, optionalNamespace prefix to be used for every element and/or attribute
in document. This should be one of the keys innamespacesdict.encodingstr, default ‘utf-8’Encoding of the resulting document.xml_declarationbool, default TrueWhether to include the XML declaration at start of document.pretty_printbool, default TrueWhether output should be pretty printed with indentation and
line breaks.parser{‘lxml’,’etree’}, default ‘lxml’Parser module to use for building of tree. Only ‘lxml’ and
‘etree’ are supported. With ‘lxml’, the ability to use XSLT
stylesheet is supported.stylesheetstr, path object or file-like object, optionalA URL, file-like object, or a raw string containing an XSLT
script used to transform the raw XML output. Script should use
layout of elements and attributes from original output. This
argument requireslxmlto be installed. Only XSLT 1.0
scripts and not later versions is currently supported.compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.Returns:None or strIfiois None, returns the resulting XML format as a
string. Otherwise returns None.See alsoto_jsonConvert the pandas object to a JSON string.to_htmlConvert DataFrame to a html.Examples>>>df=pd.DataFrame({'shape':['square','circle','triangle'],...'degrees':[360,360,180],...'sides':[4,np.nan,3]})>>>df.to_xml()<?xml version='1.0' encoding='utf-8'?><data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data>>>>df.to_xml(attr_cols=[...'index','shape','degrees','sides'...])<?xml version='1.0' encoding='utf-8'?><data><row index=""0"" shape=""square"" degrees=""360"" sides=""4.0""/><row index=""1"" shape=""circle"" degrees=""360""/><row index=""2"" shape=""triangle"" degrees=""180"" sides=""3.0""/></data>>>>df.to_xml(namespaces={""doc"":""https://example.com""},...prefix=""doc"")<?xml version='1.0' encoding='utf-8'?><doc:data xmlns:doc=""https://example.com""><doc:row><doc:index>0</doc:index><doc:shape>square</doc:shape><doc:degrees>360</doc:degrees><doc:sides>4.0</doc:sides></doc:row><doc:row><doc:index>1</doc:index><doc:shape>circle</doc:shape><doc:degrees>360</doc:degrees><doc:sides/></doc:row><doc:row><doc:index>2</doc:index><doc:shape>triangle</doc:shape><doc:degrees>180</doc:degrees><doc:sides>3.0</doc:sides></doc:row></doc:data>"
Pandas,Input output,pandas.DataFrame.to_latex,"pandas.DataFrame.to_latex#DataFrame.to_latex(buf=None,columns=None,header=True,index=True,na_rep='NaN',formatters=None,float_format=None,sparsify=None,index_names=True,bold_rows=False,column_format=None,longtable=None,escape=None,encoding=None,decimal='.',multicolumn=None,multicolumn_format=None,multirow=None,caption=None,label=None,position=None)[source]#Render object to a LaTeX tabular, longtable, or nested table.Requires\usepackage{{booktabs}}. The output can be copy/pasted
into a main LaTeX document or read from an external file
with\input{{table.tex}}.Changed in version 1.2.0:Added position argument, changed meaning of caption argument.Changed in version 2.0.0:Refactored to use the Styler implementation via jinja2 templating.Parameters:bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string.columnslist of label, optionalThe subset of columns to write. Writes all columns by default.headerbool or list of str, default TrueWrite out the column names. If a list of strings is given,
it is assumed to be aliases for the column names.indexbool, default TrueWrite row names (index).na_repstr, default ‘NaN’Missing data representation.formatterslist of functions or dict of {{str: function}}, optionalFormatter functions to apply to columns’ elements by position or
name. The result of each function must be a unicode string.
List must be of length equal to the number of columns.float_formatone-parameter function or str, optional, default NoneFormatter for floating point numbers. For examplefloat_format=""%.2f""andfloat_format=""{{:0.2f}}"".formatwill
both result in 0.1234 being formatted as 0.12.sparsifybool, optionalSet to False for a DataFrame with a hierarchical index to print
every multiindex key at each row. By default, the value will be
read from the config module.index_namesbool, default TruePrints the names of the indexes.bold_rowsbool, default FalseMake the row labels bold in the output.column_formatstr, optionalThe columns format as specified inLaTeX table formate.g. ‘rcl’ for 3
columns. By default, ‘l’ will be used for all columns except
columns of numbers, which default to ‘r’.longtablebool, optionalUse a longtable environment instead of tabular. Requires
adding a usepackage{{longtable}} to your LaTeX preamble.
By default, the value will be read from the pandas config
module, and set toTrueif the optionstyler.latex.environmentis“longtable”.Changed in version 2.0.0:The pandas option affecting this argument has changed.escapebool, optionalBy default, the value will be read from the pandas config
module and set toTrueif the optionstyler.format.escapeis“latex”. When set to False prevents from escaping latex special
characters in column names.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value toFalse.encodingstr, optionalA string representing the encoding to use in the output file,
defaults to ‘utf-8’.decimalstr, default ‘.’Character recognized as decimal separator, e.g. ‘,’ in Europe.multicolumnbool, default TrueUse multicolumn to enhance MultiIndex columns.
The default will be read from the config module, and is set
as the optionstyler.sparse.columns.Changed in version 2.0.0:The pandas option affecting this argument has changed.multicolumn_formatstr, default ‘r’The alignment for multicolumns, similar tocolumn_formatThe default will be read from the config module, and is set as the optionstyler.latex.multicol_align.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value to “r”.multirowbool, default TrueUse multirow to enhance MultiIndex rows. Requires adding a
usepackage{{multirow}} to your LaTeX preamble. Will print
centered labels (instead of top-aligned) across the contained
rows, separating groups via clines. The default will be read
from the pandas config module, and is set as the optionstyler.sparse.index.Changed in version 2.0.0:The pandas option affecting this argument has changed, as has the
default value toTrue.captionstr or tuple, optionalTuple (full_caption, short_caption),
which results in\caption[short_caption]{{full_caption}};
if a single string is passed, no short caption will be set.Changed in version 1.2.0:Optionally allow caption to be a tuple(full_caption,short_caption).labelstr, optionalThe LaTeX label to be placed inside\label{{}}in the output.
This is used with\ref{{}}in the main.texfile.positionstr, optionalThe LaTeX positional argument for tables, to be placed after\begin{{}}in the output.New in version 1.2.0.Returns:str or NoneIf buf is None, returns the result as a string. Otherwise returns None.See alsoio.formats.style.Styler.to_latexRender a DataFrame to LaTeX with conditional formatting.DataFrame.to_stringRender a DataFrame to a console-friendly tabular output.DataFrame.to_htmlRender a DataFrame as an HTML table.NotesAs of v2.0.0 this method has changed to use the Styler implementation as
part ofStyler.to_latex()viajinja2templating. This means
thatjinja2is a requirement, and needs to be installed, for this method
to function. It is advised that users switch to using Styler, since that
implementation is more frequently updated and contains much more
flexibility with the output.ExamplesConvert a general DataFrame to LaTeX with formatting:>>>df=pd.DataFrame(dict(name=['Raphael','Donatello'],...age=[26,45],...height=[181.23,177.65]))>>>print(df.to_latex(index=False,...formatters={""name"":str.upper},...float_format=""{:.1f}"".format,...))\begin{tabular}{lrr}\toprulename & age & height \\\midruleRAPHAEL & 26 & 181.2 \\DONATELLO & 45 & 177.7 \\\bottomrule\end{tabular}"
Pandas,Input output,pandas.io.formats.style.Styler.to_latex,"pandas.io.formats.style.Styler.to_latex#Styler.to_latex(buf=None,*,column_format=None,position=None,position_float=None,hrules=None,clines=None,label=None,caption=None,sparse_index=None,sparse_columns=None,multirow_align=None,multicol_align=None,siunitx=False,environment=None,encoding=None,convert_css=False)[source]#Write Styler to a file, buffer or string in LaTeX format.New in version 1.3.0.Parameters:bufstr, path object, file-like object, or None, default NoneString, path object (implementingos.PathLike[str]), or file-like
object implementing a stringwrite()function. If None, the result is
returned as a string.column_formatstr, optionalThe LaTeX column specification placed in location:\begin{tabular}{<column_format>}Defaults to ‘l’ for index and
non-numeric data columns, and, for numeric data columns,
to ‘r’ by default, or ‘S’ ifsiunitxisTrue.positionstr, optionalThe LaTeX positional argument (e.g. ‘h!’) for tables, placed in location:\\begin{table}[<position>].position_float{“centering”, “raggedleft”, “raggedright”}, optionalThe LaTeX float command placed in location:\begin{table}[<position>]\<position_float>Cannot be used ifenvironmentis “longtable”.hrulesboolSet toTrueto add \toprule, \midrule and \bottomrule from the
{booktabs} LaTeX package.
Defaults topandas.options.styler.latex.hrules, which isFalse.Changed in version 1.4.0.clinesstr, optionalUse to control adding \cline commands for the index labels separation.
Possible values are:None: no cline commands are added (default).“all;data”: a cline is added for every index value extending the
width of the table, including data entries.“all;index”: as above with lines extending only the width of the
index entries.“skip-last;data”: a cline is added for each index value except the
last level (which is never sparsified), extending the widtn of the
table.“skip-last;index”: as above with lines extending only the width of the
index entries.New in version 1.4.0.labelstr, optionalThe LaTeX label included as: \label{<label>}.
This is used with \ref{<label>} in the main .tex file.captionstr, tuple, optionalIf string, the LaTeX table caption included as: \caption{<caption>}.
If tuple, i.e (“full caption”, “short caption”), the caption included
as: \caption[<caption[1]>]{<caption[0]>}.sparse_indexbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False
will display each explicit level element in a hierarchical key for each row.
Defaults topandas.options.styler.sparse.index, which isTrue.sparse_columnsbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False
will display each explicit level element in a hierarchical key for each
column. Defaults topandas.options.styler.sparse.columns, which
isTrue.multirow_align{“c”, “t”, “b”, “naive”}, optionalIf sparsifying hierarchical MultiIndexes whether to align text centrally,
at the top or bottom using the multirow package. If not given defaults topandas.options.styler.latex.multirow_align, which is“c”.
If “naive” is given renders without multirow.Changed in version 1.4.0.multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}, optionalIf sparsifying hierarchical MultiIndex columns whether to align text at
the left, centrally, or at the right. If not given defaults topandas.options.styler.latex.multicol_align, which is “r”.
If a naive option is given renders without multicol.
Pipe decorators can also be added to non-naive values to draw vertical
rules, e.g. “|r” will draw a rule on the left side of right aligned merged
cells.Changed in version 1.4.0.siunitxbool, default FalseSet toTrueto structure LaTeX compatible with the {siunitx} package.environmentstr, optionalIf given, the environment that will replace ‘table’ in\\begin{table}.
If ‘longtable’ is specified then a more suitable template is
rendered. If not given defaults topandas.options.styler.latex.environment, which isNone.New in version 1.4.0.encodingstr, optionalCharacter encoding setting. Defaults
topandas.options.styler.render.encoding, which is “utf-8”.convert_cssbool, default FalseConvert simple cell-styles from CSS to LaTeX format. Any CSS not found in
conversion table is dropped. A style can be forced by adding option–latex. See notes.Returns:str or NoneIfbufis None, returns the result as a string. Otherwise returnsNone.See alsoStyler.formatFormat the text display value of cells.NotesLatex PackagesFor the following features we recommend the following LaTeX inclusions:FeatureInclusionsparse columnsnone: included within default {tabular} environmentsparse rows\usepackage{multirow}hrules\usepackage{booktabs}colors\usepackage[table]{xcolor}siunitx\usepackage{siunitx}bold (with siunitx)\usepackage{etoolbox}\robustify\bfseries\sisetup{detect-all = true}(within {document})italic (with siunitx)\usepackage{etoolbox}\robustify\itshape\sisetup{detect-all = true}(within {document})environment\usepackage{longtable} if arg is “longtable”
| or any other relevant environment packagehyperlinks\usepackage{hyperref}Cell StylesLaTeX styling can only be rendered if the accompanying styling functions have
been constructed with appropriate LaTeX commands. All styling
functionality is built around the concept of a CSS(<attribute>,<value>)pair (seeTable Visualization), and this
should be replaced by a LaTeX(<command>,<options>)approach. Each cell will be styled individually
using nested LaTeX commands with their accompanied options.For example the following code will highlight and bold a cell in HTML-CSS:>>>df=pd.DataFrame([[1,2],[3,4]])>>>s=df.style.highlight_max(axis=None,...props='background-color:red; font-weight:bold;')>>>s.to_html()The equivalent using LaTeX only commands is the following:>>>s=df.style.highlight_max(axis=None,...props='cellcolor:{red}; bfseries: ;')>>>s.to_latex()Internally these structured LaTeX(<command>,<options>)pairs
are translated to thedisplay_valuewith the default structure:\<command><options><display_value>.
Where there are multiple commands the latter is nested recursively, so that
the above example highlighted cell is rendered as\cellcolor{red}\bfseries4.Occasionally this format does not suit the applied command, or
combination of LaTeX packages that is in use, so additional flags can be
added to the<options>, within the tuple, to result in different
positions of required braces (thedefaultbeing the same as--nowrap):Tuple FormatOutput Structure(<command>,<options>)\<command><options> <display_value>(<command>,<options>--nowrap)\<command><options> <display_value>(<command>,<options>--rwrap)\<command><options>{<display_value>}(<command>,<options>--wrap){\<command><options> <display_value>}(<command>,<options>--lwrap){\<command><options>} <display_value>(<command>,<options>--dwrap){\<command><options>}{<display_value>}For example thetextbfcommand for font-weight
should always be used with–rwrapso('textbf','--rwrap')will render a
working cell, wrapped with braces, as\textbf{<display_value>}.A more comprehensive example is as follows:>>>df=pd.DataFrame([[1,2.2,""dogs""],[3,4.4,""cats""],[2,6.6,""cows""]],...index=[""ix1"",""ix2"",""ix3""],...columns=[""Integers"",""Floats"",""Strings""])>>>s=df.style.highlight_max(...props='cellcolor:[HTML]{FFFF00}; color:{red};'...'textit:--rwrap; textbf:--rwrap;'...)>>>s.to_latex()Table StylesInternally Styler uses itstable_stylesobject to parse thecolumn_format,position,position_float, andlabelinput arguments. These arguments are added to table styles in the format:set_table_styles([{""selector"":""column_format"",""props"":f"":{column_format};""},{""selector"":""position"",""props"":f"":{position};""},{""selector"":""position_float"",""props"":f"":{position_float};""},{""selector"":""label"",""props"":f"":{{{label.replace(':','§')}}};""}],overwrite=False)Exception is made for thehrulesargument which, in fact, controls all three
commands:toprule,bottomruleandmidrulesimultaneously. Instead of
settinghrulestoTrue, it is also possible to set each
individual rule definition, by manually setting thetable_styles,
for example below we set a regulartoprule, set anhlineforbottomruleand exclude themidrule:set_table_styles([{'selector':'toprule','props':':toprule;'},{'selector':'bottomrule','props':':hline;'},],overwrite=False)If othercommandsare added to table styles they will be detected, and
positioned immediately above the ‘\begin{tabular}’ command. For example to
add odd and even row coloring, from the {colortbl} package, in format\rowcolors{1}{pink}{red}, use:set_table_styles([{'selector':'rowcolors','props':':{1}{pink}{red};'}],overwrite=False)A more comprehensive example using these arguments is as follows:>>>df.columns=pd.MultiIndex.from_tuples([...(""Numeric"",""Integers""),...(""Numeric"",""Floats""),...(""Non-Numeric"",""Strings"")...])>>>df.index=pd.MultiIndex.from_tuples([...(""L0"",""ix1""),(""L0"",""ix2""),(""L1"",""ix3"")...])>>>s=df.style.highlight_max(...props='cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;'...)>>>s.to_latex(...column_format=""rrrrr"",position=""h"",position_float=""centering"",...hrules=True,label=""table:5"",caption=""Styled LaTeX Table"",...multirow_align=""t"",multicol_align=""r""...)FormattingTo format valuesStyler.format()should be used prior to callingStyler.to_latex, as well as other methods such asStyler.hide()for example:>>>s.clear()>>>s.table_styles=[]>>>s.caption=None>>>s.format({...(""Numeric"",""Integers""):'\${}',...(""Numeric"",""Floats""):'{:.3f}',...(""Non-Numeric"",""Strings""):str.upper...})Numeric      Non-NumericIntegers   Floats    StringsL0    ix1       $1   2.200      DOGSix2       $3   4.400      CATSL1    ix3       $2   6.600      COWS>>>s.to_latex()\begin{tabular}{llrrl}{} & {} & \multicolumn{2}{r}{Numeric} & {Non-Numeric} \\{} & {} & {Integers} & {Floats} & {Strings} \\\multirow[c]{2}{*}{L0} & ix1 & \\$1 & 2.200 & DOGS \\& ix2 & \$3 & 4.400 & CATS \\L1 & ix3 & \$2 & 6.600 & COWS \\\end{tabular}CSS ConversionThis method can convert a Styler constructured with HTML-CSS to LaTeX using
the following limited conversions.CSS AttributeCSS valueLaTeX CommandLaTeX Optionsfont-weightboldbolderbfseriesbfseriesfont-styleitalicobliqueitshapeslshapebackground-colorred#fe01ea#f0ergb(128,255,0)rgba(128,0,0,0.5)rgb(25%,255,50%)cellcolor{red}–lwrap[HTML]{FE01EA}–lwrap[HTML]{FF00EE}–lwrap[rgb]{0.5,1,0}–lwrap[rgb]{0.5,0,0}–lwrap[rgb]{0.25,1,0.5}–lwrapcolorred#fe01ea#f0ergb(128,255,0)rgba(128,0,0,0.5)rgb(25%,255,50%)color{red}[HTML]{FE01EA}[HTML]{FF00EE}[rgb]{0.5,1,0}[rgb]{0.5,0,0}[rgb]{0.25,1,0.5}It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler
using the--latexflag, and to add LaTeX parsing options that the
converter will detect within a CSS-comment.>>>df=pd.DataFrame([[1]])>>>df.style.set_properties(...**{""font-weight"":""bold /* --dwrap */"",""Huge"":""--latex--rwrap""}...).to_latex(convert_css=True)\begin{tabular}{lr}{} & {0} \\0 & {\bfseries}{\Huge{1}} \\\end{tabular}ExamplesBelow we give a complete step by step example adding some advanced features
and noting some common gotchas.First we create the DataFrame and Styler as usual, including MultiIndex rows
and columns, which allow for more advanced formatting options:>>>cidx=pd.MultiIndex.from_arrays([...[""Equity"",""Equity"",""Equity"",""Equity"",...""Stats"",""Stats"",""Stats"",""Stats"",""Rating""],...[""Energy"",""Energy"",""Consumer"",""Consumer"","""","""","""","""",""""],...[""BP"",""Shell"",""H&M"",""Unilever"",...""Std Dev"",""Variance"",""52w High"",""52w Low"",""""]...])>>>iidx=pd.MultiIndex.from_arrays([...[""Equity"",""Equity"",""Equity"",""Equity""],...[""Energy"",""Energy"",""Consumer"",""Consumer""],...[""BP"",""Shell"",""H&M"",""Unilever""]...])>>>styler=pd.DataFrame([...[1,0.8,0.66,0.72,32.1678,32.1678**2,335.12,240.89,""Buy""],...[0.8,1.0,0.69,0.79,1.876,1.876**2,14.12,19.78,""Hold""],...[0.66,0.69,1.0,0.86,7,7**2,210.9,140.6,""Buy""],...[0.72,0.79,0.86,1.0,213.76,213.76**2,2807,3678,""Sell""],...],columns=cidx,index=iidx).styleSecond we will format the display and, since our table is quite wide, will
hide the repeated level-0 of the index:>>>(styler.format(subset=""Equity"",precision=2)....format(subset=""Stats"",precision=1,thousands="","")....format(subset=""Rating"",formatter=str.upper)....format_index(escape=""latex"",axis=1)....format_index(escape=""latex"",axis=0)....hide(level=0,axis=0))Note that one of the string entries of the index and column headers is “H&M”.
Without applying theescape=”latex”option to theformat_indexmethod the
resultant LaTeX will fail to render, and the error returned is quite
difficult to debug. Using the appropriate escape the “&” is converted to “\&”.Thirdly we will apply some (CSS-HTML) styles to our object. We will use a
builtin method and also define our own method to highlight the stock
recommendation:>>>defrating_color(v):...ifv==""Buy"":color=""#33ff85""...elifv==""Sell"":color=""#ff5933""...else:color=""#ffdd33""...returnf""color:{color}; font-weight: bold;"">>>(styler.background_gradient(cmap=""inferno"",subset=""Equity"",vmin=0,vmax=1)....map(rating_color,subset=""Rating""))All the above styles will work with HTML (see below) and LaTeX upon conversion:However, we finally want to add one LaTeX only style
(from the {graphicx} package), that is not easy to convert from CSS and
pandas does not support it. Notice the–latexflag used here,
as well as–rwrapto ensure this is formatted correctly and
not ignored upon conversion.>>>styler.map_index(...lambdav:""rotatebox:{45}--rwrap--latex;"",level=2,axis=1...)Finally we render our LaTeX adding in other options as required:>>>styler.to_latex(...caption=""Selected stock correlation and simple statistics."",...clines=""skip-last;data"",...convert_css=True,...position_float=""centering"",...multicol_align=""|c|"",...hrules=True,...)\begin{table}\centering\caption{Selected stock correlation and simple statistics.}\begin{tabular}{llrrrrrrrrl}\toprule&  & \multicolumn{4}{|c|}{Equity} & \multicolumn{4}{|c|}{Stats} & Rating \\&  & \multicolumn{2}{|c|}{Energy} & \multicolumn{2}{|c|}{Consumer} &\multicolumn{4}{|c|}{} &  \\&  & \rotatebox{45}{BP} & \rotatebox{45}{Shell} & \rotatebox{45}{H\&M} &\rotatebox{45}{Unilever} & \rotatebox{45}{Std Dev} & \rotatebox{45}{Variance} &\rotatebox{45}{52w High} & \rotatebox{45}{52w Low} & \rotatebox{45}{} \\\midrule\multirow[c]{2}{*}{Energy} & BP & {\cellcolor[HTML]{FCFFA4}}\color[HTML]{000000} 1.00 & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000}0.80 & {\cellcolor[HTML]{EB6628}} \color[HTML]{F1F1F1} 0.66 &{\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 & 32.2 & 1,034.8 & 335.1& 240.9 & \color[HTML]{33FF85} \bfseries BUY \\& Shell & {\cellcolor[HTML]{FCA50A}} \color[HTML]{000000} 0.80 &{\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &{\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1} 0.69 &{\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 & 1.9 & 3.5 & 14.1 &19.8 & \color[HTML]{FFDD33} \bfseries HOLD \\\cline{1-11}\multirow[c]{2}{*}{Consumer} & H\&M & {\cellcolor[HTML]{EB6628}}\color[HTML]{F1F1F1} 0.66 & {\cellcolor[HTML]{F1731D}} \color[HTML]{F1F1F1}0.69 & {\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 &{\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 & 7.0 & 49.0 & 210.9 &140.6 & \color[HTML]{33FF85} \bfseries BUY \\& Unilever & {\cellcolor[HTML]{F68013}} \color[HTML]{F1F1F1} 0.72 &{\cellcolor[HTML]{FCA108}} \color[HTML]{000000} 0.79 &{\cellcolor[HTML]{FAC42A}} \color[HTML]{000000} 0.86 &{\cellcolor[HTML]{FCFFA4}} \color[HTML]{000000} 1.00 & 213.8 & 45,693.3 &2,807.0 & 3,678.0 & \color[HTML]{FF5933} \bfseries SELL \\\cline{1-11}\bottomrule\end{tabular}\end{table}"
Pandas,Input output,pandas.read_hdf,"pandas.read_hdf#pandas.read_hdf(path_or_buf,key=None,mode='r',errors='strict',where=None,start=None,stop=None,columns=None,iterator=False,chunksize=None,**kwargs)[source]#Read from the store, close it if we opened it.Retrieve pandas object stored in file, optionally based on where
criteria.WarningPandas uses PyTables for reading and writing HDF5 files, which allows
serializing object-dtype data with pickle when using the “fixed” format.
Loading pickled data received from untrusted sources can be unsafe.See:https://docs.python.org/3/library/pickle.htmlfor more.Parameters:path_or_bufstr, path object, pandas.HDFStoreAny valid string path is acceptable. Only supports the local file system,
remote URLs and file-like objects are not supported.If you want to pass in a path object, pandas accepts anyos.PathLike.Alternatively, pandas accepts an openpandas.HDFStoreobject.keyobject, optionalThe group identifier in the store. Can be omitted if the HDF file
contains a single pandas object.mode{‘r’, ‘r+’, ‘a’}, default ‘r’Mode to use when opening the file. Ignored if path_or_buf is apandas.HDFStore. Default is ‘r’.errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled.
See the errors argument foropen()for a full list
of options.wherelist, optionalA list of Term (or convertible) objects.startint, optionalRow number to start selection.stopint, optionalRow number to stop selection.columnslist, optionalA list of columns names to return.iteratorbool, optionalReturn an iterator object.chunksizeint, optionalNumber of rows to include in an iteration when using an iterator.**kwargsAdditional keyword arguments passed to HDFStore.Returns:objectThe selected object. Return type depends on the object stored.See alsoDataFrame.to_hdfWrite a HDF file from a DataFrame.HDFStoreLow-level access to HDF files.Examples>>>df=pd.DataFrame([[1,1.0,'a']],columns=['x','y','z'])>>>df.to_hdf('./store.h5','data')>>>reread=pd.read_hdf('./store.h5')"
Pandas,Input output,pandas.HDFStore.put,"pandas.HDFStore.put#HDFStore.put(key,value,format=None,index=True,append=False,complib=None,complevel=None,min_itemsize=None,nan_rep=None,data_columns=None,encoding=None,errors='strict',track_times=True,dropna=False)[source]#Store object in HDFStore.Parameters:keystrvalue{Series, DataFrame}format‘fixed(f)|table(t)’, default is ‘fixed’Format to use when storing object in HDFStore. Value can be one of:'fixed'Fixed format. Fast writing/reading. Not-appendable, nor searchable.'table'Table format. Write as a PyTables Table structure which may perform
worse but allow more flexible operations like searching / selecting
subsets of the data.indexbool, default TrueWrite DataFrame index as a column.appendbool, default FalseThis will force Table format, append the input data to the existing.data_columnslist of columns or True, default NoneList of columns to create as data columns, or True to use all columns.
Seehere.encodingstr, default NoneProvide an encoding for strings.track_timesbool, default TrueParameter is propagated to ‘create_table’ method of ‘PyTables’.
If set to False it enables to have the same h5 files (same hashes)
independent on creation time.dropnabool, default False, optionalRemove missing values.Examples>>>df=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df)"
Pandas,Input output,pandas.HDFStore.append,"pandas.HDFStore.append#HDFStore.append(key,value,format=None,axes=None,index=True,append=True,complib=None,complevel=None,columns=None,min_itemsize=None,nan_rep=None,chunksize=None,expectedrows=None,dropna=None,data_columns=None,encoding=None,errors='strict')[source]#Append to Table in file.Node must already exist and be Table format.Parameters:keystrvalue{Series, DataFrame}format‘table’ is the defaultFormat to use when storing object in HDFStore. Value can be one of:'table'Table format. Write as a PyTables Table structure which may perform
worse but allow more flexible operations like searching / selecting
subsets of the data.indexbool, default TrueWrite DataFrame index as a column.appendbool, default TrueAppend the input data to the existing.data_columnslist of columns, or True, default NoneList of columns to create as indexed data columns for on-disk
queries, or True to use all columns. By default only the axes
of the object are indexed. Seehere.min_itemsizedict of columns that specify minimum str sizesnan_repstr to use as str nan representationchunksizesize to chunk the writingexpectedrowsexpected TOTAL row size of this tableencodingdefault None, provide an encoding for strdropnabool, default False, optionalDo not write an ALL nan row to the store settable
by the option ‘io.hdf.dropna_table’.NotesDoesnotcheck if data being appended overlaps with existing
data in the table, so be carefulExamples>>>df1=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df1,format='table')>>>df2=pd.DataFrame([[5,6],[7,8]],columns=['A','B'])>>>store.append('data',df2)>>>store.close()A  B0  1  21  3  40  5  61  7  8"
Pandas,Input output,pandas.HDFStore.get,"pandas.HDFStore.get#HDFStore.get(key)[source]#Retrieve pandas object stored in file.Parameters:keystrReturns:objectSame type as object stored in file.Examples>>>df=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df)>>>store.get('data')>>>store.close()"
Pandas,Input output,pandas.HDFStore.select,"pandas.HDFStore.select#HDFStore.select(key,where=None,start=None,stop=None,columns=None,iterator=False,chunksize=None,auto_close=False)[source]#Retrieve pandas object stored in file, optionally based on where criteria.WarningPandas uses PyTables for reading and writing HDF5 files, which allows
serializing object-dtype data with pickle when using the “fixed” format.
Loading pickled data received from untrusted sources can be unsafe.See:https://docs.python.org/3/library/pickle.htmlfor more.Parameters:keystrObject being retrieved from file.wherelist or NoneList of Term (or convertible) objects, optional.startint or NoneRow number to start selection.stopint, default NoneRow number to stop selection.columnslist or NoneA list of columns that if not None, will limit the return columns.iteratorbool or FalseReturns an iterator.chunksizeint or NoneNumber or rows to include in iteration, return an iterator.auto_closebool or FalseShould automatically close the store when finished.Returns:objectRetrieved object from file.Examples>>>df=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df)>>>store.get('data')>>>print(store.keys())['/data1', '/data2']>>>store.select('/data1')A  B0  1  21  3  4>>>store.select('/data1',where='columns == A')A0  11  3>>>store.close()"
Pandas,Input output,pandas.HDFStore.info,"pandas.HDFStore.info#HDFStore.info()[source]#Print detailed information on the store.Returns:strExamples>>>df=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df)>>>print(store.info())>>>store.close()<class 'pandas.io.pytables.HDFStore'>File path: store.h5/data    frame    (shape->[2,2])"
Pandas,Input output,pandas.HDFStore.keys,"pandas.HDFStore.keys#HDFStore.keys(include='pandas')[source]#Return a list of keys corresponding to objects stored in HDFStore.Parameters:includestr, default ‘pandas’When kind equals ‘pandas’ return pandas objects.
When kind equals ‘native’ return native HDF5 Table objects.Returns:listList of ABSOLUTE path-names (e.g. have the leading ‘/’).Raises:raises ValueError if kind has an illegal valueExamples>>>df=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df)>>>store.get('data')>>>print(store.keys())['/data1', '/data2']>>>store.close()"
Pandas,Input output,pandas.HDFStore.groups,"pandas.HDFStore.groups#HDFStore.groups()[source]#Return a list of all the top-level nodes.Each node returned is not a pandas storage object.Returns:listList of objects.Examples>>>df=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df)>>>print(store.groups())>>>store.close()[/data (Group) ''children := ['axis0' (Array), 'axis1' (Array), 'block0_values' (Array),'block0_items' (Array)]]"
Pandas,Input output,pandas.HDFStore.walk,"pandas.HDFStore.walk#HDFStore.walk(where='/')[source]#Walk the pytables group hierarchy for pandas objects.This generator will yield the group path, subgroups and pandas object
names for each group.Any non-pandas PyTables objects that are not a group will be ignored.Thewheregroup itself is listed first (preorder), then each of its
child groups (following an alphanumerical order) is also traversed,
following the same procedure.Parameters:wherestr, default “/”Group where to start walking.Yields:pathstrFull path to a group (without trailing ‘/’).groupslistNames (strings) of the groups contained inpath.leaveslistNames (strings) of the pandas objects contained inpath.Examples>>>df1=pd.DataFrame([[1,2],[3,4]],columns=['A','B'])>>>store=pd.HDFStore(""store.h5"",'w')>>>store.put('data',df1,format='table')>>>df2=pd.DataFrame([[5,6],[7,8]],columns=['A','B'])>>>store.append('data',df2)>>>store.close()>>>forgroupinstore.walk():...print(group)>>>store.close()"
Pandas,Input output,pandas.read_feather,"pandas.read_feather#pandas.read_feather(path,columns=None,use_threads=True,storage_options=None,dtype_backend=_NoDefault.no_default)[source]#Load a feather-format object from the file path.Parameters:pathstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binaryread()function. The string could be a URL.
Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.feather.columnssequence, default NoneIf not provided, all columns are read.use_threadsbool, default TrueWhether to parallelize reading using multiple threads.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:type of object stored in fileExamples>>>df=pd.read_feather(""path/to/file.feather"")"
Pandas,Input output,pandas.DataFrame.to_feather,"pandas.DataFrame.to_feather#DataFrame.to_feather(path,**kwargs)[source]#Write a DataFrame to the binary Feather format.Parameters:pathstr, path object, file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. If a string or a path,
it will be used as Root Directory path when writing a partitioned dataset.**kwargsAdditional keywords passed topyarrow.feather.write_feather().
This includes thecompression,compression_level,chunksizeandversionkeywords.NotesThis function writes the dataframe as afeather file. Requires a default
index. For saving the DataFrame with your custom index use a method that
supports custom indices e.g.to_parquet.Examples>>>df=pd.DataFrame([[1,2,3],[4,5,6]])>>>df.to_feather(""file.feather"")"
Pandas,Input output,pandas.read_parquet,"pandas.read_parquet#pandas.read_parquet(path,engine='auto',columns=None,storage_options=None,use_nullable_dtypes=_NoDefault.no_default,dtype_backend=_NoDefault.no_default,filesystem=None,filters=None,**kwargs)[source]#Load a parquet object from the file path, returning a DataFrame.Parameters:pathstr, path object or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binaryread()function.
The string could be a URL. Valid URL schemes include http, ftp, s3,
gs, and file. For file URLs, a host is expected. A local file could be:file://localhost/path/to/table.parquet.
A file URL can also be a path to a directory that contains multiple
partitioned parquet files. Both pyarrow and fastparquet support
paths to directories as well as file URLs. A directory path could be:file://localhost/path/to/tablesors3://bucket/partition_dir.engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’Parquet library to use. If ‘auto’, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if
‘pyarrow’ is unavailable.When using the'pyarrow'engine and no storage options are provided
and a filesystem is implemented by bothpyarrow.fsandfsspec(e.g. “s3://”), then thepyarrow.fsfilesystem is attempted first.
Use the filesystem keyword with an instantiated fsspec filesystem
if you wish to use its implementation.columnslist, default=NoneIf not None, only these columns will be read from the file.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.3.0.use_nullable_dtypesbool, default FalseIf True, use dtypes that usepd.NAas missing value indicator
for the resulting DataFrame. (only applicable for thepyarrowengine)
As new dtypes are added that supportpd.NAin the future, the
output with this option will change to use those dtypes.
Note: this is an experimental option, and behaviour (e.g. additional
support dtypes) may change without notice.Deprecated since version 2.0.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file. Only implemented
forengine=""pyarrow"".New in version 2.1.0.filtersList[Tuple] or List[List[Tuple]], default NoneTo filter out data.
Filter syntax: [[(column, op, val), …],…]
where op is [==, =, >, >=, <, <=, !=, in, not in]
The innermost tuples are transposed into a set of filters applied
through anANDoperation.
The outer list combines these sets of filters through anORoperation.
A single list of tuples can also be used, meaning that noORoperation between set of filters is to be conducted.Using this argument will NOT result in row-wise filtering of the final
partitions unlessengine=""pyarrow""is also specified. For
other engines, filtering is only performed at the partition level, that is,
to prevent the loading of some row-groups and/or files.New in version 2.1.0.**kwargsAny additional kwargs are passed to the engine.Returns:DataFrameSee alsoDataFrame.to_parquetCreate a parquet object that serializes a DataFrame.Examples>>>original_df=pd.DataFrame(...{""foo"":range(5),""bar"":range(5,10)}...)>>>original_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>df_parquet_bytes=original_df.to_parquet()>>>fromioimportBytesIO>>>restored_df=pd.read_parquet(BytesIO(df_parquet_bytes))>>>restored_dffoo  bar0    0    51    1    62    2    73    3    84    4    9>>>restored_df.equals(original_df)True>>>restored_bar=pd.read_parquet(BytesIO(df_parquet_bytes),columns=[""bar""])>>>restored_barbar0    51    62    73    84    9>>>restored_bar.equals(original_df[['bar']])TrueThe function useskwargsthat are passed directly to the engine.
In the following example, we use thefiltersargument of the pyarrow
engine to filter the rows of the DataFrame.Sincepyarrowis the default engine, we can omit theengineargument.
Note that thefiltersargument is implemented by thepyarrowengine,
which can benefit from multithreading and also potentially be more
economical in terms of memory.>>>sel=[(""foo"","">"",2)]>>>restored_part=pd.read_parquet(BytesIO(df_parquet_bytes),filters=sel)>>>restored_partfoo  bar0    3    81    4    9"
Pandas,Input output,pandas.DataFrame.to_parquet,"pandas.DataFrame.to_parquet#DataFrame.to_parquet(path=None,engine='auto',compression='snappy',index=None,partition_cols=None,storage_options=None,**kwargs)[source]#Write a DataFrame to the binary parquet format.This function writes the dataframe as aparquet file. You can choose different parquet
backends, and have the option of compression. Seethe user guidefor more details.Parameters:pathstr, path object, file-like object, or None, default NoneString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function. If None, the result is
returned as bytes. If a string or path, it will be used as Root Directory
path when writing a partitioned dataset.Changed in version 1.2.0.Previously this was “fname”engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’Parquet library to use. If ‘auto’, then the optionio.parquet.engineis used. The defaultio.parquet.enginebehavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if
‘pyarrow’ is unavailable.compressionstr or None, default ‘snappy’Name of the compression to use. UseNonefor no compression.
Supported options: ‘snappy’, ‘gzip’, ‘brotli’, ‘lz4’, ‘zstd’.indexbool, default NoneIfTrue, include the dataframe’s index(es) in the file output.
IfFalse, they will not be written to the file.
IfNone, similar toTruethe dataframe’s index(es)
will be saved. However, instead of being saved as values,
the RangeIndex will be stored as a range in the metadata so it
doesn’t require much space and is faster. Other indexes will
be included as columns in the file output.partition_colslist, optional, default NoneColumn names by which to partition the dataset.
Columns are partitioned in the order they are given.
Must be None if path is not a string.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.**kwargsAdditional arguments passed to the parquet library. Seepandas iofor more details.Returns:bytes if no path argument is provided else NoneSee alsoread_parquetRead a parquet file.DataFrame.to_orcWrite an orc file.DataFrame.to_csvWrite a csv file.DataFrame.to_sqlWrite to a sql table.DataFrame.to_hdfWrite to hdf.NotesThis function requires either thefastparquetorpyarrowlibrary.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[3,4]})>>>df.to_parquet('df.parquet.gzip',...compression='gzip')>>>pd.read_parquet('df.parquet.gzip')col1  col20     1     31     2     4If you want to get a buffer to the parquet content you can use a io.BytesIO
object, as long as you don’t use partition_cols, which creates multiple files.>>>importio>>>f=io.BytesIO()>>>df.to_parquet(f)>>>f.seek(0)0>>>content=f.read()"
Pandas,Input output,pandas.read_orc,"pandas.read_orc#pandas.read_orc(path,columns=None,dtype_backend=_NoDefault.no_default,filesystem=None,**kwargs)[source]#Load an ORC object from the file path, returning a DataFrame.Parameters:pathstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binaryread()function. The string could be a URL.
Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.orc.columnslist, default NoneIf not None, only these columns will be read from the file.
Output always follows the ordering of the file and not the columns list.
This mirrors the original behaviour ofpyarrow.orc.ORCFile.read().dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file.New in version 2.1.0.**kwargsAny additional kwargs are passed to pyarrow.Returns:DataFrameNotesBefore using this function you should read theuser guide about ORCandinstall optional dependencies.Ifpathis a URI scheme pointing to a local or remote file (e.g. “s3://”),
apyarrow.fsfilesystem will be attempted to read the file. You can also pass a
pyarrow or fsspec filesystem object into the filesystem keyword to override this
behavior.Examples>>>result=pd.read_orc(""example_pa.orc"")"
Pandas,Input output,pandas.DataFrame.to_orc,"pandas.DataFrame.to_orc#DataFrame.to_orc(path=None,*,engine='pyarrow',index=None,engine_kwargs=None)[source]#Write a DataFrame to the ORC format.New in version 1.5.0.Parameters:pathstr, file-like object or None, default NoneIf a string, it will be used as Root Directory path
when writing a partitioned dataset. By file-like object,
we refer to objects with a write() method, such as a file handle
(e.g. via builtin open function). If path is None,
a bytes object is returned.engine{‘pyarrow’}, default ‘pyarrow’ORC library to use. Pyarrow must be >= 7.0.0.indexbool, optionalIfTrue, include the dataframe’s index(es) in the file output.
IfFalse, they will not be written to the file.
IfNone, similar toinferthe dataframe’s index(es)
will be saved. However, instead of being saved as values,
the RangeIndex will be stored as a range in the metadata so it
doesn’t require much space and is faster. Other indexes will
be included as columns in the file output.engine_kwargsdict[str, Any] or None, default NoneAdditional keyword arguments passed topyarrow.orc.write_table().Returns:bytes if no path argument is provided else NoneRaises:NotImplementedErrorDtype of one or more columns is category, unsigned integers, interval,
period or sparse.ValueErrorengine is not pyarrow.See alsoread_orcRead a ORC file.DataFrame.to_parquetWrite a parquet file.DataFrame.to_csvWrite a csv file.DataFrame.to_sqlWrite to a sql table.DataFrame.to_hdfWrite to hdf.NotesBefore using this function you should read theuser guide about
ORCandinstall optional dependencies.This function requirespyarrowlibrary.For supported dtypes please refer tosupported ORC features in Arrow.Currently timezones in datetime columns are not preserved when a
dataframe is converted into ORC files.Examples>>>df=pd.DataFrame(data={'col1':[1,2],'col2':[4,3]})>>>df.to_orc('df.orc')>>>pd.read_orc('df.orc')col1  col20     1     41     2     3If you want to get a buffer to the orc content you can write it to io.BytesIO>>>importio>>>b=io.BytesIO(df.to_orc())>>>b.seek(0)0>>>content=b.read()"
Pandas,Input output,pandas.read_sas,"pandas.read_sas#pandas.read_sas(filepath_or_buffer,*,format=None,index=None,encoding=None,chunksize=None,iterator=False,compression='infer')[source]#Read SAS files stored as either XPORT or SAS7BDAT format files.Parameters:filepath_or_bufferstr, path object, or file-like objectString, path object (implementingos.PathLike[str]), or file-like
object implementing a binaryread()function. The string could be a URL.
Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.sas7bdat.formatstr {‘xport’, ‘sas7bdat’} or NoneIf None, file format is inferred from file extension. If ‘xport’ or
‘sas7bdat’, uses the corresponding format.indexidentifier of index column, defaults to NoneIdentifier of column that should be used as index of the DataFrame.encodingstr, default is NoneEncoding for text data. If None, text data are stored as raw bytes.chunksizeintRead filechunksizelines at a time, returns iterator.Changed in version 1.2:TextFileReaderis a context manager.iteratorbool, defaults to FalseIf True, returns an iterator for reading the file incrementally.Changed in version 1.2:TextFileReaderis a context manager.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.Returns:DataFrame if iterator=False and chunksize=None, else SAS7BDATReaderor XportReaderExamples>>>df=pd.read_sas(""sas_data.sas7bdat"")"
Pandas,Input output,pandas.read_spss,"pandas.read_spss#pandas.read_spss(path,usecols=None,convert_categoricals=True,dtype_backend=_NoDefault.no_default)[source]#Load an SPSS file from the file path, returning a DataFrame.Parameters:pathstr or PathFile path.usecolslist-like, optionalReturn a subset of the columns. If None, return all columns.convert_categoricalsbool, default is TrueConvert categorical columns into pd.Categorical.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrameExamples>>>df=pd.read_spss(""spss_data.sav"")"
Pandas,Input output,pandas.read_sql_table,"pandas.read_sql_table#pandas.read_sql_table(table_name,con,schema=None,index_col=None,coerce_float=True,parse_dates=None,columns=None,chunksize=None,dtype_backend=_NoDefault.no_default)[source]#Read SQL database table into a DataFrame.Given a table name and a SQLAlchemy connectable, returns a DataFrame.
This function does not support DBAPI connections.Parameters:table_namestrName of SQL table in database.conSQLAlchemy connectable or strA database URI could be provided as str.
SQLite DBAPI connection mode not supported.schemastr, default NoneName of SQL schema in database to query (if database flavor
supports this). Uses default schema if None (default).index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point. Can result in loss of Precision.parse_dateslist or dict, default NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is
strftime compatible in case of parsing string times or is one of
(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds
to the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,
such as SQLite.columnslist, default NoneList of column names to select from SQL table.chunksizeint, default NoneIf specified, returns an iterator wherechunksizeis the number of
rows to include in each chunk.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or Iterator[DataFrame]A SQL table is returned as two-dimensional data structure with labeled
axes.See alsoread_sql_queryRead SQL query into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.NotesAny datetime values with time zone information will be converted to UTC.Examples>>>pd.read_sql_table('table_name','postgres:///db_name')"
Pandas,Input output,pandas.read_sql_query,"pandas.read_sql_query#pandas.read_sql_query(sql,con,index_col=None,coerce_float=True,params=None,parse_dates=None,chunksize=None,dtype=None,dtype_backend=_NoDefault.no_default)[source]#Read SQL query into a DataFrame.Returns a DataFrame corresponding to the result set of the query
string. Optionally provide anindex_colparameter to use one of the
columns as the index, otherwise default integer index will be used.Parameters:sqlstr SQL query or SQLAlchemy Selectable (select or text object)SQL query to be executed.conSQLAlchemy connectable, str, or sqlite3 connectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. If a DBAPI2 object, only sqlite3 is supported.index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point. Useful for SQL result sets.paramslist, tuple or mapping, optional, default: NoneList of parameters to pass to execute method. The syntax used
to pass parameters is database driver dependent. Check your
database driver documentation for which of the five syntax styles,
described in PEP 249’s paramstyle, is supported.
Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.parse_dateslist or dict, default: NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is
strftime compatible in case of parsing string times, or is one of
(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds
to the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,
such as SQLite.chunksizeint, default NoneIf specified, return an iterator wherechunksizeis the number of
rows to include in each chunk.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or
{‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.New in version 1.3.0.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sqlRead SQL query or database table into a DataFrame.NotesAny datetime values with time zone information parsed via theparse_datesparameter will be converted to UTC.Examples>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine(""sqlite:///database.db"")>>>withengine.connect()asconn,conn.begin():...data=pd.read_sql_table(""data"",conn)"
Pandas,Input output,pandas.read_sql,"pandas.read_sql#pandas.read_sql(sql,con,index_col=None,coerce_float=True,params=None,parse_dates=None,columns=None,chunksize=None,dtype_backend=_NoDefault.no_default,dtype=None)[source]#Read SQL query or database table into a DataFrame.This function is a convenience wrapper aroundread_sql_tableandread_sql_query(for backward compatibility). It will delegate
to the specific function depending on the provided input. A SQL query
will be routed toread_sql_query, while a database table name will
be routed toread_sql_table. Note that the delegated function might
have more specific notes about their functionality not listed here.Parameters:sqlstr or SQLAlchemy Selectable (select or text object)SQL query to be executed or a table name.conSQLAlchemy connectable, str, or sqlite3 connectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible
for engine disposal and connection closure for the SQLAlchemy connectable; str
connections are closed automatically. Seehere.index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex).coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like
decimal.Decimal) to floating point, useful for SQL result sets.paramslist, tuple or dict, optional, default: NoneList of parameters to pass to execute method. The syntax used
to pass parameters is database driver dependent. Check your
database driver documentation for which of the five syntax styles,
described in PEP 249’s paramstyle, is supported.
Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}.parse_dateslist or dict, default: NoneList of column names to parse as dates.Dict of{column_name:formatstring}where format string is
strftime compatible in case of parsing string times, or is one of
(D, s, ns, ms, us) in case of parsing integer timestamps.Dict of{column_name:argdict}, where the arg dict corresponds
to the keyword arguments ofpandas.to_datetime()Especially useful with databases without native Datetime support,
such as SQLite.columnslist, default: NoneList of column names to select from SQL table (only used when reading
a table).chunksizeint, default NoneIf specified, return an iterator wherechunksizeis the
number of rows to include in each chunk.dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultantDataFrame(still experimental). Behaviour is as follows:""numpy_nullable"": returns nullable-dtype-backedDataFrame(default).""pyarrow"": returns pyarrow-backed nullableArrowDtypeDataFrame.New in version 2.0.dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or
{‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}.
The argument is ignored if a table is passed instead of a query.New in version 2.0.0.Returns:DataFrame or Iterator[DataFrame]See alsoread_sql_tableRead SQL database table into a DataFrame.read_sql_queryRead SQL query into a DataFrame.ExamplesRead data from SQL via either a SQL query or a SQL tablename.
When using a SQLite database only SQL queries are accepted,
providing only the SQL tablename will result in an error.>>>fromsqlite3importconnect>>>conn=connect(':memory:')>>>df=pd.DataFrame(data=[[0,'10/11/12'],[1,'12/11/10']],...columns=['int_column','date_column'])>>>df.to_sql(name='test_data',con=conn)2>>>pd.read_sql('SELECT int_column, date_column FROM test_data',conn)int_column date_column0           0    10/11/121           1    12/11/10>>>pd.read_sql('test_data','postgres:///db_name')Apply date parsing to columns through theparse_datesargument
Theparse_datesargument callspd.to_datetimeon the provided columns.
Custom argument values for applyingpd.to_datetimeon a column are specified
via a dictionary format:>>>pd.read_sql('SELECT int_column, date_column FROM test_data',...conn,...parse_dates={""date_column"":{""format"":""%d/%m/%y""}})int_column date_column0           0  2012-11-101           1  2010-11-12"
Pandas,Input output,pandas.DataFrame.to_sql,"pandas.DataFrame.to_sql#DataFrame.to_sql(name,con,*,schema=None,if_exists='fail',index=True,index_label=None,chunksize=None,dtype=None,method=None)[source]#Write records stored in a DataFrame to a SQL database.Databases supported by SQLAlchemy[1]are supported. Tables can be
newly created, appended to, or overwritten.Parameters:namestrName of SQL table.consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that
library. Legacy support is provided for sqlite3.Connection objects. The user
is responsible for engine disposal and connection closure for the SQLAlchemy
connectable. Seehere.
If passing a sqlalchemy.engine.Connection which is already in a transaction,
the transaction will not be committed. If passing a sqlite3.Connection,
it will not be possible to roll back the record insertion.schemastr, optionalSpecify the schema (if database flavor supports this). If None, use
default schema.if_exists{‘fail’, ‘replace’, ‘append’}, default ‘fail’How to behave if the table already exists.fail: Raise a ValueError.replace: Drop the table before inserting new values.append: Insert new values to the existing table.indexbool, default TrueWrite DataFrame index as a column. Usesindex_labelas the column
name in the table.index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) andindexis True, then the index names are used.
A sequence should be given if the DataFrame uses MultiIndex.chunksizeint, optionalSpecify the number of rows in each batch to be written at a time.
By default, all rows will be written at once.dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the
keys should be the column names and the values should be the
SQLAlchemy types or strings for the sqlite3 legacy mode. If a
scalar is provided, it will be applied to all columns.method{None, ‘multi’, callable}, optionalControls the SQL insertion clause used:None : Uses standard SQLINSERTclause (one per row).‘multi’: Pass multiple values in a singleINSERTclause.callable with signature(pd_table,conn,keys,data_iter).Details and a sample callable implementation can be found in the
sectioninsert method.Returns:None or intNumber of rows affected by to_sql. None is returned if the callable
passed intomethoddoes not return an integer number of rows.The number of returned rows affected is the sum of therowcountattribute ofsqlite3.Cursoror SQLAlchemy connectable which may not
reflect the exact number of written rows as stipulated in thesqlite3orSQLAlchemy.New in version 1.4.0.Raises:ValueErrorWhen the table already exists andif_existsis ‘fail’ (the
default).See alsoread_sqlRead a DataFrame from a table.NotesTimezone aware datetime columns will be written asTimestampwithtimezonetype with SQLAlchemy if supported by the
database. Otherwise, the datetimes will be stored as timezone unaware
timestamps local to the original timezone.References[1]https://docs.sqlalchemy.org[2]https://www.python.org/dev/peps/pep-0249/ExamplesCreate an in-memory SQLite database.>>>fromsqlalchemyimportcreate_engine>>>engine=create_engine('sqlite://',echo=False)Create a table from scratch with 3 rows.>>>df=pd.DataFrame({'name':['User 1','User 2','User 3']})>>>dfname0  User 11  User 22  User 3>>>df.to_sql(name='users',con=engine)3>>>fromsqlalchemyimporttext>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]Ansqlalchemy.engine.Connectioncan also be passed tocon:>>>withengine.begin()asconnection:...df1=pd.DataFrame({'name':['User 4','User 5']})...df1.to_sql(name='users',con=connection,if_exists='append')2This is allowed to support operations that require that the same
DBAPI connection is used for the entire operation.>>>df2=pd.DataFrame({'name':['User 6','User 7']})>>>df2.to_sql(name='users',con=engine,if_exists='append')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),(0, 'User 4'), (1, 'User 5'), (0, 'User 6'),(1, 'User 7')]Overwrite the table with justdf2.>>>df2.to_sql(name='users',con=engine,if_exists='replace',...index_label='id')2>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM users"")).fetchall()[(0, 'User 6'), (1, 'User 7')]Usemethodto define a callable insertion method to do nothing
if there’s a primary key conflict on a table in a PostgreSQL database.>>>fromsqlalchemy.dialects.postgresqlimportinsert>>>definsert_on_conflict_nothing(table,conn,keys,data_iter):...# ""a"" is the primary key in ""conflict_table""...data=[dict(zip(keys,row))forrowindata_iter]...stmt=insert(table.table).values(data).on_conflict_do_nothing(index_elements=[""a""])...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_nothing)0For MySQL, a callable to update columnsbandcif there’s a conflict
on a primary key.>>>fromsqlalchemy.dialects.mysqlimportinsert>>>definsert_on_conflict_update(table,conn,keys,data_iter):...# update columns ""b"" and ""c"" on primary key conflict...data=[dict(zip(keys,row))forrowindata_iter]...stmt=(...insert(table.table)....values(data)...)...stmt=stmt.on_duplicate_key_update(b=stmt.inserted.b,c=stmt.inserted.c)...result=conn.execute(stmt)...returnresult.rowcount>>>df_conflict.to_sql(name=""conflict_table"",con=conn,if_exists=""append"",method=insert_on_conflict_update)2Specify the dtype (especially useful for integers with missing values).
Notice that while pandas is forced to store the data as floating point,
the database supports nullable integers. When fetching the data with
Python, we get back integer scalars.>>>df=pd.DataFrame({""A"":[1,None,2]})>>>dfA0  1.01  NaN2  2.0>>>fromsqlalchemy.typesimportInteger>>>df.to_sql(name='integers',con=engine,index=False,...dtype={""A"":Integer()})3>>>withengine.connect()asconn:...conn.execute(text(""SELECT * FROM integers"")).fetchall()[(1,), (None,), (2,)]"
Pandas,Input output,pandas.read_gbq,"pandas.read_gbq#pandas.read_gbq(query,project_id=None,index_col=None,col_order=None,reauth=False,auth_local_webserver=True,dialect=None,location=None,configuration=None,credentials=None,use_bqstorage_api=None,max_results=None,progress_bar_type=None)[source]#Load data from Google BigQuery.This function requires thepandas-gbq package.See theHow to authenticate with Google BigQueryguide for authentication instructions.Parameters:querystrSQL-Like Query to return data values.project_idstr, optionalGoogle BigQuery Account project ID. Optional when available from
the environment.index_colstr, optionalName of result column to use for index in results DataFrame.col_orderlist(str), optionalList of BigQuery column names in the desired order for results
DataFrame.reauthbool, default FalseForce Google BigQuery to re-authenticate the user. This is useful
if multiple accounts are used.auth_local_webserverbool, default TrueUse thelocal webserver flowinstead of theconsole flowwhen getting user credentials.New in version 0.2.0 of pandas-gbq.Changed in version 1.5.0:Default value is changed toTrue. Google has deprecated theauth_local_webserver=False“out of band” (copy-paste)
flow.dialectstr, default ‘legacy’Note: The default value is changing to ‘standard’ in a future version.SQL syntax dialect to use. Value can be one of:'legacy'Use BigQuery’s legacy SQL dialect. For more information seeBigQuery Legacy SQL Reference.'standard'Use BigQuery’s standard SQL, which is
compliant with the SQL 2011 standard. For more information
seeBigQuery Standard SQL Reference.locationstr, optionalLocation where the query job should run. See theBigQuery locations
documentationfor a
list of available locations. The location must match that of any
datasets used in the query.New in version 0.5.0 of pandas-gbq.configurationdict, optionalQuery config parameters for job processing.
For example:configuration = {‘query’: {‘useQueryCache’: False}}For more information seeBigQuery REST API Reference.credentialsgoogle.auth.credentials.Credentials, optionalCredentials for accessing Google APIs. Use this parameter to override
default credentials, such as to use Compute Enginegoogle.auth.compute_engine.Credentialsor Service Accountgoogle.oauth2.service_account.Credentialsdirectly.New in version 0.8.0 of pandas-gbq.use_bqstorage_apibool, default FalseUse theBigQuery Storage APIto
download query results quickly, but at an increased cost. To use this
API, firstenable it in the Cloud Console.
You must also have thebigquery.readsessions.createpermission on the project you are billing queries to.This feature requires version 0.10.0 or later of thepandas-gbqpackage. It also requires thegoogle-cloud-bigquery-storageandfastavropackages.max_resultsint, optionalIf set, limit the maximum number of rows to fetch from the query
results.progress_bar_typeOptional, strIf set, use thetqdmlibrary to
display a progress bar while the data downloads. Install thetqdmpackage to use this feature.Possible values ofprogress_bar_typeinclude:NoneNo progress bar.'tqdm'Use thetqdm.tqdm()function to print a progress bar
tosys.stderr.'tqdm_notebook'Use thetqdm.tqdm_notebook()function to display a
progress bar as a Jupyter notebook widget.'tqdm_gui'Use thetqdm.tqdm_gui()function to display a
progress bar as a graphical dialog box.Returns:df: DataFrameDataFrame representing results of query.See alsopandas_gbq.read_gbqThis function in the pandas-gbq library.DataFrame.to_gbqWrite a DataFrame to Google BigQuery.ExamplesExample taken fromGoogle BigQuery documentation>>>sql=""SELECT name FROM table_name WHERE state = 'TX' LIMIT 100;"">>>df=pd.read_gbq(sql,dialect=""standard"")>>>project_id=""your-project-id"">>>df=pd.read_gbq(sql,...project_id=project_id,...dialect=""standard""...)"
Pandas,Input output,pandas.read_stata,"pandas.read_stata#pandas.read_stata(filepath_or_buffer,*,convert_dates=True,convert_categoricals=True,index_col=None,convert_missing=False,preserve_dtypes=True,columns=None,order_categoricals=True,chunksize=None,iterator=False,compression='infer',storage_options=None)[source]#Read Stata file into DataFrame.Parameters:filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid
URL schemes include http, ftp, s3, and file. For file URLs, a host is
expected. A local file could be:file://localhost/path/to/table.dta.If you want to pass in a path object, pandas accepts anyos.PathLike.By file-like object, we refer to objects with aread()method,
such as a file handle (e.g. via builtinopenfunction)
orStringIO.convert_datesbool, default TrueConvert date variables to DataFrame time values.convert_categoricalsbool, default TrueRead value labels and convert columns to Categorical/Factor variables.index_colstr, optionalColumn to set as index.convert_missingbool, default FalseFlag indicating whether to convert missing values to their Stata
representations. If False, missing values are replaced with nan.
If True, columns containing missing values are returned with
object data types and missing values are represented by
StataMissingValue objects.preserve_dtypesbool, default TruePreserve Stata datatypes. If False, numeric data are upcast to pandas
default types for foreign data (float64 or int64).columnslist or NoneColumns to retain. Columns will be returned in the given order. None
returns all columns.order_categoricalsbool, default TrueFlag indicating whether converted categorical data are ordered.chunksizeint, default NoneReturn StataReader object for iterations, returns chunks with
given number of lines.iteratorbool, default FalseReturn StataReader object.compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in.
Set toNonefor no decompression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdDecompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for Zstandard decompression using a
custom compression dictionary:compression={'method':'zstd','dict_data':my_compression_dict}.New in version 1.5.0:Added support for.tarfiles.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.Returns:DataFrame or pandas.api.typing.StataReaderSee alsoio.stata.StataReaderLow-level reader for Stata data files.DataFrame.to_stataExport Stata data files.NotesCategorical variables read through an iterator may not have the same
categories and dtype. This occurs when a variable stored in a DTA
file is associated to an incomplete set of value labels that only
label a strict subset of the values.ExamplesCreating a dummy stata for this example>>>df=pd.DataFrame({'animal':['falcon','parrot','falcon','parrot'],...'speed':[350,18,361,15]})>>>df.to_stata('animals.dta')Read a Stata dta file:>>>df=pd.read_stata('animals.dta')Read a Stata dta file in 10,000 line chunks:>>>values=np.random.randint(0,10,size=(20_000,1),dtype=""uint8"")>>>df=pd.DataFrame(values,columns=[""i""])>>>df.to_stata('filename.dta')>>>withpd.read_stata('filename.dta',chunksize=10000)asitr:>>>forchunkinitr:...# Operate on a single chunk, e.g., chunk.mean()...pass"
Pandas,Input output,pandas.DataFrame.to_stata,"pandas.DataFrame.to_stata#DataFrame.to_stata(path,*,convert_dates=None,write_index=True,byteorder=None,time_stamp=None,data_label=None,variable_labels=None,version=114,convert_strl=None,compression='infer',storage_options=None,value_labels=None)[source]#Export DataFrame object to Stata dta format.Writes the DataFrame to a Stata dataset file.
“dta” files contain a Stata dataset.Parameters:pathstr, path object, or bufferString, path object (implementingos.PathLike[str]), or file-like
object implementing a binarywrite()function.convert_datesdictDictionary mapping columns containing datetime types to stata
internal format to use when writing the dates. Options are ‘tc’,
‘td’, ‘tm’, ‘tw’, ‘th’, ‘tq’, ‘ty’. Column can be either an integer
or a name. Datetime columns that do not have a conversion type
specified will be converted to ‘tc’. Raises NotImplementedError if
a datetime column has timezone information.write_indexboolWrite the index to Stata dataset.byteorderstrCan be “>”, “<”, “little”, or “big”. default issys.byteorder.time_stampdatetimeA datetime to use as file creation date. Default is the current
time.data_labelstr, optionalA label for the data set. Must be 80 characters or smaller.variable_labelsdictDictionary containing columns as keys and variable labels as
values. Each label must be 80 characters or smaller.version{114, 117, 118, 119, None}, default 114Version to use in the output dta file. Set to None to let pandas
decide between 118 or 119 formats depending on the number of
columns in the frame. Version 114 can be read by Stata 10 and
later. Version 117 can be read by Stata 13 or later. Version 118
is supported in Stata 14 and later. Version 119 is supported in
Stata 15 and later. Version 114 limits string variables to 244
characters or fewer while versions 117 and later allow strings
with lengths up to 2,000,000 characters. Versions 118 and 119
support Unicode characters, and version 119 supports more than
32,767 variables.Version 119 should usually only be used when the number of
variables exceeds the capacity of dta format 118. Exporting
smaller datasets in format 119 may have unintended consequences,
and, as of November 2020, Stata SE cannot read version 119 files.convert_strllist, optionalList of column names to convert to string columns to Stata StrL
format. Only available if version is 117. Storing strings in the
StrL format can produce smaller dta files if strings have more than
8 characters and values are repeated.compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is
path-like, then detect compression from the following extensions: ‘.gz’,
‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’
(otherwise no compression).
Set toNonefor no compression.
Can also be a dict with key'method'set
to one of {'zip','gzip','bz2','zstd','xz','tar'} and
other key-value pairs are forwarded tozipfile.ZipFile,gzip.GzipFile,bz2.BZ2File,zstandard.ZstdCompressor,lzma.LZMAFileortarfile.TarFile, respectively.
As an example, the following could be passed for faster compression and to create
a reproducible gzip archive:compression={'method':'gzip','compresslevel':1,'mtime':1}.New in version 1.5.0:Added support for.tarfiles.Changed in version 1.4.0:Zstandard support.storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g.
host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
are forwarded tourllib.request.Requestas header options. For other
URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are
forwarded tofsspec.open. Please seefsspecandurllibfor more
details, and for more examples on storage options referhere.New in version 1.2.0.value_labelsdict of dictsDictionary containing columns as keys and dictionaries of column value
to labels as values. Labels for a single variable must be 32,000
characters or smaller.New in version 1.4.0.Raises:NotImplementedErrorIf datetimes contain timezone informationColumn dtype is not representable in StataValueErrorColumns listed in convert_dates are neither datetime64[ns]
or datetime.datetimeColumn listed in convert_dates is not in DataFrameCategorical label contains more than 32,000 charactersSee alsoread_stataImport Stata data files.io.stata.StataWriterLow-level writer for Stata data files.io.stata.StataWriter117Low-level writer for version 117 files.Examples>>>df=pd.DataFrame({'animal':['falcon','parrot','falcon',...'parrot'],...'speed':[350,18,361,15]})>>>df.to_stata('animals.dta')"
Pandas,Input output,pandas.io.stata.StataReader.data_label,"pandas.io.stata.StataReader.data_label#propertyStataReader.data_label[source]#Return data label of Stata file.Examples>>>df=pd.DataFrame([(1,)],columns=[""variable""])>>>time_stamp=pd.Timestamp(2000,2,29,14,21)>>>data_label=""This is a data file."">>>path=""/My_path/filename.dta"">>>df.to_stata(path,time_stamp=time_stamp,...data_label=data_label,...version=None)>>>withpd.io.stata.StataReader(path)asreader:...print(reader.data_label)This is a data file."
Pandas,Input output,pandas.io.stata.StataReader.value_labels,"pandas.io.stata.StataReader.value_labels#StataReader.value_labels()[source]#Return a nested dict associating each variable name to its value and label.Returns:dictExamples>>>df=pd.DataFrame([[1,2],[3,4]],columns=[""col_1"",""col_2""])>>>time_stamp=pd.Timestamp(2000,2,29,14,21)>>>path=""/My_path/filename.dta"">>>value_labels={""col_1"":{3:""x""}}>>>df.to_stata(path,time_stamp=time_stamp,...value_labels=value_labels,version=None)>>>withpd.io.stata.StataReader(path)asreader:...print(reader.value_labels()){'col_1': {3: 'x'}}>>>pd.read_stata(path)index col_1 col_20       0    1    21       1    x    4"
Pandas,Input output,pandas.io.stata.StataReader.variable_labels,"pandas.io.stata.StataReader.variable_labels#StataReader.variable_labels()[source]#Return a dict associating each variable name with corresponding label.Returns:dictExamples>>>df=pd.DataFrame([[1,2],[3,4]],columns=[""col_1"",""col_2""])>>>time_stamp=pd.Timestamp(2000,2,29,14,21)>>>path=""/My_path/filename.dta"">>>variable_labels={""col_1"":""This is an example""}>>>df.to_stata(path,time_stamp=time_stamp,...variable_labels=variable_labels,version=None)>>>withpd.io.stata.StataReader(path)asreader:...print(reader.variable_labels()){'index': '', 'col_1': 'This is an example', 'col_2': ''}>>>pd.read_stata(path)index col_1 col_20       0    1    21       1    3    4"
Pandas,Input output,pandas.io.stata.StataWriter.write_file,"pandas.io.stata.StataWriter.write_file#StataWriter.write_file()[source]#Export DataFrame object to Stata dta format.Examples>>>df=pd.DataFrame({""fully_labelled"":[1,2,3,3,1],...""partially_labelled"":[1.0,2.0,np.nan,9.0,np.nan],...""Y"":[7,7,9,8,10],...""Z"":pd.Categorical([""j"",""k"",""l"",""k"",""j""]),...})>>>path=""/My_path/filename.dta"">>>labels={""fully_labelled"":{1:""one"",2:""two"",3:""three""},...""partially_labelled"":{1.0:""one"",2.0:""two""},...}>>>writer=pd.io.stata.StataWriter(path,...df,...value_labels=labels)>>>writer.write_file()>>>df=pd.read_stata(path)>>>dfindex fully_labelled  partially_labeled  Y  Z0       0            one                one  7  j1       1            two                two  7  k2       2          three                NaN  9  l3       3          three                9.0  8  k4       4            one                NaN 10  j"
